"""
ãƒˆãƒ¼ã‚¯ãƒ³åŠ¹ç‡åˆ†æãƒ»ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ 
Issue #813å¯¾å¿œ - performance_metrics.pyåˆ†å‰²ç‰ˆï¼ˆåˆ†æç³»ï¼‰

è²¬ä»»ç¯„å›²:
- ãƒˆãƒ¼ã‚¯ãƒ³åŠ¹ç‡åˆ†æ
- éåŠ¹ç‡ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
- ã‚¢ãƒ©ãƒ¼ãƒˆãƒ»é€šçŸ¥ã‚·ã‚¹ãƒ†ãƒ 
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æãƒ¬ãƒãƒ¼ãƒˆ
"""

import re
import time
from collections import Counter
from dataclasses import dataclass
from typing import Any, Dict, List, Optional

from ..logger import get_logger


@dataclass
class TokenEfficiencyMetrics:
    """ãƒˆãƒ¼ã‚¯ãƒ³åŠ¹ç‡ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""

    total_tokens: int = 0
    effective_tokens: int = 0
    efficiency_rate: float = 0.0
    waste_tokens: int = 0
    optimization_potential: float = 0.0


@dataclass
class InefficiencyPattern:
    """éåŠ¹ç‡ãƒ‘ã‚¿ãƒ¼ãƒ³"""

    pattern_type: str
    description: str
    occurrences: int = 0
    waste_estimate: float = 0.0
    severity: str = "medium"  # low, medium, high, critical


class TokenEfficiencyAnalyzer:
    """
    ãƒˆãƒ¼ã‚¯ãƒ³åŠ¹ç‡åˆ†æã‚·ã‚¹ãƒ†ãƒ 

    æ©Ÿèƒ½:
    - ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨åŠ¹ç‡ã®è©³ç´°åˆ†æ
    - éåŠ¹ç‡ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è‡ªå‹•æ¤œå‡º
    - æœ€é©åŒ–ææ¡ˆã®ç”Ÿæˆ
    - åŠ¹ç‡æ”¹å–„ã®å®šé‡çš„æ¸¬å®š
    """

    def __init__(self, analysis_window: int = 1000):
        self.logger = get_logger(__name__)
        self.analysis_window = analysis_window

        # åˆ†æãƒ‡ãƒ¼ã‚¿
        self.token_history: List[int] = []
        self.pattern_history: List[Dict[str, Any]] = []

        # éåŠ¹ç‡ãƒ‘ã‚¿ãƒ¼ãƒ³å®šç¾©
        self.inefficiency_patterns = self._initialize_patterns()

        self.logger.info(
            f"TokenEfficiencyAnalyzer initialized with window size: {analysis_window}"
        )

    def _initialize_patterns(self) -> Dict[str, InefficiencyPattern]:
        """éåŠ¹ç‡ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆæœŸåŒ–"""
        patterns = {
            "repetitive_operations": InefficiencyPattern(
                pattern_type="repetitive_operations",
                description="åŒä¸€æ“ä½œã®åå¾©å®Ÿè¡Œ",
                severity="high",
            ),
            "inefficient_regex": InefficiencyPattern(
                pattern_type="inefficient_regex",
                description="éåŠ¹ç‡ãªæ­£è¦è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³",
                severity="medium",
            ),
            "memory_leaks": InefficiencyPattern(
                pattern_type="memory_leaks",
                description="ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯å‚¾å‘",
                severity="critical",
            ),
            "excessive_io": InefficiencyPattern(
                pattern_type="excessive_io",
                description="éå‰°ãªI/Oæ“ä½œ",
                severity="high",
            ),
            "redundant_processing": InefficiencyPattern(
                pattern_type="redundant_processing",
                description="å†—é•·ãªå‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯",
                severity="medium",
            ),
        }

        return patterns

    def analyze_token_efficiency(
        self, execution_data: Dict[str, Any]
    ) -> TokenEfficiencyMetrics:
        """
        å®Ÿè¡Œãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒˆãƒ¼ã‚¯ãƒ³åŠ¹ç‡ã‚’åˆ†æ

        Args:
            execution_data: å®Ÿè¡Œæ™‚ãƒ‡ãƒ¼ã‚¿ï¼ˆå‡¦ç†æ™‚é–“ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç­‰ï¼‰

        Returns:
            TokenEfficiencyMetrics: åŠ¹ç‡ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        """
        # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®æ¨å®šï¼ˆå‡¦ç†æ™‚é–“ã¨ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‹ã‚‰ï¼‰
        processing_time = execution_data.get("processing_time", 0)
        memory_usage = execution_data.get("memory_usage_mb", 0)
        operations_count = execution_data.get("operations_count", 0)

        # åŠ¹ç‡æ€§è¨ˆç®—ã®ç°¡æ˜“å®Ÿè£…
        estimated_tokens = int(processing_time * 1000 + memory_usage * 10)

        # åŸºæº–åŠ¹ç‡ã¨æ¯”è¼ƒ
        baseline_efficiency = 0.75  # 75%ã‚’åŸºæº–ã¨ã™ã‚‹
        actual_efficiency = min(1.0, operations_count / max(1, estimated_tokens))

        effective_tokens = int(estimated_tokens * actual_efficiency)
        waste_tokens = estimated_tokens - effective_tokens

        optimization_potential = max(0, baseline_efficiency - actual_efficiency) * 100

        metrics = TokenEfficiencyMetrics(
            total_tokens=estimated_tokens,
            effective_tokens=effective_tokens,
            efficiency_rate=actual_efficiency,
            waste_tokens=waste_tokens,
            optimization_potential=optimization_potential,
        )

        # å±¥æ­´ã«è¿½åŠ 
        self.token_history.append(estimated_tokens)
        if len(self.token_history) > self.analysis_window:
            self.token_history.pop(0)

        return metrics

    def detect_inefficiency_patterns(
        self, execution_logs: List[str]
    ) -> List[InefficiencyPattern]:
        """
        å®Ÿè¡Œãƒ­ã‚°ã‹ã‚‰éåŠ¹ç‡ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º

        Args:
            execution_logs: å®Ÿè¡Œãƒ­ã‚°ãƒªã‚¹ãƒˆ

        Returns:
            List[InefficiencyPattern]: æ¤œå‡ºã•ã‚ŒãŸãƒ‘ã‚¿ãƒ¼ãƒ³
        """
        detected_patterns = []

        # ãƒ­ã‚°ã‚’çµåˆã—ã¦åˆ†æ
        log_text = "\n".join(execution_logs)

        # åå¾©æ“ä½œã®æ¤œå‡º
        repetitive_count = self._detect_repetitive_operations(log_text)
        if repetitive_count > 10:
            pattern = self.inefficiency_patterns["repetitive_operations"].copy()
            pattern.occurrences = repetitive_count
            pattern.waste_estimate = repetitive_count * 0.1
            detected_patterns.append(pattern)

        # éåŠ¹ç‡ãªæ­£è¦è¡¨ç¾ã®æ¤œå‡º
        regex_issues = self._detect_inefficient_regex(log_text)
        if regex_issues > 5:
            pattern = self.inefficiency_patterns["inefficient_regex"].copy()
            pattern.occurrences = regex_issues
            pattern.waste_estimate = regex_issues * 0.05
            detected_patterns.append(pattern)

        # ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ã®æ¤œå‡º
        memory_trend = self._analyze_memory_trend()
        if memory_trend > 0.2:  # 20%ä»¥ä¸Šã®å¢—åŠ å‚¾å‘
            pattern = self.inefficiency_patterns["memory_leaks"].copy()
            pattern.occurrences = 1
            pattern.waste_estimate = memory_trend * 100
            detected_patterns.append(pattern)

        return detected_patterns

    def _detect_repetitive_operations(self, log_text: str) -> int:
        """åå¾©æ“ä½œã‚’æ¤œå‡º"""
        # åŒã˜ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚„è­¦å‘Šã®ç¹°ã‚Šè¿”ã—ã‚’æ¤œå‡º
        lines = log_text.split("\n")
        line_counts = Counter(line.strip() for line in lines if line.strip())

        repetitive_count = 0
        for line, count in line_counts.items():
            if count > 5 and ("error" in line.lower() or "warning" in line.lower()):
                repetitive_count += count - 1  # åˆå›ä»¥å¤–ã‚’ã‚«ã‚¦ãƒ³ãƒˆ

        return repetitive_count

    def _detect_inefficient_regex(self, log_text: str) -> int:
        """éåŠ¹ç‡ãªæ­£è¦è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º"""
        # æ­£è¦è¡¨ç¾ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«å¤±æ•—ã‚„æ™‚é–“ã®ã‹ã‹ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º
        regex_error_patterns = [
            r"regex.*error",
            r"pattern.*timeout",
            r"compilation.*failed",
        ]

        issues = 0
        for pattern in regex_error_patterns:
            matches = re.findall(pattern, log_text, re.IGNORECASE)
            issues += len(matches)

        return issues

    def _analyze_memory_trend(self) -> float:
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®å¢—åŠ å‚¾å‘ã‚’åˆ†æ"""
        if len(self.token_history) < 10:
            return 0.0

        # ç°¡æ˜“çš„ãªç·šå½¢å›å¸°ã§å‚¾å‘ã‚’è¨ˆç®—
        recent_data = self.token_history[-10:]
        n = len(recent_data)

        x_sum = sum(range(n))
        y_sum = sum(recent_data)
        xy_sum = sum(i * recent_data[i] for i in range(n))
        x2_sum = sum(i * i for i in range(n))

        if n * x2_sum - x_sum * x_sum == 0:
            return 0.0

        slope = (n * xy_sum - x_sum * y_sum) / (n * x2_sum - x_sum * x_sum)

        # æ­£è¦åŒ–ï¼ˆæœ€å¤§å€¤ã«å¯¾ã™ã‚‹å¢—åŠ ç‡ï¼‰
        if max(recent_data) > 0:
            return slope / max(recent_data)
        return 0.0

    def generate_optimization_recommendations(
        self, patterns: List[InefficiencyPattern]
    ) -> List[str]:
        """éåŠ¹ç‡ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ãæœ€é©åŒ–æ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ"""
        recommendations = []

        for pattern in patterns:
            if pattern.pattern_type == "repetitive_operations":
                recommendations.append(
                    f"ğŸ”„ åå¾©æ“ä½œã®æœ€é©åŒ–: {pattern.occurrences}å›ã®é‡è¤‡æ“ä½œã‚’çµ±åˆã™ã‚‹ã“ã¨ã§"
                    f"ç´„{pattern.waste_estimate:.1f}%ã®åŠ¹ç‡å‘ä¸ŠãŒæœŸå¾…ã§ãã¾ã™"
                )

            elif pattern.pattern_type == "inefficient_regex":
                recommendations.append(
                    f"ğŸ” æ­£è¦è¡¨ç¾ã®æœ€é©åŒ–: {pattern.occurrences}å€‹ã®éåŠ¹ç‡ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’"
                    f"æ”¹å–„ã™ã‚‹ã“ã¨ã§å‡¦ç†é€Ÿåº¦ãŒå‘ä¸Šã—ã¾ã™"
                )

            elif pattern.pattern_type == "memory_leaks":
                recommendations.append(
                    "ğŸ’¾ ãƒ¡ãƒ¢ãƒªç®¡ç†ã®æ”¹å–„: ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®å¢—åŠ å‚¾å‘ã‚’æ”¹å–„ã™ã‚‹ã“ã¨ã§"
                    "é•·æœŸå®Ÿè¡Œã®å®‰å®šæ€§ãŒå‘ä¸Šã—ã¾ã™"
                )

        if not recommendations:
            recommendations.append(
                "âœ… ç¾åœ¨ã®ã¨ã“ã‚ã€å¤§ããªæœ€é©åŒ–ã®ä½™åœ°ã¯æ¤œå‡ºã•ã‚Œã¦ã„ã¾ã›ã‚“"
            )

        return recommendations

    def get_efficiency_trend_report(self) -> str:
        """åŠ¹ç‡æ€§ã®å‚¾å‘ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        if len(self.token_history) < 5:
            return "åŠ¹ç‡æ€§åˆ†æã«ã¯ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“"

        recent_avg = sum(self.token_history[-5:]) / 5
        older_avg = (
            sum(self.token_history[-10:-5]) / 5
            if len(self.token_history) >= 10
            else recent_avg
        )

        if older_avg > 0:
            trend_percent = (recent_avg - older_avg) / older_avg * 100
        else:
            trend_percent = 0

        if trend_percent > 5:
            trend_text = f"åŠ¹ç‡æ€§ãŒ{trend_percent:.1f}%å‘ä¸Šã—ã¦ã„ã¾ã™ ğŸ“ˆ"
        elif trend_percent < -5:
            trend_text = f"åŠ¹ç‡æ€§ãŒ{abs(trend_percent):.1f}%ä½ä¸‹ã—ã¦ã„ã¾ã™ ğŸ“‰"
        else:
            trend_text = "åŠ¹ç‡æ€§ã¯å®‰å®šã—ã¦ã„ã¾ã™ â¡ï¸"

        return f"ãƒˆãƒ¼ã‚¯ãƒ³åŠ¹ç‡åˆ†æ: {trend_text}"


class PatternDetector:
    """ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºãƒ»åˆ†æã‚¨ãƒ³ã‚¸ãƒ³"""

    def __init__(self):
        self.logger = get_logger(__name__)
        self.detection_rules = self._initialize_rules()

    def _initialize_rules(self) -> Dict[str, Any]:
        """æ¤œå‡ºãƒ«ãƒ¼ãƒ«ã‚’åˆæœŸåŒ–"""
        return {
            "performance_degradation": {
                "threshold": 0.2,
                "window": 10,
                "severity": "high",
            },
            "error_spike": {"threshold": 5, "window": 5, "severity": "critical"},
        }

    def analyze_patterns(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ"""
        patterns = []

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŠ£åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³
        if self._detect_performance_degradation(data):
            patterns.append(
                {
                    "type": "performance_degradation",
                    "severity": "high",
                    "description": "å‡¦ç†æ€§èƒ½ã®ç¶™ç¶šçš„ãªåŠ£åŒ–ã‚’æ¤œå‡º",
                }
            )

        return patterns

    def _detect_performance_degradation(self, data: List[Dict[str, Any]]) -> bool:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŠ£åŒ–ã‚’æ¤œå‡º"""
        if len(data) < 10:
            return False

        recent_times = [d.get("processing_time", 0) for d in data[-5:]]
        older_times = [d.get("processing_time", 0) for d in data[-10:-5]]

        if older_times and recent_times:
            recent_avg = sum(recent_times) / len(recent_times)
            older_avg = sum(older_times) / len(older_times)

            if older_avg > 0:
                degradation = (recent_avg - older_avg) / older_avg
                return degradation > 0.2  # 20%ä»¥ä¸Šã®åŠ£åŒ–

        return False


class AlertSystem:
    """ã‚¢ãƒ©ãƒ¼ãƒˆãƒ»é€šçŸ¥ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, alert_thresholds: Optional[Dict[str, float]] = None):
        self.logger = get_logger(__name__)

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚¢ãƒ©ãƒ¼ãƒˆé–¾å€¤
        self.thresholds = alert_thresholds or {
            "memory_usage": 80.0,  # %
            "processing_time": 10.0,  # seconds
            "error_rate": 5.0,  # %
            "efficiency_drop": 20.0,  # %
        }

        self.alert_history: List[Dict[str, Any]] = []

    def check_alerts(self, metrics: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ã‚¢ãƒ©ãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        alerts = []

        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚¢ãƒ©ãƒ¼ãƒˆ
        memory_percent = metrics.get("memory_percent", 0)
        if memory_percent > self.thresholds["memory_usage"]:
            alerts.append(
                {
                    "type": "high_memory",
                    "severity": "warning",
                    "message": f"é«˜ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {memory_percent:.1f}%",
                    "value": memory_percent,
                    "threshold": self.thresholds["memory_usage"],
                }
            )

        # å‡¦ç†æ™‚é–“ã‚¢ãƒ©ãƒ¼ãƒˆ
        processing_time = metrics.get("processing_time", 0)
        if processing_time > self.thresholds["processing_time"]:
            alerts.append(
                {
                    "type": "slow_processing",
                    "severity": "warning",
                    "message": f"å‡¦ç†æ™‚é–“ãŒé•·ã„: {processing_time:.2f}ç§’",
                    "value": processing_time,
                    "threshold": self.thresholds["processing_time"],
                }
            )

        # ã‚¢ãƒ©ãƒ¼ãƒˆå±¥æ­´ã«è¿½åŠ 
        for alert in alerts:
            alert["timestamp"] = time.time()
            self.alert_history.append(alert)

        return alerts

    def get_alert_summary(self) -> str:
        """ã‚¢ãƒ©ãƒ¼ãƒˆã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ"""
        if not self.alert_history:
            return "ã‚¢ãƒ©ãƒ¼ãƒˆã¯ç™ºç”Ÿã—ã¦ã„ã¾ã›ã‚“"

        recent_alerts = self.alert_history[-10:]  # ç›´è¿‘10ä»¶
        alert_counts = Counter(alert["type"] for alert in recent_alerts)

        summary_lines = ["ğŸš¨ æœ€è¿‘ã®ã‚¢ãƒ©ãƒ¼ãƒˆ:"]
        for alert_type, count in alert_counts.items():
            summary_lines.append(f"  {alert_type}: {count}ä»¶")

        return "\n".join(summary_lines)
