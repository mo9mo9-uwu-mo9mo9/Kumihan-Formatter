"""
ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ»ãƒ†ã‚¹ãƒˆã‚·ã‚¹ãƒ†ãƒ 
Issue #813å¯¾å¿œ - performance_metrics.pyåˆ†å‰²ç‰ˆï¼ˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç³»ï¼‰

è²¬ä»»ç¯„å›²:
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
- ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœåˆ†æ
- æ€§èƒ½æ¯”è¼ƒãƒ»è©•ä¾¡
- æœ€é©åŒ–åŠ¹æœæ¸¬å®š
"""

import statistics
import time
from dataclasses import dataclass, field
from typing import Any, Callable, Dict, List, Optional

from ..logger import get_logger


@dataclass
class BenchmarkResult:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ"""

    name: str
    execution_time: float
    iterations: int
    avg_time_per_iteration: float
    min_time: float
    max_time: float
    std_deviation: float
    memory_usage_mb: Optional[float] = None
    success_rate: float = 100.0
    metadata: Dict[str, Any] = field(default_factory=dict)


class PerformanceBenchmark:
    """
    ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆãƒ»ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚·ã‚¹ãƒ†ãƒ 

    æ©Ÿèƒ½:
    - é–¢æ•°ãƒ»ãƒ¡ã‚½ãƒƒãƒ‰ã®æ€§èƒ½æ¸¬å®š
    - è¤‡æ•°å›å®Ÿè¡Œã«ã‚ˆã‚‹çµ±è¨ˆåˆ†æ
    - ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç›£è¦–
    - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆ
    """

    def __init__(self, default_iterations: int = 100):
        self.logger = get_logger(__name__)
        self.default_iterations = default_iterations
        self.results: List[BenchmarkResult] = []

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–
        self._memory_available = self._check_memory_monitoring()

        self.logger.info(
            f"PerformanceBenchmark initialized with {default_iterations} default iterations"
        )

    def _check_memory_monitoring(self) -> bool:
        """ãƒ¡ãƒ¢ãƒªç›£è¦–æ©Ÿèƒ½ã®åˆ©ç”¨å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
        try:
            import psutil  # noqa: F401

            return True
        except ImportError:
            self.logger.warning("psutil not available, memory monitoring disabled")
            return False

    def benchmark_function(
        self,
        func: Callable,
        args: tuple = (),
        kwargs: dict = None,
        iterations: Optional[int] = None,
        name: Optional[str] = None,
        warm_up: int = 5,
    ) -> BenchmarkResult:
        """
        é–¢æ•°ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ

        Args:
            func: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å¯¾è±¡é–¢æ•°
            args: é–¢æ•°å¼•æ•°
            kwargs: é–¢æ•°ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¼•æ•°
            iterations: å®Ÿè¡Œå›æ•°
            name: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å
            warm_up: ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Ÿè¡Œå›æ•°

        Returns:
            BenchmarkResult: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ
        """
        if kwargs is None:
            kwargs = {}
        if iterations is None:
            iterations = self.default_iterations
        if name is None:
            name = func.__name__

        self.logger.info(f"Starting benchmark: {name} ({iterations} iterations)")

        # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Ÿè¡Œ
        for _ in range(warm_up):
            try:
                func(*args, **kwargs)
            except Exception:
                pass  # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã§ã¯ã‚¨ãƒ©ãƒ¼ã‚’ç„¡è¦–

        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®åˆæœŸå€¤
        initial_memory = self._get_memory_usage()

        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
        execution_times = []
        successful_runs = 0

        start_time = time.time()

        for i in range(iterations):
            iteration_start = time.perf_counter()

            try:
                func(*args, **kwargs)
                iteration_end = time.perf_counter()
                execution_times.append(iteration_end - iteration_start)
                successful_runs += 1

            except Exception as e:
                self.logger.warning(f"Benchmark iteration {i+1} failed: {e}")

        total_time = time.time() - start_time

        # çµ±è¨ˆè¨ˆç®—
        if execution_times:
            avg_time = statistics.mean(execution_times)
            min_time = min(execution_times)
            max_time = max(execution_times)
            std_dev = (
                statistics.stdev(execution_times) if len(execution_times) > 1 else 0.0
            )
        else:
            avg_time = min_time = max_time = std_dev = 0.0

        # æˆåŠŸç‡è¨ˆç®—
        success_rate = (successful_runs / iterations) * 100 if iterations > 0 else 0.0

        # æœ€çµ‚ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
        final_memory = self._get_memory_usage()
        memory_delta = final_memory - initial_memory if self._memory_available else None

        # çµæœä½œæˆ
        result = BenchmarkResult(
            name=name,
            execution_time=total_time,
            iterations=iterations,
            avg_time_per_iteration=avg_time,
            min_time=min_time,
            max_time=max_time,
            std_deviation=std_dev,
            memory_usage_mb=memory_delta,
            success_rate=success_rate,
            metadata={
                "successful_runs": successful_runs,
                "failed_runs": iterations - successful_runs,
                "warm_up_iterations": warm_up,
            },
        )

        self.results.append(result)

        self.logger.info(
            f"Benchmark completed: {name} - "
            f"avg: {avg_time:.4f}s, "
            f"success: {success_rate:.1f}%"
        )

        return result

    def _get_memory_usage(self) -> float:
        """ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—ï¼ˆMBï¼‰"""
        if not self._memory_available:
            return 0.0

        try:
            import psutil

            process = psutil.Process()
            return process.memory_info().rss / 1024 / 1024
        except Exception:
            return 0.0

    def benchmark_multiple_functions(
        self,
        functions: Dict[str, Callable],
        args: tuple = (),
        kwargs: dict = None,
        iterations: Optional[int] = None,
    ) -> Dict[str, BenchmarkResult]:
        """
        è¤‡æ•°é–¢æ•°ã®æ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

        Args:
            functions: {åå‰: é–¢æ•°} ã®è¾æ›¸
            args: å…±é€šå¼•æ•°
            kwargs: å…±é€šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¼•æ•°
            iterations: å®Ÿè¡Œå›æ•°

        Returns:
            Dict[str, BenchmarkResult]: å„é–¢æ•°ã®çµæœ
        """
        if kwargs is None:
            kwargs = {}

        results = {}

        for name, func in functions.items():
            result = self.benchmark_function(
                func=func, args=args, kwargs=kwargs, iterations=iterations, name=name
            )
            results[name] = result

        return results

    def compare_performance(self, baseline_name: str) -> str:
        """
        ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³é–¢æ•°ã¨ã®æ€§èƒ½æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ

        Args:
            baseline_name: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³é–¢æ•°å

        Returns:
            str: æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆ
        """
        baseline = None
        for result in self.results:
            if result.name == baseline_name:
                baseline = result
                break

        if baseline is None:
            return f"Baseline '{baseline_name}' not found in results"

        report_lines = [
            f"ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆ (ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³: {baseline_name})",
            "=" * 60,
            f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³: {baseline.avg_time_per_iteration:.4f}s",
            "",
        ]

        for result in self.results:
            if result.name == baseline_name:
                continue

            if baseline.avg_time_per_iteration > 0:
                speedup = (
                    baseline.avg_time_per_iteration / result.avg_time_per_iteration
                )
                speedup_percent = (speedup - 1) * 100

                if speedup > 1:
                    performance_text = (
                        f"{speedup:.2f}x faster ({speedup_percent:.1f}% improvement)"
                    )
                else:
                    slowdown_percent = (1 - speedup) * 100
                    performance_text = (
                        f"{1/speedup:.2f}x slower ({slowdown_percent:.1f}% slower)"
                    )
            else:
                performance_text = "æ¯”è¼ƒä¸å¯èƒ½"

            report_lines.extend(
                [
                    f"{result.name}:",
                    f"  å¹³å‡æ™‚é–“: {result.avg_time_per_iteration:.4f}s",
                    f"  ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹: {performance_text}",
                    f"  æˆåŠŸç‡: {result.success_rate:.1f}%",
                    "",
                ]
            )

        return "\n".join(report_lines)

    def get_summary_report(self) -> str:
        """å…¨ä½“çš„ãªçµ±è¨ˆãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        if not self.results:
            return "ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœãŒã‚ã‚Šã¾ã›ã‚“"

        report_lines = [
            "ğŸ“ˆ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµ±è¨ˆãƒ¬ãƒãƒ¼ãƒˆ",
            "=" * 40,
            f"ç·å®Ÿè¡Œãƒ†ã‚¹ãƒˆæ•°: {len(self.results)}",
            "",
        ]

        # æœ€é€Ÿãƒ»æœ€é…ã®é–¢æ•°
        fastest = min(self.results, key=lambda x: x.avg_time_per_iteration)
        slowest = max(self.results, key=lambda x: x.avg_time_per_iteration)

        report_lines.extend(
            [
                f"ğŸ† æœ€é«˜æ€§èƒ½: {fastest.name} ({fastest.avg_time_per_iteration:.4f}s)",
                f"ğŸŒ æœ€ä½æ€§èƒ½: {slowest.name} ({slowest.avg_time_per_iteration:.4f}s)",
                "",
            ]
        )

        # å„çµæœã®è©³ç´°
        for result in sorted(self.results, key=lambda x: x.avg_time_per_iteration):
            memory_info = (
                f", ãƒ¡ãƒ¢ãƒª: {result.memory_usage_mb:.2f}MB"
                if result.memory_usage_mb
                else ""
            )

            report_lines.append(
                f"{result.name}: {result.avg_time_per_iteration:.4f}s "
                f"(æˆåŠŸç‡: {result.success_rate:.1f}%{memory_info})"
            )

        return "\n".join(report_lines)

    def clear_results(self):
        """çµæœã‚’ã‚¯ãƒªã‚¢"""
        cleared_count = len(self.results)
        self.results.clear()
        self.logger.info(f"Cleared {cleared_count} benchmark results")

    def export_results_to_dict(self) -> List[Dict[str, Any]]:
        """çµæœã‚’è¾æ›¸å½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        return [
            {
                "name": result.name,
                "execution_time": result.execution_time,
                "iterations": result.iterations,
                "avg_time_per_iteration": result.avg_time_per_iteration,
                "min_time": result.min_time,
                "max_time": result.max_time,
                "std_deviation": result.std_deviation,
                "memory_usage_mb": result.memory_usage_mb,
                "success_rate": result.success_rate,
                "metadata": result.metadata,
            }
            for result in self.results
        ]
