"""
ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚·ã‚¹ãƒ†ãƒ 
Issue #694 Phase 3å¯¾å¿œ - è©³ç´°ãªæ€§èƒ½æ¸¬å®šãƒ»åˆ†æ
"""

import json
import os
import sys
import threading
import time
from collections import deque
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional

import psutil

from .logger import get_logger


@dataclass
class PerformanceSnapshot:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ"""

    timestamp: float
    cpu_percent: float
    memory_mb: float
    memory_percent: float
    processing_rate: float  # items/sec
    items_processed: int
    total_items: int
    stage: str
    thread_count: int = 0
    disk_io_read_mb: float = 0.0
    disk_io_write_mb: float = 0.0


@dataclass
class ProcessingStats:
    """å‡¦ç†çµ±è¨ˆæƒ…å ±"""

    start_time: float = field(default_factory=time.time)
    end_time: Optional[float] = None
    total_items: int = 0
    items_processed: int = 0
    errors_count: int = 0
    warnings_count: int = 0
    peak_memory_mb: float = 0.0
    avg_cpu_percent: float = 0.0
    processing_phases: List[str] = field(default_factory=list)

    @property
    def duration_seconds(self) -> float:
        """å‡¦ç†æ™‚é–“ï¼ˆç§’ï¼‰"""
        end = self.end_time or time.time()
        return end - self.start_time

    @property
    def items_per_second(self) -> float:
        """å‡¦ç†é€Ÿåº¦ï¼ˆã‚¢ã‚¤ãƒ†ãƒ /ç§’ï¼‰"""
        duration = self.duration_seconds
        return self.items_processed / duration if duration > 0 else 0
    
    @property
    def completion_rate(self) -> float:
        """å®Œäº†ç‡ï¼ˆ%ï¼‰"""
        if self.total_items == 0:
            return 0.0
        return (self.items_processed / self.total_items) * 100


class PerformanceMonitor:
    """
    ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 

    æ©Ÿèƒ½:
    - CPUãƒ»ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ç¶™ç¶šç›£è¦–
    - å‡¦ç†é€Ÿåº¦ãƒ»ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæ¸¬å®š
    - ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æ¤œå‡º
    - ãƒ¡ãƒˆãƒªã‚¯ã‚¹å±¥æ­´ä¿å­˜
    - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    """

    def __init__(self, monitoring_interval: float = 1.0, history_size: int = 1000):
        self.logger = get_logger(__name__)
        self.monitoring_interval = monitoring_interval
        self.history_size = history_size

        # ç›£è¦–ãƒ‡ãƒ¼ã‚¿
        self.snapshots: deque[PerformanceSnapshot] = deque(maxlen=history_size)
        self.stats = ProcessingStats()

        # ç›£è¦–åˆ¶å¾¡
        self._monitoring = False
        self._monitor_thread: Optional[threading.Thread] = None
        self._lock = threading.Lock()

        # ãƒ—ãƒ­ã‚»ã‚¹æƒ…å ±
        self.process = psutil.Process()
        self.initial_memory = self.process.memory_info().rss

        # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
        self.alert_callbacks: List[Callable[[str, Dict], None]] = []

        self.logger.info(
            f"PerformanceMonitor initialized: interval={monitoring_interval}s, history={history_size}"
        )

    def start_monitoring(self, total_items: int, initial_stage: str = "é–‹å§‹"):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚’é–‹å§‹"""
        with self._lock:
            if self._monitoring:
                self.logger.warning("Performance monitoring already started")
                return

            # çµ±è¨ˆæƒ…å ±åˆæœŸåŒ–
            self.stats = ProcessingStats(
                start_time=time.time(), total_items=total_items
            )
            self.stats.processing_phases.append(initial_stage)

            # ç›£è¦–é–‹å§‹
            self._monitoring = True
            self._monitor_thread = threading.Thread(
                target=self._monitoring_loop, daemon=True
            )
            self._monitor_thread.start()

            self.logger.info(
                f"Performance monitoring started: {total_items} items, stage: {initial_stage}"
            )

    def stop_monitoring(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚’åœæ­¢"""
        with self._lock:
            if not self._monitoring:
                return

            self._monitoring = False
            self.stats.end_time = time.time()

            if self._monitor_thread and self._monitor_thread.is_alive():
                self._monitor_thread.join(timeout=2.0)

            self.logger.info(
                f"Performance monitoring stopped after {self.stats.duration_seconds:.2f}s"
            )

    def update_progress(self, items_processed: int, current_stage: str = ""):
        """é€²æ—ã‚’æ›´æ–°"""
        with self._lock:
            self.stats.items_processed = items_processed

            if current_stage and current_stage not in self.stats.processing_phases:
                self.stats.processing_phases.append(current_stage)

    def add_error(self):
        """ã‚¨ãƒ©ãƒ¼ã‚«ã‚¦ãƒ³ãƒˆã‚’å¢—åŠ """
        with self._lock:
            self.stats.errors_count += 1

    def add_warning(self):
        """è­¦å‘Šã‚«ã‚¦ãƒ³ãƒˆã‚’å¢—åŠ """
        with self._lock:
            self.stats.warnings_count += 1

    def add_alert_callback(self, callback: Callable[[str, Dict], None]):
        """ã‚¢ãƒ©ãƒ¼ãƒˆã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’è¿½åŠ """
        self.alert_callbacks.append(callback)

    def get_current_snapshot(self) -> PerformanceSnapshot:
        """ç¾åœ¨ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’å–å¾—"""
        try:
            # CPUãƒ»ãƒ¡ãƒ¢ãƒªæƒ…å ±
            cpu_percent = self.process.cpu_percent()
            memory_info = self.process.memory_info()
            memory_mb = memory_info.rss / 1024 / 1024
            memory_percent = self.process.memory_percent()

            # å‡¦ç†é€Ÿåº¦è¨ˆç®—
            processing_rate = self.stats.items_per_second

            # ãƒ‡ã‚£ã‚¹ã‚¯I/Oï¼ˆå¯èƒ½ãªå ´åˆï¼‰
            try:
                io_counters = self.process.io_counters()
                disk_io_read_mb = io_counters.read_bytes / 1024 / 1024
                disk_io_write_mb = io_counters.write_bytes / 1024 / 1024
            except (AttributeError, psutil.AccessDenied):
                disk_io_read_mb = disk_io_write_mb = 0.0

            # ã‚¹ãƒ¬ãƒƒãƒ‰æ•°
            thread_count = self.process.num_threads()

            # ç¾åœ¨ã®ã‚¹ãƒ†ãƒ¼ã‚¸
            current_stage = (
                self.stats.processing_phases[-1]
                if self.stats.processing_phases
                else "unknown"
            )

            return PerformanceSnapshot(
                timestamp=time.time(),
                cpu_percent=cpu_percent,
                memory_mb=memory_mb,
                memory_percent=memory_percent,
                processing_rate=processing_rate,
                items_processed=self.stats.items_processed,
                total_items=self.stats.total_items,
                stage=current_stage,
                thread_count=thread_count,
                disk_io_read_mb=disk_io_read_mb,
                disk_io_write_mb=disk_io_write_mb,
            )

        except Exception as e:
            self.logger.error(f"Error creating performance snapshot: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬çš„ãªæƒ…å ±ã®ã¿
            return PerformanceSnapshot(
                timestamp=time.time(),
                cpu_percent=0.0,
                memory_mb=0.0,
                memory_percent=0.0,
                processing_rate=0.0,
                items_processed=self.stats.items_processed,
                total_items=self.stats.total_items,
                stage="error",
            )

    def _monitoring_loop(self):
        """ç›£è¦–ãƒ«ãƒ¼ãƒ—ï¼ˆåˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œï¼‰"""
        self.logger.debug("Performance monitoring loop started")

        while self._monitoring:
            try:
                # ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆå–å¾—
                snapshot = self.get_current_snapshot()

                with self._lock:
                    self.snapshots.append(snapshot)

                    # ãƒ”ãƒ¼ã‚¯ãƒ¡ãƒ¢ãƒªæ›´æ–°
                    if snapshot.memory_mb > self.stats.peak_memory_mb:
                        self.stats.peak_memory_mb = snapshot.memory_mb

                    # CPUå¹³å‡è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰
                    if len(self.snapshots) > 0:
                        recent_snapshots = list(self.snapshots)[-10:]  # ç›´è¿‘10ã‚µãƒ³ãƒ—ãƒ«
                        self.stats.avg_cpu_percent = sum(
                            s.cpu_percent for s in recent_snapshots
                        ) / len(recent_snapshots)

                # ã‚¢ãƒ©ãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯
                self._check_performance_alerts(snapshot)

                time.sleep(self.monitoring_interval)

            except Exception as e:
                self.logger.error(f"Error in monitoring loop: {e}")
                time.sleep(self.monitoring_interval)

        self.logger.debug("Performance monitoring loop ended")

    def _check_performance_alerts(self, snapshot: PerformanceSnapshot):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¢ãƒ©ãƒ¼ãƒˆã‚’ãƒã‚§ãƒƒã‚¯"""
        alerts = []

        # é«˜CPUä½¿ç”¨ç‡ã‚¢ãƒ©ãƒ¼ãƒˆ
        if snapshot.cpu_percent > 90:
            alerts.append(
                {
                    "type": "high_cpu",
                    "severity": "warning",
                    "message": f"é«˜CPUä½¿ç”¨ç‡: {snapshot.cpu_percent:.1f}%",
                    "value": snapshot.cpu_percent,
                }
            )

        # é«˜ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ã‚¢ãƒ©ãƒ¼ãƒˆ
        if snapshot.memory_percent > 80:
            alerts.append(
                {
                    "type": "high_memory",
                    "severity": "warning",
                    "message": f"é«˜ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {snapshot.memory_percent:.1f}% ({snapshot.memory_mb:.1f}MB)",
                    "value": snapshot.memory_percent,
                }
            )

        # ä½å‡¦ç†é€Ÿåº¦ã‚¢ãƒ©ãƒ¼ãƒˆ
        if (
            snapshot.processing_rate > 0 and snapshot.processing_rate < 100
        ):  # 100 items/secæœªæº€
            alerts.append(
                {
                    "type": "low_processing_rate",
                    "severity": "info",
                    "message": f"ä½å‡¦ç†é€Ÿåº¦: {snapshot.processing_rate:.0f} items/sec",
                    "value": snapshot.processing_rate,
                }
            )

        # ã‚¢ãƒ©ãƒ¼ãƒˆé€šçŸ¥
        for alert in alerts:
            for callback in self.alert_callbacks:
                try:
                    callback(alert["type"], alert)
                except Exception as e:
                    self.logger.error(f"Error in alert callback: {e}")

    def get_performance_summary(self) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¦‚è¦ã‚’å–å¾—"""
        with self._lock:
            recent_snapshots = list(self.snapshots)[-10:] if self.snapshots else []

            return {
                "duration_seconds": self.stats.duration_seconds,
                "items_processed": self.stats.items_processed,
                "total_items": self.stats.total_items,
                "completion_rate": self.stats.completion_rate,
                "items_per_second": self.stats.items_per_second,
                "errors_count": self.stats.errors_count,
                "warnings_count": self.stats.warnings_count,
                "peak_memory_mb": self.stats.peak_memory_mb,
                "avg_cpu_percent": self.stats.avg_cpu_percent,
                "processing_phases": self.stats.processing_phases,
                "current_memory_mb": (
                    recent_snapshots[-1].memory_mb if recent_snapshots else 0
                ),
                "current_cpu_percent": (
                    recent_snapshots[-1].cpu_percent if recent_snapshots else 0
                ),
                "snapshots_count": len(self.snapshots),
            }

    def generate_performance_report(self) -> str:
        """è©³ç´°ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        summary = self.get_performance_summary()

        report_lines = [
            "ğŸ” ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æãƒ¬ãƒãƒ¼ãƒˆ",
            "=" * 50,
            f"å‡¦ç†æ™‚é–“: {summary['duration_seconds']:.2f}ç§’",
            f"å‡¦ç†é …ç›®: {summary['items_processed']:,} / "
            f"{summary['total_items']:,} ({summary['completion_rate']:.1f}%)",
            f"å‡¦ç†é€Ÿåº¦: {summary['items_per_second']:,.0f} items/ç§’",
            f"ã‚¨ãƒ©ãƒ¼: {summary['errors_count']}, è­¦å‘Š: {summary['warnings_count']}",
            "",
            "ğŸ’¾ ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡:",
            f"  ãƒ”ãƒ¼ã‚¯ãƒ¡ãƒ¢ãƒª: {summary['peak_memory_mb']:.1f}MB",
            f"  å¹³å‡CPU: {summary['avg_cpu_percent']:.1f}%",
            f"  ç¾åœ¨ãƒ¡ãƒ¢ãƒª: {summary['current_memory_mb']:.1f}MB",
            f"  ç¾åœ¨CPU: {summary['current_cpu_percent']:.1f}%",
            "",
            "ğŸ”„ å‡¦ç†ãƒ•ã‚§ãƒ¼ã‚º:",
        ]

        for phase in summary["processing_phases"]:
            report_lines.append(f"  - {phase}")

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‚¾å‘åˆ†æ
        if len(self.snapshots) >= 5:
            report_lines.extend(
                [
                    "",
                    "ğŸ“ˆ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‚¾å‘:",
                ]
            )

            snapshots_list = list(self.snapshots)

            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å‚¾å‘
            memory_trend = self._calculate_trend(
                [s.memory_mb for s in snapshots_list[-10:]]
            )
            memory_status = (
                "å¢—åŠ "
                if memory_trend > 0.5
                else "å®‰å®š" if memory_trend > -0.5 else "æ¸›å°‘"
            )
            report_lines.append(f"  ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {memory_status}")

            # å‡¦ç†é€Ÿåº¦å‚¾å‘
            rates = [
                s.processing_rate for s in snapshots_list[-10:] if s.processing_rate > 0
            ]
            if rates:
                rate_trend = self._calculate_trend(rates)
                rate_status = (
                    "å‘ä¸Š"
                    if rate_trend > 0.5
                    else "å®‰å®š" if rate_trend > -0.5 else "ä½ä¸‹"
                )
                report_lines.append(f"  å‡¦ç†é€Ÿåº¦: {rate_status}")

        return "\n".join(report_lines)

    def _calculate_trend(self, values: List[float]) -> float:
        """å€¤ã®å‚¾å‘ã‚’è¨ˆç®—ï¼ˆç°¡æ˜“ç·šå½¢å›å¸°ï¼‰"""
        if len(values) < 2:
            return 0.0

        n = len(values)
        x_avg = sum(range(n)) / n
        y_avg = sum(values) / n

        numerator = sum((i - x_avg) * (values[i] - y_avg) for i in range(n))
        denominator = sum((i - x_avg) ** 2 for i in range(n))

        return numerator / denominator if denominator != 0 else 0.0

    def save_metrics_to_file(self, file_path: Path):
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        try:
            metrics_data = {
                "summary": self.get_performance_summary(),
                "snapshots": [
                    {
                        "timestamp": s.timestamp,
                        "cpu_percent": s.cpu_percent,
                        "memory_mb": s.memory_mb,
                        "memory_percent": s.memory_percent,
                        "processing_rate": s.processing_rate,
                        "items_processed": s.items_processed,
                        "stage": s.stage,
                    }
                    for s in self.snapshots
                ],
            }

            with open(file_path, "w", encoding="utf-8") as f:
                json.dump(metrics_data, f, indent=2, ensure_ascii=False)

            self.logger.info(f"Performance metrics saved to {file_path}")

        except Exception as e:
            self.logger.error(f"Failed to save metrics to file: {e}")


class ProgressiveOutputSystem:
    """
    ãƒ—ãƒ­ã‚°ãƒ¬ãƒƒã‚·ãƒ–å‡ºåŠ›ã‚·ã‚¹ãƒ†ãƒ ï¼ˆIssue #727 å¯¾å¿œï¼‰

    æ©Ÿèƒ½:
    - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ çµæœå‡ºåŠ›
    - æ®µéšçš„HTMLç”Ÿæˆ
    - ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“å‘ä¸Š
    - å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã®å¯è¦–æ€§æ”¹å–„
    """

    def __init__(self, output_path: Optional[Path] = None, buffer_size: int = 1000):
        self.logger = get_logger(__name__)
        self.output_path = output_path
        self.buffer_size = buffer_size

        # å‡ºåŠ›ç®¡ç†
        self.html_buffer = []
        self.total_nodes_processed = 0
        self.current_section = "header"

        # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆéƒ¨åˆ†
        self.html_header = ""
        self.html_footer = ""
        self.css_content = ""

        # ã‚¹ãƒˆãƒªãƒ¼ãƒ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«
        self.output_stream = None

        self.logger.info(
            f"Progressive output system initialized: buffer_size={buffer_size}"
        )

    def initialize_output_stream(
        self, template_content: str = "", css_content: str = ""
    ):
        """å‡ºåŠ›ã‚¹ãƒˆãƒªãƒ¼ãƒ ã®åˆæœŸåŒ–"""

        if not self.output_path:
            return  # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ç„¡åŠ¹

        try:
            self.output_stream = open(
                self.output_path, "w", encoding="utf-8", buffering=1
            )

            # HTMLãƒ˜ãƒƒãƒ€ãƒ¼ã®æº–å‚™
            self.css_content = css_content
            self.html_header = self._create_html_header(template_content)
            self.html_footer = self._create_html_footer()

            # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’å³åº§ã«å‡ºåŠ›
            self.output_stream.write(self.html_header)
            self.output_stream.flush()

            self.logger.info(f"Progressive output stream started: {self.output_path}")

        except Exception as e:
            self.logger.error(f"Failed to initialize output stream: {e}")
            self.output_stream = None

    def add_processed_node(self, node_html: str, node_info: dict = None):
        """å‡¦ç†æ¸ˆã¿ãƒãƒ¼ãƒ‰ã®è¿½åŠ """

        if not node_html.strip():
            return

        self.html_buffer.append(node_html)
        self.total_nodes_processed += 1

        # ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚ºã«é”ã—ãŸã‚‰å‡ºåŠ›
        if len(self.html_buffer) >= self.buffer_size:
            self.flush_buffer()

        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¡¨ç¤º
        if self.total_nodes_processed % 100 == 0:
            self.logger.info(
                f"Progressive output: {self.total_nodes_processed} nodes processed"
            )

    def flush_buffer(self):
        """ãƒãƒƒãƒ•ã‚¡ã®å¼·åˆ¶å‡ºåŠ›"""

        if not self.html_buffer or not self.output_stream:
            return

        try:
            # ãƒãƒƒãƒ•ã‚¡å†…å®¹ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿
            content = "\n".join(self.html_buffer)
            self.output_stream.write(content + "\n")
            self.output_stream.flush()

            # ãƒãƒƒãƒ•ã‚¡ã‚¯ãƒªã‚¢
            self.html_buffer.clear()

            self.logger.debug(f"Buffer flushed: {len(self.html_buffer)} items")

        except Exception as e:
            self.logger.error(f"Buffer flush error: {e}")

    def add_section_marker(self, section_name: str, section_content: str = ""):
        """ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãƒãƒ¼ã‚«ãƒ¼ã®è¿½åŠ """

        self.current_section = section_name

        if section_content:
            section_html = f"""
<!-- ===== {section_name.upper()} SECTION START ===== -->
{section_content}
<!-- ===== {section_name.upper()} SECTION END ===== -->
"""
            self.add_processed_node(section_html)

    def finalize_output(self):
        """å‡ºåŠ›ã®æœ€çµ‚åŒ–"""

        if not self.output_stream:
            return

        try:
            # æ®‹ã‚Šãƒãƒƒãƒ•ã‚¡ã‚’å‡ºåŠ›
            self.flush_buffer()

            # ãƒ•ãƒƒã‚¿ãƒ¼ã‚’å‡ºåŠ›
            self.output_stream.write(self.html_footer)
            self.output_stream.flush()

            # ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚¯ãƒ­ãƒ¼ã‚º
            self.output_stream.close()
            self.output_stream = None

            self.logger.info(
                f"Progressive output finalized: {self.total_nodes_processed} nodes, "
                f"output: {self.output_path}"
            )

        except Exception as e:
            self.logger.error(f"Output finalization error: {e}")

    def _create_html_header(self, template_content: str) -> str:
        """HTMLãƒ˜ãƒƒãƒ€ãƒ¼ã®ä½œæˆ"""

        return f"""<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Kumihan Formatter - Progressive Output</title>
    <style>
{self.css_content}
/* Progressive output styles */
.kumihan-progressive-info {{
    position: fixed;
    top: 10px;
    right: 10px;
    background: rgba(0,0,0,0.8);
    color: white;
    padding: 10px;
    border-radius: 5px;
    font-family: monospace;
    font-size: 12px;
    z-index: 1000;
}}
.kumihan-processing {{
    opacity: 0.7;
    transition: opacity 0.3s ease;
}}
    </style>
    <script>
// Progressive output JavaScript
let processedNodes = 0;
function updateProgressInfo() {{
    const info = document.querySelector('.kumihan-progressive-info');
    if (info) {{
        info.textContent = 'Kumihan-Formatter å‡¦ç†ä¸­... ' + (window.processedNodes || 0) + ' nodes';
    }}
}}
setInterval(updateProgressInfo, 1000);
    </script>
</head>
<body>
<div class="kumihan-progressive-info">Kumihan Progressive Output - å‡¦ç†é–‹å§‹</div>
<div class="kumihan-content">
<!-- PROGRESSIVE CONTENT START -->
"""

    def _create_html_footer(self) -> str:
        """HTMLãƒ•ãƒƒã‚¿ãƒ¼ã®ä½œæˆ"""

        return f"""
<!-- PROGRESSIVE CONTENT END -->
</div>
<script>
// Final processing info
const info = document.querySelector('.kumihan-progressive-info');
if (info) {{
    info.textContent = 'âœ… å‡¦ç†å®Œäº† - {self.total_nodes_processed} nodes';
    info.style.backgroundColor = 'rgba(0,128,0,0.8)';
}}
document.querySelectorAll('.kumihan-processing').forEach(el => {{
    el.classList.remove('kumihan-processing');
}});
</script>
</body>
</html>"""

    def create_progress_html(self, current: int, total: int, stage: str = "") -> str:
        """ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¡¨ç¤ºHTMLç”Ÿæˆ"""

        progress_percent = (current / total * 100) if total > 0 else 0

        return f"""
<div class="kumihan-progress-update" data-current="{current}" data-total="{total}">
    <div class="progress-bar" style="width: {progress_percent:.1f}%; background: linear-gradient(90deg, #4CAF50, #2196F3);"></div>
    <div class="progress-text">{stage} - {current}/{total} ({progress_percent:.1f}%)</div>
</div>
"""

    def get_output_statistics(self) -> dict:
        """å‡ºåŠ›çµ±è¨ˆã®å–å¾—"""

        return {
            "total_nodes_processed": self.total_nodes_processed,
            "buffer_size": len(self.html_buffer),
            "current_section": self.current_section,
            "output_active": self.output_stream is not None,
            "output_path": str(self.output_path) if self.output_path else None,
        }

    def __enter__(self):
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼é–‹å§‹"""
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼çµ‚äº†"""
        self.finalize_output()


class PerformanceBenchmark:
    """
    ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚·ã‚¹ãƒ†ãƒ ï¼ˆIssue #727 å¯¾å¿œï¼‰

    æ©Ÿèƒ½:
    - ãƒ‘ãƒ¼ã‚µãƒ¼æ€§èƒ½æ¸¬å®š
    - ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãƒ†ã‚¹ãƒˆ
    - ä¸¦åˆ—å‡¦ç†åŠ¹æœæ¸¬å®š
    - ç›®æ¨™é”æˆè©•ä¾¡
    """

    def __init__(self):
        self.logger = get_logger(__name__)
        self.results = {}
        self.test_data_cache = {}

    def run_comprehensive_benchmark(self) -> dict:
        """åŒ…æ‹¬çš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""

        self.logger.info("ğŸš€ Starting comprehensive performance benchmark...")

        benchmark_results = {
            "metadata": {
                "timestamp": time.time(),
                "python_version": sys.version,
                "platform": sys.platform,
                "cpu_count": os.cpu_count(),
            },
            "tests": {},
        }

        # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹å®šç¾©
        test_cases = [
            {"name": "small", "lines": 1000, "description": "å°è¦æ¨¡ãƒ•ã‚¡ã‚¤ãƒ«(1Kè¡Œ)"},
            {"name": "medium", "lines": 5000, "description": "ä¸­è¦æ¨¡ãƒ•ã‚¡ã‚¤ãƒ«(5Kè¡Œ)"},
            {"name": "large", "lines": 10000, "description": "å¤§è¦æ¨¡ãƒ•ã‚¡ã‚¤ãƒ«(10Kè¡Œ)"},
            {
                "name": "extra_large",
                "lines": 50000,
                "description": "è¶…å¤§è¦æ¨¡ãƒ•ã‚¡ã‚¤ãƒ«(50Kè¡Œ)",
            },
        ]

        for test_case in test_cases:
            self.logger.info(f"ğŸ“Š Testing {test_case['description']}...")

            test_results = self._run_single_benchmark(
                test_case["name"], test_case["lines"]
            )

            benchmark_results["tests"][test_case["name"]] = test_results

        # ç›®æ¨™é”æˆè©•ä¾¡
        benchmark_results["goal_assessment"] = self._assess_performance_goals(
            benchmark_results["tests"]
        )

        # ã‚µãƒãƒªãƒ¼ç”Ÿæˆ
        benchmark_results["summary"] = self._generate_benchmark_summary(
            benchmark_results
        )

        self.logger.info("âœ… Comprehensive benchmark completed")
        return benchmark_results

    def _run_single_benchmark(self, test_name: str, line_count: int) -> dict:
        """å˜ä¸€ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®å®Ÿè¡Œ"""

        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        test_text = self._generate_test_data(line_count)

        results = {
            "test_info": {
                "name": test_name,
                "line_count": line_count,
                "text_length": len(test_text),
                "text_size_mb": len(test_text) / 1024 / 1024,
            },
            "traditional_parser": {},
            "optimized_parser": {},
            "streaming_parser": {},
            "parallel_parser": {},
            "improvement_ratios": {},
        }

        # Traditional Parser ãƒ†ã‚¹ãƒˆ
        try:
            results["traditional_parser"] = self._benchmark_traditional_parser(
                test_text
            )
        except Exception as e:
            self.logger.error(f"Traditional parser test failed: {e}")
            results["traditional_parser"] = {"error": str(e)}

        # Optimized Parser ãƒ†ã‚¹ãƒˆ
        try:
            results["optimized_parser"] = self._benchmark_optimized_parser(test_text)
        except Exception as e:
            self.logger.error(f"Optimized parser test failed: {e}")
            results["optimized_parser"] = {"error": str(e)}

        # Streaming Parser ãƒ†ã‚¹ãƒˆ
        try:
            results["streaming_parser"] = self._benchmark_streaming_parser(test_text)
        except Exception as e:
            self.logger.error(f"Streaming parser test failed: {e}")
            results["streaming_parser"] = {"error": str(e)}

        # æ”¹å–„ç‡è¨ˆç®—
        results["improvement_ratios"] = self._calculate_improvement_ratios(results)

        return results

    def _benchmark_traditional_parser(self, test_text: str) -> dict:
        """å¾“æ¥ãƒ‘ãƒ¼ã‚µãƒ¼ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""

        from ...parser import Parser

        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¸¬å®šé–‹å§‹
        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB

        start_time = time.time()

        # ãƒ‘ãƒ¼ã‚µãƒ¼å®Ÿè¡Œ
        parser = Parser()
        nodes = parser.parse(test_text)

        parse_time = time.time() - start_time
        peak_memory = process.memory_info().rss / 1024 / 1024  # MB
        memory_used = peak_memory - initial_memory

        return {
            "parse_time_seconds": parse_time,
            "memory_used_mb": memory_used,
            "peak_memory_mb": peak_memory,
            "nodes_count": len(nodes),
            "throughput_lines_per_second": (
                len(test_text.split("\n")) / parse_time if parse_time > 0 else 0
            ),
            "errors_count": len(parser.get_errors()),
        }

    def _benchmark_optimized_parser(self, test_text: str) -> dict:
        """æœ€é©åŒ–ãƒ‘ãƒ¼ã‚µãƒ¼ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""

        from ...parser import Parser

        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024

        start_time = time.time()

        # æœ€é©åŒ–ãƒ‘ãƒ¼ã‚µãƒ¼å®Ÿè¡Œ
        parser = Parser()
        nodes = parser.parse_optimized(test_text)

        parse_time = time.time() - start_time
        peak_memory = process.memory_info().rss / 1024 / 1024
        memory_used = peak_memory - initial_memory

        return {
            "parse_time_seconds": parse_time,
            "memory_used_mb": memory_used,
            "peak_memory_mb": peak_memory,
            "nodes_count": len(nodes),
            "throughput_lines_per_second": (
                len(test_text.split("\n")) / parse_time if parse_time > 0 else 0
            ),
            "errors_count": len(parser.get_errors()),
        }

    def _benchmark_streaming_parser(self, test_text: str) -> dict:
        """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ‘ãƒ¼ã‚µãƒ¼ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""

        from ...parser import StreamingParser

        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024

        start_time = time.time()

        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ‘ãƒ¼ã‚µãƒ¼å®Ÿè¡Œ
        parser = StreamingParser()
        nodes = list(parser.parse_streaming_from_text(test_text))

        parse_time = time.time() - start_time
        peak_memory = process.memory_info().rss / 1024 / 1024
        memory_used = peak_memory - initial_memory

        return {
            "parse_time_seconds": parse_time,
            "memory_used_mb": memory_used,
            "peak_memory_mb": peak_memory,
            "nodes_count": len(nodes),
            "throughput_lines_per_second": (
                len(test_text.split("\n")) / parse_time if parse_time > 0 else 0
            ),
            "errors_count": len(parser.get_errors()),
        }

    def _generate_test_data(self, line_count: int) -> str:
        """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ"""

        if line_count in self.test_data_cache:
            return self.test_data_cache[line_count]

        lines = []

        # å¤šæ§˜ãªKumihanè¨˜æ³•ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å«ã‚€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
        patterns = [
            "ã“ã‚Œã¯é€šå¸¸ã®ãƒ‘ãƒ©ã‚°ãƒ©ãƒ•ã§ã™ã€‚",
            "# å¤ªå­— # ã“ã®ãƒ†ã‚­ã‚¹ãƒˆã¯å¤ªå­—ã«ãªã‚Šã¾ã™",
            "# ã‚¤ã‚¿ãƒªãƒƒã‚¯ # ã“ã®ãƒ†ã‚­ã‚¹ãƒˆã¯ã‚¤ã‚¿ãƒªãƒƒã‚¯ã«ãªã‚Šã¾ã™",
            "# è¦‹å‡ºã—1 # å¤§ããªè¦‹å‡ºã—",
            "- ãƒªã‚¹ãƒˆé …ç›®1",
            "- ãƒªã‚¹ãƒˆé …ç›®2",
            "1. é †åºä»˜ããƒªã‚¹ãƒˆ1",
            "2. é †åºä»˜ããƒªã‚¹ãƒˆ2",
            "# æ ç·š # æ ã§å›²ã¾ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ",
            "# ãƒã‚¤ãƒ©ã‚¤ãƒˆ color=yellow # é»„è‰²ã§ãƒã‚¤ãƒ©ã‚¤ãƒˆ",
            "",  # ç©ºè¡Œ
            "è¤‡æ•°è¡Œã«ã‚ãŸã‚‹\\né•·ã„ãƒ†ã‚­ã‚¹ãƒˆã®\\nä¾‹ã§ã™ã€‚",
        ]

        for i in range(line_count):
            pattern = patterns[i % len(patterns)]
            if "é …ç›®" in pattern or "ãƒªã‚¹ãƒˆ" in pattern:
                lines.append(
                    pattern.replace("é …ç›®", f"é …ç›®{i+1}").replace(
                        "ãƒªã‚¹ãƒˆ", f"ãƒªã‚¹ãƒˆ{i+1}"
                    )
                )
            else:
                lines.append(f"{pattern} (è¡Œ {i+1})")

        test_text = "\n".join(lines)
        self.test_data_cache[line_count] = test_text

        return test_text

    def _calculate_improvement_ratios(self, results: dict) -> dict:
        """æ”¹å–„ç‡ã®è¨ˆç®—"""

        improvement = {}

        traditional = results.get("traditional_parser", {})
        optimized = results.get("optimized_parser", {})
        streaming = results.get("streaming_parser", {})

        if traditional.get("parse_time_seconds") and optimized.get(
            "parse_time_seconds"
        ):
            improvement["optimized_vs_traditional_speed"] = (
                traditional["parse_time_seconds"] / optimized["parse_time_seconds"]
            )

        if traditional.get("memory_used_mb") and optimized.get("memory_used_mb"):
            improvement["optimized_vs_traditional_memory"] = (
                traditional["memory_used_mb"] / optimized["memory_used_mb"]
            )

        if traditional.get("parse_time_seconds") and streaming.get(
            "parse_time_seconds"
        ):
            improvement["streaming_vs_traditional_speed"] = (
                traditional["parse_time_seconds"] / streaming["parse_time_seconds"]
            )

        return improvement

    def _assess_performance_goals(self, test_results: dict) -> dict:
        """Issue #727 ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›®æ¨™ã®é”æˆè©•ä¾¡"""

        assessment = {
            "goals": {
                "10k_lines_under_15s": False,
                "memory_reduction_66_percent": False,
                "100k_lines_under_180s": False,
                "10k_lines_under_30s": False,
            },
            "details": {},
        }

        # 10Kè¡Œãƒ•ã‚¡ã‚¤ãƒ«15ç§’ä»¥å†…ç›®æ¨™
        large_test = test_results.get("large", {})
        if large_test:
            optimized_time = large_test.get("optimized_parser", {}).get(
                "parse_time_seconds", float("inf")
            )
            streaming_time = large_test.get("streaming_parser", {}).get(
                "parse_time_seconds", float("inf")
            )

            best_time = min(optimized_time, streaming_time)
            assessment["goals"]["10k_lines_under_15s"] = best_time <= 15.0
            assessment["details"]["10k_best_time"] = best_time

        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡66%å‰Šæ¸›ç›®æ¨™
        if large_test:
            traditional_memory = large_test.get("traditional_parser", {}).get(
                "memory_used_mb", 0
            )
            optimized_memory = large_test.get("optimized_parser", {}).get(
                "memory_used_mb", 0
            )

            if traditional_memory > 0:
                memory_reduction = (
                    (traditional_memory - optimized_memory) / traditional_memory * 100
                )
                assessment["goals"]["memory_reduction_66_percent"] = (
                    memory_reduction >= 66.0
                )
                assessment["details"]["memory_reduction_percent"] = memory_reduction

        return assessment

    def _generate_benchmark_summary(self, benchmark_results: dict) -> dict:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚µãƒãƒªãƒ¼ç”Ÿæˆ"""

        summary = {
            "overall_performance": "unknown",
            "key_achievements": [],
            "areas_for_improvement": [],
            "recommendations": [],
        }

        goals = benchmark_results.get("goal_assessment", {}).get("goals", {})

        achieved_goals = sum(1 for achieved in goals.values() if achieved)
        total_goals = len(goals)

        if achieved_goals >= total_goals * 0.8:
            summary["overall_performance"] = "excellent"
            summary["key_achievements"].append("ã»ã¼å…¨ã¦ã®æ€§èƒ½ç›®æ¨™ã‚’é”æˆ")
        elif achieved_goals >= total_goals * 0.6:
            summary["overall_performance"] = "good"
            summary["key_achievements"].append("ä¸»è¦ãªæ€§èƒ½ç›®æ¨™ã‚’é”æˆ")
        else:
            summary["overall_performance"] = "needs_improvement"
            summary["areas_for_improvement"].append("æ€§èƒ½ç›®æ¨™ã®é”æˆç‡ãŒä½ã„")

        # æ¨å¥¨äº‹é …
        if not goals.get("10k_lines_under_15s"):
            summary["recommendations"].append("å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã®æ›´ãªã‚‹æœ€é©åŒ–ãŒå¿…è¦")

        if not goals.get("memory_reduction_66_percent"):
            summary["recommendations"].append("ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®æ”¹å–„ãŒå¿…è¦")

        return summary

    def generate_benchmark_report(self, results: dict) -> str:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""

        report_lines = [
            "ğŸ”¬ Kumihan-Formatter ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ¬ãƒãƒ¼ãƒˆ",
            "=" * 60,
            f"å®Ÿè¡Œæ—¥æ™‚: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(results['metadata']['timestamp']))}",
            f"ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ : {results['metadata']['platform']}",
            f"CPUã‚³ã‚¢æ•°: {results['metadata']['cpu_count']}",
            "",
            "ğŸ“Š ãƒ†ã‚¹ãƒˆçµæœ:",
        ]

        for test_name, test_data in results["tests"].items():
            info = test_data["test_info"]
            report_lines.extend(
                [
                    f"\nğŸ” {info['name'].upper()} ({info['line_count']:,}è¡Œ, {info['text_size_mb']:.1f}MB):",
                    f"  å¾“æ¥ãƒ‘ãƒ¼ã‚µãƒ¼: {test_data['traditional_parser'].get('parse_time_seconds', 'N/A'):.2f}s, "
                    f"{test_data['traditional_parser'].get('memory_used_mb', 'N/A'):.1f}MB",
                    f"  æœ€é©åŒ–ãƒ‘ãƒ¼ã‚µãƒ¼: {test_data['optimized_parser'].get('parse_time_seconds', 'N/A'):.2f}s, "
                    f"{test_data['optimized_parser'].get('memory_used_mb', 'N/A'):.1f}MB",
                    f"  ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°: {test_data['streaming_parser'].get('parse_time_seconds', 'N/A'):.2f}s, "
                    f"{test_data['streaming_parser'].get('memory_used_mb', 'N/A'):.1f}MB",
                ]
            )

            # æ”¹å–„ç‡
            improvements = test_data.get("improvement_ratios", {})
            if improvements:
                speed_improve = improvements.get("optimized_vs_traditional_speed", 1)
                memory_improve = improvements.get("optimized_vs_traditional_memory", 1)
                report_lines.append(
                    f"  æ”¹å–„ç‡: é€Ÿåº¦ {speed_improve:.1f}x, ãƒ¡ãƒ¢ãƒª {memory_improve:.1f}x"
                )

        # ç›®æ¨™é”æˆçŠ¶æ³
        goals = results.get("goal_assessment", {}).get("goals", {})
        report_lines.extend(
            [
                "",
                "ğŸ¯ ç›®æ¨™é”æˆçŠ¶æ³:",
                f"  10Kè¡Œ15ç§’ä»¥å†…: {'âœ…' if goals.get('10k_lines_under_15s') else 'âŒ'}",
                f"  ãƒ¡ãƒ¢ãƒª66%å‰Šæ¸›: {'âœ…' if goals.get('memory_reduction_66_percent') else 'âŒ'}",
                f"  100Kè¡Œ180ç§’ä»¥å†…: {'âœ…' if goals.get('100k_lines_under_180s') else 'âŒ'}",
            ]
        )

        # ã‚µãƒãƒªãƒ¼
        summary = results.get("summary", {})
        report_lines.extend(
            [
                "",
                f"ğŸ“ˆ ç·åˆè©•ä¾¡: {summary.get('overall_performance', 'unknown').upper()}",
            ]
        )

        if summary.get("key_achievements"):
            report_lines.append("âœ¨ ä¸»ãªæˆæœ:")
            for achievement in summary["key_achievements"]:
                report_lines.append(f"  â€¢ {achievement}")

        if summary.get("recommendations"):
            report_lines.append("ğŸ’¡ æ¨å¥¨æ”¹å–„:")
            for rec in summary["recommendations"]:
                report_lines.append(f"  â€¢ {rec}")

        return "\n".join(report_lines)


# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ç”¨ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿
def monitor_performance(task_name: str = "å‡¦ç†"):
    """
    ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼

    Args:
        task_name: ã‚¿ã‚¹ã‚¯å

    Returns:
        PerformanceContext: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
    """
    return PerformanceContext(task_name)


class PerformanceContext:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼"""

    def __init__(self, task_name: str):
        self.task_name = task_name
        self.monitor = PerformanceMonitor()
        self.start_time = None

    def __enter__(self):
        self.start_time = time.time()
        self.monitor.start_monitoring(total_items=1000, initial_stage=self.task_name)
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.monitor.stop_monitoring()

    def record_item_processed(self):
        """ã‚¢ã‚¤ãƒ†ãƒ å‡¦ç†ã®è¨˜éŒ²"""
        if hasattr(self.monitor, "update_progress"):
            # ç°¡æ˜“çš„ãªé€²æ—æ›´æ–°
            pass
