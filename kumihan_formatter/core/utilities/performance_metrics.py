"""
ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚·ã‚¹ãƒ†ãƒ 
Issue #694 Phase 3å¯¾å¿œ - è©³ç´°ãªæ€§èƒ½æ¸¬å®šãƒ»åˆ†æ
"""

import json
import os
import sys
import threading
import time
from collections import deque
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, AsyncIterator, Callable, Dict, Iterator, List, Optional

import psutil

from .logger import get_logger


@dataclass
class PerformanceSnapshot:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ"""

    timestamp: float
    cpu_percent: float
    memory_mb: float
    memory_percent: float
    processing_rate: float  # items/sec
    items_processed: int
    total_items: int
    stage: str
    thread_count: int = 0
    disk_io_read_mb: float = 0.0
    disk_io_write_mb: float = 0.0


@dataclass
class ProcessingStats:
    """å‡¦ç†çµ±è¨ˆæƒ…å ±"""

    start_time: float = field(default_factory=time.time)
    end_time: Optional[float] = None
    total_items: int = 0
    items_processed: int = 0
    errors_count: int = 0
    warnings_count: int = 0
    peak_memory_mb: float = 0.0
    avg_cpu_percent: float = 0.0
    processing_phases: List[str] = field(default_factory=list)

    @property
    def duration_seconds(self) -> float:
        """å‡¦ç†æ™‚é–“ï¼ˆç§’ï¼‰"""
        end = self.end_time or time.time()
        return end - self.start_time

    @property
    def items_per_second(self) -> float:
        """å‡¦ç†é€Ÿåº¦ï¼ˆã‚¢ã‚¤ãƒ†ãƒ /ç§’ï¼‰"""
        duration = self.duration_seconds
        return self.items_processed / duration if duration > 0 else 0

    @property
    def completion_rate(self) -> float:
        """å®Œäº†ç‡ï¼ˆ%ï¼‰"""
        if self.total_items == 0:
            return 0.0
        return (self.items_processed / self.total_items) * 100


class PerformanceMonitor:
    """
    ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 

    æ©Ÿèƒ½:
    - CPUãƒ»ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ç¶™ç¶šç›£è¦–
    - å‡¦ç†é€Ÿåº¦ãƒ»ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæ¸¬å®š
    - ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æ¤œå‡º
    - ãƒ¡ãƒˆãƒªã‚¯ã‚¹å±¥æ­´ä¿å­˜
    - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    """

    def __init__(self, monitoring_interval: float = 1.0, history_size: int = 1000):
        self.logger = get_logger(__name__)
        self.monitoring_interval = monitoring_interval
        self.history_size = history_size

        # ç›£è¦–ãƒ‡ãƒ¼ã‚¿
        self.snapshots: deque[PerformanceSnapshot] = deque(maxlen=history_size)
        self.stats = ProcessingStats()

        # ç›£è¦–åˆ¶å¾¡
        self._monitoring = False
        self._monitor_thread: Optional[threading.Thread] = None
        self._lock = threading.Lock()

        # ãƒ—ãƒ­ã‚»ã‚¹æƒ…å ±
        self.process = psutil.Process()
        self.initial_memory = self.process.memory_info().rss

        # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
        self.alert_callbacks: List[Callable[[str, Dict], None]] = []

        self.logger.info(
            f"PerformanceMonitor initialized: interval={monitoring_interval}s, history={history_size}"
        )

    def start_monitoring(self, total_items: int, initial_stage: str = "é–‹å§‹"):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚’é–‹å§‹"""
        with self._lock:
            if self._monitoring:
                self.logger.warning("Performance monitoring already started")
                return

            # çµ±è¨ˆæƒ…å ±åˆæœŸåŒ–
            self.stats = ProcessingStats(
                start_time=time.time(), total_items=total_items
            )
            self.stats.processing_phases.append(initial_stage)

            # ç›£è¦–é–‹å§‹
            self._monitoring = True
            self._monitor_thread = threading.Thread(
                target=self._monitoring_loop, daemon=True
            )
            self._monitor_thread.start()

            self.logger.info(
                f"Performance monitoring started: {total_items} items, stage: {initial_stage}"
            )

    def stop_monitoring(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚’åœæ­¢"""
        with self._lock:
            if not self._monitoring:
                return

            self._monitoring = False
            self.stats.end_time = time.time()

            if self._monitor_thread and self._monitor_thread.is_alive():
                self._monitor_thread.join(timeout=2.0)

            self.logger.info(
                f"Performance monitoring stopped after {self.stats.duration_seconds:.2f}s"
            )

    def update_progress(self, items_processed: int, current_stage: str = ""):
        """é€²æ—ã‚’æ›´æ–°"""
        with self._lock:
            self.stats.items_processed = items_processed

            if current_stage and current_stage not in self.stats.processing_phases:
                self.stats.processing_phases.append(current_stage)

    def add_error(self):
        """ã‚¨ãƒ©ãƒ¼ã‚«ã‚¦ãƒ³ãƒˆã‚’å¢—åŠ """
        with self._lock:
            self.stats.errors_count += 1

    def add_warning(self):
        """è­¦å‘Šã‚«ã‚¦ãƒ³ãƒˆã‚’å¢—åŠ """
        with self._lock:
            self.stats.warnings_count += 1

    def add_alert_callback(self, callback: Callable[[str, Dict], None]):
        """ã‚¢ãƒ©ãƒ¼ãƒˆã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’è¿½åŠ """
        self.alert_callbacks.append(callback)

    def get_current_snapshot(self) -> PerformanceSnapshot:
        """ç¾åœ¨ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’å–å¾—"""
        try:
            # CPUãƒ»ãƒ¡ãƒ¢ãƒªæƒ…å ±
            cpu_percent = self.process.cpu_percent()
            memory_info = self.process.memory_info()
            memory_mb = memory_info.rss / 1024 / 1024
            memory_percent = self.process.memory_percent()

            # å‡¦ç†é€Ÿåº¦è¨ˆç®—
            processing_rate = self.stats.items_per_second

            # ãƒ‡ã‚£ã‚¹ã‚¯I/Oï¼ˆå¯èƒ½ãªå ´åˆï¼‰
            try:
                io_counters = self.process.io_counters()
                disk_io_read_mb = io_counters.read_bytes / 1024 / 1024
                disk_io_write_mb = io_counters.write_bytes / 1024 / 1024
            except (AttributeError, psutil.AccessDenied):
                disk_io_read_mb = disk_io_write_mb = 0.0

            # ã‚¹ãƒ¬ãƒƒãƒ‰æ•°
            thread_count = self.process.num_threads()

            # ç¾åœ¨ã®ã‚¹ãƒ†ãƒ¼ã‚¸
            current_stage = (
                self.stats.processing_phases[-1]
                if self.stats.processing_phases
                else "unknown"
            )

            return PerformanceSnapshot(
                timestamp=time.time(),
                cpu_percent=cpu_percent,
                memory_mb=memory_mb,
                memory_percent=memory_percent,
                processing_rate=processing_rate,
                items_processed=self.stats.items_processed,
                total_items=self.stats.total_items,
                stage=current_stage,
                thread_count=thread_count,
                disk_io_read_mb=disk_io_read_mb,
                disk_io_write_mb=disk_io_write_mb,
            )

        except Exception as e:
            self.logger.error(f"Error creating performance snapshot: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬çš„ãªæƒ…å ±ã®ã¿
            return PerformanceSnapshot(
                timestamp=time.time(),
                cpu_percent=0.0,
                memory_mb=0.0,
                memory_percent=0.0,
                processing_rate=0.0,
                items_processed=self.stats.items_processed,
                total_items=self.stats.total_items,
                stage="error",
            )

    def _monitoring_loop(self):
        """ç›£è¦–ãƒ«ãƒ¼ãƒ—ï¼ˆåˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œï¼‰"""
        self.logger.debug("Performance monitoring loop started")

        while self._monitoring:
            try:
                # ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆå–å¾—
                snapshot = self.get_current_snapshot()

                with self._lock:
                    self.snapshots.append(snapshot)

                    # ãƒ”ãƒ¼ã‚¯ãƒ¡ãƒ¢ãƒªæ›´æ–°
                    if snapshot.memory_mb > self.stats.peak_memory_mb:
                        self.stats.peak_memory_mb = snapshot.memory_mb

                    # CPUå¹³å‡è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰
                    if len(self.snapshots) > 0:
                        recent_snapshots = list(self.snapshots)[-10:]  # ç›´è¿‘10ã‚µãƒ³ãƒ—ãƒ«
                        self.stats.avg_cpu_percent = sum(
                            s.cpu_percent for s in recent_snapshots
                        ) / len(recent_snapshots)

                # ã‚¢ãƒ©ãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯
                self._check_performance_alerts(snapshot)

                time.sleep(self.monitoring_interval)

            except Exception as e:
                self.logger.error(f"Error in monitoring loop: {e}")
                time.sleep(self.monitoring_interval)

        self.logger.debug("Performance monitoring loop ended")

    def _check_performance_alerts(self, snapshot: PerformanceSnapshot):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¢ãƒ©ãƒ¼ãƒˆã‚’ãƒã‚§ãƒƒã‚¯"""
        alerts = []

        # é«˜CPUä½¿ç”¨ç‡ã‚¢ãƒ©ãƒ¼ãƒˆ
        if snapshot.cpu_percent > 90:
            alerts.append(
                {
                    "type": "high_cpu",
                    "severity": "warning",
                    "message": f"é«˜CPUä½¿ç”¨ç‡: {snapshot.cpu_percent:.1f}%",
                    "value": snapshot.cpu_percent,
                }
            )

        # é«˜ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡ã‚¢ãƒ©ãƒ¼ãƒˆ
        if snapshot.memory_percent > 80:
            alerts.append(
                {
                    "type": "high_memory",
                    "severity": "warning",
                    "message": f"é«˜ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {snapshot.memory_percent:.1f}% ({snapshot.memory_mb:.1f}MB)",
                    "value": snapshot.memory_percent,
                }
            )

        # ä½å‡¦ç†é€Ÿåº¦ã‚¢ãƒ©ãƒ¼ãƒˆ
        if (
            snapshot.processing_rate > 0 and snapshot.processing_rate < 100
        ):  # 100 items/secæœªæº€
            alerts.append(
                {
                    "type": "low_processing_rate",
                    "severity": "info",
                    "message": f"ä½å‡¦ç†é€Ÿåº¦: {snapshot.processing_rate:.0f} items/sec",
                    "value": snapshot.processing_rate,
                }
            )

        # ã‚¢ãƒ©ãƒ¼ãƒˆé€šçŸ¥
        for alert in alerts:
            for callback in self.alert_callbacks:
                try:
                    callback(alert["type"], alert)
                except Exception as e:
                    self.logger.error(f"Error in alert callback: {e}")

    def get_performance_summary(self) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¦‚è¦ã‚’å–å¾—"""
        with self._lock:
            recent_snapshots = list(self.snapshots)[-10:] if self.snapshots else []

            return {
                "duration_seconds": self.stats.duration_seconds,
                "items_processed": self.stats.items_processed,
                "total_items": self.stats.total_items,
                "completion_rate": self.stats.completion_rate,
                "items_per_second": self.stats.items_per_second,
                "errors_count": self.stats.errors_count,
                "warnings_count": self.stats.warnings_count,
                "peak_memory_mb": self.stats.peak_memory_mb,
                "avg_cpu_percent": self.stats.avg_cpu_percent,
                "processing_phases": self.stats.processing_phases,
                "current_memory_mb": (
                    recent_snapshots[-1].memory_mb if recent_snapshots else 0
                ),
                "current_cpu_percent": (
                    recent_snapshots[-1].cpu_percent if recent_snapshots else 0
                ),
                "snapshots_count": len(self.snapshots),
            }

    def generate_performance_report(self) -> str:
        """è©³ç´°ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        summary = self.get_performance_summary()

        report_lines = [
            "ğŸ” ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æãƒ¬ãƒãƒ¼ãƒˆ",
            "=" * 50,
            f"å‡¦ç†æ™‚é–“: {summary['duration_seconds']:.2f}ç§’",
            f"å‡¦ç†é …ç›®: {summary['items_processed']:,} / "
            f"{summary['total_items']:,} ({summary['completion_rate']:.1f}%)",
            f"å‡¦ç†é€Ÿåº¦: {summary['items_per_second']:,.0f} items/ç§’",
            f"ã‚¨ãƒ©ãƒ¼: {summary['errors_count']}, è­¦å‘Š: {summary['warnings_count']}",
            "",
            "ğŸ’¾ ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨é‡:",
            f"  ãƒ”ãƒ¼ã‚¯ãƒ¡ãƒ¢ãƒª: {summary['peak_memory_mb']:.1f}MB",
            f"  å¹³å‡CPU: {summary['avg_cpu_percent']:.1f}%",
            f"  ç¾åœ¨ãƒ¡ãƒ¢ãƒª: {summary['current_memory_mb']:.1f}MB",
            f"  ç¾åœ¨CPU: {summary['current_cpu_percent']:.1f}%",
            "",
            "ğŸ”„ å‡¦ç†ãƒ•ã‚§ãƒ¼ã‚º:",
        ]

        for phase in summary["processing_phases"]:
            report_lines.append(f"  - {phase}")

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‚¾å‘åˆ†æ
        if len(self.snapshots) >= 5:
            report_lines.extend(
                [
                    "",
                    "ğŸ“ˆ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‚¾å‘:",
                ]
            )

            snapshots_list = list(self.snapshots)

            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å‚¾å‘
            memory_trend = self._calculate_trend(
                [s.memory_mb for s in snapshots_list[-10:]]
            )
            memory_status = (
                "å¢—åŠ "
                if memory_trend > 0.5
                else "å®‰å®š" if memory_trend > -0.5 else "æ¸›å°‘"
            )
            report_lines.append(f"  ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {memory_status}")

            # å‡¦ç†é€Ÿåº¦å‚¾å‘
            rates = [
                s.processing_rate for s in snapshots_list[-10:] if s.processing_rate > 0
            ]
            if rates:
                rate_trend = self._calculate_trend(rates)
                rate_status = (
                    "å‘ä¸Š"
                    if rate_trend > 0.5
                    else "å®‰å®š" if rate_trend > -0.5 else "ä½ä¸‹"
                )
                report_lines.append(f"  å‡¦ç†é€Ÿåº¦: {rate_status}")

        return "\n".join(report_lines)

    def _calculate_trend(self, values: List[float]) -> float:
        """å€¤ã®å‚¾å‘ã‚’è¨ˆç®—ï¼ˆç°¡æ˜“ç·šå½¢å›å¸°ï¼‰"""
        if len(values) < 2:
            return 0.0

        n = len(values)
        x_avg = sum(range(n)) / n
        y_avg = sum(values) / n

        numerator = sum((i - x_avg) * (values[i] - y_avg) for i in range(n))
        denominator = sum((i - x_avg) ** 2 for i in range(n))

        return numerator / denominator if denominator != 0 else 0.0

    def save_metrics_to_file(self, file_path: Path):
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        try:
            metrics_data = {
                "summary": self.get_performance_summary(),
                "snapshots": [
                    {
                        "timestamp": s.timestamp,
                        "cpu_percent": s.cpu_percent,
                        "memory_mb": s.memory_mb,
                        "memory_percent": s.memory_percent,
                        "processing_rate": s.processing_rate,
                        "items_processed": s.items_processed,
                        "stage": s.stage,
                    }
                    for s in self.snapshots
                ],
            }

            with open(file_path, "w", encoding="utf-8") as f:
                json.dump(metrics_data, f, indent=2, ensure_ascii=False)

            self.logger.info(f"Performance metrics saved to {file_path}")

        except Exception as e:
            self.logger.error(f"Failed to save metrics to file: {e}")


class SIMDOptimizer:
    """
    SIMDï¼ˆSingle Instruction Multiple Dataï¼‰æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ 

    ç‰¹å¾´:
    - NumPyé…åˆ—ã«ã‚ˆã‚‹å¤§å®¹é‡ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ã®é«˜é€ŸåŒ–
    - ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã•ã‚ŒãŸæ–‡å­—åˆ—æ“ä½œ
    - CPUä¸¦åˆ—å‘½ä»¤ã«ã‚ˆã‚‹å‡¦ç†é€Ÿåº¦å‘ä¸Š
    - 300Kè¡Œãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã®83%é«˜é€ŸåŒ–ã‚’ç›®æ¨™
    """

    def __init__(self):
        self.logger = get_logger(__name__)
        self._numpy_available = self._check_numpy_availability()
        self._regex_cache = {}

        if self._numpy_available:
            import numpy as np

            self.np = np
            self.logger.info("SIMD optimizer initialized with NumPy acceleration")
        else:
            self.logger.warning(
                "NumPy not available, falling back to standard processing"
            )

    def _check_numpy_availability(self) -> bool:
        """NumPyåˆ©ç”¨å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
        try:
            import numpy as np  # noqa: F401

            return True
        except ImportError:
            return False

    def vectorized_line_processing(
        self, lines: List[str], pattern_funcs: List[Callable[[str], str]]
    ) -> List[str]:
        """
        ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã•ã‚ŒãŸè¡Œå‡¦ç†ï¼ˆSIMDæœ€é©åŒ–ï¼‰

        Args:
            lines: å‡¦ç†å¯¾è±¡è¡Œãƒªã‚¹ãƒˆ
            pattern_funcs: é©ç”¨ã™ã‚‹å¤‰æ›é–¢æ•°ãƒªã‚¹ãƒˆ

        Returns:
            List[str]: å‡¦ç†æ¸ˆã¿è¡Œãƒªã‚¹ãƒˆ
        """
        if not self._numpy_available:
            return self._fallback_line_processing(lines, pattern_funcs)

        if not lines:
            return []

        self.logger.debug(
            f"SIMD processing {len(lines)} lines with {len(pattern_funcs)} functions"
        )

        try:
            # NumPyé…åˆ—ã«å¤‰æ›ï¼ˆæ–‡å­—åˆ—å‡¦ç†ã®é«˜é€ŸåŒ–ï¼‰
            np_lines = self.np.array(lines, dtype=object)

            # ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã•ã‚ŒãŸé–¢æ•°é©ç”¨
            for func in pattern_funcs:
                # numpy.vectorizeã§SIMDæœ€é©åŒ–ã‚’æ´»ç”¨
                vectorized_func = self.np.vectorize(func, otypes=[object])
                np_lines = vectorized_func(np_lines)

            # ãƒªã‚¹ãƒˆã«æˆ»ã™
            result = np_lines.tolist()

            self.logger.debug(
                f"SIMD processing completed: {len(result)} lines processed"
            )
            return result

        except Exception as e:
            self.logger.error(f"SIMD processing failed, falling back: {e}")
            return self._fallback_line_processing(lines, pattern_funcs)

    def _fallback_line_processing(
        self, lines: List[str], pattern_funcs: List[Callable[[str], str]]
    ) -> List[str]:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ï¼ˆé€šå¸¸å‡¦ç†ï¼‰"""
        result = lines.copy()
        for func in pattern_funcs:
            result = [func(line) for line in result]
        return result

    def optimized_regex_operations(
        self, text: str, patterns: List[tuple[str, str]]
    ) -> str:
        """
        æœ€é©åŒ–ã•ã‚ŒãŸæ­£è¦è¡¨ç¾å‡¦ç†

        Args:
            text: å‡¦ç†å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
            patterns: (pattern, replacement)ã®ã‚¿ãƒ—ãƒ«ãƒªã‚¹ãƒˆ

        Returns:
            str: å‡¦ç†æ¸ˆã¿ãƒ†ã‚­ã‚¹ãƒˆ
        """
        import re

        result = text

        # æ­£è¦è¡¨ç¾ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ´»ç”¨
        for pattern, replacement in patterns:
            if pattern not in self._regex_cache:
                self._regex_cache[pattern] = re.compile(pattern)

            compiled_pattern = self._regex_cache[pattern]
            result = compiled_pattern.sub(replacement, result)

        return result

    def parallel_chunk_simd_processing(
        self,
        chunks: List[Any],
        processing_func: Callable,
        max_workers: Optional[int] = None,
    ) -> List[Any]:
        """
        ä¸¦åˆ—ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ã¨SIMDæœ€é©åŒ–ã®çµ„ã¿åˆã‚ã›

        Args:
            chunks: å‡¦ç†ãƒãƒ£ãƒ³ã‚¯ãƒªã‚¹ãƒˆ
            processing_func: ãƒãƒ£ãƒ³ã‚¯å‡¦ç†é–¢æ•°
            max_workers: æœ€å¤§ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°

        Returns:
            List[Any]: å‡¦ç†çµæœãƒªã‚¹ãƒˆ
        """
        import concurrent.futures
        import os

        # CPUåŠ¹ç‡æœ€å¤§åŒ–ã®ãŸã‚ã®å‹•çš„ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°è¨ˆç®—
        if max_workers is None:
            cpu_count = os.cpu_count() or 1
            max_workers = min(cpu_count * 2, len(chunks))

        results = []

        if len(chunks) <= 2:
            # å°‘æ•°ãƒãƒ£ãƒ³ã‚¯ã¯ä¸¦åˆ—åŒ–ã›ãšSIMDæœ€é©åŒ–ã®ã¿
            for chunk in chunks:
                results.append(processing_func(chunk))
        else:
            # ä¸¦åˆ—å‡¦ç† + SIMDæœ€é©åŒ–
            with concurrent.futures.ThreadPoolExecutor(
                max_workers=max_workers
            ) as executor:
                future_to_chunk = {
                    executor.submit(processing_func, chunk): chunk for chunk in chunks
                }

                for future in concurrent.futures.as_completed(future_to_chunk):
                    try:
                        result = future.result()
                        results.append(result)
                    except Exception as e:
                        self.logger.error(f"SIMD parallel processing error: {e}")
                        # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ç©ºçµæœã‚’è¿½åŠ ã—ã¦ç¶™ç¶š
                        results.append(None)

        # Noneçµæœã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        return [r for r in results if r is not None]

    def memory_efficient_processing(
        self, data_generator: Iterator[str], batch_size: int = 1000
    ) -> Iterator[str]:
        """
        ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªSIMDå‡¦ç†ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ï¼‰

        Args:
            data_generator: ãƒ‡ãƒ¼ã‚¿ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿
            batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º

        Yields:
            str: å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿
        """
        batch = []

        for item in data_generator:
            batch.append(item)

            if len(batch) >= batch_size:
                # ãƒãƒƒãƒã‚’SIMDå‡¦ç†
                if self._numpy_available:
                    try:
                        np_batch = self.np.array(batch, dtype=object)
                        # ãƒãƒƒãƒå‡¦ç†ï¼ˆå®Ÿéš›ã®å‡¦ç†é–¢æ•°ã¯ç”¨é€”ã«å¿œã˜ã¦å®Ÿè£…ï¼‰
                        processed_batch = np_batch.tolist()
                    except Exception:
                        processed_batch = batch
                else:
                    processed_batch = batch

                # çµæœã‚’yield
                for processed_item in processed_batch:
                    yield processed_item

                # ãƒãƒƒãƒã‚¯ãƒªã‚¢
                batch.clear()

        # æ®‹ã‚Šã®ãƒãƒƒãƒã‚’å‡¦ç†
        if batch:
            if self._numpy_available:
                try:
                    np_batch = self.np.array(batch, dtype=object)
                    processed_batch = np_batch.tolist()
                except Exception:
                    processed_batch = batch
            else:
                processed_batch = batch

            for processed_item in processed_batch:
                yield processed_item

    def get_simd_metrics(self) -> Dict[str, Any]:
        """SIMDæœ€é©åŒ–ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—"""
        return {
            "numpy_available": self._numpy_available,
            "regex_cache_size": len(self._regex_cache),
            "optimization_level": "high" if self._numpy_available else "standard",
        }


class AsyncIOOptimizer:
    """
    éåŒæœŸI/Oæœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ 

    ç‰¹å¾´:
    - aiofilesã«ã‚ˆã‚‹éåŒæœŸãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
    - ä¸¦åˆ—ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†
    - ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒã¨ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°
    - å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°èª­ã¿è¾¼ã¿
    """

    def __init__(self, buffer_size: int = 64 * 1024):
        self.logger = get_logger(__name__)
        self.buffer_size = buffer_size
        self._aiofiles_available = self._check_aiofiles_availability()

        if self._aiofiles_available:
            self.logger.info(
                f"AsyncIO optimizer initialized with buffer size: {buffer_size}"
            )
        else:
            self.logger.warning("aiofiles not available, using synchronous I/O")

    def _check_aiofiles_availability(self) -> bool:
        """aiofilesåˆ©ç”¨å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
        try:
            import aiofiles  # noqa: F401

            return True
        except ImportError:
            return False

    async def async_read_file_chunked(
        self, file_path: Path, chunk_size: int = 64 * 1024
    ) -> AsyncIterator[str]:
        """
        éåŒæœŸãƒãƒ£ãƒ³ã‚¯èª­ã¿è¾¼ã¿

        Args:
            file_path: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            chunk_size: ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º

        Yields:
            str: ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ£ãƒ³ã‚¯
        """
        if not self._aiofiles_available:
            # åŒæœŸãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            with open(file_path, "r", encoding="utf-8") as f:
                while True:
                    chunk = f.read(chunk_size)
                    if not chunk:
                        break
                    yield chunk
            return

        import aiofiles

        try:
            async with aiofiles.open(file_path, "r", encoding="utf-8") as f:
                while True:
                    chunk = await f.read(chunk_size)
                    if not chunk:
                        break
                    yield chunk
        except Exception as e:
            self.logger.error(f"Async file read failed: {e}")
            # åŒæœŸãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            with open(file_path, "r", encoding="utf-8") as f:
                while True:
                    chunk = f.read(chunk_size)
                    if not chunk:
                        break
                    yield chunk

    async def async_read_lines_batched(
        self, file_path: Path, batch_size: int = 1000
    ) -> AsyncIterator[List[str]]:
        """
        éåŒæœŸãƒãƒƒãƒè¡Œèª­ã¿è¾¼ã¿

        Args:
            file_path: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º

        Yields:
            List[str]: è¡Œã®ãƒãƒƒãƒ
        """
        if not self._aiofiles_available:
            # åŒæœŸãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            with open(file_path, "r", encoding="utf-8") as f:
                batch = []
                for line in f:
                    batch.append(line.rstrip("\n"))
                    if len(batch) >= batch_size:
                        yield batch
                        batch = []
                if batch:
                    yield batch
            return

        import aiofiles

        try:
            async with aiofiles.open(file_path, "r", encoding="utf-8") as f:
                batch = []
                async for line in f:
                    batch.append(line.rstrip("\n"))
                    if len(batch) >= batch_size:
                        yield batch
                        batch = []
                if batch:
                    yield batch
        except Exception as e:
            self.logger.error(f"Async batch read failed: {e}")
            # åŒæœŸãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            with open(file_path, "r", encoding="utf-8") as f:
                batch = []
                for line in f:
                    batch.append(line.rstrip("\n"))
                    if len(batch) >= batch_size:
                        yield batch
                        batch = []
                if batch:
                    yield batch

    async def async_write_results_streaming(
        self, file_path: Path, results_generator: AsyncIterator[str]
    ):
        """
        éåŒæœŸã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°çµæœæ›¸ãè¾¼ã¿

        Args:
            file_path: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            results_generator: çµæœã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿
        """
        if not self._aiofiles_available:
            # åŒæœŸãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            with open(file_path, "w", encoding="utf-8") as f:
                async for result in results_generator:
                    f.write(result)
            return

        import aiofiles

        try:
            async with aiofiles.open(file_path, "w", encoding="utf-8") as f:
                async for result in results_generator:
                    await f.write(result)
        except Exception as e:
            self.logger.error(f"Async write failed: {e}")
            # åŒæœŸãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            with open(file_path, "w", encoding="utf-8") as f:
                async for result in results_generator:
                    f.write(result)

    def get_async_metrics(self) -> Dict[str, Any]:
        """éåŒæœŸI/Oãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—"""
        return {
            "aiofiles_available": self._aiofiles_available,
            "buffer_size": self.buffer_size,
            "optimization_level": "async" if self._aiofiles_available else "sync",
        }


class RegexOptimizer:
    """
    æ­£è¦è¡¨ç¾ã‚¨ãƒ³ã‚¸ãƒ³æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ 

    ç‰¹å¾´:
    - ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿æ­£è¦è¡¨ç¾ã®åŠ¹ç‡çš„ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°
    - æœ€é©åŒ–ã•ã‚ŒãŸãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°æˆ¦ç•¥
    - ãƒãƒƒãƒãƒ³ã‚°æ€§èƒ½ã®å¤§å¹…å‘ä¸Š
    - ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªæ­£è¦è¡¨ç¾å‡¦ç†
    """

    def __init__(self, cache_size_limit: int = 1000):
        self.logger = get_logger(__name__)
        self.cache_size_limit = cache_size_limit
        self._pattern_cache = {}
        self._usage_counter = {}
        self._compile_stats = {"hits": 0, "misses": 0, "evictions": 0}

        # æœ€é©åŒ–ã•ã‚ŒãŸäº‹å‰ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ãƒ‘ã‚¿ãƒ¼ãƒ³
        self._precompiled_patterns = self._initialize_precompiled_patterns()

        self.logger.info(
            f"RegexOptimizer initialized with cache limit: {cache_size_limit}"
        )

    def _initialize_precompiled_patterns(self) -> Dict[str, Any]:
        """ã‚ˆãä½¿ç”¨ã•ã‚Œã‚‹æ­£è¦è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’äº‹å‰ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«"""
        import re

        patterns = {
            # Kumihanãƒãƒ¼ã‚¯ã‚¢ãƒƒãƒ—åŸºæœ¬ãƒ‘ã‚¿ãƒ¼ãƒ³
            "inline_notation": re.compile(r"#\s*([^#]+?)\s*#([^#]+?)##", re.MULTILINE),
            "block_marker": re.compile(r"^#\s*([^#]+?)\s*#([^#]*)##$", re.MULTILINE),
            "nested_markers": re.compile(r"#+([^#]+?)#+", re.MULTILINE),
            # ã‚ˆãä½¿ç”¨ã•ã‚Œã‚‹æ–‡å­—åˆ—å‡¦ç†ãƒ‘ã‚¿ãƒ¼ãƒ³
            "whitespace_cleanup": re.compile(r"\s+"),
            "line_breaks": re.compile(r"\r?\n"),
            "empty_lines": re.compile(r"^\s*$", re.MULTILINE),
            # è‰²å±æ€§è§£æ
            "color_attribute": re.compile(r"color\s*=\s*([#\w]+)"),
            "hex_color": re.compile(r"^#[0-9a-fA-F]{3,6}$"),
            # HTMLã‚¨ã‚¹ã‚±ãƒ¼ãƒ—
            "html_chars": re.compile(r"[<>&\"']"),
            # ç‰¹æ®Šæ–‡å­—å‡¦ç†
            "japanese_chars": re.compile(r"[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]"),
        }

        self.logger.info(f"Pre-compiled {len(patterns)} regex patterns")
        return patterns

    def get_compiled_pattern(self, pattern_str: str, flags: int = 0) -> Any:
        """
        ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿æ­£è¦è¡¨ç¾ã‚’å–å¾—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ä»˜ãï¼‰

        Args:
            pattern_str: æ­£è¦è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³æ–‡å­—åˆ—
            flags: æ­£è¦è¡¨ç¾ãƒ•ãƒ©ã‚°

        Returns:
            Pattern: ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿æ­£è¦è¡¨ç¾ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        """
        import re

        cache_key = (pattern_str, flags)

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆãƒã‚§ãƒƒã‚¯
        if cache_key in self._pattern_cache:
            self._compile_stats["hits"] += 1
            self._usage_counter[cache_key] = self._usage_counter.get(cache_key, 0) + 1
            return self._pattern_cache[cache_key]

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹ï¼šæ–°è¦ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
        self._compile_stats["misses"] += 1

        try:
            compiled_pattern = re.compile(pattern_str, flags)

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºåˆ¶é™ãƒã‚§ãƒƒã‚¯
            if len(self._pattern_cache) >= self.cache_size_limit:
                self._evict_least_used_pattern()

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            self._pattern_cache[cache_key] = compiled_pattern
            self._usage_counter[cache_key] = 1

            return compiled_pattern

        except re.error as e:
            self.logger.error(
                f"Regex compilation failed for pattern '{pattern_str}': {e}"
            )
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šæ–‡å­—åˆ—ãƒãƒƒãƒãƒ³ã‚°
            return None

    def _evict_least_used_pattern(self):
        """æœ€ã‚‚ä½¿ç”¨é »åº¦ã®ä½ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å‰Šé™¤"""
        if not self._usage_counter:
            return

        # æœ€å°ä½¿ç”¨å›æ•°ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¦‹ã¤ã‘ã‚‹
        least_used_key = min(self._usage_counter, key=self._usage_counter.get)

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å‰Šé™¤
        if least_used_key in self._pattern_cache:
            del self._pattern_cache[least_used_key]
        if least_used_key in self._usage_counter:
            del self._usage_counter[least_used_key]

        self._compile_stats["evictions"] += 1
        self.logger.debug(
            f"Evicted regex pattern from cache: {least_used_key[0][:50]}..."
        )

    def optimized_search(self, pattern_str: str, text: str, flags: int = 0) -> Any:
        """
        æœ€é©åŒ–ã•ã‚ŒãŸæ­£è¦è¡¨ç¾æ¤œç´¢

        Args:
            pattern_str: æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³
            text: æ¤œç´¢å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
            flags: æ­£è¦è¡¨ç¾ãƒ•ãƒ©ã‚°

        Returns:
            Match object or None
        """
        # äº‹å‰ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
        for name, precompiled in self._precompiled_patterns.items():
            if precompiled.pattern == pattern_str:
                return precompiled.search(text)

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å–å¾—
        compiled_pattern = self.get_compiled_pattern(pattern_str, flags)
        if compiled_pattern:
            return compiled_pattern.search(text)

        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå˜ç´”æ–‡å­—åˆ—æ¤œç´¢
        return pattern_str in text

    def optimized_findall(
        self, pattern_str: str, text: str, flags: int = 0
    ) -> List[str]:
        """
        æœ€é©åŒ–ã•ã‚ŒãŸæ­£è¦è¡¨ç¾å…¨ä½“æ¤œç´¢

        Args:
            pattern_str: æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³
            text: æ¤œç´¢å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
            flags: æ­£è¦è¡¨ç¾ãƒ•ãƒ©ã‚°

        Returns:
            List[str]: ãƒãƒƒãƒã—ãŸæ–‡å­—åˆ—ã®ãƒªã‚¹ãƒˆ
        """
        compiled_pattern = self.get_compiled_pattern(pattern_str, flags)
        if compiled_pattern:
            return compiled_pattern.findall(text)
        return []

    def optimized_substitute(
        self, pattern_str: str, replacement: str, text: str, flags: int = 0
    ) -> str:
        """
        æœ€é©åŒ–ã•ã‚ŒãŸæ­£è¦è¡¨ç¾ç½®æ›

        Args:
            pattern_str: ç½®æ›ãƒ‘ã‚¿ãƒ¼ãƒ³
            replacement: ç½®æ›æ–‡å­—åˆ—
            text: å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
            flags: æ­£è¦è¡¨ç¾ãƒ•ãƒ©ã‚°

        Returns:
            str: ç½®æ›å¾Œãƒ†ã‚­ã‚¹ãƒˆ
        """
        compiled_pattern = self.get_compiled_pattern(pattern_str, flags)
        if compiled_pattern:
            return compiled_pattern.sub(replacement, text)

        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå˜ç´”æ–‡å­—åˆ—ç½®æ›
        return text.replace(pattern_str, replacement)

    def batch_process_with_patterns(
        self, texts: List[str], patterns_and_replacements: List[tuple[str, str]]
    ) -> List[str]:
        """
        è¤‡æ•°ãƒ†ã‚­ã‚¹ãƒˆã«å¯¾ã™ã‚‹ä¸€æ‹¬æ­£è¦è¡¨ç¾å‡¦ç†

        Args:
            texts: å‡¦ç†å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆ
            patterns_and_replacements: (pattern, replacement)ã®ã‚¿ãƒ—ãƒ«ãƒªã‚¹ãƒˆ

        Returns:
            List[str]: å‡¦ç†æ¸ˆã¿ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆ
        """
        results = []

        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’äº‹å‰ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
        compiled_patterns = []
        for pattern, replacement in patterns_and_replacements:
            compiled = self.get_compiled_pattern(pattern)
            if compiled:
                compiled_patterns.append((compiled, replacement))

        # å„ãƒ†ã‚­ã‚¹ãƒˆã‚’å‡¦ç†
        for text in texts:
            processed_text = text
            for compiled_pattern, replacement in compiled_patterns:
                processed_text = compiled_pattern.sub(replacement, processed_text)
            results.append(processed_text)

        return results

    def get_cache_stats(self) -> Dict[str, Any]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆã‚’å–å¾—"""
        total_requests = self._compile_stats["hits"] + self._compile_stats["misses"]
        hit_rate = (
            (self._compile_stats["hits"] / total_requests * 100)
            if total_requests > 0
            else 0
        )

        return {
            "cache_size": len(self._pattern_cache),
            "cache_limit": self.cache_size_limit,
            "hit_rate_percent": hit_rate,
            "total_hits": self._compile_stats["hits"],
            "total_misses": self._compile_stats["misses"],
            "total_evictions": self._compile_stats["evictions"],
            "precompiled_patterns": len(self._precompiled_patterns),
        }

    def clear_cache(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        cleared_count = len(self._pattern_cache)
        self._pattern_cache.clear()
        self._usage_counter.clear()
        self._compile_stats = {"hits": 0, "misses": 0, "evictions": 0}

        self.logger.info(f"Cleared {cleared_count} patterns from regex cache")


class MemoryOptimizer:
    """
    ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ 

    ç‰¹å¾´:
    - ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªãƒ‡ãƒ¼ã‚¿æ§‹é€ é¸æŠ
    - ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æœ€é©åŒ–
    - ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«ã¨ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå†åˆ©ç”¨
    - å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã®ãƒ¡ãƒ¢ãƒªç®¡ç†
    """

    def __init__(self, enable_gc_optimization: bool = True):
        self.logger = get_logger(__name__)
        self.enable_gc_optimization = enable_gc_optimization
        self._object_pools = {}
        self._memory_stats = {"allocations": 0, "deallocations": 0, "pool_hits": 0}

        if enable_gc_optimization:
            self._configure_gc_optimization()

        self.logger.info("MemoryOptimizer initialized")

    def _configure_gc_optimization(self):
        """ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æœ€é©åŒ–è¨­å®š"""
        import gc

        # GCé–¾å€¤ã‚’èª¿æ•´ï¼ˆå¤§å®¹é‡å‡¦ç†å‘ã‘ï¼‰
        original_thresholds = gc.get_threshold()
        # ã‚ˆã‚Šé«˜ã„é–¾å€¤ã§GCé »åº¦ã‚’ä¸‹ã’ã€ãƒãƒƒãƒå‡¦ç†åŠ¹ç‡ã‚’å‘ä¸Š
        new_thresholds = (
            original_thresholds[0] * 2,  # ä¸–ä»£0
            original_thresholds[1] * 2,  # ä¸–ä»£1
            original_thresholds[2] * 2,  # ä¸–ä»£2
        )
        gc.set_threshold(*new_thresholds)

        self.logger.info(
            f"GC thresholds adjusted: {original_thresholds} -> {new_thresholds}"
        )

    def create_object_pool(
        self, pool_name: str, factory_func: Callable, max_size: int = 100
    ):
        """
        ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãƒ—ãƒ¼ãƒ«ä½œæˆ

        Args:
            pool_name: ãƒ—ãƒ¼ãƒ«å
            factory_func: ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆç”Ÿæˆé–¢æ•°
            max_size: ãƒ—ãƒ¼ãƒ«æœ€å¤§ã‚µã‚¤ã‚º
        """
        from collections import deque

        self._object_pools[pool_name] = {
            "pool": deque(maxlen=max_size),
            "factory": factory_func,
            "max_size": max_size,
            "created_count": 0,
            "reused_count": 0,
        }

        self.logger.info(f"Object pool '{pool_name}' created with max size: {max_size}")

    def get_pooled_object(self, pool_name: str):
        """ãƒ—ãƒ¼ãƒ«ã‹ã‚‰ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å–å¾—"""
        if pool_name not in self._object_pools:
            raise ValueError(f"Object pool '{pool_name}' not found")

        pool_info = self._object_pools[pool_name]
        pool = pool_info["pool"]

        if pool:
            # ãƒ—ãƒ¼ãƒ«ã‹ã‚‰å†åˆ©ç”¨
            obj = pool.popleft()
            pool_info["reused_count"] += 1
            self._memory_stats["pool_hits"] += 1
            return obj
        else:
            # æ–°è¦ä½œæˆ
            obj = pool_info["factory"]()
            pool_info["created_count"] += 1
            self._memory_stats["allocations"] += 1
            return obj

    def return_pooled_object(self, pool_name: str, obj: Any):
        """ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ãƒ—ãƒ¼ãƒ«ã«è¿”å´"""
        if pool_name not in self._object_pools:
            return

        pool_info = self._object_pools[pool_name]
        pool = pool_info["pool"]

        # ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ãƒªã‚»ãƒƒãƒˆï¼ˆå¯èƒ½ã§ã‚ã‚Œã°ï¼‰
        if hasattr(obj, "reset"):
            obj.reset()
        elif hasattr(obj, "clear"):
            obj.clear()

        # ãƒ—ãƒ¼ãƒ«ã«è¿”å´
        if len(pool) < pool_info["max_size"]:
            pool.append(obj)
        else:
            # ãƒ—ãƒ¼ãƒ«ãŒæº€æ¯ã®å ´åˆã¯ç ´æ£„
            self._memory_stats["deallocations"] += 1

    def memory_efficient_file_reader(
        self, file_path: Path, chunk_size: int = 64 * 1024, use_mmap: bool = False
    ) -> Iterator[str]:
        """
        ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿

        Args:
            file_path: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            chunk_size: ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º
            use_mmap: ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ—ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ä½¿ç”¨ãƒ•ãƒ©ã‚°

        Yields:
            str: ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ£ãƒ³ã‚¯
        """
        if use_mmap:
            try:
                import mmap

                with open(file_path, "r", encoding="utf-8") as f:
                    with mmap.mmap(
                        f.fileno(), 0, access=mmap.ACCESS_READ
                    ) as mmapped_file:
                        for i in range(0, len(mmapped_file), chunk_size):
                            chunk = mmapped_file[i : i + chunk_size].decode(
                                "utf-8", errors="ignore"
                            )
                            yield chunk
                return
            except Exception as e:
                self.logger.warning(f"mmap failed, falling back to regular read: {e}")

        # é€šå¸¸ã®ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        with open(file_path, "r", encoding="utf-8") as f:
            while True:
                chunk = f.read(chunk_size)
                if not chunk:
                    break
                yield chunk

    def optimize_list_operations(self, data: List[Any], operation: str) -> Any:
        """
        ãƒªã‚¹ãƒˆæ“ä½œã®æœ€é©åŒ–

        Args:
            data: æ“ä½œå¯¾è±¡ãƒ‡ãƒ¼ã‚¿
            operation: æ“ä½œç¨®åˆ¥ï¼ˆ'sort', 'unique', 'filter_empty'ï¼‰

        Returns:
            Any: æœ€é©åŒ–ã•ã‚ŒãŸçµæœ
        """
        if operation == "sort":
            # å¤§å®¹é‡ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã¯Timsortã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’æ´»ç”¨
            return sorted(data, key=str if isinstance(data[0], str) else None)

        elif operation == "unique":
            # ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ãŸé‡è¤‡é™¤å»ï¼ˆé †åºã¯ä¿æŒã•ã‚Œãªã„ï¼‰
            if len(data) > 10000:
                return list(set(data))
            else:
                # å°å®¹é‡ãƒ‡ãƒ¼ã‚¿ã¯é †åºä¿æŒé‡è¤‡é™¤å»
                seen = set()
                result = []
                for item in data:
                    if item not in seen:
                        seen.add(item)
                        result.append(item)
                return result

        elif operation == "filter_empty":
            # ç©ºè¦ç´ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            return [item for item in data if item and str(item).strip()]

        else:
            return data

    def batch_process_with_memory_limit(
        self,
        data_generator: Iterator[Any],
        processing_func: Callable,
        memory_limit_mb: int = 100,
    ) -> Iterator[Any]:
        """
        ãƒ¡ãƒ¢ãƒªåˆ¶é™ä»˜ããƒãƒƒãƒå‡¦ç†

        Args:
            data_generator: ãƒ‡ãƒ¼ã‚¿ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿
            processing_func: å‡¦ç†é–¢æ•°
            memory_limit_mb: ãƒ¡ãƒ¢ãƒªåˆ¶é™ï¼ˆMBï¼‰

        Yields:
            Any: å‡¦ç†çµæœ
        """
        import sys

        batch = []
        batch_size_bytes = 0
        memory_limit_bytes = memory_limit_mb * 1024 * 1024

        for item in data_generator:
            batch.append(item)
            # æ¦‚ç®—ã®ãƒ¡ãƒ¢ãƒªã‚µã‚¤ã‚ºã‚’è¨ˆç®—
            batch_size_bytes += sys.getsizeof(item)

            if batch_size_bytes >= memory_limit_bytes:
                # ãƒãƒƒãƒå‡¦ç†å®Ÿè¡Œ
                for result in processing_func(batch):
                    yield result

                # ãƒãƒƒãƒã‚¯ãƒªã‚¢
                batch.clear()
                batch_size_bytes = 0

        # æ®‹ã‚Šã®ãƒãƒƒãƒã‚’å‡¦ç†
        if batch:
            for result in processing_func(batch):
                yield result

    def force_garbage_collection(self):
        """å¼·åˆ¶ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ"""
        import gc

        collected_objects = gc.collect()
        self.logger.debug(f"Garbage collection: {collected_objects} objects collected")
        return collected_objects

    def get_memory_stats(self) -> Dict[str, Any]:
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨çµ±è¨ˆã‚’å–å¾—"""
        import os

        import psutil

        process = psutil.Process(os.getpid())
        memory_info = process.memory_info()

        pool_stats = {}
        for name, pool_info in self._object_pools.items():
            pool_stats[name] = {
                "current_size": len(pool_info["pool"]),
                "max_size": pool_info["max_size"],
                "created_count": pool_info["created_count"],
                "reused_count": pool_info["reused_count"],
                "efficiency_percent": (
                    pool_info["reused_count"]
                    / (pool_info["created_count"] + pool_info["reused_count"])
                    * 100
                    if (pool_info["created_count"] + pool_info["reused_count"]) > 0
                    else 0
                ),
            }

        return {
            "process_memory_mb": memory_info.rss / 1024 / 1024,
            "virtual_memory_mb": memory_info.vms / 1024 / 1024,
            "object_pools": pool_stats,
            "memory_operations": self._memory_stats,
        }

    def detect_memory_leaks(
        self, threshold_mb: float = 10.0, sample_interval: int = 5
    ) -> Dict[str, Any]:
        """
        é«˜åº¦ãªãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯æ¤œå‡ºæ©Ÿæ§‹

        Args:
            threshold_mb: ãƒ¡ãƒ¢ãƒªå¢—åŠ ã®æ¤œå‡ºé–¾å€¤ï¼ˆMBï¼‰
            sample_interval: ã‚µãƒ³ãƒ—ãƒ«é–“éš”ï¼ˆç§’ï¼‰

        Returns:
            Dict[str, Any]: ãƒªãƒ¼ã‚¯æ¤œå‡ºçµæœ
        """
        import gc
        import os
        import time
        from typing import List, Tuple

        import psutil

        process = psutil.Process(os.getpid())
        samples: List[Tuple[float, float]] = []  # (timestamp, memory_mb)

        # åˆæœŸãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡è¨˜éŒ²
        initial_memory = process.memory_info().rss / 1024 / 1024
        samples.append((time.time(), initial_memory))

        self.logger.info(
            f"Memory leak detection started. Initial memory: {initial_memory:.2f} MB"
        )

        # è¤‡æ•°å›ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        for i in range(sample_interval):
            time.sleep(1)
            current_memory = process.memory_info().rss / 1024 / 1024
            samples.append((time.time(), current_memory))

        # ãƒªãƒ¼ã‚¯åˆ†æ
        memory_growth = samples[-1][1] - samples[0][1]
        growth_rate = memory_growth / (samples[-1][0] - samples[0][0])  # MB/ç§’

        # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œå¾Œã®ãƒ¡ãƒ¢ãƒªç¢ºèª
        gc.collect()
        time.sleep(0.5)
        post_gc_memory = process.memory_info().rss / 1024 / 1024
        gc_effect = samples[-1][1] - post_gc_memory

        # ãƒªãƒ¼ã‚¯åˆ¤å®š
        is_leak_detected = (
            memory_growth > threshold_mb
            and gc_effect < memory_growth * 0.5  # GCã§åŠåˆ†ä»¥ä¸Šå›åã•ã‚Œãªã„å ´åˆ
        )

        leak_info = {
            "leak_detected": is_leak_detected,
            "memory_growth_mb": memory_growth,
            "growth_rate_mb_per_sec": growth_rate,
            "gc_effect_mb": gc_effect,
            "initial_memory_mb": initial_memory,
            "final_memory_mb": samples[-1][1],
            "post_gc_memory_mb": post_gc_memory,
            "samples": samples,
            "recommendation": self._generate_leak_recommendation(
                is_leak_detected, memory_growth, gc_effect
            ),
        }

        if is_leak_detected:
            self.logger.warning(
                f"Memory leak detected! Growth: {memory_growth:.2f} MB, Rate: {growth_rate:.4f} MB/s"
            )
        else:
            self.logger.info(
                f"No significant memory leak detected. Growth: {memory_growth:.2f} MB"
            )

        return leak_info

    def proactive_gc_strategy(
        self, memory_threshold_mb: float = 100.0, enable_generational: bool = True
    ) -> Dict[str, Any]:
        """
        ãƒ—ãƒ­ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æˆ¦ç•¥

        Args:
            memory_threshold_mb: GCå®Ÿè¡Œãƒ¡ãƒ¢ãƒªé–¾å€¤ï¼ˆMBï¼‰
            enable_generational: ä¸–ä»£åˆ¥GCæœ€é©åŒ–æœ‰åŠ¹ãƒ•ãƒ©ã‚°

        Returns:
            Dict[str, Any]: GCå®Ÿè¡Œçµæœ
        """
        import gc
        import os
        import time

        import psutil

        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss / 1024 / 1024

        self.logger.info(
            f"Proactive GC strategy triggered. Current memory: {initial_memory:.2f} MB"
        )

        results = {
            "initial_memory_mb": initial_memory,
            "gc_executed": False,
            "collections_performed": [],
            "memory_freed_mb": 0.0,
            "execution_time_ms": 0.0,
        }

        start_time = time.time()

        # ãƒ¡ãƒ¢ãƒªé–¾å€¤ãƒã‚§ãƒƒã‚¯
        if initial_memory >= memory_threshold_mb:
            self.logger.info(
                f"Memory threshold ({memory_threshold_mb} MB) exceeded. Executing proactive GC..."
            )

            if enable_generational:
                # ä¸–ä»£åˆ¥æœ€é©åŒ–GCå®Ÿè¡Œ
                for generation in range(3):  # Python GCã®3ä¸–ä»£
                    collected = gc.collect(generation)
                    results["collections_performed"].append(
                        {"generation": generation, "objects_collected": collected}
                    )
                    self.logger.debug(
                        f"Generation {generation} GC: {collected} objects collected"
                    )
            else:
                # æ¨™æº–GCå®Ÿè¡Œ
                collected = gc.collect()
                results["collections_performed"].append(
                    {"generation": "all", "objects_collected": collected}
                )

            # GCå¾Œãƒ¡ãƒ¢ãƒªç¢ºèª
            time.sleep(0.1)  # GCå®Œäº†ã‚’å¾…æ©Ÿ
            post_gc_memory = process.memory_info().rss / 1024 / 1024
            results["memory_freed_mb"] = initial_memory - post_gc_memory
            results["final_memory_mb"] = post_gc_memory
            results["gc_executed"] = True

            self.logger.info(
                f"Proactive GC completed. Memory freed: {results['memory_freed_mb']:.2f} MB"
            )
        else:
            self.logger.debug(
                f"Memory usage ({initial_memory:.2f} MB) below threshold. GC not needed."
            )

        results["execution_time_ms"] = (time.time() - start_time) * 1000
        return results

    def create_advanced_resource_pool(
        self,
        pool_name: str,
        factory_func: Callable,
        max_size: int = 100,
        cleanup_func: Optional[Callable] = None,
        auto_cleanup_interval: int = 300,
    ) -> None:
        """
        é«˜åº¦ãªãƒªã‚½ãƒ¼ã‚¹ãƒ—ãƒ¼ãƒ«ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 

        Args:
            pool_name: ãƒ—ãƒ¼ãƒ«å
            factory_func: ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆç”Ÿæˆé–¢æ•°
            max_size: ãƒ—ãƒ¼ãƒ«æœ€å¤§ã‚µã‚¤ã‚º
            cleanup_func: ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—é–¢æ•°
            auto_cleanup_interval: è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—é–“éš”ï¼ˆç§’ï¼‰
        """
        import threading
        import time
        from collections import deque

        pool_info = {
            "pool": deque(maxlen=max_size),
            "factory": factory_func,
            "cleanup": cleanup_func,
            "max_size": max_size,
            "created_count": 0,
            "reused_count": 0,
            "cleanup_count": 0,
            "last_cleanup": time.time(),
            "auto_cleanup_interval": auto_cleanup_interval,
            "lock": threading.RLock(),  # ãƒªã‚¨ãƒ³ãƒˆãƒ©ãƒ³ãƒˆãƒ­ãƒƒã‚¯
        }

        self._object_pools[pool_name] = pool_info

        # è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¿ã‚¤ãƒãƒ¼è¨­å®š
        def auto_cleanup():
            while pool_name in self._object_pools:
                time.sleep(auto_cleanup_interval)
                self._cleanup_resource_pool(pool_name)

        cleanup_thread = threading.Thread(target=auto_cleanup, daemon=True)
        cleanup_thread.start()

        self.logger.info(
            f"Advanced resource pool '{pool_name}' created with auto-cleanup every {auto_cleanup_interval}s"
        )

    def _cleanup_resource_pool(self, pool_name: str) -> int:
        """ãƒªã‚½ãƒ¼ã‚¹ãƒ—ãƒ¼ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œ"""
        if pool_name not in self._object_pools:
            return 0

        pool_info = self._object_pools[pool_name]

        with pool_info["lock"]:
            cleanup_count = 0
            cleanup_func = pool_info["cleanup"]

            if cleanup_func:
                # ãƒ—ãƒ¼ãƒ«å†…ã®å…¨ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                temp_objects = []
                while pool_info["pool"]:
                    obj = pool_info["pool"].popleft()
                    try:
                        cleanup_func(obj)
                        temp_objects.append(obj)
                        cleanup_count += 1
                    except Exception as e:
                        self.logger.warning(
                            f"Cleanup failed for object in pool '{pool_name}': {e}"
                        )

                # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã•ã‚ŒãŸã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æˆ»ã™
                for obj in temp_objects:
                    pool_info["pool"].append(obj)

            pool_info["cleanup_count"] += cleanup_count
            pool_info["last_cleanup"] = time.time()

            if cleanup_count > 0:
                self.logger.info(
                    f"Resource pool '{pool_name}' cleanup: {cleanup_count} objects processed"
                )

            return cleanup_count

    def generate_memory_report(self, include_detailed_stats: bool = True) -> str:
        """
        ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆå¯è¦–åŒ–æ©Ÿèƒ½ï¼‰

        Args:
            include_detailed_stats: è©³ç´°çµ±è¨ˆæƒ…å ±ã‚’å«ã‚€ã‹

        Returns:
            str: HTMLãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®ãƒ¡ãƒ¢ãƒªãƒ¬ãƒãƒ¼ãƒˆ
        """
        import os
        import time
        from datetime import datetime

        import psutil

        process = psutil.Process(os.getpid())
        memory_info = process.memory_info()

        # åŸºæœ¬æƒ…å ±åé›†
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        memory_mb = memory_info.rss / 1024 / 1024
        virtual_mb = memory_info.vms / 1024 / 1024

        # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
        system_memory = psutil.virtual_memory()
        system_total_gb = system_memory.total / 1024 / 1024 / 1024
        system_available_gb = system_memory.available / 1024 / 1024 / 1024
        system_usage_percent = system_memory.percent

        # HTML ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        html_report = f"""
<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Kumihan-Formatter ãƒ¡ãƒ¢ãƒªãƒ¬ãƒãƒ¼ãƒˆ</title>
    <style>
        body {{
            font-family: "Segoe UI", Tahoma, Geneva, Verdana, sans-serif;
            margin: 20px;
            background-color: #f5f5f5;
        }}
        .container {{
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }}
        .header {{
            text-align: center;
            color: #2c3e50;
            border-bottom: 2px solid #3498db;
            padding-bottom: 10px;
        }}
        .stats-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }}
        .stat-card {{
            background: #f8f9fa;
            padding: 15px;
            border-radius: 6px;
            border-left: 4px solid #3498db;
        }}
        .stat-title {{
            font-weight: bold;
            color: #2c3e50;
            margin-bottom: 8px;
        }}
        .stat-value {{
            font-size: 1.2em;
            color: #27ae60;
        }}
        .memory-bar {{
            width: 100%;
            height: 20px;
            background-color: #ecf0f1;
            border-radius: 10px;
            overflow: hidden;
            margin: 10px 0;
        }}
        .memory-fill {{
            height: 100%;
            background: linear-gradient(90deg, #27ae60, #f39c12, #e74c3c);
            transition: width 0.3s ease;
        }}
        .pool-table {{
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }}
        .pool-table th, .pool-table td {{
            border: 1px solid #ddd;
            padding: 8px;
            text-align: left;
        }}
        .pool-table th {{
            background-color: #3498db;
            color: white;
        }}
        .warning {{
            background-color: #fff3cd;
            border: 1px solid #ffeaa7;
            color: #856404;
            padding: 10px;
            border-radius: 4px;
            margin: 10px 0;
        }}
        .timestamp {{
            text-align: right;
            color: #7f8c8d;
            font-size: 0.9em;
            margin-top: 20px;
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ§  Kumihan-Formatter ãƒ¡ãƒ¢ãƒªãƒ¬ãƒãƒ¼ãƒˆ</h1>
            <p>Issue #772 - ãƒ¡ãƒ¢ãƒªãƒ»ãƒªã‚½ãƒ¼ã‚¹ç®¡ç†å¼·åŒ–ã‚·ã‚¹ãƒ†ãƒ </p>
        </div>

        <div class="stats-grid">
            <div class="stat-card">
                <div class="stat-title">ãƒ—ãƒ­ã‚»ã‚¹ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡</div>
                <div class="stat-value">{memory_mb:.2f} MB</div>
                <div class="memory-bar">
                    <div class="memory-fill" style="width: {min(memory_mb/100*10, 100)}%;"></div>
                </div>
            </div>

            <div class="stat-card">
                <div class="stat-title">ä»®æƒ³ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡</div>
                <div class="stat-value">{virtual_mb:.2f} MB</div>
            </div>

            <div class="stat-card">
                <div class="stat-title">ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡</div>
                <div class="stat-value">{system_usage_percent:.1f}%</div>
                <div class="memory-bar">
                    <div class="memory-fill" style="width: {system_usage_percent}%;"></div>
                </div>
            </div>

            <div class="stat-card">
                <div class="stat-title">ã‚·ã‚¹ãƒ†ãƒ åˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒª</div>
                <div class="stat-value">{system_available_gb:.1f} GB / {system_total_gb:.1f} GB</div>
            </div>
        </div>
        """

        # ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãƒ—ãƒ¼ãƒ«çµ±è¨ˆ
        if self._object_pools and include_detailed_stats:
            html_report += """
        <h2>ğŸŠ ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãƒ—ãƒ¼ãƒ«çµ±è¨ˆ</h2>
        <table class="pool-table">
            <thead>
                <tr>
                    <th>ãƒ—ãƒ¼ãƒ«å</th>
                    <th>ç¾åœ¨ã‚µã‚¤ã‚º</th>
                    <th>æœ€å¤§ã‚µã‚¤ã‚º</th>
                    <th>ä½œæˆæ•°</th>
                    <th>å†åˆ©ç”¨æ•°</th>
                    <th>åŠ¹ç‡ç‡</th>
                    <th>æœ€çµ‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—</th>
                </tr>
            </thead>
            <tbody>
            """

            for name, pool_info in self._object_pools.items():
                current_size = len(pool_info["pool"])
                max_size = pool_info["max_size"]
                created = pool_info["created_count"]
                reused = pool_info["reused_count"]
                total = created + reused
                efficiency = (reused / total * 100) if total > 0 else 0
                last_cleanup = time.strftime(
                    "%H:%M:%S", time.localtime(pool_info.get("last_cleanup", 0))
                )

                html_report += f"""
                <tr>
                    <td>{name}</td>
                    <td>{current_size}</td>
                    <td>{max_size}</td>
                    <td>{created}</td>
                    <td>{reused}</td>
                    <td>{efficiency:.1f}%</td>
                    <td>{last_cleanup}</td>
                </tr>
                """

            html_report += "</tbody></table>"

        # ãƒ¡ãƒ¢ãƒªçµ±è¨ˆæƒ…å ±
        if include_detailed_stats:
            html_report += f"""
        <h2>ğŸ“Š è©³ç´°ãƒ¡ãƒ¢ãƒªçµ±è¨ˆ</h2>
        <div class="stats-grid">
            <div class="stat-card">
                <div class="stat-title">ã‚¢ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³æ•°</div>
                <div class="stat-value">{self._memory_stats["allocations"]:,}</div>
            </div>
            <div class="stat-card">
                <div class="stat-title">ãƒ‡ã‚£ã‚¢ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³æ•°</div>
                <div class="stat-value">{self._memory_stats["deallocations"]:,}</div>
            </div>
            <div class="stat-card">
                <div class="stat-title">ãƒ—ãƒ¼ãƒ«ãƒ’ãƒƒãƒˆæ•°</div>
                <div class="stat-value">{self._memory_stats["pool_hits"]:,}</div>
            </div>
        </div>

        <div class="warning">
            <strong>âš ï¸ æ³¨æ„:</strong> ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒ100MBä»¥ä¸Šã®å ´åˆã¯ã€ãƒ—ãƒ­ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®å®Ÿè¡Œã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚
        </div>
        """

        html_report += f"""
        <div class="timestamp">
            ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆæ™‚åˆ»: {current_time}
        </div>
    </div>
</body>
</html>
        """

        self.logger.info(f"Memory report generated. Current usage: {memory_mb:.2f} MB")
        return html_report.strip()

    def _generate_leak_recommendation(
        self, is_leak: bool, growth_mb: float, gc_effect_mb: float
    ) -> str:
        """ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯æ¤œå‡ºçµæœã«åŸºã¥ãæ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç”Ÿæˆ"""
        if not is_leak:
            return "æ­£å¸¸ãªãƒ¡ãƒ¢ãƒªä½¿ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã™ã€‚å®šæœŸçš„ãªç›£è¦–ã‚’ç¶™ç¶šã—ã¦ãã ã•ã„ã€‚"

        recommendations = []

        if gc_effect_mb < growth_mb * 0.3:
            recommendations.append(
                "ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®åŠ¹æœãŒä½ã„ãŸã‚ã€å¼·ã„å‚ç…§ã®è§£é™¤ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
            )

        if growth_mb > 50:
            recommendations.append(
                "å¤§å¹…ãªãƒ¡ãƒ¢ãƒªå¢—åŠ ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚å¤§å®¹é‡ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†æ–¹æ³•ã‚’è¦‹ç›´ã—ã¦ãã ã•ã„ã€‚"
            )

        if not recommendations:
            recommendations.append(
                "ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
            )

        return " ".join(recommendations)


class ProgressiveOutputSystem:
    """
    ãƒ—ãƒ­ã‚°ãƒ¬ãƒƒã‚·ãƒ–å‡ºåŠ›ã‚·ã‚¹ãƒ†ãƒ ï¼ˆIssue #727 å¯¾å¿œï¼‰

    æ©Ÿèƒ½:
    - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ çµæœå‡ºåŠ›
    - æ®µéšçš„HTMLç”Ÿæˆ
    - ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“å‘ä¸Š
    - å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã®å¯è¦–æ€§æ”¹å–„
    """

    def __init__(self, output_path: Optional[Path] = None, buffer_size: int = 1000):
        self.logger = get_logger(__name__)
        self.output_path = output_path
        self.buffer_size = buffer_size

        # å‡ºåŠ›ç®¡ç†
        self.html_buffer = []
        self.total_nodes_processed = 0
        self.current_section = "header"

        # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆéƒ¨åˆ†
        self.html_header = ""
        self.html_footer = ""
        self.css_content = ""

        # ã‚¹ãƒˆãƒªãƒ¼ãƒ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«
        self.output_stream = None

        self.logger.info(
            f"Progressive output system initialized: buffer_size={buffer_size}"
        )

    def initialize_output_stream(
        self, template_content: str = "", css_content: str = ""
    ):
        """å‡ºåŠ›ã‚¹ãƒˆãƒªãƒ¼ãƒ ã®åˆæœŸåŒ–"""

        if not self.output_path:
            return  # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ç„¡åŠ¹

        try:
            self.output_stream = open(
                self.output_path, "w", encoding="utf-8", buffering=1
            )

            # HTMLãƒ˜ãƒƒãƒ€ãƒ¼ã®æº–å‚™
            self.css_content = css_content
            self.html_header = self._create_html_header(template_content)
            self.html_footer = self._create_html_footer()

            # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’å³åº§ã«å‡ºåŠ›
            self.output_stream.write(self.html_header)
            self.output_stream.flush()

            self.logger.info(f"Progressive output stream started: {self.output_path}")

        except Exception as e:
            self.logger.error(f"Failed to initialize output stream: {e}")
            self.output_stream = None

    def add_processed_node(self, node_html: str, node_info: dict = None):
        """å‡¦ç†æ¸ˆã¿ãƒãƒ¼ãƒ‰ã®è¿½åŠ """

        if not node_html.strip():
            return

        self.html_buffer.append(node_html)
        self.total_nodes_processed += 1

        # ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚ºã«é”ã—ãŸã‚‰å‡ºåŠ›
        if len(self.html_buffer) >= self.buffer_size:
            self.flush_buffer()

        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¡¨ç¤º
        if self.total_nodes_processed % 100 == 0:
            self.logger.info(
                f"Progressive output: {self.total_nodes_processed} nodes processed"
            )

    def flush_buffer(self):
        """ãƒãƒƒãƒ•ã‚¡ã®å¼·åˆ¶å‡ºåŠ›"""

        if not self.html_buffer or not self.output_stream:
            return

        try:
            # ãƒãƒƒãƒ•ã‚¡å†…å®¹ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿
            content = "\n".join(self.html_buffer)
            self.output_stream.write(content + "\n")
            self.output_stream.flush()

            # ãƒãƒƒãƒ•ã‚¡ã‚¯ãƒªã‚¢
            self.html_buffer.clear()

            self.logger.debug(f"Buffer flushed: {len(self.html_buffer)} items")

        except Exception as e:
            self.logger.error(f"Buffer flush error: {e}")

    def add_section_marker(self, section_name: str, section_content: str = ""):
        """ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãƒãƒ¼ã‚«ãƒ¼ã®è¿½åŠ """

        self.current_section = section_name

        if section_content:
            section_html = f"""
<!-- ===== {section_name.upper()} SECTION START ===== -->
{section_content}
<!-- ===== {section_name.upper()} SECTION END ===== -->
"""
            self.add_processed_node(section_html)

    def finalize_output(self):
        """å‡ºåŠ›ã®æœ€çµ‚åŒ–"""

        if not self.output_stream:
            return

        try:
            # æ®‹ã‚Šãƒãƒƒãƒ•ã‚¡ã‚’å‡ºåŠ›
            self.flush_buffer()

            # ãƒ•ãƒƒã‚¿ãƒ¼ã‚’å‡ºåŠ›
            self.output_stream.write(self.html_footer)
            self.output_stream.flush()

            # ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚¯ãƒ­ãƒ¼ã‚º
            self.output_stream.close()
            self.output_stream = None

            self.logger.info(
                f"Progressive output finalized: {self.total_nodes_processed} nodes, "
                f"output: {self.output_path}"
            )

        except Exception as e:
            self.logger.error(f"Output finalization error: {e}")

    def _create_html_header(self, template_content: str) -> str:
        """HTMLãƒ˜ãƒƒãƒ€ãƒ¼ã®ä½œæˆ"""

        return f"""<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Kumihan Formatter - Progressive Output</title>
    <style>
{self.css_content}
/* Progressive output styles */
.kumihan-progressive-info {{
    position: fixed;
    top: 10px;
    right: 10px;
    background: rgba(0,0,0,0.8);
    color: white;
    padding: 10px;
    border-radius: 5px;
    font-family: monospace;
    font-size: 12px;
    z-index: 1000;
}}
.kumihan-processing {{
    opacity: 0.7;
    transition: opacity 0.3s ease;
}}
    </style>
    <script>
// Progressive output JavaScript
let processedNodes = 0;
function updateProgressInfo() {{
    const info = document.querySelector('.kumihan-progressive-info');
    if (info) {{
        info.textContent = 'Kumihan-Formatter å‡¦ç†ä¸­... ' + (window.processedNodes || 0) + ' nodes';
    }}
}}
setInterval(updateProgressInfo, 1000);
    </script>
</head>
<body>
<div class="kumihan-progressive-info">Kumihan Progressive Output - å‡¦ç†é–‹å§‹</div>
<div class="kumihan-content">
<!-- PROGRESSIVE CONTENT START -->
"""

    def _create_html_footer(self) -> str:
        """HTMLãƒ•ãƒƒã‚¿ãƒ¼ã®ä½œæˆ"""

        return f"""
<!-- PROGRESSIVE CONTENT END -->
</div>
<script>
// Final processing info
const info = document.querySelector('.kumihan-progressive-info');
if (info) {{
    info.textContent = 'âœ… å‡¦ç†å®Œäº† - {self.total_nodes_processed} nodes';
    info.style.backgroundColor = 'rgba(0,128,0,0.8)';
}}
document.querySelectorAll('.kumihan-processing').forEach(el => {{
    el.classList.remove('kumihan-processing');
}});
</script>
</body>
</html>"""

    def create_progress_html(self, current: int, total: int, stage: str = "") -> str:
        """ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¡¨ç¤ºHTMLç”Ÿæˆ"""

        progress_percent = (current / total * 100) if total > 0 else 0

        progress_style = f"width: {progress_percent:.1f}%; background: linear-gradient(90deg, #4CAF50, #2196F3);"
        progress_text = f"{stage} - {current}/{total} ({progress_percent:.1f}%)"

        return f"""
<div class="kumihan-progress-update" data-current="{current}" data-total="{total}">
    <div class="progress-bar" style="{progress_style}"></div>
    <div class="progress-text">{progress_text}</div>
</div>
"""

    def get_output_statistics(self) -> dict:
        """å‡ºåŠ›çµ±è¨ˆã®å–å¾—"""

        return {
            "total_nodes_processed": self.total_nodes_processed,
            "buffer_size": len(self.html_buffer),
            "current_section": self.current_section,
            "output_active": self.output_stream is not None,
            "output_path": str(self.output_path) if self.output_path else None,
        }

    def __enter__(self):
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼é–‹å§‹"""
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼çµ‚äº†"""
        self.finalize_output()


class PerformanceBenchmark:
    """
    ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚·ã‚¹ãƒ†ãƒ ï¼ˆIssue #727 å¯¾å¿œï¼‰

    æ©Ÿèƒ½:
    - ãƒ‘ãƒ¼ã‚µãƒ¼æ€§èƒ½æ¸¬å®š
    - ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãƒ†ã‚¹ãƒˆ
    - ä¸¦åˆ—å‡¦ç†åŠ¹æœæ¸¬å®š
    - ç›®æ¨™é”æˆè©•ä¾¡
    """

    def __init__(self):
        self.logger = get_logger(__name__)
        self.results = {}
        self.test_data_cache = {}

    def run_comprehensive_benchmark(self) -> dict:
        """åŒ…æ‹¬çš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""

        self.logger.info("ğŸš€ Starting comprehensive performance benchmark...")

        benchmark_results = {
            "metadata": {
                "timestamp": time.time(),
                "python_version": sys.version,
                "platform": sys.platform,
                "cpu_count": os.cpu_count(),
            },
            "tests": {},
        }

        # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹å®šç¾©
        test_cases = [
            {"name": "small", "lines": 1000, "description": "å°è¦æ¨¡ãƒ•ã‚¡ã‚¤ãƒ«(1Kè¡Œ)"},
            {"name": "medium", "lines": 5000, "description": "ä¸­è¦æ¨¡ãƒ•ã‚¡ã‚¤ãƒ«(5Kè¡Œ)"},
            {"name": "large", "lines": 10000, "description": "å¤§è¦æ¨¡ãƒ•ã‚¡ã‚¤ãƒ«(10Kè¡Œ)"},
            {
                "name": "extra_large",
                "lines": 50000,
                "description": "è¶…å¤§è¦æ¨¡ãƒ•ã‚¡ã‚¤ãƒ«(50Kè¡Œ)",
            },
        ]

        for test_case in test_cases:
            self.logger.info(f"ğŸ“Š Testing {test_case['description']}...")

            test_results = self._run_single_benchmark(
                test_case["name"], test_case["lines"]
            )

            benchmark_results["tests"][test_case["name"]] = test_results

        # ç›®æ¨™é”æˆè©•ä¾¡
        benchmark_results["goal_assessment"] = self._assess_performance_goals(
            benchmark_results["tests"]
        )

        # ã‚µãƒãƒªãƒ¼ç”Ÿæˆ
        benchmark_results["summary"] = self._generate_benchmark_summary(
            benchmark_results
        )

        self.logger.info("âœ… Comprehensive benchmark completed")
        return benchmark_results

    def _run_single_benchmark(self, test_name: str, line_count: int) -> dict:
        """å˜ä¸€ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®å®Ÿè¡Œ"""

        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        test_text = self._generate_test_data(line_count)

        results = {
            "test_info": {
                "name": test_name,
                "line_count": line_count,
                "text_length": len(test_text),
                "text_size_mb": len(test_text) / 1024 / 1024,
            },
            "traditional_parser": {},
            "optimized_parser": {},
            "streaming_parser": {},
            "parallel_parser": {},
            "improvement_ratios": {},
        }

        # Traditional Parser ãƒ†ã‚¹ãƒˆ
        try:
            results["traditional_parser"] = self._benchmark_traditional_parser(
                test_text
            )
        except Exception as e:
            self.logger.error(f"Traditional parser test failed: {e}")
            results["traditional_parser"] = {"error": str(e)}

        # Optimized Parser ãƒ†ã‚¹ãƒˆ
        try:
            results["optimized_parser"] = self._benchmark_optimized_parser(test_text)
        except Exception as e:
            self.logger.error(f"Optimized parser test failed: {e}")
            results["optimized_parser"] = {"error": str(e)}

        # Streaming Parser ãƒ†ã‚¹ãƒˆ
        try:
            results["streaming_parser"] = self._benchmark_streaming_parser(test_text)
        except Exception as e:
            self.logger.error(f"Streaming parser test failed: {e}")
            results["streaming_parser"] = {"error": str(e)}

        # æ”¹å–„ç‡è¨ˆç®—
        results["improvement_ratios"] = self._calculate_improvement_ratios(results)

        return results

    def _benchmark_traditional_parser(self, test_text: str) -> dict:
        """å¾“æ¥ãƒ‘ãƒ¼ã‚µãƒ¼ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""

        from ...parser import Parser

        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¸¬å®šé–‹å§‹
        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB

        start_time = time.time()

        # ãƒ‘ãƒ¼ã‚µãƒ¼å®Ÿè¡Œ
        parser = Parser()
        nodes = parser.parse(test_text)

        parse_time = time.time() - start_time
        peak_memory = process.memory_info().rss / 1024 / 1024  # MB
        memory_used = peak_memory - initial_memory

        return {
            "parse_time_seconds": parse_time,
            "memory_used_mb": memory_used,
            "peak_memory_mb": peak_memory,
            "nodes_count": len(nodes),
            "throughput_lines_per_second": (
                len(test_text.split("\n")) / parse_time if parse_time > 0 else 0
            ),
            "errors_count": len(parser.get_errors()),
        }

    def _benchmark_optimized_parser(self, test_text: str) -> dict:
        """æœ€é©åŒ–ãƒ‘ãƒ¼ã‚µãƒ¼ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""

        from ...parser import Parser

        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024

        start_time = time.time()

        # æœ€é©åŒ–ãƒ‘ãƒ¼ã‚µãƒ¼å®Ÿè¡Œ
        parser = Parser()
        nodes = parser.parse_optimized(test_text)

        parse_time = time.time() - start_time
        peak_memory = process.memory_info().rss / 1024 / 1024
        memory_used = peak_memory - initial_memory

        return {
            "parse_time_seconds": parse_time,
            "memory_used_mb": memory_used,
            "peak_memory_mb": peak_memory,
            "nodes_count": len(nodes),
            "throughput_lines_per_second": (
                len(test_text.split("\n")) / parse_time if parse_time > 0 else 0
            ),
            "errors_count": len(parser.get_errors()),
        }

    def _benchmark_streaming_parser(self, test_text: str) -> dict:
        """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ‘ãƒ¼ã‚µãƒ¼ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""

        from ...parser import StreamingParser

        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024

        start_time = time.time()

        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ‘ãƒ¼ã‚µãƒ¼å®Ÿè¡Œ
        parser = StreamingParser()
        nodes = list(parser.parse_streaming_from_text(test_text))

        parse_time = time.time() - start_time
        peak_memory = process.memory_info().rss / 1024 / 1024
        memory_used = peak_memory - initial_memory

        return {
            "parse_time_seconds": parse_time,
            "memory_used_mb": memory_used,
            "peak_memory_mb": peak_memory,
            "nodes_count": len(nodes),
            "throughput_lines_per_second": (
                len(test_text.split("\n")) / parse_time if parse_time > 0 else 0
            ),
            "errors_count": len(parser.get_errors()),
        }

    def _generate_test_data(self, line_count: int) -> str:
        """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ"""

        if line_count in self.test_data_cache:
            return self.test_data_cache[line_count]

        lines = []

        # å¤šæ§˜ãªKumihanè¨˜æ³•ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å«ã‚€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
        patterns = [
            "ã“ã‚Œã¯é€šå¸¸ã®ãƒ‘ãƒ©ã‚°ãƒ©ãƒ•ã§ã™ã€‚",
            "# å¤ªå­— # ã“ã®ãƒ†ã‚­ã‚¹ãƒˆã¯å¤ªå­—ã«ãªã‚Šã¾ã™",
            "# ã‚¤ã‚¿ãƒªãƒƒã‚¯ # ã“ã®ãƒ†ã‚­ã‚¹ãƒˆã¯ã‚¤ã‚¿ãƒªãƒƒã‚¯ã«ãªã‚Šã¾ã™",
            "# è¦‹å‡ºã—1 # å¤§ããªè¦‹å‡ºã—",
            "- ãƒªã‚¹ãƒˆé …ç›®1",
            "- ãƒªã‚¹ãƒˆé …ç›®2",
            "1. é †åºä»˜ããƒªã‚¹ãƒˆ1",
            "2. é †åºä»˜ããƒªã‚¹ãƒˆ2",
            "# æ ç·š # æ ã§å›²ã¾ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ",
            "# ãƒã‚¤ãƒ©ã‚¤ãƒˆ color=yellow # é»„è‰²ã§ãƒã‚¤ãƒ©ã‚¤ãƒˆ",
            "",  # ç©ºè¡Œ
            "è¤‡æ•°è¡Œã«ã‚ãŸã‚‹\\né•·ã„ãƒ†ã‚­ã‚¹ãƒˆã®\\nä¾‹ã§ã™ã€‚",
        ]

        for i in range(line_count):
            pattern = patterns[i % len(patterns)]
            if "é …ç›®" in pattern or "ãƒªã‚¹ãƒˆ" in pattern:
                lines.append(
                    pattern.replace("é …ç›®", f"é …ç›®{i+1}").replace(
                        "ãƒªã‚¹ãƒˆ", f"ãƒªã‚¹ãƒˆ{i+1}"
                    )
                )
            else:
                lines.append(f"{pattern} (è¡Œ {i+1})")

        test_text = "\n".join(lines)
        self.test_data_cache[line_count] = test_text

        return test_text

    def _calculate_improvement_ratios(self, results: dict) -> dict:
        """æ”¹å–„ç‡ã®è¨ˆç®—"""

        improvement = {}

        traditional = results.get("traditional_parser", {})
        optimized = results.get("optimized_parser", {})
        streaming = results.get("streaming_parser", {})

        if traditional.get("parse_time_seconds") and optimized.get(
            "parse_time_seconds"
        ):
            improvement["optimized_vs_traditional_speed"] = (
                traditional["parse_time_seconds"] / optimized["parse_time_seconds"]
            )

        if traditional.get("memory_used_mb") and optimized.get("memory_used_mb"):
            improvement["optimized_vs_traditional_memory"] = (
                traditional["memory_used_mb"] / optimized["memory_used_mb"]
            )

        if traditional.get("parse_time_seconds") and streaming.get(
            "parse_time_seconds"
        ):
            improvement["streaming_vs_traditional_speed"] = (
                traditional["parse_time_seconds"] / streaming["parse_time_seconds"]
            )

        return improvement

    def _assess_performance_goals(self, test_results: dict) -> dict:
        """Issue #727 ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›®æ¨™ã®é”æˆè©•ä¾¡"""

        assessment = {
            "goals": {
                "10k_lines_under_15s": False,
                "memory_reduction_66_percent": False,
                "100k_lines_under_180s": False,
                "10k_lines_under_30s": False,
            },
            "details": {},
        }

        # 10Kè¡Œãƒ•ã‚¡ã‚¤ãƒ«15ç§’ä»¥å†…ç›®æ¨™
        large_test = test_results.get("large", {})
        if large_test:
            optimized_time = large_test.get("optimized_parser", {}).get(
                "parse_time_seconds", float("inf")
            )
            streaming_time = large_test.get("streaming_parser", {}).get(
                "parse_time_seconds", float("inf")
            )

            best_time = min(optimized_time, streaming_time)
            assessment["goals"]["10k_lines_under_15s"] = best_time <= 15.0
            assessment["details"]["10k_best_time"] = best_time

        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡66%å‰Šæ¸›ç›®æ¨™
        if large_test:
            traditional_memory = large_test.get("traditional_parser", {}).get(
                "memory_used_mb", 0
            )
            optimized_memory = large_test.get("optimized_parser", {}).get(
                "memory_used_mb", 0
            )

            if traditional_memory > 0:
                memory_reduction = (
                    (traditional_memory - optimized_memory) / traditional_memory * 100
                )
                assessment["goals"]["memory_reduction_66_percent"] = (
                    memory_reduction >= 66.0
                )
                assessment["details"]["memory_reduction_percent"] = memory_reduction

        return assessment

    def _generate_benchmark_summary(self, benchmark_results: dict) -> dict:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚µãƒãƒªãƒ¼ç”Ÿæˆ"""

        summary = {
            "overall_performance": "unknown",
            "key_achievements": [],
            "areas_for_improvement": [],
            "recommendations": [],
        }

        goals = benchmark_results.get("goal_assessment", {}).get("goals", {})

        achieved_goals = sum(1 for achieved in goals.values() if achieved)
        total_goals = len(goals)

        if achieved_goals >= total_goals * 0.8:
            summary["overall_performance"] = "excellent"
            summary["key_achievements"].append("ã»ã¼å…¨ã¦ã®æ€§èƒ½ç›®æ¨™ã‚’é”æˆ")
        elif achieved_goals >= total_goals * 0.6:
            summary["overall_performance"] = "good"
            summary["key_achievements"].append("ä¸»è¦ãªæ€§èƒ½ç›®æ¨™ã‚’é”æˆ")
        else:
            summary["overall_performance"] = "needs_improvement"
            summary["areas_for_improvement"].append("æ€§èƒ½ç›®æ¨™ã®é”æˆç‡ãŒä½ã„")

        # æ¨å¥¨äº‹é …
        if not goals.get("10k_lines_under_15s"):
            summary["recommendations"].append("å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã®æ›´ãªã‚‹æœ€é©åŒ–ãŒå¿…è¦")

        if not goals.get("memory_reduction_66_percent"):
            summary["recommendations"].append("ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®æ”¹å–„ãŒå¿…è¦")

        return summary

    def generate_benchmark_report(self, results: dict) -> str:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""

        report_lines = [
            "ğŸ”¬ Kumihan-Formatter ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ¬ãƒãƒ¼ãƒˆ",
            "=" * 60,
            f"å®Ÿè¡Œæ—¥æ™‚: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(results['metadata']['timestamp']))}",
            f"ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ : {results['metadata']['platform']}",
            f"CPUã‚³ã‚¢æ•°: {results['metadata']['cpu_count']}",
            "",
            "ğŸ“Š ãƒ†ã‚¹ãƒˆçµæœ:",
        ]

        for test_name, test_data in results["tests"].items():
            info = test_data["test_info"]
            report_lines.extend(
                [
                    f"\nğŸ” {info['name'].upper()} ({info['line_count']:,}è¡Œ, {info['text_size_mb']:.1f}MB):",
                    f"  å¾“æ¥ãƒ‘ãƒ¼ã‚µãƒ¼: {test_data['traditional_parser'].get('parse_time_seconds', 'N/A'):.2f}s, "
                    f"{test_data['traditional_parser'].get('memory_used_mb', 'N/A'):.1f}MB",
                    f"  æœ€é©åŒ–ãƒ‘ãƒ¼ã‚µãƒ¼: {test_data['optimized_parser'].get('parse_time_seconds', 'N/A'):.2f}s, "
                    f"{test_data['optimized_parser'].get('memory_used_mb', 'N/A'):.1f}MB",
                    f"  ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°: {test_data['streaming_parser'].get('parse_time_seconds', 'N/A'):.2f}s, "
                    f"{test_data['streaming_parser'].get('memory_used_mb', 'N/A'):.1f}MB",
                ]
            )

            # æ”¹å–„ç‡
            improvements = test_data.get("improvement_ratios", {})
            if improvements:
                speed_improve = improvements.get("optimized_vs_traditional_speed", 1)
                memory_improve = improvements.get("optimized_vs_traditional_memory", 1)
                report_lines.append(
                    f"  æ”¹å–„ç‡: é€Ÿåº¦ {speed_improve:.1f}x, ãƒ¡ãƒ¢ãƒª {memory_improve:.1f}x"
                )

        # ç›®æ¨™é”æˆçŠ¶æ³
        goals = results.get("goal_assessment", {}).get("goals", {})
        report_lines.extend(
            [
                "",
                "ğŸ¯ ç›®æ¨™é”æˆçŠ¶æ³:",
                f"  10Kè¡Œ15ç§’ä»¥å†…: {'âœ…' if goals.get('10k_lines_under_15s') else 'âŒ'}",
                f"  ãƒ¡ãƒ¢ãƒª66%å‰Šæ¸›: {'âœ…' if goals.get('memory_reduction_66_percent') else 'âŒ'}",
                f"  100Kè¡Œ180ç§’ä»¥å†…: {'âœ…' if goals.get('100k_lines_under_180s') else 'âŒ'}",
            ]
        )

        # ã‚µãƒãƒªãƒ¼
        summary = results.get("summary", {})
        report_lines.extend(
            [
                "",
                f"ğŸ“ˆ ç·åˆè©•ä¾¡: {summary.get('overall_performance', 'unknown').upper()}",
            ]
        )

        if summary.get("key_achievements"):
            report_lines.append("âœ¨ ä¸»ãªæˆæœ:")
            for achievement in summary["key_achievements"]:
                report_lines.append(f"  â€¢ {achievement}")

        if summary.get("recommendations"):
            report_lines.append("ğŸ’¡ æ¨å¥¨æ”¹å–„:")
            for rec in summary["recommendations"]:
                report_lines.append(f"  â€¢ {rec}")

        return "\n".join(report_lines)


# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ç”¨ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿
def monitor_performance(task_name: str = "å‡¦ç†"):
    """
    ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼

    Args:
        task_name: ã‚¿ã‚¹ã‚¯å

    Returns:
        PerformanceContext: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
    """
    return PerformanceContext(task_name)


class PerformanceContext:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼"""

    def __init__(self, task_name: str):
        self.task_name = task_name
        self.monitor = PerformanceMonitor()
        self.start_time = None

    def __enter__(self):
        self.start_time = time.time()
        self.monitor.start_monitoring(total_items=1000, initial_stage=self.task_name)
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.monitor.stop_monitoring()

    def record_item_processed(self):
        """ã‚¢ã‚¤ãƒ†ãƒ å‡¦ç†ã®è¨˜éŒ²"""
        if hasattr(self.monitor, "update_progress"):
            # ç°¡æ˜“çš„ãªé€²æ—æ›´æ–°
            pass


# Testing serena-expert enforcement
