"""
ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ 
Issue #813å¯¾å¿œ - performance_metrics.pyã‹ã‚‰åˆ†é›¢
"""

import os
import threading
import time
from collections import deque
from datetime import datetime
from pathlib import Path
from typing import Any, Callable, Dict, Iterator, List, Optional, Tuple, Union

import psutil

# from ...utilities.logger import get_logger  # Removed: unused import


class MemoryOptimizer:
    """
    ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ 

    ç‰¹å¾´:
    - ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªãƒ‡ãƒ¼ã‚¿æ§‹é€ é¸æŠ
    - ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æœ€é©åŒ–
    - ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«ã¨ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå†åˆ©ç”¨
    - å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã®ãƒ¡ãƒ¢ãƒªç®¡ç†
    """

    def __init__(self, enable_gc_optimization: bool = True):
        from kumihan_formatter.core.utilities.logger import get_logger

        self.logger = get_logger(__name__)
        self.enable_gc_optimization = enable_gc_optimization
        self._object_pools: Dict[str, Dict[str, Any]] = {}  # mypyå‹ä¿®æ­£
        self._memory_stats = {"allocations": 0, "deallocations": 0, "pool_hits": 0}

        if enable_gc_optimization:
            self._configure_gc_optimization()

        self.logger.info("MemoryOptimizer initialized")

    def _configure_gc_optimization(self) -> None:
        """ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æœ€é©åŒ–è¨­å®š"""
        import gc

        # GCé–¾å€¤ã‚’èª¿æ•´ï¼ˆå¤§å®¹é‡å‡¦ç†å‘ã‘ï¼‰
        original_thresholds = gc.get_threshold()
        # ã‚ˆã‚Šé«˜ã„é–¾å€¤ã§GCé »åº¦ã‚’ä¸‹ã’ã€ãƒãƒƒãƒå‡¦ç†åŠ¹ç‡ã‚’å‘ä¸Š
        new_thresholds = (
            original_thresholds[0] * 2,  # ä¸–ä»£0
            original_thresholds[1] * 2,  # ä¸–ä»£1
            original_thresholds[2] * 2,  # ä¸–ä»£2
        )
        gc.set_threshold(*new_thresholds)

        self.logger.info(
            f"GC thresholds adjusted: {original_thresholds} -> {new_thresholds}"
        )

    def create_object_pool(
        self, pool_name: str, factory_func: Callable[[], Any], max_size: int = 100
    ) -> None:
        """
        ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãƒ—ãƒ¼ãƒ«ä½œæˆ

        Args:
            pool_name: ãƒ—ãƒ¼ãƒ«å
            factory_func: ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆç”Ÿæˆé–¢æ•°
            max_size: ãƒ—ãƒ¼ãƒ«æœ€å¤§ã‚µã‚¤ã‚º
        """
        self._object_pools[pool_name] = {
            "pool": deque(maxlen=max_size),
            "factory": factory_func,
            "max_size": max_size,
            "created_count": 0,
            "reused_count": 0,
        }

        self.logger.info(f"Object pool '{pool_name}' created with max size: {max_size}")

    def get_pooled_object(self, pool_name: str) -> None:
        """ãƒ—ãƒ¼ãƒ«ã‹ã‚‰ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å–å¾—"""
        if pool_name not in self._object_pools:
            raise ValueError(f"Object pool '{pool_name}' not found")

        pool_info = self._object_pools[pool_name]
        pool = pool_info["pool"]

        if pool:
            # ãƒ—ãƒ¼ãƒ«ã‹ã‚‰å†åˆ©ç”¨
            obj = pool.popleft()
            pool_info["reused_count"] += 1
            self._memory_stats["pool_hits"] += 1
            return obj
        else:
            # æ–°è¦ä½œæˆ
            obj = pool_info["factory"]()
            pool_info["created_count"] += 1
            self._memory_stats["allocations"] += 1
            return obj

    def return_pooled_object(self, pool_name: str, obj: Any) -> None:
        """ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ãƒ—ãƒ¼ãƒ«ã«è¿”å´"""
        if pool_name not in self._object_pools:
            return

        pool_info = self._object_pools[pool_name]
        pool = pool_info["pool"]

        # ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ãƒªã‚»ãƒƒãƒˆï¼ˆå¯èƒ½ã§ã‚ã‚Œã°ï¼‰
        if hasattr(obj, "reset"):
            obj.reset()
        elif hasattr(obj, "clear"):
            obj.clear()

        # ãƒ—ãƒ¼ãƒ«ã«è¿”å´
        if len(pool) < pool_info["max_size"]:
            pool.append(obj)
        else:
            # ãƒ—ãƒ¼ãƒ«ãŒæº€æ¯ã®å ´åˆã¯ç ´æ£„
            self._memory_stats["deallocations"] += 1

    def memory_efficient_file_reader(
        self,
        file_path: Union[str, Path],
        chunk_size: int = 64 * 1024,
        use_mmap: bool = False,
    ) -> Iterator[str]:
        """
        ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿

        Args:
            file_path: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            chunk_size: ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º
            use_mmap: ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ—ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ä½¿ç”¨ãƒ•ãƒ©ã‚°

        Yields:
            str: ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ£ãƒ³ã‚¯
        """
        if use_mmap:
            try:
                import mmap

                with open(file_path, "r", encoding="utf-8") as f:
                    with mmap.mmap(
                        f.fileno(), 0, access=mmap.ACCESS_READ
                    ) as mmapped_file:
                        for i in range(0, len(mmapped_file), chunk_size):
                            chunk = mmapped_file[i : i + chunk_size].decode(
                                "utf-8", errors="ignore"
                            )
                            yield chunk
                return
            except Exception as e:
                self.logger.warning(f"mmap failed, falling back to regular read: {e}")

        # é€šå¸¸ã®ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        with open(file_path, "r", encoding="utf-8") as f:
            while True:
                chunk = f.read(chunk_size)
                if not chunk:
                    break
                yield chunk

    def optimize_list_operations(self, data: List[Any], operation: str) -> Any:
        """
        ãƒªã‚¹ãƒˆæ“ä½œã®æœ€é©åŒ–

        Args:
            data: æ“ä½œå¯¾è±¡ãƒ‡ãƒ¼ã‚¿
            operation: æ“ä½œç¨®åˆ¥ï¼ˆ'sort', 'unique', 'filter_empty'ï¼‰

        Returns:
            Any: æœ€é©åŒ–ã•ã‚ŒãŸçµæœ
        """
        if operation == "sort":
            # å¤§å®¹é‡ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã¯Timsortã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’æ´»ç”¨
            return sorted(data, key=str if isinstance(data[0], str) else None)

        elif operation == "unique":
            # é«˜é€Ÿuniqueå‡¦ç†
            if len(data) < 1000:
                return list(set(data))
            else:
                # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡é‡è¦–ã®é‡è¤‡å‰Šé™¤
                seen = set()
                result = []
                for item in data:
                    if item not in seen:
                        seen.add(item)
                        result.append(item)
                return result

        elif operation == "filter_empty":
            # ç©ºè¦ç´ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            return [item for item in data if item]

        else:
            # ãã®ä»–ã®æ“ä½œã¯ãã®ã¾ã¾è¿”ã™
            return data

    def batch_process_with_memory_limit(
        self,
        data_generator: Iterator[Any],
        processing_func: Callable,
        memory_limit_mb: int = 100,
    ) -> Iterator[Any]:
        """
        ãƒ¡ãƒ¢ãƒªåˆ¶é™ä»˜ããƒãƒƒãƒå‡¦ç†

        Args:
            data_generator: ãƒ‡ãƒ¼ã‚¿ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿
            processing_func: å‡¦ç†é–¢æ•°
            memory_limit_mb: ãƒ¡ãƒ¢ãƒªåˆ¶é™ï¼ˆMBï¼‰

        Yields:
            Any: å‡¦ç†çµæœ
        """
        import sys

        batch = []
        batch_size_bytes = 0
        memory_limit_bytes = memory_limit_mb * 1024 * 1024

        for item in data_generator:
            batch.append(item)
            # æ¦‚ç®—ã®ãƒ¡ãƒ¢ãƒªã‚µã‚¤ã‚ºã‚’è¨ˆç®—
            batch_size_bytes += sys.getsizeof(item)

            if batch_size_bytes >= memory_limit_bytes:
                # ãƒãƒƒãƒå‡¦ç†å®Ÿè¡Œ
                for result in processing_func(batch):
                    yield result

                # ãƒãƒƒãƒã‚¯ãƒªã‚¢
                batch.clear()
                batch_size_bytes = 0

        # æ®‹ã‚Šã®ãƒãƒƒãƒã‚’å‡¦ç†
        if batch:
            for result in processing_func(batch):
                yield result

    def force_garbage_collection(self) -> None:
        """å¼·åˆ¶ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ"""
        import gc

        collected_objects = gc.collect()
        self.logger.debug(f"Garbage collection: {collected_objects} objects collected")
        return collected_objects

    def get_memory_stats(self) -> Dict[str, Any]:
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨çµ±è¨ˆã‚’å–å¾—"""
        process = psutil.Process(os.getpid())
        memory_info = process.memory_info()

        pool_stats = {}
        for name, pool_info in self._object_pools.items():
            pool_stats[name] = {
                "current_size": len(pool_info["pool"]),
                "max_size": pool_info["max_size"],
                "created_count": pool_info["created_count"],
                "reused_count": pool_info["reused_count"],
                "efficiency_percent": (
                    pool_info["reused_count"]
                    / (pool_info["created_count"] + pool_info["reused_count"])
                    * 100
                    if (pool_info["created_count"] + pool_info["reused_count"]) > 0
                    else 0
                ),
            }

        return {
            "process_memory_mb": memory_info.rss / 1024 / 1024,
            "virtual_memory_mb": memory_info.vms / 1024 / 1024,
            "object_pools": pool_stats,
            "memory_operations": self._memory_stats,
        }

    def detect_memory_leaks(
        self, threshold_mb: float = 10.0, sample_interval: int = 5
    ) -> Dict[str, Any]:
        """
        é«˜åº¦ãªãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯æ¤œå‡ºæ©Ÿæ§‹

        Args:
            threshold_mb: ãƒ¡ãƒ¢ãƒªå¢—åŠ ã®æ¤œå‡ºé–¾å€¤ï¼ˆMBï¼‰
            sample_interval: ã‚µãƒ³ãƒ—ãƒ«é–“éš”ï¼ˆç§’ï¼‰

        Returns:
            Dict[str, Any]: ãƒªãƒ¼ã‚¯æ¤œå‡ºçµæœ
        """
        import gc

        process = psutil.Process(os.getpid())
        samples: List[Tuple[float, float]] = []  # (timestamp, memory_mb)

        # åˆæœŸãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡è¨˜éŒ²
        initial_memory = process.memory_info().rss / 1024 / 1024
        samples.append((time.time(), initial_memory))

        self.logger.info(
            f"Memory leak detection started. Initial memory: {initial_memory:.2f} MB"
        )

        # è¤‡æ•°å›ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        for i in range(sample_interval):
            time.sleep(1)
            current_memory = process.memory_info().rss / 1024 / 1024
            samples.append((time.time(), current_memory))

        # ãƒªãƒ¼ã‚¯åˆ†æ
        memory_growth = samples[-1][1] - samples[0][1]
        growth_rate = memory_growth / (samples[-1][0] - samples[0][0])  # MB/ç§’

        # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œå¾Œã®ãƒ¡ãƒ¢ãƒªç¢ºèª
        gc.collect()
        time.sleep(0.5)
        post_gc_memory = process.memory_info().rss / 1024 / 1024
        gc_effect = samples[-1][1] - post_gc_memory

        # ãƒªãƒ¼ã‚¯åˆ¤å®š
        is_leak_detected = (
            memory_growth > threshold_mb
            and gc_effect < memory_growth * 0.5  # GCã§åŠåˆ†ä»¥ä¸Šå›åã•ã‚Œãªã„å ´åˆ
        )

        leak_info = {
            "leak_detected": is_leak_detected,
            "memory_growth_mb": memory_growth,
            "growth_rate_mb_per_sec": growth_rate,
            "gc_effect_mb": gc_effect,
            "initial_memory_mb": initial_memory,
            "final_memory_mb": samples[-1][1],
            "post_gc_memory_mb": post_gc_memory,
            "samples": samples,
            "recommendation": self._generate_leak_recommendation(
                is_leak_detected, memory_growth, gc_effect
            ),
        }

        if is_leak_detected:
            self.logger.warning(
                f"Memory leak detected! Growth: {memory_growth:.2f} MB, "
                f"Rate: {growth_rate:.4f} MB/s"
            )
        else:
            self.logger.info(
                f"No significant memory leak detected. Growth: {memory_growth:.2f} MB"
            )

        return leak_info

    def proactive_gc_strategy(
        self, memory_threshold_mb: float = 100.0, enable_generational: bool = True
    ) -> Dict[str, Any]:
        """
        ãƒ—ãƒ­ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³æˆ¦ç•¥

        Args:
            memory_threshold_mb: GCå®Ÿè¡Œãƒ¡ãƒ¢ãƒªé–¾å€¤ï¼ˆMBï¼‰
            enable_generational: ä¸–ä»£åˆ¥GCæœ€é©åŒ–æœ‰åŠ¹ãƒ•ãƒ©ã‚°

        Returns:
            Dict[str, Any]: GCå®Ÿè¡Œçµæœ
        """
        import gc

        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss / 1024 / 1024

        self.logger.info(
            f"Proactive GC strategy triggered. Current memory: {initial_memory:.2f} MB"
        )

        results = {
            "initial_memory_mb": initial_memory,
            "gc_executed": False,
            "collections_performed": [],
            "memory_freed_mb": 0.0,
            "execution_time_ms": 0.0,
        }

        start_time = time.time()

        # ãƒ¡ãƒ¢ãƒªé–¾å€¤ãƒã‚§ãƒƒã‚¯
        if initial_memory >= memory_threshold_mb:
            self.logger.info(
                f"Memory threshold ({memory_threshold_mb} MB) exceeded. Executing proactive GC..."
            )

            if enable_generational:
                # ä¸–ä»£åˆ¥æœ€é©åŒ–GCå®Ÿè¡Œ
                for generation in range(3):  # Python GCã®3ä¸–ä»£
                    collected = gc.collect(generation)
                    results["collections_performed"].append(
                        {"generation": generation, "objects_collected": collected}
                    )
                    self.logger.debug(
                        f"Generation {generation} GC: {collected} objects collected"
                    )
            else:
                # æ¨™æº–GCå®Ÿè¡Œ
                collected = gc.collect()
                results["collections_performed"].append(
                    {"generation": "all", "objects_collected": collected}
                )

            # GCå¾Œãƒ¡ãƒ¢ãƒªç¢ºèª
            time.sleep(0.1)  # GCå®Œäº†ã‚’å¾…æ©Ÿ
            post_gc_memory = process.memory_info().rss / 1024 / 1024
            results["memory_freed_mb"] = initial_memory - post_gc_memory
            results["final_memory_mb"] = post_gc_memory
            results["gc_executed"] = True

            self.logger.info(
                f"Proactive GC completed. Memory freed: {results['memory_freed_mb']:.2f} MB"
            )
        else:
            self.logger.debug(
                f"Memory usage ({initial_memory:.2f} MB) below threshold. GC not needed."
            )

        results["execution_time_ms"] = (time.time() - start_time) * 1000
        return results

    def create_advanced_resource_pool(
        self,
        pool_name: str,
        factory_func: Callable,
        max_size: int = 100,
        cleanup_func: Optional[Callable] = None,
        auto_cleanup_interval: int = 300,
    ) -> None:
        """
        é«˜åº¦ãªãƒªã‚½ãƒ¼ã‚¹ãƒ—ãƒ¼ãƒ«ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 

        Args:
            pool_name: ãƒ—ãƒ¼ãƒ«å
            factory_func: ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆç”Ÿæˆé–¢æ•°
            max_size: ãƒ—ãƒ¼ãƒ«æœ€å¤§ã‚µã‚¤ã‚º
            cleanup_func: ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—é–¢æ•°
            auto_cleanup_interval: è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—é–“éš”ï¼ˆç§’ï¼‰
        """
        pool_info = {
            "pool": deque(maxlen=max_size),
            "factory": factory_func,
            "cleanup": cleanup_func,
            "max_size": max_size,
            "created_count": 0,
            "reused_count": 0,
            "cleanup_count": 0,
            "last_cleanup": time.time(),
            "auto_cleanup_interval": auto_cleanup_interval,
            "lock": threading.RLock(),  # ãƒªã‚¨ãƒ³ãƒˆãƒ©ãƒ³ãƒˆãƒ­ãƒƒã‚¯
        }

        self._object_pools[pool_name] = pool_info

        # è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¿ã‚¤ãƒãƒ¼è¨­å®š
        def auto_cleanup() -> None:
            while pool_name in self._object_pools:
                time.sleep(auto_cleanup_interval)
                self._cleanup_resource_pool(pool_name)

        cleanup_thread = threading.Thread(target=auto_cleanup, daemon=True)
        cleanup_thread.start()

        self.logger.info(
            f"Advanced resource pool '{pool_name}' created with auto-cleanup "
            f"every {auto_cleanup_interval}s"
        )

    def _cleanup_resource_pool(self, pool_name: str) -> int:
        """ãƒªã‚½ãƒ¼ã‚¹ãƒ—ãƒ¼ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œ"""
        if pool_name not in self._object_pools:
            return 0

        pool_info = self._object_pools[pool_name]
        cleanup_count = 0

        with pool_info["lock"]:
            cleanup_func = pool_info.get("cleanup")
            if cleanup_func:
                # ãƒ—ãƒ¼ãƒ«å†…ã®å…¨ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                temp_objects = []
                while pool_info["pool"]:
                    obj = pool_info["pool"].popleft()
                    try:
                        cleanup_func(obj)
                        temp_objects.append(obj)
                        cleanup_count += 1
                    except Exception as e:
                        self.logger.warning(
                            f"Cleanup failed for object in pool '{pool_name}': {e}"
                        )

                # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã•ã‚ŒãŸã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æˆ»ã™
                for obj in temp_objects:
                    pool_info["pool"].append(obj)

            pool_info["cleanup_count"] += cleanup_count
            pool_info["last_cleanup"] = time.time()

            if cleanup_count > 0:
                self.logger.info(
                    f"Resource pool '{pool_name}' cleanup: {cleanup_count} objects processed"
                )

        return cleanup_count

    def generate_memory_report(self, include_detailed_stats: bool = True) -> str:
        """
        ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆå¯è¦–åŒ–æ©Ÿèƒ½ï¼‰

        Args:
            include_detailed_stats: è©³ç´°çµ±è¨ˆæƒ…å ±ã‚’å«ã‚€ã‹

        Returns:
            str: HTMLãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®ãƒ¡ãƒ¢ãƒªãƒ¬ãƒãƒ¼ãƒˆ
        """
        # tmp/é…ä¸‹ã«ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        from pathlib import Path

        process = psutil.Process(os.getpid())
        memory_info = process.memory_info()

        # åŸºæœ¬æƒ…å ±åé›†
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        memory_mb = memory_info.rss / 1024 / 1024
        _ = memory_info.vms / 1024 / 1024  # Virtual memory (unused)

        # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
        system_memory = psutil.virtual_memory()
        _ = system_memory.total / 1024 / 1024 / 1024  # System total (unused)
        _ = system_memory.available / 1024 / 1024 / 1024  # System available (unused)
        system_usage_percent = system_memory.percent

        # HTML ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        html_report = f"""<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <title>Kumihan-Formatter ãƒ¡ãƒ¢ãƒªãƒ¬ãƒãƒ¼ãƒˆ</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        .container {{ max-width: 1200px; margin: 0 auto; }}
        .stat-card {{ background: #f8f9fa; padding: 15px; margin: 10px; border-radius: 6px; }}
        .memory-bar {{ width: 100%; height: 20px; background-color: #ecf0f1; border-radius: 10px; }}
        .memory-fill {{
            height: 100%;
            background: linear-gradient(90deg, #27ae60, #f39c12, #e74c3c);
        }}
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸ§  Kumihan-Formatter ãƒ¡ãƒ¢ãƒªãƒ¬ãƒãƒ¼ãƒˆ</h1>
        <div class="stat-card">
            <h3>ãƒ—ãƒ­ã‚»ã‚¹ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {memory_mb:.2f} MB</h3>
            <div class="memory-bar">
                <div class="memory-fill" style="width: {min(memory_mb/100*10, 100)}%;"></div>
            </div>
        </div>
        <div class="stat-card">
            <h3>ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡: {system_usage_percent:.1f}%</h3>
            <div class="memory-bar">
                <div class="memory-fill" style="width: {system_usage_percent}%;"></div>
            </div>
        </div>
        <p>ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆæ™‚åˆ»: {current_time}</p>
    </div>
</body>
</html>"""

        # tmp/é…ä¸‹ã«ä¿å­˜
        tmp_dir = Path("tmp")
        tmp_dir.mkdir(exist_ok=True)
        report_path = (
            tmp_dir / f"memory_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
        )

        with open(report_path, "w", encoding="utf-8") as f:
            f.write(html_report)

        self.logger.info(f"Memory report generated and saved to {report_path}")
        return str(report_path)

    def _generate_leak_recommendation(
        self, is_leak: bool, growth_mb: float, gc_effect_mb: float
    ) -> str:
        """ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯æ¤œå‡ºçµæœã«åŸºã¥ãæ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç”Ÿæˆ"""
        if not is_leak:
            return "æ­£å¸¸ãªãƒ¡ãƒ¢ãƒªä½¿ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã™ã€‚å®šæœŸçš„ãªç›£è¦–ã‚’ç¶™ç¶šã—ã¦ãã ã•ã„ã€‚"

        recommendations = []

        if gc_effect_mb < growth_mb * 0.3:
            recommendations.append(
                "ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã®åŠ¹æœãŒä½ã„ãŸã‚ã€å¼·ã„å‚ç…§ã®è§£é™¤ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
            )

        if growth_mb > 50:
            recommendations.append(
                "å¤§å¹…ãªãƒ¡ãƒ¢ãƒªå¢—åŠ ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚å¤§å®¹é‡ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†æ–¹æ³•ã‚’è¦‹ç›´ã—ã¦ãã ã•ã„ã€‚"
            )

        if not recommendations:
            recommendations.append(
                "ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
            )

        return " ".join(recommendations)
