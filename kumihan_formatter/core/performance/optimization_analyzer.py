"""
æœ€é©åŒ–åŠ¹æœåˆ†æã‚·ã‚¹ãƒ†ãƒ çµ±åˆ - åˆ†å‰²ã•ã‚ŒãŸã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®çµ±åˆ
åˆ†å‰²ã•ã‚ŒãŸæœ€é©åŒ–åˆ†æã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’çµ±åˆã—ã€
å…ƒã®OptimizationAnalyzerã‚¯ãƒ©ã‚¹ã¨åŒç­‰ã®æ©Ÿèƒ½ã‚’æä¾›
Issue #476å¯¾å¿œ - ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºåˆ¶é™éµå®ˆ
"""

import json
from dataclasses import asdict
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any

from ..utilities.logger import get_logger
from .optimization_comparison import OptimizationComparisonEngine
from .optimization_measurement import OptimizationMeasurementSystem
from .optimization_types import OptimizationReport


class OptimizationAnalyzer:
    """æœ€é©åŒ–åŠ¹æœåˆ†æã‚·ã‚¹ãƒ†ãƒ çµ±åˆ
    æ©Ÿèƒ½:
    - Before/After ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ
    - çµ±è¨ˆçš„æœ‰æ„æ€§æ¤œå®š
    - æ”¹å–„åŠ¹æœã®å®šé‡çš„è©•ä¾¡
    - å›å¸°ãƒªã‚¹ã‚¯ã®è©•ä¾¡
    - æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    """

    def __init__(self, baseline_dir: Path = None):  # type: ignore
        """æœ€é©åŒ–åˆ†æå™¨ã‚’åˆæœŸåŒ–
        Args:
            baseline_dir: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        self.logger = get_logger(__name__)
        self.baseline_dir = baseline_dir or Path("./performance_baselines")
        self.baseline_dir.mkdir(parents=True, exist_ok=True)
        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
        self.measurement_system = OptimizationMeasurementSystem(self.baseline_dir)
        self.comparison_engine = OptimizationComparisonEngine()
        # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        self.optimization_history: list[OptimizationReport] = []

    def capture_baseline(self, name: str, description: str = "") -> dict[str, Any]:
        """æœ€é©åŒ–å‰ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ€§èƒ½ã‚’è¨˜éŒ²
        Args:
            name: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å
            description: èª¬æ˜
        Returns:
            ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿
        """
        return self.measurement_system.capture_baseline(name, description)

    def measure_optimization_impact(
        self,
        optimization_name: str,
        baseline_name: str,
        description: str = "",
    ) -> OptimizationReport:
        """æœ€é©åŒ–å¾Œã®åŠ¹æœã‚’æ¸¬å®š
        Args:
            optimization_name: æœ€é©åŒ–å
            baseline_name: æ¯”è¼ƒå¯¾è±¡ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å
            description: æœ€é©åŒ–ã®èª¬æ˜
        Returns:
            æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆ
        """
        print(f"ğŸ” Measuring optimization impact: {optimization_name}")
        self.logger.info(f"æœ€é©åŒ–åŠ¹æœæ¸¬å®šé–‹å§‹: {optimization_name}")
        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        baseline_data = self.measurement_system.load_baseline(baseline_name)
        if not baseline_data:
            raise ValueError(f"Baseline '{baseline_name}' not found")
        # æœ€é©åŒ–å¾Œã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œ
        optimized_results = self.measurement_system.measure_optimized_performance(
            optimization_name
        )
        # æ¯”è¼ƒåˆ†æ
        metrics = self.comparison_engine.compare_performance(
            baseline_data["benchmark_results"], optimized_results
        )
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report = OptimizationReport(
            timestamp=datetime.now().isoformat(),
            optimization_name=optimization_name,
            total_improvement_score=self.comparison_engine.calculate_total_improvement_score(
                metrics
            ),
            metrics=metrics,
            performance_summary=self.comparison_engine.create_performance_summary(
                metrics
            ),
            recommendations=self.comparison_engine.generate_recommendations(metrics),
            regression_warnings=self.comparison_engine.detect_regressions(metrics),
        )
        # å±¥æ­´ã«ä¿å­˜
        self.optimization_history.append(report)
        # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        report_file = (
            self.baseline_dir / f"{optimization_name}_optimization_report.json"
        )
        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(asdict(report), f, indent=2, ensure_ascii=False)
        print(f"ğŸ“ˆ Optimization report saved to: {report_file}")
        self.logger.info(f"æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜å®Œäº†: {report_file}")
        return report

    def generate_comprehensive_report(self, optimization_name: str) -> str:
        """åŒ…æ‹¬çš„ãªæœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
        Args:
            optimization_name: æœ€é©åŒ–å
        Returns:
            ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆæ–‡å­—åˆ—
        """
        # å¯¾å¿œã™ã‚‹ãƒ¬ãƒãƒ¼ãƒˆã‚’æ¤œç´¢
        report = None
        for r in self.optimization_history:
            if r.optimization_name == optimization_name:
                report = r
                break
        if not report:
            return f"Optimization report for '{optimization_name}' not found."
        lines = [
            f"ğŸš€ Optimization Impact Report: {optimization_name}",
            "=" * 80,
            f"Generated: {report.timestamp}",
            f"Total Improvement Score: {report.total_improvement_score:.2f}",
            "",
        ]
        # ä¸»è¦æ”¹å–„ç‚¹
        significant_improvements = report.get_significant_improvements()
        if significant_improvements:
            lines.extend(
                [
                    "ğŸ“ˆ Significant Improvements:",
                    "-" * 40,
                ]
            )
            for metric in significant_improvements:
                lines.append(
                    f"  âœ… {metric.name}: {metric.improvement_percent:.1f}% improvement "
                    f"({metric.before_value:.3f}s â†’ {metric.after_value:.3f}s)"
                )
            lines.append("")
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ
        categories = ["performance", "memory", "cache"]
        for category in categories:
            category_metrics = report.get_metrics_by_category(category)
            if category_metrics:
                lines.extend(
                    [
                        f"ğŸ“Š {category.title()} Metrics:",
                        "-" * 25,
                    ]
                )
                for metric in category_metrics:
                    status = "âœ…" if metric.is_improvement else "âš ï¸"
                    lines.append(
                        f"  {status} {metric.name}: {metric.improvement_percent:+.1f}% "
                        f"({metric.significance} significance)"
                    )
                lines.append("")
        # æ¨å¥¨äº‹é …
        if report.recommendations:
            lines.extend(
                [
                    "ğŸ’¡ Recommendations:",
                    "-" * 20,
                ]
            )
            for rec in report.recommendations:
                lines.append(f"  â€¢ {rec}")
            lines.append("")
        # å›å¸°è­¦å‘Š
        if report.regression_warnings:
            lines.extend(
                [
                    "âš ï¸  Regression Warnings:",
                    "-" * 25,
                ]
            )
            for warning in report.regression_warnings:
                lines.append(f"  âš ï¸  {warning}")
            lines.append("")
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¦ç´„
        summary = report.performance_summary
        lines.extend(
            [
                "ğŸ“‹ Performance Summary:",
                "-" * 25,
                f"  Total Benchmarks: {summary.get('total_benchmarks', 0)}",
                f"  Improved Metrics: {summary.get('improved_metrics', 0)}",
                f"  Degraded Metrics: {summary.get('degraded_metrics', 0)}",
                f"  Stable Metrics: {summary.get('stable_metrics', 0)}",
            ]
        )
        if summary.get("cache_effectiveness"):
            cache_eff = summary["cache_effectiveness"]
            lines.extend(
                [
                    "",
                    "ğŸ’¾ Cache Effectiveness:",
                    "-" * 22,
                    f"  Total Cache Metrics: {cache_eff.get('total_cache_metrics', 0)}",
                    f"  Cache Improvements: {cache_eff.get('cache_improvements', 0)}",
                    f"  Avg Cache Improvement: {cache_eff.get('avg_cache_improvement', 0):.1f}%",
                ]
            )
        return "\n".join(lines)

    def compare_optimizations(self, optimization_names: list[str]) -> dict[str, Any]:
        """è¤‡æ•°ã®æœ€é©åŒ–ã‚’æ¯”è¼ƒ
        Args:
            optimization_names: æ¯”è¼ƒã™ã‚‹æœ€é©åŒ–åã®ãƒªã‚¹ãƒˆ
        Returns:
            æ¯”è¼ƒçµæœ
        """
        comparison_data = {  # type: ignore
            "optimizations": {},
            "ranking": [],
            "best_practices": [],
        }
        reports = []
        for name in optimization_names:
            for report in self.optimization_history:
                if report.optimization_name == name:
                    reports.append(report)
                    break
        if not reports:
            return {"error": "No optimization reports found for comparison"}
        # å„æœ€é©åŒ–ã®ã‚¹ã‚³ã‚¢ã‚’æ¯”è¼ƒ
        for report in reports:
            comparison_data["optimizations"][report.optimization_name] = {  # type: ignore
                "total_score": report.total_improvement_score,
                "significant_improvements": len(report.get_significant_improvements()),
                "regression_warnings": len(report.regression_warnings),
            }
        # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ä½œæˆ
        ranking = sorted(
            comparison_data["optimizations"].items(),  # type: ignore
            key=lambda x: x[1]["total_score"],
            reverse=True,
        )
        comparison_data["ranking"] = [name for name, _ in ranking]
        # ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹æŠ½å‡º
        best_practices = set()
        for report in reports:
            if report.total_improvement_score > 10:  # é«˜ã‚¹ã‚³ã‚¢ã®æœ€é©åŒ–ã‹ã‚‰
                for rec in report.recommendations[:2]:  # ä¸Šä½2ã¤ã®æ¨å¥¨äº‹é …
                    best_practices.add(rec)
        comparison_data["best_practices"] = list(best_practices)
        return comparison_data

    # ãƒ‡ãƒ¼ã‚¿ç®¡ç†ãƒ¡ã‚½ãƒƒãƒ‰
    def list_baselines(self) -> list[str]:
        """åˆ©ç”¨å¯èƒ½ãªãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ä¸€è¦§ã‚’å–å¾—"""
        return self.measurement_system.list_baselines()

    def validate_baseline_consistency(self, baseline_name: str) -> dict[str, Any]:
        """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã®ä¸€è²«æ€§ã‚’æ¤œè¨¼"""
        return self.measurement_system.validate_baseline_consistency(baseline_name)

    def cleanup_old_data(self, days_old: int = 30):  # type: ignore
        """å¤ã„ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        Args:
            days_old: å‰Šé™¤å¯¾è±¡ã®æ—¥æ•°
        """
        cutoff_date = datetime.now() - timedelta(days=days_old)
        # å¤ã„ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        for baseline_file in self.baseline_dir.glob("*_baseline.json"):
            if baseline_file.stat().st_mtime < cutoff_date.timestamp():
                baseline_file.unlink()
                print(f"ğŸ—‘ï¸  Deleted old baseline: {baseline_file}")
        # å¤ã„ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        for report_file in self.baseline_dir.glob("*_optimization_report.json"):
            if report_file.stat().st_mtime < cutoff_date.timestamp():
                report_file.unlink()
                print(f"ğŸ—‘ï¸  Deleted old report: {report_file}")

    def export_optimization_summary(self, output_file: Path):  # type: ignore
        """æœ€é©åŒ–ã®è¦ç´„ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
        Args:
            output_file: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        summary_data = {
            "export_timestamp": datetime.now().isoformat(),
            "total_optimizations": len(self.optimization_history),
            "optimization_reports": [
                asdict(report) for report in self.optimization_history
            ],
            "baseline_data": self.measurement_system.baseline_data,
        }
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(summary_data, f, indent=2, ensure_ascii=False)
        print(f"ğŸ“¤ Optimization summary exported to: {output_file}")
