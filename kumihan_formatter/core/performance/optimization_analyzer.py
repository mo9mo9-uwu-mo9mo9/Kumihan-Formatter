"""
æœ€é©åŒ–åŠ¹æœåˆ†æãƒ„ãƒ¼ãƒ« - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ã®å®šé‡çš„è©•ä¾¡

ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šã®åŠ¹æœã‚’æ¸¬å®šãƒ»åˆ†æ
Issue #402å¯¾å¿œ - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
"""

import json
import statistics
from dataclasses import asdict, dataclass
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any

from ..performance import get_global_monitor
from .benchmark import BenchmarkConfig, PerformanceBenchmarkSuite
from .memory_monitor import MemoryMonitor
from .profiler import AdvancedProfiler


@dataclass
class OptimizationMetrics:
    """æœ€é©åŒ–ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""

    name: str
    before_value: float
    after_value: float
    improvement_percent: float
    improvement_absolute: float
    significance: str  # low, medium, high, critical
    category: str  # performance, memory, cache, etc.

    @property
    def is_improvement(self) -> bool:
        """æ”¹å–„ãŒã‚ã£ãŸã‹ã©ã†ã‹"""
        return self.improvement_percent > 0


@dataclass
class OptimizationReport:
    """æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆ"""

    timestamp: str
    optimization_name: str
    total_improvement_score: float
    metrics: list[OptimizationMetrics]
    performance_summary: dict[str, Any]
    recommendations: list[str]
    regression_warnings: list[str]

    def get_metrics_by_category(self, category: str) -> list[OptimizationMetrics]:
        """ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—"""
        return [m for m in self.metrics if m.category == category]

    def get_significant_improvements(self) -> list[OptimizationMetrics]:
        """é‡è¦ãªæ”¹å–„ã‚’å–å¾—"""
        return [
            m
            for m in self.metrics
            if m.significance in ["high", "critical"] and m.is_improvement
        ]


class OptimizationAnalyzer:
    """æœ€é©åŒ–åŠ¹æœåˆ†æã‚·ã‚¹ãƒ†ãƒ 

    æ©Ÿèƒ½:
    - Before/After ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ
    - çµ±è¨ˆçš„æœ‰æ„æ€§æ¤œå®š
    - æ”¹å–„åŠ¹æœã®å®šé‡çš„è©•ä¾¡
    - å›å¸°ãƒªã‚¹ã‚¯ã®è©•ä¾¡
    - æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    """

    def __init__(self, baseline_dir: Path = None):  # type: ignore
        """æœ€é©åŒ–åˆ†æå™¨ã‚’åˆæœŸåŒ–

        Args:
            baseline_dir: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        self.baseline_dir = baseline_dir or Path("./performance_baselines")
        self.baseline_dir.mkdir(parents=True, exist_ok=True)

        # æ¸¬å®šãƒ„ãƒ¼ãƒ«
        self.monitor = get_global_monitor()
        self.profiler = AdvancedProfiler()
        self.memory_monitor = MemoryMonitor()

        # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        self.baseline_data: dict[str, Any] = {}
        self.optimization_history: list[OptimizationReport] = []

    def capture_baseline(self, name: str, description: str = "") -> dict[str, Any]:
        """æœ€é©åŒ–å‰ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ€§èƒ½ã‚’è¨˜éŒ²

        Args:
            name: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å
            description: èª¬æ˜

        Returns:
            ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿
        """
        print(f"ğŸ“Š Capturing baseline: {name}")

        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¤ãƒ¼ãƒˆã‚’å®Ÿè¡Œ
        benchmark_config = BenchmarkConfig(
            iterations=5,
            warmup_iterations=2,
            enable_profiling=True,
            enable_memory_monitoring=True,
            cache_enabled=False,  # æœ€é©åŒ–å‰ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡åŠ¹
        )

        benchmark_suite = PerformanceBenchmarkSuite(benchmark_config)
        baseline_results = benchmark_suite.run_full_benchmark_suite()

        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰
        baseline_data = {
            "name": name,
            "description": description,
            "timestamp": datetime.now().isoformat(),
            "benchmark_results": baseline_results,
            "system_info": self._capture_system_info(),
        }

        # ä¿å­˜
        self.baseline_data[name] = baseline_data
        baseline_file = self.baseline_dir / f"{name}_baseline.json"
        with open(baseline_file, "w", encoding="utf-8") as f:
            json.dump(baseline_data, f, indent=2, ensure_ascii=False)

        print(f"âœ… Baseline captured and saved to: {baseline_file}")
        return baseline_data

    def measure_optimization_impact(
        self,
        optimization_name: str,
        baseline_name: str,
        description: str = "",
    ) -> OptimizationReport:
        """æœ€é©åŒ–å¾Œã®åŠ¹æœã‚’æ¸¬å®š

        Args:
            optimization_name: æœ€é©åŒ–å
            baseline_name: æ¯”è¼ƒå¯¾è±¡ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å
            description: æœ€é©åŒ–ã®èª¬æ˜

        Returns:
            æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆ
        """
        print(f"ğŸ” Measuring optimization impact: {optimization_name}")

        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        baseline_data = self._load_baseline(baseline_name)
        if not baseline_data:
            raise ValueError(f"Baseline '{baseline_name}' not found")

        # æœ€é©åŒ–å¾Œã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œ
        benchmark_config = BenchmarkConfig(
            iterations=5,
            warmup_iterations=2,
            enable_profiling=True,
            enable_memory_monitoring=True,
            cache_enabled=True,  # æœ€é©åŒ–å¾Œã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æœ‰åŠ¹
        )

        benchmark_suite = PerformanceBenchmarkSuite(benchmark_config)
        optimized_results = benchmark_suite.run_full_benchmark_suite()

        # æ¯”è¼ƒåˆ†æ
        metrics = self._compare_performance(
            baseline_data["benchmark_results"], optimized_results
        )

        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report = OptimizationReport(
            timestamp=datetime.now().isoformat(),
            optimization_name=optimization_name,
            total_improvement_score=self._calculate_total_improvement_score(metrics),
            metrics=metrics,
            performance_summary=self._create_performance_summary(metrics),
            recommendations=self._generate_recommendations(metrics),
            regression_warnings=self._detect_regressions(metrics),
        )

        # å±¥æ­´ã«ä¿å­˜
        self.optimization_history.append(report)

        # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        report_file = (
            self.baseline_dir / f"{optimization_name}_optimization_report.json"
        )
        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(asdict(report), f, indent=2, ensure_ascii=False)

        print(f"ğŸ“ˆ Optimization report saved to: {report_file}")
        return report

    def generate_comprehensive_report(self, optimization_name: str) -> str:
        """åŒ…æ‹¬çš„ãªæœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ

        Args:
            optimization_name: æœ€é©åŒ–å

        Returns:
            ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆæ–‡å­—åˆ—
        """
        # å¯¾å¿œã™ã‚‹ãƒ¬ãƒãƒ¼ãƒˆã‚’æ¤œç´¢
        report = None
        for r in self.optimization_history:
            if r.optimization_name == optimization_name:
                report = r
                break

        if not report:
            return f"Optimization report for '{optimization_name}' not found."

        lines = [
            f"ğŸš€ Optimization Impact Report: {optimization_name}",
            "=" * 80,
            f"Generated: {report.timestamp}",
            f"Total Improvement Score: {report.total_improvement_score:.2f}",
            "",
        ]

        # ä¸»è¦æ”¹å–„ç‚¹
        significant_improvements = report.get_significant_improvements()
        if significant_improvements:
            lines.extend(
                [
                    "ğŸ“ˆ Significant Improvements:",
                    "-" * 40,
                ]
            )
            for metric in significant_improvements:
                lines.append(
                    f"  âœ… {metric.name}: {metric.improvement_percent:.1f}% improvement "
                    f"({metric.before_value:.3f}s â†’ {metric.after_value:.3f}s)"
                )
            lines.append("")

        # ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ
        categories = ["performance", "memory", "cache"]
        for category in categories:
            category_metrics = report.get_metrics_by_category(category)
            if category_metrics:
                lines.extend(
                    [
                        f"ğŸ“Š {category.title()} Metrics:",
                        "-" * 25,
                    ]
                )
                for metric in category_metrics:
                    status = "âœ…" if metric.is_improvement else "âš ï¸"
                    lines.append(
                        f"  {status} {metric.name}: {metric.improvement_percent:+.1f}% "
                        f"({metric.significance} significance)"
                    )
                lines.append("")

        # æ¨å¥¨äº‹é …
        if report.recommendations:
            lines.extend(
                [
                    "ğŸ’¡ Recommendations:",
                    "-" * 20,
                ]
            )
            for rec in report.recommendations:
                lines.append(f"  â€¢ {rec}")
            lines.append("")

        # å›å¸°è­¦å‘Š
        if report.regression_warnings:
            lines.extend(
                [
                    "âš ï¸  Regression Warnings:",
                    "-" * 25,
                ]
            )
            for warning in report.regression_warnings:
                lines.append(f"  âš ï¸  {warning}")
            lines.append("")

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¦ç´„
        summary = report.performance_summary
        lines.extend(
            [
                "ğŸ“‹ Performance Summary:",
                "-" * 25,
                f"  Total Benchmarks: {summary.get('total_benchmarks', 0)}",
                f"  Improved Metrics: {summary.get('improved_metrics', 0)}",
                f"  Degraded Metrics: {summary.get('degraded_metrics', 0)}",
                f"  Stable Metrics: {summary.get('stable_metrics', 0)}",
            ]
        )

        if summary.get("cache_effectiveness"):
            cache_eff = summary["cache_effectiveness"]
            lines.extend(
                [
                    "",
                    "ğŸ’¾ Cache Effectiveness:",
                    "-" * 22,
                    f"  File Cache Hit Rate: {cache_eff.get('file_cache_hit_rate', 0):.1%}",
                    f"  Parse Cache Hit Rate: {cache_eff.get('parse_cache_hit_rate', 0):.1%}",
                    f"  Render Cache Hit Rate: {cache_eff.get('render_cache_hit_rate', 0):.1%}",
                ]
            )

        return "\n".join(lines)

    def compare_optimizations(self, optimization_names: list[str]) -> dict[str, Any]:
        """è¤‡æ•°ã®æœ€é©åŒ–ã‚’æ¯”è¼ƒ

        Args:
            optimization_names: æ¯”è¼ƒã™ã‚‹æœ€é©åŒ–åã®ãƒªã‚¹ãƒˆ

        Returns:
            æ¯”è¼ƒçµæœ
        """
        comparison_data = {  # type: ignore
            "optimizations": {},
            "ranking": [],
            "best_practices": [],
        }

        reports = []
        for name in optimization_names:
            for report in self.optimization_history:
                if report.optimization_name == name:
                    reports.append(report)
                    break

        if not reports:
            return {"error": "No optimization reports found for comparison"}

        # å„æœ€é©åŒ–ã®ã‚¹ã‚³ã‚¢ã‚’æ¯”è¼ƒ
        for report in reports:
            comparison_data["optimizations"][report.optimization_name] = {  # type: ignore
                "total_score": report.total_improvement_score,
                "significant_improvements": len(report.get_significant_improvements()),
                "regression_warnings": len(report.regression_warnings),
            }

        # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ä½œæˆ
        ranking = sorted(
            comparison_data["optimizations"].items(),  # type: ignore
            key=lambda x: x[1]["total_score"],
            reverse=True,
        )
        comparison_data["ranking"] = [name for name, _ in ranking]

        # ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹æŠ½å‡º
        best_practices = set()
        for report in reports:
            if report.total_improvement_score > 10:  # é«˜ã‚¹ã‚³ã‚¢ã®æœ€é©åŒ–ã‹ã‚‰
                for rec in report.recommendations[:2]:  # ä¸Šä½2ã¤ã®æ¨å¥¨äº‹é …
                    best_practices.add(rec)

        comparison_data["best_practices"] = list(best_practices)

        return comparison_data

    def _load_baseline(self, baseline_name: str) -> dict[str, Any] | None:
        """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        if baseline_name in self.baseline_data:
            return self.baseline_data[baseline_name]  # type: ignore

        baseline_file = self.baseline_dir / f"{baseline_name}_baseline.json"
        if baseline_file.exists():
            with open(baseline_file, "r", encoding="utf-8") as f:
                data = json.load(f)
                self.baseline_data[baseline_name] = data
                return data  # type: ignore

        return None

    def _compare_performance(
        self, baseline_results: dict[str, Any], optimized_results: dict[str, Any]
    ) -> list[OptimizationMetrics]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æ¯”è¼ƒ"""
        metrics = []

        # è©³ç´°çµæœã‹ã‚‰æ¯”è¼ƒ
        baseline_detailed = baseline_results.get("detailed_results", [])
        optimized_detailed = optimized_results.get("detailed_results", [])

        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯åã§ãƒãƒƒãƒ”ãƒ³ã‚°
        baseline_map = {result["name"]: result for result in baseline_detailed}
        optimized_map = {result["name"]: result for result in optimized_detailed}

        # å…±é€šã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’æ¯”è¼ƒ
        for name in set(baseline_map.keys()) & set(optimized_map.keys()):
            baseline_result = baseline_map[name]
            optimized_result = optimized_map[name]

            # å®Ÿè¡Œæ™‚é–“ã®æ¯”è¼ƒ
            before_time = baseline_result["avg_time"]
            after_time = optimized_result["avg_time"]

            if before_time > 0:
                improvement_percent = ((before_time - after_time) / before_time) * 100
                improvement_absolute = before_time - after_time

                metrics.append(
                    OptimizationMetrics(
                        name=f"{name}_execution_time",
                        before_value=before_time,
                        after_value=after_time,
                        improvement_percent=improvement_percent,
                        improvement_absolute=improvement_absolute,
                        significance=self._calculate_significance(improvement_percent),
                        category="performance",
                    )
                )

            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æ¯”è¼ƒï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
            baseline_memory = baseline_result.get("memory_usage", {})
            optimized_memory = optimized_result.get("memory_usage", {})

            if baseline_memory and optimized_memory:
                before_memory = baseline_memory.get("peak_mb", 0)
                after_memory = optimized_memory.get("peak_mb", 0)

                if before_memory > 0:
                    memory_improvement = (
                        (before_memory - after_memory) / before_memory
                    ) * 100

                    metrics.append(
                        OptimizationMetrics(
                            name=f"{name}_memory_usage",
                            before_value=before_memory,
                            after_value=after_memory,
                            improvement_percent=memory_improvement,
                            improvement_absolute=before_memory - after_memory,
                            significance=self._calculate_significance(
                                memory_improvement
                            ),
                            category="memory",
                        )
                    )

        return metrics

    def _calculate_significance(self, improvement_percent: float) -> str:
        """æ”¹å–„ã®æœ‰æ„æ€§ã‚’è¨ˆç®—"""
        abs_improvement = abs(improvement_percent)

        if abs_improvement >= 25:
            return "critical"
        elif abs_improvement >= 10:
            return "high"
        elif abs_improvement >= 5:
            return "medium"
        else:
            return "low"

    def _calculate_total_improvement_score(
        self, metrics: list[OptimizationMetrics]
    ) -> float:
        """ç·åˆæ”¹å–„ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        if not metrics:
            return 0.0

        total_score = 0.0
        weight_map = {
            "critical": 4.0,
            "high": 3.0,
            "medium": 2.0,
            "low": 1.0,
        }

        for metric in metrics:
            # æ”¹å–„ã®ã¿ã‚’ã‚¹ã‚³ã‚¢ã«å«ã‚ã‚‹
            if metric.is_improvement:
                weight = weight_map.get(metric.significance, 1.0)
                total_score += metric.improvement_percent * weight

        return total_score

    def _create_performance_summary(
        self, metrics: list[OptimizationMetrics]
    ) -> dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¦ç´„ã‚’ä½œæˆ"""
        improved = [m for m in metrics if m.is_improvement]
        degraded = [
            m for m in metrics if not m.is_improvement and m.improvement_percent < -1
        ]
        stable = [m for m in metrics if -1 <= m.improvement_percent <= 1]

        summary = {
            "total_benchmarks": len(metrics),
            "improved_metrics": len(improved),
            "degraded_metrics": len(degraded),
            "stable_metrics": len(stable),
        }

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹æœã®åˆ†æ
        cache_metrics = [m for m in metrics if "cache" in m.name.lower()]
        if cache_metrics:
            cache_improvements = [m for m in cache_metrics if m.is_improvement]
            summary["cache_effectiveness"] = {  # type: ignore
                "total_cache_metrics": len(cache_metrics),
                "cache_improvements": len(cache_improvements),
                "avg_cache_improvement": (
                    statistics.mean([m.improvement_percent for m in cache_improvements])
                    if cache_improvements
                    else 0
                ),
            }

        return summary

    def _generate_recommendations(
        self, metrics: list[OptimizationMetrics]
    ) -> list[str]:
        """æ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ"""
        recommendations = []

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ã®æ¨å¥¨
        perf_metrics = [m for m in metrics if m.category == "performance"]
        if perf_metrics:
            avg_improvement = statistics.mean(
                [m.improvement_percent for m in perf_metrics if m.is_improvement]
            )
            if avg_improvement > 20:
                recommendations.append(
                    "ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–ãŒéå¸¸ã«åŠ¹æœçš„ã§ã™ã€‚åŒæ§˜ã®æˆ¦ç•¥ã‚’ä»–ã®å‡¦ç†ã«ã‚‚é©ç”¨ã™ã‚‹ã“ã¨ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚"
                )
            elif avg_improvement > 10:
                recommendations.append(
                    "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ãŒç¢ºèªã•ã‚Œã¾ã—ãŸã€‚ã•ã‚‰ãªã‚‹æœ€é©åŒ–ã®ä½™åœ°ãŒã‚ã‚Šã¾ã™ã€‚"
                )

        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æ¨å¥¨
        memory_metrics = [m for m in metrics if m.category == "memory"]
        memory_improvements = [m for m in memory_metrics if m.is_improvement]
        if len(memory_improvements) > len(memory_metrics) * 0.7:
            recommendations.append(
                "ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒåŠ¹æœçš„ã«å‰Šæ¸›ã•ã‚Œã¦ã„ã¾ã™ã€‚ãƒ¡ãƒ¢ãƒªç›£è¦–ã‚’ç¶™ç¶šã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚"
            )

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥é–¢é€£ã®æ¨å¥¨
        cache_metrics = [m for m in metrics if "cache" in m.name.lower()]
        if cache_metrics:
            cache_improvements = [m for m in cache_metrics if m.is_improvement]
            if len(cache_improvements) == len(cache_metrics):
                recommendations.append(
                    "ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥ãŒå…¨é¢çš„ã«åŠ¹æœçš„ã§ã™ã€‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºã¨TTLè¨­å®šã®æœ€é©åŒ–ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚"
                )

        return recommendations

    def _detect_regressions(self, metrics: list[OptimizationMetrics]) -> list[str]:
        """å›å¸°ã‚’æ¤œå‡º"""
        warnings = []

        for metric in metrics:
            if (
                not metric.is_improvement and metric.improvement_percent < -5
            ):  # 5%ä»¥ä¸Šã®åŠ£åŒ–
                severity = "é‡å¤§" if metric.improvement_percent < -15 else "è»½å¾®"
                warnings.append(
                    f"{severity}ãªå›å¸°: {metric.name} ãŒ {abs(metric.improvement_percent):.1f}% åŠ£åŒ–"
                )

        return warnings

    def _capture_system_info(self) -> dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã‚’è¨˜éŒ²"""
        import platform
        import sys

        system_info: dict[str, Any] = {
            "platform": platform.platform(),
            "python_version": sys.version,
            "cpu_count": None,
            "memory_total": None,
        }

        try:
            import psutil

            system_info["cpu_count"] = psutil.cpu_count()
            system_info["memory_total"] = psutil.virtual_memory().total
        except ImportError:
            pass

        return system_info

    def cleanup_old_data(self, days_old: int = 30):  # type: ignore
        """å¤ã„ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—

        Args:
            days_old: å‰Šé™¤å¯¾è±¡ã®æ—¥æ•°
        """
        cutoff_date = datetime.now() - timedelta(days=days_old)

        # å¤ã„ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        for baseline_file in self.baseline_dir.glob("*_baseline.json"):
            if baseline_file.stat().st_mtime < cutoff_date.timestamp():
                baseline_file.unlink()
                print(f"ğŸ—‘ï¸  Deleted old baseline: {baseline_file}")

        # å¤ã„ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        for report_file in self.baseline_dir.glob("*_optimization_report.json"):
            if report_file.stat().st_mtime < cutoff_date.timestamp():
                report_file.unlink()
                print(f"ğŸ—‘ï¸  Deleted old report: {report_file}")

    def export_optimization_summary(self, output_file: Path):  # type: ignore
        """æœ€é©åŒ–ã®è¦ç´„ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ

        Args:
            output_file: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        summary_data = {
            "export_timestamp": datetime.now().isoformat(),
            "total_optimizations": len(self.optimization_history),
            "optimization_reports": [
                asdict(report) for report in self.optimization_history
            ],
            "baseline_data": self.baseline_data,
        }

        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(summary_data, f, indent=2, ensure_ascii=False)

        print(f"ğŸ“¤ Optimization summary exported to: {output_file}")
