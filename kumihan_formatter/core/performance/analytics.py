"""
TokenåŠ¹ç‡æ€§åˆ†æã‚·ã‚¹ãƒ†ãƒ 
Issue #813å¯¾å¿œ - performance_metrics.pyã‹ã‚‰åˆ†é›¢
"""

from collections import defaultdict, deque
from dataclasses import dataclass
from statistics import mean
from typing import Dict, List, Optional

from ..utilities.logger import get_logger


@dataclass
class TokenEfficiencyMetrics:
    """TokenåŠ¹ç‡æ€§ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""

    tokens_per_second: float
    tokens_per_mb_memory: float
    efficiency_score: float
    trend_direction: str
    baseline_comparison: float
    pattern_efficiency: Dict[str, float]


@dataclass
class InefficiencyPattern:
    """éåŠ¹ç‡æ€§ãƒ‘ã‚¿ãƒ¼ãƒ³"""

    pattern_name: str
    frequency: int
    impact_score: float
    suggested_optimization: str
    affected_operations: List[str]


class TokenEfficiencyAnalyzer:
    """
    TokenåŠ¹ç‡æ€§åˆ†æã‚·ã‚¹ãƒ†ãƒ 

    æ©Ÿèƒ½:
    - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ TokenåŠ¹ç‡ç›£è¦–
    - åŠ¹ç‡æ€§ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
    - åŠ¹ç‡æ€§å‚¾å‘åˆ†æ
    - ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¯”è¼ƒ
    """

    def __init__(self, monitoring_window_size: int = 100):
        self.logger = get_logger(__name__)
        self.monitoring_window_size = monitoring_window_size

        # åŠ¹ç‡æ€§ãƒ‡ãƒ¼ã‚¿å±¥æ­´
        self.efficiency_history: deque[TokenEfficiencyMetrics] = deque(
            maxlen=monitoring_window_size
        )
        self.baseline_efficiency: Optional[float] = None

        # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥åŠ¹ç‡æ€§è¿½è·¡
        self.pattern_efficiencies: Dict[str, List[float]] = defaultdict(list)

        # åŠ¹ç‡æ€§é–¾å€¤è¨­å®š
        self.efficiency_thresholds = {
            "excellent": 0.9,
            "good": 0.7,
            "acceptable": 0.5,
            "poor": 0.3,
        }

        self.logger.info("TokenEfficiencyAnalyzer initialized")

    def analyze_efficiency(
        self,
        tokens_used: int,
        processing_time: float,
        memory_mb: float,
        operation_pattern: str = "default",
    ) -> TokenEfficiencyMetrics:
        """TokenåŠ¹ç‡æ€§ã‚’åˆ†æ"""

        # åŸºæœ¬åŠ¹ç‡æ€§ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        tokens_per_second = tokens_used / processing_time if processing_time > 0 else 0
        tokens_per_mb_memory = tokens_used / memory_mb if memory_mb > 0 else 0

        # åŠ¹ç‡æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆ0-1æ­£è¦åŒ–ï¼‰
        efficiency_score = self._calculate_efficiency_score(
            tokens_per_second, tokens_per_mb_memory, tokens_used
        )

        # å‚¾å‘åˆ†æ
        trend_direction = self._analyze_trend()

        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¯”è¼ƒ
        baseline_comparison = self._compare_to_baseline(efficiency_score)

        # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥åŠ¹ç‡æ€§æ›´æ–°
        self.pattern_efficiencies[operation_pattern].append(efficiency_score)
        pattern_efficiency = {
            pattern: mean(scores[-10:]) if scores else 0.0
            for pattern, scores in self.pattern_efficiencies.items()
        }

        metrics = TokenEfficiencyMetrics(
            tokens_per_second=tokens_per_second,
            tokens_per_mb_memory=tokens_per_mb_memory,
            efficiency_score=efficiency_score,
            trend_direction=trend_direction,
            baseline_comparison=baseline_comparison,
            pattern_efficiency=pattern_efficiency,
        )

        # å±¥æ­´ã«è¿½åŠ 
        self.efficiency_history.append(metrics)

        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è¨­å®šï¼ˆåˆå›ï¼‰
        if self.baseline_efficiency is None:
            self.baseline_efficiency = efficiency_score

        self.logger.debug(
            f"Efficiency analysis: score={efficiency_score:.3f}, trend={trend_direction}"
        )
        return metrics

    def _calculate_efficiency_score(
        self, tokens_per_second: float, tokens_per_mb_memory: float, total_tokens: int
    ) -> float:
        """åŠ¹ç‡æ€§ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        # æ­£è¦åŒ–ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼ï¼ˆå®Ÿéš›ã®é‹ç”¨ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦èª¿æ•´ï¼‰
        time_efficiency = min(tokens_per_second / 1000, 1.0)  # 1000 tokens/sec ãŒæœ€å¤§
        memory_efficiency = min(
            tokens_per_mb_memory / 10000, 1.0
        )  # 10000 tokens/MB ãŒæœ€å¤§

        # Tokenä½¿ç”¨é‡åŠ¹ç‡æ€§ï¼ˆå°‘ãªã„ã»ã©è‰¯ã„ï¼‰
        token_efficiency = max(0, 1.0 - (total_tokens / 50000))  # 50000ãƒˆãƒ¼ã‚¯ãƒ³ã‚’åŸºæº–

        # é‡ã¿ä»˜ãçµ±åˆã‚¹ã‚³ã‚¢
        weighted_score = (
            time_efficiency * 0.4 + memory_efficiency * 0.3 + token_efficiency * 0.3
        )

        return min(max(weighted_score, 0.0), 1.0)

    def _analyze_trend(self) -> str:
        """åŠ¹ç‡æ€§ã®å‚¾å‘ã‚’åˆ†æ"""
        if len(self.efficiency_history) < 3:
            return "insufficient_data"

        recent_scores = [m.efficiency_score for m in list(self.efficiency_history)[-5:]]
        if len(recent_scores) < 2:
            return "insufficient_data"

        # ç°¡æ˜“ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
        trend_sum = sum(
            recent_scores[i] - recent_scores[i - 1]
            for i in range(1, len(recent_scores))
        )

        if trend_sum > 0.05:
            return "improving"
        elif trend_sum < -0.05:
            return "degrading"
        else:
            return "stable"

    def _compare_to_baseline(self, current_score: float) -> float:
        """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã®æ¯”è¼ƒ"""
        if self.baseline_efficiency is None:
            return 0.0

        return (current_score - self.baseline_efficiency) / self.baseline_efficiency

    def get_efficiency_summary(self) -> Dict:
        """åŠ¹ç‡æ€§æ¦‚è¦ã‚’å–å¾—"""
        if not self.efficiency_history:
            return {"status": "no_data"}

        recent_scores = [m.efficiency_score for m in self.efficiency_history]
        current_score = recent_scores[-1] if recent_scores else 0.0

        # åŠ¹ç‡æ€§ãƒ¬ãƒ™ãƒ«åˆ¤å®š
        efficiency_level = "poor"
        for level, threshold in sorted(
            self.efficiency_thresholds.items(), key=lambda x: x[1], reverse=True
        ):
            if current_score >= threshold:
                efficiency_level = level
                break

        return {
            "current_score": current_score,
            "efficiency_level": efficiency_level,
            "average_score": mean(recent_scores),
            "trend": self._analyze_trend(),
            "baseline_comparison_percent": self._compare_to_baseline(current_score)
            * 100,
            "pattern_efficiencies": {
                pattern: mean(scores[-5:]) if scores else 0.0
                for pattern, scores in self.pattern_efficiencies.items()
            },
        }


class PatternDetector:
    """
    éåŠ¹ç‡æ€§ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ 

    æ©Ÿèƒ½:
    - å®Ÿè¡Œæ™‚éåŠ¹ç‡æ€§ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è‡ªå‹•æ¤œå‡º
    - ãƒ‘ã‚¿ãƒ¼ãƒ³é »åº¦åˆ†æ
    - æœ€é©åŒ–æ¨å¥¨ã®ç”Ÿæˆ
    - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã®ç‰¹å®š
    """

    def __init__(self):
        self.logger = get_logger(__name__)

        # ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºå±¥æ­´
        self.detected_patterns: List[InefficiencyPattern] = []

        # ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºãƒ«ãƒ¼ãƒ«
        self.detection_rules = {
            "high_token_usage": {
                "threshold": 10000,
                "description": "é«˜Tokenä½¿ç”¨é‡",
                "optimization": "å‡¦ç†å˜ä½ã®åˆ†å‰²ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ´»ç”¨ã‚’æ¤œè¨",
            },
            "slow_processing": {
                "threshold": 30.0,  # ç§’
                "description": "å‡¦ç†æ™‚é–“ãŒé•·ã„",
                "optimization": "ä¸¦åˆ—å‡¦ç†ã€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ æœ€é©åŒ–ã‚’æ¤œè¨",
            },
            "high_memory_usage": {
                "threshold": 500.0,  # MB
                "description": "é«˜ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡",
                "optimization": "ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªãƒ‡ãƒ¼ã‚¿æ§‹é€ ã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ã‚’æ¤œè¨",
            },
            "frequent_gc": {
                "threshold": 10,  # å›æ•°
                "description": "é »ç¹ãªã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³",
                "optimization": "ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆç”Ÿæˆã®å‰Šæ¸›ã€ãƒ—ãƒ¼ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³é©ç”¨ã‚’æ¤œè¨",
            },
        }

        self.logger.info("PatternDetector initialized")

    def detect_patterns(
        self,
        tokens_used: int,
        processing_time: float,
        memory_mb: float,
        gc_count: int = 0,
        operation_name: str = "unknown",
    ) -> List[InefficiencyPattern]:
        """éåŠ¹ç‡æ€§ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º"""
        patterns = []

        # é«˜Tokenä½¿ç”¨é‡ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
        if tokens_used > self.detection_rules["high_token_usage"]["threshold"]:
            pattern = InefficiencyPattern(
                pattern_name="high_token_usage",
                frequency=1,
                impact_score=min(tokens_used / 50000, 1.0),
                suggested_optimization=self.detection_rules["high_token_usage"][
                    "optimization"
                ],
                affected_operations=[operation_name],
            )
            patterns.append(pattern)

        # ä½é€Ÿå‡¦ç†ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
        if processing_time > self.detection_rules["slow_processing"]["threshold"]:
            pattern = InefficiencyPattern(
                pattern_name="slow_processing",
                frequency=1,
                impact_score=min(processing_time / 60, 1.0),
                suggested_optimization=self.detection_rules["slow_processing"][
                    "optimization"
                ],
                affected_operations=[operation_name],
            )
            patterns.append(pattern)

        # é«˜ãƒ¡ãƒ¢ãƒªä½¿ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
        if memory_mb > self.detection_rules["high_memory_usage"]["threshold"]:
            pattern = InefficiencyPattern(
                pattern_name="high_memory_usage",
                frequency=1,
                impact_score=min(memory_mb / 1000, 1.0),
                suggested_optimization=self.detection_rules["high_memory_usage"][
                    "optimization"
                ],
                affected_operations=[operation_name],
            )
            patterns.append(pattern)

        # é »ç¹GCãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
        if gc_count > self.detection_rules["frequent_gc"]["threshold"]:
            pattern = InefficiencyPattern(
                pattern_name="frequent_gc",
                frequency=gc_count,
                impact_score=min(gc_count / 50, 1.0),
                suggested_optimization=self.detection_rules["frequent_gc"][
                    "optimization"
                ],
                affected_operations=[operation_name],
            )
            patterns.append(pattern)

        # æ¤œå‡ºã—ãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å±¥æ­´ã«è¿½åŠ 
        for pattern in patterns:
            self._update_pattern_history(pattern)

        if patterns:
            self.logger.info(
                f"Detected {len(patterns)} inefficiency patterns for operation: {operation_name}"
            )

        return patterns

    def _update_pattern_history(self, new_pattern: InefficiencyPattern):
        """ãƒ‘ã‚¿ãƒ¼ãƒ³å±¥æ­´ã‚’æ›´æ–°"""
        # æ—¢å­˜ã®åŒã˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œç´¢
        existing_pattern = None
        for pattern in self.detected_patterns:
            if pattern.pattern_name == new_pattern.pattern_name:
                existing_pattern = pattern
                break

        if existing_pattern:
            # æ—¢å­˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã®é »åº¦ã‚’æ›´æ–°
            existing_pattern.frequency += new_pattern.frequency
            existing_pattern.impact_score = max(
                existing_pattern.impact_score, new_pattern.impact_score
            )
            existing_pattern.affected_operations.extend(new_pattern.affected_operations)
            existing_pattern.affected_operations = list(
                set(existing_pattern.affected_operations)
            )
        else:
            # æ–°ã—ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¿½åŠ 
            self.detected_patterns.append(new_pattern)

    def get_pattern_summary(self) -> Dict:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºæ¦‚è¦ã‚’å–å¾—"""
        if not self.detected_patterns:
            return {"total_patterns": 0, "patterns": []}

        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å½±éŸ¿åº¦ã§ã‚½ãƒ¼ãƒˆ
        sorted_patterns = sorted(
            self.detected_patterns, key=lambda x: x.impact_score, reverse=True
        )

        pattern_summary = []
        for pattern in sorted_patterns[:10]:  # ä¸Šä½10ãƒ‘ã‚¿ãƒ¼ãƒ³
            pattern_summary.append(
                {
                    "name": pattern.pattern_name,
                    "frequency": pattern.frequency,
                    "impact_score": pattern.impact_score,
                    "optimization": pattern.suggested_optimization,
                    "affected_operations": pattern.affected_operations,
                }
            )

        return {
            "total_patterns": len(self.detected_patterns),
            "patterns": pattern_summary,
        }

    def generate_optimization_recommendations(self) -> List[str]:
        """æœ€é©åŒ–æ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ"""
        if not self.detected_patterns:
            return ["ç¾åœ¨ã€æœ€é©åŒ–ãŒå¿…è¦ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã¯æ¤œå‡ºã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"]

        recommendations = []

        # é«˜å½±éŸ¿åº¦ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰æ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ
        high_impact_patterns = [
            p for p in self.detected_patterns if p.impact_score > 0.5
        ]

        if high_impact_patterns:
            recommendations.append("ğŸš¨ é«˜å„ªå…ˆåº¦ã®æœ€é©åŒ–é …ç›®:")
            for pattern in sorted(
                high_impact_patterns, key=lambda x: x.impact_score, reverse=True
            ):
                recommendations.append(
                    f"  - {pattern.suggested_optimization} "
                    f"(å½±éŸ¿åº¦: {pattern.impact_score:.2f}, é »åº¦: {pattern.frequency})"
                )

        # é »å‡ºãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰æ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ
        frequent_patterns = [p for p in self.detected_patterns if p.frequency > 5]

        if frequent_patterns:
            recommendations.append("\nğŸ”„ é »å‡ºãƒ‘ã‚¿ãƒ¼ãƒ³ã®æœ€é©åŒ–:")
            for pattern in sorted(
                frequent_patterns, key=lambda x: x.frequency, reverse=True
            ):
                recommendations.append(
                    f"  - {pattern.suggested_optimization} "
                    f"(é »åº¦: {pattern.frequency}å›)"
                )

        if not recommendations:
            recommendations.append("å®šæœŸçš„ãªæœ€é©åŒ–ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ç¶™ç¶šã—ã¦ãã ã•ã„ã€‚")

        return recommendations
