"""
çµ±åˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¤ãƒ¼ãƒˆ - åˆ†å‰²ã•ã‚ŒãŸã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®çµ±åˆ

åˆ†å‰²ã•ã‚ŒãŸãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’çµ±åˆã—ã€
å…ƒã®PerformanceBenchmarkSuiteã‚¯ãƒ©ã‚¹ã¨åŒç­‰ã®æ©Ÿèƒ½ã‚’æä¾›
Issue #476å¯¾å¿œ - ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºåˆ¶é™éµå®ˆ
"""

import json
import time
from dataclasses import asdict
from pathlib import Path
from typing import Any

from ..caching.file_cache import FileCache
from ..caching.parse_cache import ParseCache
from ..caching.render_cache import RenderCache
from ..performance import get_global_monitor
from ..utilities.logger import get_logger
from .benchmark_analyzer import BenchmarkAnalyzer
from .benchmark_runner import BenchmarkRunner
from .benchmark_types import BenchmarkConfig, BenchmarkResult, DEFAULT_BENCHMARK_CONFIG
from .memory_monitor import MemoryMonitor
from .profiler import AdvancedProfiler


class PerformanceBenchmarkSuite:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç·åˆã‚¹ã‚¤ãƒ¼ãƒˆ

    æ©Ÿèƒ½:
    - ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚ã‚Š/ãªã—ã®æ€§èƒ½æ¯”è¼ƒ
    - ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æ¸¬å®š
    - ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°çµ±åˆ
    - å›å¸°æ¤œå‡º
    - ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¯”è¼ƒ
    """

    def __init__(self, config: BenchmarkConfig | None = None) -> None:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¤ãƒ¼ãƒˆã‚’åˆæœŸåŒ–

        Args:
            config: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è¨­å®š
        """
        self.logger = get_logger(__name__)
        self.config = config or DEFAULT_BENCHMARK_CONFIG
        self.logger.info(
            f"PerformanceBenchmarkSuiteåˆæœŸåŒ–: iterations={self.config.iterations}, "
            f"warmup={self.config.warmup_iterations}"
        )

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®šãƒ„ãƒ¼ãƒ«
        self.monitor = get_global_monitor()
        self.profiler = AdvancedProfiler() if self.config.enable_profiling else None
        self.memory_monitor = (
            MemoryMonitor() if self.config.enable_memory_monitoring else None
        )

        self.logger.debug(
            f"profiling={self.config.enable_profiling}, "
            f"memory_monitoring={self.config.enable_memory_monitoring}"
        )

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ 
        self.file_cache = FileCache() if self.config.cache_enabled else None
        self.parse_cache = ParseCache() if self.config.cache_enabled else None
        self.render_cache = RenderCache() if self.config.cache_enabled else None

        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
        self.runner = BenchmarkRunner(
            self.config,
            self.file_cache,
            self.parse_cache,
            self.render_cache,
            self.memory_monitor,
            self.profiler,
        )
        self.analyzer = BenchmarkAnalyzer()

        # çµæœä¿å­˜
        self.results: list[BenchmarkResult] = []
        self.baseline_results: dict[str, BenchmarkResult] | None = None

        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’èª­ã¿è¾¼ã¿
        if self.config.baseline_file and self.config.baseline_file.exists():
            self.load_baseline(self.config.baseline_file)

        self.logger.info("PerformanceBenchmarkSuiteåˆæœŸåŒ–å®Œäº†")

    def run_full_benchmark_suite(self) -> dict[str, Any]:
        """å®Œå…¨ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¤ãƒ¼ãƒˆã‚’å®Ÿè¡Œ"""
        self.logger.info("ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¤ãƒ¼ãƒˆå®Ÿè¡Œé–‹å§‹")
        print("ğŸš€ Starting Performance Benchmark Suite...")
        print("=" * 60)

        # ãƒ¡ãƒ¢ãƒªç›£è¦–é–‹å§‹
        if self.memory_monitor:
            self.memory_monitor.start_monitoring()
            self.logger.debug("ãƒ¡ãƒ¢ãƒªç›£è¦–é–‹å§‹")

        try:
            # 1. ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
            self.logger.info("ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹")
            print("\nğŸ“ File Reading Benchmarks:")
            file_results = self.runner.run_file_benchmarks()
            self.results.extend(file_results)

            # 2. ãƒ‘ãƒ¼ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
            self.logger.info("ãƒ‘ãƒ¼ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹")
            print("\nğŸ” Parsing Benchmarks:")
            parse_results = self.runner.run_parse_benchmarks()
            self.results.extend(parse_results)

            # 3. ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
            self.logger.info("ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹")
            print("\nğŸ¨ Rendering Benchmarks:")
            render_results = self.runner.run_render_benchmarks()
            self.results.extend(render_results)

            # 4. çµ±åˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
            self.logger.info("ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹")
            print("\nğŸ”„ End-to-End Benchmarks:")
            e2e_results = self.runner.run_e2e_benchmarks()
            self.results.extend(e2e_results)

            # 5. ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
            self.logger.info("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆé–‹å§‹")
            print("\nğŸ’¾ Cache Performance Tests:")
            cache_results = self.runner.run_cache_benchmarks()
            self.results.extend(cache_results)

        finally:
            # ãƒ¡ãƒ¢ãƒªç›£è¦–åœæ­¢
            if self.memory_monitor:
                self.memory_monitor.stop_monitoring()
                self.logger.debug("ãƒ¡ãƒ¢ãƒªç›£è¦–åœæ­¢")

        # çµæœåˆ†æ
        analysis = self.analyzer.analyze_results(self.results)
        self.logger.info(
            f"ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¤ãƒ¼ãƒˆå®Œäº†: {len(self.results)}å€‹ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œ"
        )

        print("\nğŸ“Š Benchmark Complete!")
        print("=" * 60)

        return analysis

    def run_regression_test(self) -> dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
        if not self.baseline_results:
            error_msg = "No baseline results available for regression testing"
            self.logger.error(error_msg)
            return {"error": error_msg}

        self.logger.info("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°ãƒ†ã‚¹ãƒˆé–‹å§‹")
        print("ğŸ” Running Performance Regression Tests...")

        # ä¸»è¦ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œ
        current_results = {}

        # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å›å¸°ãƒ†ã‚¹ãƒˆ
        current_results["file_reading_medium"] = self.runner.benchmark_file_reading()

        # ãƒ‘ãƒ¼ã‚¹å›å¸°ãƒ†ã‚¹ãƒˆ
        current_results["parsing_basic"] = self.runner.benchmark_parsing()

        # ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°å›å¸°ãƒ†ã‚¹ãƒˆ
        current_results["rendering_basic"] = self.runner.benchmark_rendering()

        # å›å¸°åˆ†æ
        regression_analysis = self.analyzer.analyze_regression(
            current_results, self.baseline_results
        )
        self.logger.info(
            f"å›å¸°ãƒ†ã‚¹ãƒˆå®Œäº†: {len(regression_analysis.get('regressions_detected', []))}å€‹ã®å›å¸°ã‚’æ¤œå‡º"
        )

        return regression_analysis

    def save_baseline(self, output_file: Path) -> None:
        """ç¾åœ¨ã®çµæœã‚’ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã—ã¦ä¿å­˜

        Args:
            output_file: ä¿å­˜å…ˆãƒ•ã‚¡ã‚¤ãƒ«
        """
        if not self.results:
            self.logger.warning("ä¿å­˜ã™ã‚‹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœãŒã‚ã‚Šã¾ã›ã‚“")
            print("âš ï¸  No results to save as baseline")
            return

        baseline_data = {
            "timestamp": time.time(),
            "config": asdict(self.config),
            "results": {result.name: asdict(result) for result in self.results},
        }

        try:
            output_file.parent.mkdir(parents=True, exist_ok=True)
            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(baseline_data, f, indent=2, ensure_ascii=False)

            self.logger.info(f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ä¿å­˜å®Œäº†: {output_file}")
            print(f"ğŸ’¾ Baseline saved to: {output_file}")
        except Exception as e:
            self.logger.error(f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def load_baseline(self, baseline_file: Path) -> Any:
        """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³çµæœã‚’èª­ã¿è¾¼ã¿

        Args:
            baseline_file: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«
        """
        try:
            with open(baseline_file, "r", encoding="utf-8") as f:
                baseline_data = json.load(f)

            self.baseline_results = {}
            for name, result_data in baseline_data["results"].items():
                self.baseline_results[name] = BenchmarkResult(**result_data)

            self.logger.info(
                f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³èª­ã¿è¾¼ã¿å®Œäº†: {baseline_file}, {len(self.baseline_results)}å€‹ã®çµæœ"
            )
            print(f"ğŸ“¥ Baseline loaded from: {baseline_file}")

        except Exception as e:
            self.logger.error(f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            print(f"âš ï¸  Failed to load baseline: {e}")

    def generate_report(self) -> dict[str, Any]:
        """åŒ…æ‹¬çš„ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        if not self.results:
            return {"error": "No benchmark results available"}

        # åŸºæœ¬åˆ†æ
        analysis = self.analyzer.analyze_results(self.results)
        
        # ã‚µãƒãƒªãƒ¼ç”Ÿæˆ
        summary = self.analyzer.generate_benchmark_summary(self.results)
        
        # å›å¸°åˆ†æï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãŒã‚ã‚‹å ´åˆï¼‰
        regression_analysis = None
        if self.baseline_results:
            current_results = {r.name: r for r in self.results}
            regression_analysis = self.analyzer.analyze_regression(
                current_results, self.baseline_results
            )

        report = {
            "summary": asdict(summary),
            "detailed_analysis": analysis,
            "regression_analysis": regression_analysis,
            "config": asdict(self.config),
            "timestamp": time.time(),
        }

        self.logger.info("ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†")
        return report

    def clear_results(self) -> None:
        """çµæœã‚’ã‚¯ãƒªã‚¢"""
        cleared_count = len(self.results)
        self.results.clear()
        self.logger.info(f"ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚’ã‚¯ãƒªã‚¢: {cleared_count}å€‹")

    # ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚¢ã‚¯ã‚»ã‚¹ï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰
    @property
    def latest_results(self) -> list[BenchmarkResult]:
        """æœ€æ–°ã®çµæœãƒªã‚¹ãƒˆã¸ã®ã‚¢ã‚¯ã‚»ã‚¹"""
        return self.results

    @property
    def has_baseline(self) -> bool:
        """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        return self.baseline_results is not None

    # çµ±è¨ˆã‚¢ã‚¯ã‚»ã‚¹
    def get_performance_summary(self) -> dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚µãƒãƒªãƒ¼ã‚’å–å¾—"""
        if not self.results:
            return {"error": "No results available"}

        summary = self.analyzer.generate_benchmark_summary(self.results)
        return {
            "total_benchmarks": summary.total_benchmarks,
            "total_runtime": summary.total_runtime,
            "performance_score": summary.performance_score,
            "fastest_benchmark": {
                "name": summary.fastest_benchmark.name,
                "time": summary.fastest_benchmark.avg_time,
            },
            "slowest_benchmark": {
                "name": summary.slowest_benchmark.name,
                "time": summary.slowest_benchmark.avg_time,
            },
            "memory_peak_mb": summary.memory_peak,
            "cache_hit_rate": summary.cache_hit_rate,
        }