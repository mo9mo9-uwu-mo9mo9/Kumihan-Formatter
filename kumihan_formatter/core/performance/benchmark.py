"""
ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š

ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–ã®åŠ¹æœæ¸¬å®šã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°æ¤œå‡º
Issue #402å¯¾å¿œ - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
"""

import json
import statistics
import time
from dataclasses import asdict, dataclass
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional

from ...utilities.logger import get_logger
from ..caching.file_cache import FileCache
from ..caching.parse_cache import ParseCache
from ..caching.render_cache import RenderCache
from ..performance import get_global_monitor
from .memory_monitor import MemoryMonitor
from .profiler import AdvancedProfiler


@dataclass
class BenchmarkResult:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ"""

    name: str
    iterations: int
    total_time: float
    avg_time: float
    min_time: float
    max_time: float
    std_dev: float
    memory_usage: Dict[str, float]
    cache_stats: Dict[str, Any]
    throughput: Optional[float] = None
    regression_score: Optional[float] = None


@dataclass
class BenchmarkConfig:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è¨­å®š"""

    iterations: int = 5
    warmup_iterations: int = 2
    enable_profiling: bool = True
    enable_memory_monitoring: bool = True
    cache_enabled: bool = True
    baseline_file: Optional[Path] = None


class PerformanceBenchmarkSuite:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç·åˆã‚¹ã‚¤ãƒ¼ãƒˆ

    æ©Ÿèƒ½:
    - ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚ã‚Š/ãªã—ã®æ€§èƒ½æ¯”è¼ƒ
    - ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æ¸¬å®š
    - ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°çµ±åˆ
    - å›å¸°æ¤œå‡º
    - ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¯”è¼ƒ
    """

    def __init__(self, config: BenchmarkConfig = None):
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¤ãƒ¼ãƒˆã‚’åˆæœŸåŒ–

        Args:
            config: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è¨­å®š
        """
        self.logger = get_logger(__name__)
        self.config = config or BenchmarkConfig()
        self.logger.info(
            f"PerformanceBenchmarkSuiteåˆæœŸåŒ–: iterations={self.config.iterations}, warmup={self.config.warmup_iterations}"
        )

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®šãƒ„ãƒ¼ãƒ«
        self.monitor = get_global_monitor()
        self.profiler = AdvancedProfiler() if self.config.enable_profiling else None
        self.memory_monitor = (
            MemoryMonitor() if self.config.enable_memory_monitoring else None
        )

        self.logger.debug(
            f"profiling={self.config.enable_profiling}, memory_monitoring={self.config.enable_memory_monitoring}"
        )

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ 
        self.file_cache = FileCache() if self.config.cache_enabled else None
        self.parse_cache = ParseCache() if self.config.cache_enabled else None
        self.render_cache = RenderCache() if self.config.cache_enabled else None

        # çµæœä¿å­˜
        self.results: List[BenchmarkResult] = []
        self.baseline_results: Optional[Dict[str, BenchmarkResult]] = None

        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’èª­ã¿è¾¼ã¿
        if self.config.baseline_file and self.config.baseline_file.exists():
            self.load_baseline(self.config.baseline_file)

        self.logger.info("PerformanceBenchmarkSuiteåˆæœŸåŒ–å®Œäº†")

    def run_full_benchmark_suite(self) -> Dict[str, Any]:
        """å®Œå…¨ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¤ãƒ¼ãƒˆã‚’å®Ÿè¡Œ"""
        self.logger.info("ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¤ãƒ¼ãƒˆå®Ÿè¡Œé–‹å§‹")
        print("ğŸš€ Starting Performance Benchmark Suite...")
        print("=" * 60)

        # ãƒ¡ãƒ¢ãƒªç›£è¦–é–‹å§‹
        if self.memory_monitor:
            self.memory_monitor.start_monitoring()
            self.logger.debug("ãƒ¡ãƒ¢ãƒªç›£è¦–é–‹å§‹")

        try:
            # 1. ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
            self.logger.info("ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹")
            print("\nğŸ“ File Reading Benchmarks:")
            self._run_file_benchmarks()

            # 2. ãƒ‘ãƒ¼ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
            self.logger.info("ãƒ‘ãƒ¼ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹")
            print("\nğŸ” Parsing Benchmarks:")
            self._run_parse_benchmarks()

            # 3. ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
            self.logger.info("ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹")
            print("\nğŸ¨ Rendering Benchmarks:")
            self._run_render_benchmarks()

            # 4. çµ±åˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
            self.logger.info("ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹")
            print("\nğŸ”„ End-to-End Benchmarks:")
            self._run_e2e_benchmarks()

            # 5. ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
            self.logger.info("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆé–‹å§‹")
            print("\nğŸ’¾ Cache Performance Tests:")
            self._run_cache_benchmarks()

        finally:
            # ãƒ¡ãƒ¢ãƒªç›£è¦–åœæ­¢
            if self.memory_monitor:
                self.memory_monitor.stop_monitoring()
                self.logger.debug("ãƒ¡ãƒ¢ãƒªç›£è¦–åœæ­¢")

        # çµæœåˆ†æ
        analysis = self._analyze_results()
        self.logger.info(
            f"ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¤ãƒ¼ãƒˆå®Œäº†: {len(self.results)}å€‹ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œ"
        )

        print("\nğŸ“Š Benchmark Complete!")
        print("=" * 60)

        return analysis

    def run_regression_test(self) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
        if not self.baseline_results:
            error_msg = "No baseline results available for regression testing"
            self.logger.error(error_msg)
            return {"error": error_msg}

        self.logger.info("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°ãƒ†ã‚¹ãƒˆé–‹å§‹")
        print("ğŸ” Running Performance Regression Tests...")

        # ä¸»è¦ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œ
        current_results = {}

        # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å›å¸°ãƒ†ã‚¹ãƒˆ
        current_results["file_reading"] = self._benchmark_file_reading()

        # ãƒ‘ãƒ¼ã‚¹å›å¸°ãƒ†ã‚¹ãƒˆ
        current_results["parsing"] = self._benchmark_parsing()

        # ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°å›å¸°ãƒ†ã‚¹ãƒˆ
        current_results["rendering"] = self._benchmark_rendering()

        # å›å¸°åˆ†æ
        regression_analysis = self._analyze_regression(current_results)
        self.logger.info(
            f"å›å¸°ãƒ†ã‚¹ãƒˆå®Œäº†: {len(regression_analysis.get('regressions_detected', []))}å€‹ã®å›å¸°ã‚’æ¤œå‡º"
        )

        return regression_analysis

    def save_baseline(self, output_file: Path):
        """ç¾åœ¨ã®çµæœã‚’ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã—ã¦ä¿å­˜

        Args:
            output_file: ä¿å­˜å…ˆãƒ•ã‚¡ã‚¤ãƒ«
        """
        if not self.results:
            self.logger.warning("ä¿å­˜ã™ã‚‹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœãŒã‚ã‚Šã¾ã›ã‚“")
            print("âš ï¸  No results to save as baseline")
            return

        baseline_data = {
            "timestamp": time.time(),
            "config": asdict(self.config),
            "results": {result.name: asdict(result) for result in self.results},
        }

        try:
            output_file.parent.mkdir(parents=True, exist_ok=True)
            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(baseline_data, f, indent=2, ensure_ascii=False)

            self.logger.info(f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ä¿å­˜å®Œäº†: {output_file}")
            print(f"ğŸ’¾ Baseline saved to: {output_file}")
        except Exception as e:
            self.logger.error(f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def load_baseline(self, baseline_file: Path):
        """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³çµæœã‚’èª­ã¿è¾¼ã¿

        Args:
            baseline_file: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«
        """
        try:
            with open(baseline_file, "r", encoding="utf-8") as f:
                baseline_data = json.load(f)

            self.baseline_results = {}
            for name, result_data in baseline_data["results"].items():
                self.baseline_results[name] = BenchmarkResult(**result_data)

            self.logger.info(
                f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³èª­ã¿è¾¼ã¿å®Œäº†: {baseline_file}, {len(self.baseline_results)}å€‹ã®çµæœ"
            )
            print(f"ğŸ“¥ Baseline loaded from: {baseline_file}")

        except Exception as e:
            self.logger.error(f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            print(f"âš ï¸  Failed to load baseline: {e}")

    def _run_file_benchmarks(self):
        """ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        # å°ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        result = self._benchmark_file_reading(file_size="small")
        self.results.append(result)
        print(f"  Small files: {result.avg_time:.3f}s avg")

        # å¤§ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        result = self._benchmark_file_reading(file_size="large")
        self.results.append(result)
        print(f"  Large files: {result.avg_time:.3f}s avg")

    def _run_parse_benchmarks(self):
        """ãƒ‘ãƒ¼ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        # åŸºæœ¬ãƒ‘ãƒ¼ã‚¹
        result = self._benchmark_parsing(complexity="basic")
        self.results.append(result)
        print(f"  Basic parsing: {result.avg_time:.3f}s avg")

        # è¤‡é›‘ãƒ‘ãƒ¼ã‚¹
        result = self._benchmark_parsing(complexity="complex")
        self.results.append(result)
        print(f"  Complex parsing: {result.avg_time:.3f}s avg")

    def _run_render_benchmarks(self):
        """ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        # åŸºæœ¬ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°
        result = self._benchmark_rendering(template="basic")
        self.results.append(result)
        print(f"  Basic rendering: {result.avg_time:.3f}s avg")

        # è¤‡é›‘ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°
        result = self._benchmark_rendering(template="complex")
        self.results.append(result)
        print(f"  Complex rendering: {result.avg_time:.3f}s avg")

    def _run_e2e_benchmarks(self):
        """ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        result = self._benchmark_full_pipeline()
        self.results.append(result)
        print(f"  Full pipeline: {result.avg_time:.3f}s avg")

    def _run_cache_benchmarks(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        if not self.config.cache_enabled:
            print("  Cache disabled, skipping cache benchmarks")
            return

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡ãƒ†ã‚¹ãƒˆ
        result = self._benchmark_cache_performance()
        self.results.append(result)
        print(f"  Cache performance: {result.avg_time:.3f}s avg")

    def _benchmark_file_reading(self, file_size: str = "medium") -> BenchmarkResult:
        """ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        name = f"file_reading_{file_size}"

        # ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ
        test_content = self._generate_test_content(file_size)
        test_file = Path(f"/tmp/benchmark_{file_size}.txt")
        test_file.write_text(test_content, encoding="utf-8")

        def benchmark_func():
            if self.file_cache:
                return self.file_cache.get_file_content(test_file)
            else:
                return test_file.read_text(encoding="utf-8")

        return self._run_benchmark(name, benchmark_func)

    def _benchmark_parsing(self, complexity: str = "basic") -> BenchmarkResult:
        """ãƒ‘ãƒ¼ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        name = f"parsing_{complexity}"

        # ãƒ†ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆ
        test_content = self._generate_parse_test_content(complexity)

        def benchmark_func():
            if self.parse_cache:
                return self.parse_cache.get_parse_or_compute(
                    test_content, self._mock_parse_function
                )
            else:
                return self._mock_parse_function(test_content)

        return self._run_benchmark(name, benchmark_func)

    def _benchmark_rendering(self, template: str = "basic") -> BenchmarkResult:
        """ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        name = f"rendering_{template}"

        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
        test_data = self._generate_render_test_data(template)
        content_hash = "test_hash"

        def benchmark_func():
            if self.render_cache:
                return self.render_cache.get_render_or_compute(
                    content_hash, template, self._mock_render_function, data=test_data
                )
            else:
                return self._mock_render_function(data=test_data)

        return self._run_benchmark(name, benchmark_func)

    def _benchmark_full_pipeline(self) -> BenchmarkResult:
        """ãƒ•ãƒ«ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        name = "full_pipeline"

        # ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’æº–å‚™
        test_content = self._generate_test_content("medium")
        test_file = Path("/tmp/benchmark_pipeline.txt")
        test_file.write_text(test_content, encoding="utf-8")

        def benchmark_func():
            # 1. ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
            if self.file_cache:
                content = self.file_cache.get_file_content(test_file)
            else:
                content = test_file.read_text(encoding="utf-8")

            # 2. ãƒ‘ãƒ¼ã‚¹
            if self.parse_cache:
                ast_nodes = self.parse_cache.get_parse_or_compute(
                    content, self._mock_parse_function
                )
            else:
                ast_nodes = self._mock_parse_function(content)

            # 3. ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°
            content_hash = "pipeline_hash"
            if self.render_cache:
                html = self.render_cache.get_render_or_compute(
                    content_hash,
                    "basic",
                    self._mock_render_function,
                    ast_nodes=ast_nodes,
                )
            else:
                html = self._mock_render_function(ast_nodes=ast_nodes)

            return html

        return self._run_benchmark(name, benchmark_func)

    def _benchmark_cache_performance(self) -> BenchmarkResult:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        name = "cache_performance"

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’äº‹å‰ã«ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
        test_content = self._generate_test_content("medium")

        if self.file_cache:
            test_file = Path("/tmp/benchmark_cache.txt")
            test_file.write_text(test_content, encoding="utf-8")
            # ä¸€åº¦èª­ã¿è¾¼ã‚“ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            self.file_cache.get_file_content(test_file)

        if self.parse_cache:
            # ä¸€åº¦ãƒ‘ãƒ¼ã‚¹ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            self.parse_cache.get_parse_or_compute(
                test_content, self._mock_parse_function
            )

        def benchmark_func():
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆã‚’æœŸå¾…
            if self.file_cache:
                self.file_cache.get_file_content(test_file)

            if self.parse_cache:
                self.parse_cache.get_parse_or_compute(
                    test_content, self._mock_parse_function
                )

            return "cached_result"

        return self._run_benchmark(name, benchmark_func)

    def _run_benchmark(self, name: str, func: Callable) -> BenchmarkResult:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œ"""
        # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
        self.logger.debug(f"ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—é–‹å§‹: {self.config.warmup_iterations}å›")
        for i in range(self.config.warmup_iterations):
            func()
            self.logger.debug(
                f"ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ— {i+1}/{self.config.warmup_iterations} å®Œäº†"
            )

        # ãƒ¡ãƒ¢ãƒªç›£è¦–é–‹å§‹
        start_memory = None
        if self.memory_monitor:
            start_memory = self.memory_monitor.take_snapshot()

        # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°é–‹å§‹
        session_name = f"benchmark_{name}"
        if self.profiler:
            profiler_context = self.profiler.profile_session(session_name)
            profiler_context.__enter__()

        # å®Ÿéš›ã®æ¸¬å®š
        self.logger.debug(f"ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ¸¬å®šé–‹å§‹: {self.config.iterations}å›")
        times = []
        for i in range(self.config.iterations):
            start_time = time.perf_counter()
            func()
            end_time = time.perf_counter()
            execution_time = end_time - start_time
            times.append(execution_time)
            self.logger.debug(
                f"ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ {i+1}/{self.config.iterations}: {execution_time:.4f}s"
            )

        # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°çµ‚äº†
        if self.profiler:
            profiler_context.__exit__(None, None, None)

        # ãƒ¡ãƒ¢ãƒªç›£è¦–çµ‚äº†
        end_memory = None
        if self.memory_monitor:
            end_memory = self.memory_monitor.take_snapshot()

        # çµ±è¨ˆè¨ˆç®—
        total_time = sum(times)
        avg_time = statistics.mean(times)
        min_time = min(times)
        max_time = max(times)
        std_dev = statistics.stdev(times) if len(times) > 1 else 0

        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
        memory_usage = {}
        if start_memory and end_memory:
            memory_usage = {
                "start_mb": start_memory.memory_mb,
                "end_mb": end_memory.memory_mb,
                "delta_mb": end_memory.memory_mb - start_memory.memory_mb,
                "peak_mb": max(start_memory.memory_mb, end_memory.memory_mb),
            }

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆ
        cache_stats = {}
        if self.file_cache:
            cache_stats["file_cache"] = self.file_cache.get_cache_stats()
        if self.parse_cache:
            cache_stats["parse_cache"] = self.parse_cache.get_parse_statistics()
        if self.render_cache:
            cache_stats["render_cache"] = self.render_cache.get_render_statistics()

        result = BenchmarkResult(
            name=name,
            iterations=self.config.iterations,
            total_time=total_time,
            avg_time=avg_time,
            min_time=min_time,
            max_time=max_time,
            std_dev=std_dev,
            memory_usage=memory_usage,
            cache_stats=cache_stats,
        )

        self.logger.info(
            f"ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†: {name}, å¹³å‡: {avg_time:.4f}s, "
            f"æœ€å°: {min_time:.4f}s, æœ€å¤§: {max_time:.4f}s"
        )

        return result

    def _analyze_results(self) -> Dict[str, Any]:
        """çµæœã‚’åˆ†æ"""
        analysis = {
            "summary": {
                "total_benchmarks": len(self.results),
                "fastest_benchmark": (
                    min(self.results, key=lambda x: x.avg_time).name
                    if self.results
                    else None
                ),
                "slowest_benchmark": (
                    max(self.results, key=lambda x: x.avg_time).name
                    if self.results
                    else None
                ),
            },
            "detailed_results": [asdict(result) for result in self.results],
            "performance_insights": self._generate_performance_insights(),
        }

        # å›å¸°åˆ†æ
        if self.baseline_results:
            analysis["regression_analysis"] = self._analyze_regression(
                {result.name: result for result in self.results}
            )

        return analysis

    def _analyze_regression(
        self, current_results: Dict[str, BenchmarkResult]
    ) -> Dict[str, Any]:
        """å›å¸°åˆ†æã‚’å®Ÿè¡Œ"""
        regression_analysis = {
            "regressions_detected": [],
            "improvements_detected": [],
            "stable_benchmarks": [],
        }

        threshold = 0.1  # 10%ã®å¤‰åŒ–ã‚’é–¾å€¤ã¨ã™ã‚‹

        for name, baseline in self.baseline_results.items():
            if name in current_results:
                current = current_results[name]

                # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å¤‰åŒ–ã‚’è¨ˆç®—
                change_percent = (
                    (current.avg_time - baseline.avg_time) / baseline.avg_time
                ) * 100

                if change_percent > threshold * 100:
                    regression_analysis["regressions_detected"].append(
                        {
                            "benchmark": name,
                            "baseline_time": baseline.avg_time,
                            "current_time": current.avg_time,
                            "change_percent": change_percent,
                            "severity": (
                                "high"
                                if change_percent > 25
                                else "medium" if change_percent > 10 else "low"
                            ),
                        }
                    )
                elif change_percent < -threshold * 100:
                    regression_analysis["improvements_detected"].append(
                        {
                            "benchmark": name,
                            "baseline_time": baseline.avg_time,
                            "current_time": current.avg_time,
                            "change_percent": abs(change_percent),
                        }
                    )
                else:
                    regression_analysis["stable_benchmarks"].append(name)

        return regression_analysis

    def _generate_performance_insights(self) -> List[str]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¤ãƒ³ã‚µã‚¤ãƒˆã‚’ç”Ÿæˆ"""
        insights = []

        if not self.results:
            return insights

        # å¹³å‡æ™‚é–“ã®åˆ†æ
        avg_times = [result.avg_time for result in self.results]
        overall_avg = statistics.mean(avg_times)

        if overall_avg > 1.0:
            insights.append(
                "å…¨ä½“çš„ã«ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒé…ã„: å¹³å‡å®Ÿè¡Œæ™‚é–“ãŒ1ç§’ã‚’è¶…ãˆã¦ã„ã¾ã™"
            )

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹æœã®åˆ†æ
        cache_results = [result for result in self.results if "cache" in result.name]
        if cache_results:
            cache_avg = statistics.mean([result.avg_time for result in cache_results])
            if cache_avg < overall_avg * 0.5:
                insights.append("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒåŠ¹æœçš„ã«å‹•ä½œã—ã¦ã„ã¾ã™")
            else:
                insights.append(
                    "ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®åŠ¹æœãŒé™å®šçš„ã§ã™: è¨­å®šã®è¦‹ç›´ã—ã‚’æ¤œè¨ã—ã¦ãã ã•ã„"
                )

        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®åˆ†æ
        memory_results = [result for result in self.results if result.memory_usage]
        if memory_results:
            high_memory_results = [
                result
                for result in memory_results
                if result.memory_usage.get("delta_mb", 0) > 50
            ]
            if high_memory_results:
                insights.append(
                    f"{len(high_memory_results)}å€‹ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§é«˜ã„ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ"
                )

        return insights

    def _generate_test_content(self, size: str) -> str:
        """ãƒ†ã‚¹ãƒˆç”¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆ"""
        base_content = """
# ãƒ†ã‚¹ãƒˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

## æ¦‚è¦
ã“ã‚Œã¯ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆç”¨ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã™ã€‚

## å†…å®¹
- é …ç›®1: åŸºæœ¬çš„ãªå†…å®¹
- é …ç›®2: ã‚ˆã‚Šè©³ç´°ãªå†…å®¹
- é …ç›®3: è¤‡é›‘ãªå†…å®¹

### è©³ç´°ã‚»ã‚¯ã‚·ãƒ§ãƒ³
ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«ã¯è©³ç´°ãªèª¬æ˜ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚
"""

        if size == "small":
            return base_content
        elif size == "medium":
            return base_content * 10
        elif size == "large":
            return base_content * 100
        else:
            return base_content

    def _generate_parse_test_content(self, complexity: str) -> str:
        """ãƒ‘ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆç”¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆ"""
        if complexity == "basic":
            return self._generate_test_content("small")
        else:
            return self._generate_test_content("large")

    def _generate_render_test_data(self, template: str) -> Dict[str, Any]:
        """ãƒ¬ãƒ³ãƒ€ãƒ¼ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""
        if template == "basic":
            return {"title": "Test", "content": "Basic content"}
        else:
            return {
                "title": "Complex Test",
                "content": "Complex content" * 100,
                "items": [f"Item {i}" for i in range(100)],
            }

    def _mock_parse_function(self, content: str) -> List[Any]:
        """ãƒ¢ãƒƒã‚¯ãƒ‘ãƒ¼ã‚¹é–¢æ•°"""
        # å®Ÿéš›ã®ãƒ‘ãƒ¼ã‚¹å‡¦ç†ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        time.sleep(0.001)  # 1ms ã®ãƒ‘ãƒ¼ã‚¹æ™‚é–“ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        return [{"type": "text", "content": line} for line in content.split("\n")]

    def _mock_render_function(self, **kwargs) -> str:
        """ãƒ¢ãƒƒã‚¯ãƒ¬ãƒ³ãƒ€ãƒ¼é–¢æ•°"""
        # å®Ÿéš›ã®ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°å‡¦ç†ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        time.sleep(0.002)  # 2ms ã®ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°æ™‚é–“ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        return f"<html><body>{kwargs}</body></html>"
