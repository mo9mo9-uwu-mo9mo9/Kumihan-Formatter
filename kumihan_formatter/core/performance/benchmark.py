"""
ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚·ã‚¹ãƒ†ãƒ 
Issue #813å¯¾å¿œ - performance_metrics.pyã‹ã‚‰åˆ†é›¢
"""

import json
import time
from datetime import datetime
from pathlib import Path
from statistics import mean, median, stdev
from typing import Any, Callable, Dict, List, Optional

from ..utilities.logger import get_logger


class PerformanceBenchmark:
    """
    ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œã‚·ã‚¹ãƒ†ãƒ 

    æ©Ÿèƒ½:
    - æ¨™æº–åŒ–ã•ã‚ŒãŸãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
    - è¤‡æ•°å®Ÿè¡Œã§ã®çµ±è¨ˆåˆ†æ
    - çµæœæ¯”è¼ƒãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
    - ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã®æ°¸ç¶šåŒ–
    """

    def __init__(self):
        self.logger = get_logger(__name__)
        self.benchmark_results: List[Dict[str, Any]] = []

        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚«ãƒ†ã‚´ãƒªè¨­å®š
        self.benchmark_categories = {
            "text_processing": {
                "description": "ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†æ€§èƒ½",
                "metrics": ["processing_time", "memory_usage", "tokens_per_second"],
            },
            "file_operations": {
                "description": "ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œæ€§èƒ½",
                "metrics": ["read_time", "write_time", "throughput_mb_s"],
            },
            "memory_management": {
                "description": "ãƒ¡ãƒ¢ãƒªç®¡ç†æ€§èƒ½",
                "metrics": ["allocation_time", "gc_time", "peak_memory"],
            },
            "optimization": {
                "description": "æœ€é©åŒ–æ©Ÿèƒ½æ€§èƒ½",
                "metrics": [
                    "optimization_time",
                    "efficiency_gain",
                    "resource_reduction",
                ],
            },
        }

        self.logger.info("PerformanceBenchmark initialized")

    def run_benchmark(
        self,
        benchmark_name: str,
        benchmark_func: Callable,
        category: str = "general",
        iterations: int = 5,
        warmup_iterations: int = 1,
        **kwargs,
    ) -> Dict[str, Any]:
        """
        ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ

        Args:
            benchmark_name: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å
            benchmark_func: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å¯¾è±¡é–¢æ•°
            category: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚«ãƒ†ã‚´ãƒª
            iterations: å®Ÿè¡Œå›æ•°
            warmup_iterations: ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å›æ•°
            **kwargs: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–¢æ•°ã¸ã®å¼•æ•°

        Returns:
            Dict[str, Any]: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ
        """
        self.logger.info(
            f"Starting benchmark: {benchmark_name} ({iterations} iterations)"
        )

        # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Ÿè¡Œ
        for i in range(warmup_iterations):
            try:
                benchmark_func(**kwargs)
                self.logger.debug(f"Warmup iteration {i+1} completed")
            except Exception as e:
                self.logger.warning(f"Warmup iteration {i+1} failed: {e}")

        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
        execution_times = []
        memory_usages = []
        errors = []

        for iteration in range(iterations):
            try:
                # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¸¬å®šé–‹å§‹
                import os

                import psutil

                process = psutil.Process(os.getpid())
                start_memory = process.memory_info().rss / 1024 / 1024

                # å®Ÿè¡Œæ™‚é–“æ¸¬å®š
                start_time = time.perf_counter()
                _ = benchmark_func(**kwargs)  # Benchmark execution only
                end_time = time.perf_counter()

                # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¸¬å®šçµ‚äº†
                end_memory = process.memory_info().rss / 1024 / 1024
                memory_used = end_memory - start_memory

                execution_time = end_time - start_time
                execution_times.append(execution_time)
                memory_usages.append(memory_used)

                self.logger.debug(
                    f"Iteration {iteration+1}: {execution_time:.4f}s, "
                    f"Memory: {memory_used:.2f}MB"
                )

            except Exception as e:
                errors.append(f"Iteration {iteration+1}: {str(e)}")
                self.logger.error(f"Benchmark iteration {iteration+1} failed: {e}")

        # çµ±è¨ˆåˆ†æ
        if execution_times:
            stats = self._calculate_statistics(execution_times, memory_usages)
        else:
            stats = {"error": "All benchmark iterations failed"}

        # çµæœæ§‹ç¯‰
        benchmark_result = {
            "name": benchmark_name,
            "category": category,
            "timestamp": datetime.now().isoformat(),
            "iterations": iterations,
            "successful_iterations": len(execution_times),
            "failed_iterations": len(errors),
            "statistics": stats,
            "errors": errors,
            "metadata": {
                "warmup_iterations": warmup_iterations,
                "python_version": self._get_python_version(),
                "system_info": self._get_system_info(),
            },
        }

        # çµæœã‚’å±¥æ­´ã«è¿½åŠ 
        self.benchmark_results.append(benchmark_result)

        # çµæœã‚’tmp/é…ä¸‹ã«ä¿å­˜
        self._save_benchmark_result(benchmark_result)

        self.logger.info(
            f"Benchmark {benchmark_name} completed: "
            f"{len(execution_times)}/{iterations} successful iterations"
        )

        return benchmark_result

    def _calculate_statistics(
        self, execution_times: List[float], memory_usages: List[float]
    ) -> Dict[str, Any]:
        """çµ±è¨ˆæƒ…å ±ã‚’è¨ˆç®—"""
        if not execution_times:
            return {}

        stats = {
            "execution_time": {
                "mean": mean(execution_times),
                "median": median(execution_times),
                "min": min(execution_times),
                "max": max(execution_times),
                "std_dev": stdev(execution_times) if len(execution_times) > 1 else 0.0,
                "cv_percent": (
                    (stdev(execution_times) / mean(execution_times)) * 100
                    if len(execution_times) > 1 and mean(execution_times) > 0
                    else 0.0
                ),
            }
        }

        if memory_usages:
            stats["memory_usage"] = {
                "mean": mean(memory_usages),
                "median": median(memory_usages),
                "min": min(memory_usages),
                "max": max(memory_usages),
                "std_dev": stdev(memory_usages) if len(memory_usages) > 1 else 0.0,
            }

        return stats

    def _get_python_version(self) -> str:
        """Python ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±ã‚’å–å¾—"""
        import sys

        return f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}"

    def _get_system_info(self) -> Dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã‚’å–å¾—"""
        import platform

        import psutil

        return {
            "platform": platform.platform(),
            "processor": platform.processor(),
            "cpu_count": psutil.cpu_count(),
            "total_memory_gb": psutil.virtual_memory().total / 1024 / 1024 / 1024,
        }

    def _save_benchmark_result(self, result: Dict[str, Any]):
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚’tmp/é…ä¸‹ã«ä¿å­˜"""
        tmp_dir = Path("tmp")
        tmp_dir.mkdir(exist_ok=True)

        filename = f"benchmark_{result['name']}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        file_path = tmp_dir / filename

        try:
            with open(file_path, "w", encoding="utf-8") as f:
                json.dump(result, f, indent=2, ensure_ascii=False)

            self.logger.info(f"Benchmark result saved to {file_path}")
        except Exception as e:
            self.logger.error(f"Failed to save benchmark result: {e}")

    def compare_benchmarks(
        self, benchmark_names: List[str], metric: str = "execution_time.mean"
    ) -> Dict[str, Any]:
        """
        è¤‡æ•°ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®æ¯”è¼ƒåˆ†æ

        Args:
            benchmark_names: æ¯”è¼ƒå¯¾è±¡ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯åã®ãƒªã‚¹ãƒˆ
            metric: æ¯”è¼ƒãƒ¡ãƒˆãƒªã‚¯ã‚¹

        Returns:
            Dict[str, Any]: æ¯”è¼ƒçµæœ
        """
        if not benchmark_names:
            return {"error": "No benchmark names provided"}

        # è©²å½“ã™ã‚‹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚’å–å¾—
        comparison_data = {}
        for name in benchmark_names:
            matching_results = [
                result
                for result in self.benchmark_results
                if result["name"] == name and "statistics" in result
            ]

            if matching_results:
                latest_result = max(matching_results, key=lambda x: x["timestamp"])
                comparison_data[name] = latest_result
            else:
                self.logger.warning(f"No results found for benchmark: {name}")

        if not comparison_data:
            return {"error": "No valid benchmark data found for comparison"}

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹å€¤ã‚’æŠ½å‡º
        metric_values = {}
        for name, result in comparison_data.items():
            try:
                # ãƒã‚¹ãƒˆã•ã‚ŒãŸãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ‘ã‚¹ã‚’ã‚µãƒãƒ¼ãƒˆï¼ˆä¾‹: "execution_time.mean"ï¼‰
                value = result["statistics"]
                for key in metric.split("."):
                    value = value[key]
                metric_values[name] = value
            except (KeyError, TypeError):
                self.logger.warning(f"Metric {metric} not found for {name}")

        if not metric_values:
            return {"error": f"Metric {metric} not found in any benchmark"}

        # æ¯”è¼ƒåˆ†æ
        best_performance = min(metric_values.items(), key=lambda x: x[1])
        worst_performance = max(metric_values.items(), key=lambda x: x[1])

        improvements = {}
        baseline_value = worst_performance[1]
        for name, value in metric_values.items():
            if baseline_value > 0:
                improvement_percent = ((baseline_value - value) / baseline_value) * 100
                improvements[name] = improvement_percent

        comparison_result = {
            "metric": metric,
            "values": metric_values,
            "best_performance": {
                "benchmark": best_performance[0],
                "value": best_performance[1],
            },
            "worst_performance": {
                "benchmark": worst_performance[0],
                "value": worst_performance[1],
            },
            "improvements_percent": improvements,
            "analysis_timestamp": datetime.now().isoformat(),
        }

        return comparison_result

    def generate_benchmark_report(self, category: Optional[str] = None) -> str:
        """
        ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ

        Args:
            category: ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¯¾è±¡ã‚«ãƒ†ã‚´ãƒª

        Returns:
            str: HTMLãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®ãƒ¬ãƒãƒ¼ãƒˆ
        """
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        if category:
            filtered_results = [
                r for r in self.benchmark_results if r.get("category") == category
            ]
        else:
            filtered_results = self.benchmark_results

        if not filtered_results:
            return "<html><body><h1>No benchmark results found</h1></body></html>"

        # HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        html_content = f"""<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <title>Kumihan-Formatter ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ¬ãƒãƒ¼ãƒˆ</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        .container {{ max-width: 1200px; margin: 0 auto; }}
        .benchmark-card {{
            background: #f8f9fa; padding: 15px; margin: 10px 0;
            border-radius: 6px; border-left: 4px solid #007bff;
        }}
        .stats-table {{ width: 100%; border-collapse: collapse; margin: 10px 0; }}
        .stats-table th, .stats-table td {{
            border: 1px solid #ddd; padding: 8px; text-align: left;
        }}
        .stats-table th {{ background-color: #007bff; color: white; }}
        .success {{ color: #28a745; }}
        .warning {{ color: #ffc107; }}
        .error {{ color: #dc3545; }}
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸƒâ€â™‚ï¸ Kumihan-Formatter ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ¬ãƒãƒ¼ãƒˆ</h1>
        <p>ç”Ÿæˆæ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        <p>ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ: {len(filtered_results)}ä»¶</p>
"""

        for result in filtered_results[-10:]:  # æœ€æ–°10ä»¶
            success_rate = (
                result["successful_iterations"] / result["iterations"] * 100
                if result["iterations"] > 0
                else 0
            )

            status_class = (
                "success"
                if success_rate == 100
                else "warning" if success_rate > 50 else "error"
            )

            html_content += f"""
        <div class="benchmark-card">
            <h3>{result['name']} ({result.get('category', 'general')})</h3>
            <p>å®Ÿè¡Œæ™‚åˆ»: {result['timestamp']}</p>
            <p class="{status_class}">æˆåŠŸç‡: {success_rate:.1f}%
               ({result['successful_iterations']}/{result['iterations']} iterations)</p>
"""

            if "statistics" in result and "execution_time" in result["statistics"]:
                stats = result["statistics"]["execution_time"]
                html_content += f"""
            <table class="stats-table">
                <tr><th>ãƒ¡ãƒˆãƒªã‚¯ã‚¹</th><th>å€¤</th></tr>
                <tr><td>å¹³å‡å®Ÿè¡Œæ™‚é–“</td><td>{stats['mean']:.4f}ç§’</td></tr>
                <tr><td>ä¸­å¤®å€¤</td><td>{stats['median']:.4f}ç§’</td></tr>
                <tr><td>æœ€å°å€¤</td><td>{stats['min']:.4f}ç§’</td></tr>
                <tr><td>æœ€å¤§å€¤</td><td>{stats['max']:.4f}ç§’</td></tr>
                <tr><td>æ¨™æº–åå·®</td><td>{stats['std_dev']:.4f}ç§’</td></tr>
                <tr><td>å¤‰å‹•ä¿‚æ•°</td><td>{stats['cv_percent']:.2f}%</td></tr>
            </table>
"""

            if result.get("errors"):
                html_content += """
            <div class="error">
                <h4>ã‚¨ãƒ©ãƒ¼:</h4>
                <ul>
"""
                for error in result["errors"][:5]:  # æœ€åˆã®5å€‹ã®ã‚¨ãƒ©ãƒ¼
                    html_content += f"<li>{error}</li>"
                html_content += "</ul></div>"

            html_content += "</div>"

        html_content += """
    </div>
</body>
</html>"""

        # tmp/é…ä¸‹ã«ä¿å­˜
        tmp_dir = Path("tmp")
        tmp_dir.mkdir(exist_ok=True)
        report_path = (
            tmp_dir
            / f"benchmark_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
        )

        with open(report_path, "w", encoding="utf-8") as f:
            f.write(html_content)

        self.logger.info(f"Benchmark report generated: {report_path}")
        return str(report_path)

    def get_benchmark_summary(self) -> Dict[str, Any]:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ¦‚è¦ã‚’å–å¾—"""
        if not self.benchmark_results:
            return {"total_benchmarks": 0}

        # ã‚«ãƒ†ã‚´ãƒªåˆ¥é›†è¨ˆ
        category_counts = {}
        total_iterations = 0
        total_successful = 0

        for result in self.benchmark_results:
            category = result.get("category", "general")
            category_counts[category] = category_counts.get(category, 0) + 1
            total_iterations += result["iterations"]
            total_successful += result["successful_iterations"]

        success_rate = (
            (total_successful / total_iterations * 100) if total_iterations > 0 else 0
        )

        return {
            "total_benchmarks": len(self.benchmark_results),
            "categories": category_counts,
            "total_iterations": total_iterations,
            "successful_iterations": total_successful,
            "overall_success_rate_percent": success_rate,
            "latest_benchmark": (
                self.benchmark_results[-1]["timestamp"]
                if self.benchmark_results
                else None
            ),
        }
