"""
æœ€é©åŒ–åˆ†æžã‚³ã‚¢ãƒ­ã‚¸ãƒƒã‚¯ - Issue #402å¯¾å¿œ

æœ€é©åŒ–åŠ¹æžœã®æ¸¬å®šã€ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¯”è¼ƒã€çµ±è¨ˆåˆ†æžã€‚
"""

import json

# statistics removed as unused
from datetime import datetime
from pathlib import Path
from typing import Any

from ...performance import get_global_monitor
from ...utilities.logger import get_logger
from ..benchmark import PerformanceBenchmarkSuite
from ..benchmark_types import BenchmarkConfig
from ..memory_monitor import MemoryMonitor
from ..profiler import AdvancedProfiler
from .models import OptimizationMetrics, OptimizationReport
from .utils import (
    calculate_significance,
    calculate_total_improvement_score,
    capture_system_info,
    create_performance_summary,
    detect_regressions,
    generate_recommendations,
)


class OptimizationAnalyzer:
    """æœ€é©åŒ–åŠ¹æžœåˆ†æžã‚·ã‚¹ãƒ†ãƒ 

    æ©Ÿèƒ½:
    - Before/After ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ¯”è¼ƒ
    - çµ±è¨ˆçš„æœ‰æ„æ€§æ¤œå®š
    - æ”¹å–„åŠ¹æžœã®å®šé‡çš„è©•ä¾¡
    - å›žå¸°ãƒªã‚¹ã‚¯ã®è©•ä¾¡
    """

    def __init__(self, baseline_dir: Path | None = None) -> None:
        """æœ€é©åŒ–åˆ†æžå™¨ã‚’åˆæœŸåŒ–

        Args:
            baseline_dir: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        self.baseline_dir = baseline_dir or Path("./performance_baselines")
        self.baseline_dir.mkdir(parents=True, exist_ok=True)
        self.logger = get_logger(__name__)

        # æ¸¬å®šãƒ„ãƒ¼ãƒ«
        self.monitor = get_global_monitor()
        self.profiler = AdvancedProfiler()
        self.memory_monitor = MemoryMonitor()

        # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        self.baseline_data: dict[str, Any] = {}
        self.optimization_history: list[OptimizationReport] = []

        self.logger.info(
            f"OptimizationAnalyzer initialized: baseline_dir={self.baseline_dir}"
        )

    def capture_baseline(self, name: str, description: str = "") -> dict[str, Any]:
        """æœ€é©åŒ–å‰ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ€§èƒ½ã‚’è¨˜éŒ²

        Args:
            name: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å
            description: èª¬æ˜Ž

        Returns:
            ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿
        """
        self.logger.info(f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚­ãƒ£ãƒ—ãƒãƒ£é–‹å§‹: {name}")
        print(f"ðŸ“ˆ Capturing baseline performance: {name}")

        # ãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯è¨­å®šï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—ã§ç´”ç²¹ãªæ€§èƒ½ã‚’æ¸¬å®šï¼‰
        benchmark_config = BenchmarkConfig(
            iterations=5,
            warmup_iterations=2,
            enable_profiling=True,
            enable_memory_monitoring=True,
            cache_enabled=False,  # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—
        )

        # ãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯å®Ÿè¡Œ
        benchmark_suite = PerformanceBenchmarkSuite(benchmark_config)
        benchmark_results = benchmark_suite.run_full_benchmark_suite()

        # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã‚’åŽé›†
        system_info = capture_system_info()

        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
        baseline_data = {
            "name": name,
            "description": description,
            "timestamp": datetime.now().isoformat(),
            "benchmark_config": benchmark_config.__dict__,
            "benchmark_results": benchmark_results,
            "system_info": system_info,
        }

        # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        baseline_file = self.baseline_dir / f"{name}_baseline.json"
        with open(baseline_file, "w", encoding="utf-8") as f:
            json.dump(baseline_data, f, indent=2, ensure_ascii=False)

        # ãƒ¡ãƒ¢ãƒªã«ä¿å­˜
        self.baseline_data[name] = baseline_data

        self.logger.info(f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ä¿å­˜å®Œäº†: {baseline_file}")
        print(f"ðŸ“ Baseline saved to: {baseline_file}")
        return baseline_data

    def measure_optimization_impact(
        self,
        optimization_name: str,
        baseline_name: str,
        description: str = "",
    ) -> OptimizationReport:
        """æœ€é©åŒ–å¾Œã®åŠ¹æžœã‚’æ¸¬å®š

        Args:
            optimization_name: æœ€é©åŒ–å
            baseline_name: æ¯”è¼ƒå¯¾è±¡ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å
            description: æœ€é©åŒ–ã®èª¬æ˜Ž

        Returns:
            æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆ
        """
        self.logger.info(f"æœ€é©åŒ–åŠ¹æžœæ¸¬å®šé–‹å§‹: {optimization_name}")
        print(f"ðŸ” Measuring optimization impact: {optimization_name}")

        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        baseline_data = self._load_baseline(baseline_name)
        if not baseline_data:
            raise ValueError(f"Baseline '{baseline_name}' not found")

        # æœ€é©åŒ–å¾Œã®ãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯ã‚’å®Ÿè¡Œ
        benchmark_config = BenchmarkConfig(
            iterations=5,
            warmup_iterations=2,
            enable_profiling=True,
            enable_memory_monitoring=True,
            cache_enabled=True,  # æœ€é©åŒ–å¾Œã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æœ‰åŠ¹
        )

        benchmark_suite = PerformanceBenchmarkSuite(benchmark_config)
        optimized_results = benchmark_suite.run_full_benchmark_suite()

        # æ¯”è¼ƒåˆ†æž
        metrics = self._compare_performance(
            baseline_data["benchmark_results"], optimized_results
        )

        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report = OptimizationReport(
            timestamp=datetime.now().isoformat(),
            optimization_name=optimization_name,
            total_improvement_score=calculate_total_improvement_score(metrics),
            metrics=metrics,
            performance_summary=create_performance_summary(metrics),
            recommendations=generate_recommendations(metrics),
            regression_warnings=detect_regressions(metrics),
        )

        # å±¥æ­´ã«ä¿å­˜
        self.optimization_history.append(report)

        # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        report_file = (
            self.baseline_dir / f"{optimization_name}_optimization_report.json"
        )
        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(report.to_dict(), f, indent=2, ensure_ascii=False)

        self.logger.info(f"æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜å®Œäº†: {report_file}")
        print(f"ðŸ“ˆ Optimization report saved to: {report_file}")
        return report

    def _load_baseline(self, baseline_name: str) -> dict[str, Any] | None:
        """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        # ãƒ¡ãƒ¢ãƒªã‹ã‚‰æ¤œç´¢
        if baseline_name in self.baseline_data:
            cached_data: dict[str, Any] = self.baseline_data[baseline_name]
            return cached_data

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿
        baseline_file = self.baseline_dir / f"{baseline_name}_baseline.json"
        if baseline_file.exists():
            try:
                with open(baseline_file, "r", encoding="utf-8") as f:
                    loaded_data: dict[str, Any] = json.load(f)
                self.baseline_data[baseline_name] = loaded_data
                return loaded_data
            except Exception as e:
                self.logger.error(f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

        return None

    def _compare_performance(
        self, baseline_results: dict[str, Any], optimized_results: dict[str, Any]
    ) -> list[OptimizationMetrics]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ã‚’æ¯”è¼ƒã—ã¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ç”Ÿæˆ"""
        metrics = []

        # ãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯çµæžœã®æ¯”è¼ƒ
        if (
            "benchmark_results" in baseline_results
            and "benchmark_results" in optimized_results
        ):
            baseline_benchmarks = baseline_results["benchmark_results"]
            optimized_benchmarks = optimized_results["benchmark_results"]

            for name, baseline_data in baseline_benchmarks.items():
                if name in optimized_benchmarks:
                    optimized_data = optimized_benchmarks[name]

                    # å®Ÿè¡Œæ™‚é–“ã®æ¯”è¼ƒ
                    if "avg_time" in baseline_data and "avg_time" in optimized_data:
                        baseline_time = baseline_data["avg_time"]
                        optimized_time = optimized_data["avg_time"]

                        improvement_percent = (
                            (baseline_time - optimized_time) / baseline_time * 100
                            if baseline_time > 0
                            else 0
                        )

                        significance = calculate_significance(
                            baseline_time, optimized_time, improvement_percent
                        )

                        metrics.append(
                            OptimizationMetrics(
                                name=f"{name}_execution_time",
                                before_value=baseline_time,
                                after_value=optimized_time,
                                improvement_percent=improvement_percent,
                                improvement_absolute=baseline_time - optimized_time,
                                significance=significance,
                                category="performance",
                            )
                        )

        return metrics
