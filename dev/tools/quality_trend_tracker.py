#!/usr/bin/env python3
"""
å“è³ªãƒˆãƒ¬ãƒ³ãƒ‰ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 

å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å±¥æ­´ã‚’è¨˜éŒ²ã—ã€ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã‚„åŠ£åŒ–æ¤œå‡ºã‚’è¡Œã†
"""

import json
import sqlite3
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
import argparse


@dataclass
class QualitySnapshot:
    """å“è³ªã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ"""
    timestamp: str
    commit_hash: str
    branch_name: str
    total_score: float
    syntax_pass_rate: float
    test_pass_rate: float
    test_coverage: float
    performance_score: float
    file_count: int
    test_count: int
    
    # è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    conversion_time_avg: float
    conversion_time_max: float
    syntax_errors: int
    failed_tests: int
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    environment: str  # CI, local, etc.
    version: str
    
    @classmethod
    def from_quality_metrics(cls, metrics_data: Dict[str, Any], 
                           commit_hash: str = "", branch_name: str = "", 
                           environment: str = "local", version: str = ""):
        """å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’ä½œæˆ"""
        return cls(
            timestamp=datetime.now().isoformat(),
            commit_hash=commit_hash,
            branch_name=branch_name,
            total_score=metrics_data.get('overall_quality_score', 0.0),
            syntax_pass_rate=metrics_data.get('syntax_pass_rate', 0.0),
            test_pass_rate=metrics_data.get('test_pass_rate', 0.0),
            test_coverage=metrics_data.get('test_coverage', 0.0),
            performance_score=100.0 - (metrics_data.get('conversion_time_avg', 0.0) * 20),
            file_count=metrics_data.get('total_files', 0),
            test_count=metrics_data.get('passed_count', 0) + metrics_data.get('failed_count', 0),
            conversion_time_avg=metrics_data.get('conversion_time_avg', 0.0),
            conversion_time_max=metrics_data.get('conversion_time_max', 0.0),
            syntax_errors=metrics_data.get('syntax_errors', 0),
            failed_tests=metrics_data.get('failed_count', 0),
            environment=environment,
            version=version
        )


class QualityTrendTracker:
    """å“è³ªãƒˆãƒ¬ãƒ³ãƒ‰è¿½è·¡ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, db_path: Path = None):
        if db_path is None:
            db_path = Path("dev/data/quality_history.db")
        
        self.db_path = db_path
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        self._init_database()
    
    def _init_database(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åˆæœŸåŒ–"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS quality_snapshots (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    commit_hash TEXT,
                    branch_name TEXT,
                    total_score REAL NOT NULL,
                    syntax_pass_rate REAL,
                    test_pass_rate REAL,
                    test_coverage REAL,
                    performance_score REAL,
                    file_count INTEGER,
                    test_count INTEGER,
                    conversion_time_avg REAL,
                    conversion_time_max REAL,
                    syntax_errors INTEGER,
                    failed_tests INTEGER,
                    environment TEXT,
                    version TEXT,
                    raw_data TEXT  -- JSONå½¢å¼ã§å…ƒãƒ‡ãƒ¼ã‚¿ã‚‚ä¿å­˜
                )
            """)
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
            conn.execute("CREATE INDEX IF NOT EXISTS idx_timestamp ON quality_snapshots(timestamp)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_branch ON quality_snapshots(branch_name)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_commit ON quality_snapshots(commit_hash)")
    
    def record_snapshot(self, snapshot: QualitySnapshot, raw_data: Dict[str, Any] = None):
        """å“è³ªã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’è¨˜éŒ²"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                INSERT INTO quality_snapshots (
                    timestamp, commit_hash, branch_name, total_score,
                    syntax_pass_rate, test_pass_rate, test_coverage, performance_score,
                    file_count, test_count, conversion_time_avg, conversion_time_max,
                    syntax_errors, failed_tests, environment, version, raw_data
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                snapshot.timestamp, snapshot.commit_hash, snapshot.branch_name,
                snapshot.total_score, snapshot.syntax_pass_rate, snapshot.test_pass_rate,
                snapshot.test_coverage, snapshot.performance_score, snapshot.file_count,
                snapshot.test_count, snapshot.conversion_time_avg, snapshot.conversion_time_max,
                snapshot.syntax_errors, snapshot.failed_tests, snapshot.environment,
                snapshot.version, json.dumps(raw_data) if raw_data else None
            ))
    
    def get_recent_snapshots(self, days: int = 30, branch: str = None) -> List[QualitySnapshot]:
        """æœ€è¿‘ã®å“è³ªã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’å–å¾—"""
        since_date = (datetime.now() - timedelta(days=days)).isoformat()
        
        query = """
            SELECT timestamp, commit_hash, branch_name, total_score,
                   syntax_pass_rate, test_pass_rate, test_coverage, performance_score,
                   file_count, test_count, conversion_time_avg, conversion_time_max,
                   syntax_errors, failed_tests, environment, version
            FROM quality_snapshots 
            WHERE timestamp >= ?
        """
        params = [since_date]
        
        if branch:
            query += " AND branch_name = ?"
            params.append(branch)
        
        query += " ORDER BY timestamp DESC"
        
        with sqlite3.connect(self.db_path) as conn:
            rows = conn.execute(query, params).fetchall()
            
        return [QualitySnapshot(*row) for row in rows]
    
    def analyze_trends(self, days: int = 30, branch: str = None) -> Dict[str, Any]:
        """å“è³ªãƒˆãƒ¬ãƒ³ãƒ‰ã‚’åˆ†æ"""
        snapshots = self.get_recent_snapshots(days, branch)
        
        if len(snapshots) < 2:
            return {
                "trend": "insufficient_data",
                "message": "ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã«ã¯æœ€ä½2ã¤ã®ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆãŒå¿…è¦ã§ã™",
                "snapshots_count": len(snapshots)
            }
        
        # æœ€æ–°ã¨æœ€å¤ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã§æ¯”è¼ƒ
        latest = snapshots[0]
        oldest = snapshots[-1]
        
        score_change = latest.total_score - oldest.total_score
        test_change = latest.test_pass_rate - oldest.test_pass_rate
        coverage_change = latest.test_coverage - oldest.test_coverage
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ¤å®š
        if score_change >= 5.0:
            trend = "improving"
            trend_emoji = "ğŸ“ˆ"
        elif score_change <= -5.0:
            trend = "declining"
            trend_emoji = "ğŸ“‰"
        else:
            trend = "stable"
            trend_emoji = "â¡ï¸"
        
        # å¹³å‡å“è³ªã‚¹ã‚³ã‚¢
        avg_score = sum(s.total_score for s in snapshots) / len(snapshots)
        
        return {
            "trend": trend,
            "trend_emoji": trend_emoji,
            "score_change": score_change,
            "test_change": test_change,
            "coverage_change": coverage_change,
            "average_score": avg_score,
            "latest_score": latest.total_score,
            "oldest_score": oldest.total_score,
            "snapshots_count": len(snapshots),
            "period_days": days,
            "analysis_date": datetime.now().isoformat()
        }
    
    def detect_quality_degradation(self, threshold: float = 5.0, 
                                 recent_count: int = 5) -> Optional[Dict[str, Any]]:
        """å“è³ªåŠ£åŒ–ã‚’æ¤œå‡º"""
        recent_snapshots = self.get_recent_snapshots(days=7)[:recent_count]
        
        if len(recent_snapshots) < recent_count:
            return None
        
        # æœ€æ–°ã®ã‚¹ã‚³ã‚¢ã¨å¹³å‡ã‚¹ã‚³ã‚¢ã‚’æ¯”è¼ƒ
        latest_score = recent_snapshots[0].total_score
        avg_score = sum(s.total_score for s in recent_snapshots[1:]) / (len(recent_snapshots) - 1)
        
        degradation = avg_score - latest_score
        
        if degradation >= threshold:
            return {
                "degradation_detected": True,
                "severity": "high" if degradation >= 10.0 else "medium",
                "degradation_amount": degradation,
                "latest_score": latest_score,
                "average_score": avg_score,
                "threshold": threshold,
                "detection_time": datetime.now().isoformat(),
                "affected_snapshots": len(recent_snapshots)
            }
        
        return {
            "degradation_detected": False,
            "latest_score": latest_score,
            "average_score": avg_score,
            "degradation_amount": degradation
        }
    
    def generate_trend_report(self, days: int = 30, branch: str = None) -> str:
        """å“è³ªãƒˆãƒ¬ãƒ³ãƒ‰ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        trends = self.analyze_trends(days, branch)
        degradation = self.detect_quality_degradation()
        recent_snapshots = self.get_recent_snapshots(days, branch)[:10]
        
        # ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã®å ´åˆã®å‡¦ç†
        if trends.get('trend') == 'insufficient_data':
            return f"""
ğŸ” Kumihan-Formatter å“è³ªãƒˆãƒ¬ãƒ³ãƒ‰ãƒ¬ãƒãƒ¼ãƒˆ
{'=' * 50}
ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
åˆ†ææœŸé–“: ç›´è¿‘{days}æ—¥é–“

âš ï¸ ãƒ‡ãƒ¼ã‚¿ä¸è¶³
-------------
{trends['message']}
è¨˜éŒ²ã•ã‚ŒãŸã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆæ•°: {trends['snapshots_count']}

ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã«ã¯æœ€ä½2ã¤ã®ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆãŒå¿…è¦ã§ã™ã€‚
ç¶™ç¶šçš„ã«å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨˜éŒ²ã—ã¦ãã ã•ã„ã€‚
"""
        
        trend_emoji = trends.get('trend_emoji', 'â“')
        
        report = f"""
ğŸ” Kumihan-Formatter å“è³ªãƒˆãƒ¬ãƒ³ãƒ‰ãƒ¬ãƒãƒ¼ãƒˆ
{'=' * 50}
ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
åˆ†ææœŸé–“: ç›´è¿‘{days}æ—¥é–“
å¯¾è±¡ãƒ–ãƒ©ãƒ³ãƒ: {branch or 'å…¨ãƒ–ãƒ©ãƒ³ãƒ'}

ğŸ“Š ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
--------------
{trend_emoji} ç·åˆãƒˆãƒ¬ãƒ³ãƒ‰: {trends['trend']}
å“è³ªã‚¹ã‚³ã‚¢å¤‰åŒ–: {trends['score_change']:+.1f} ({trends['oldest_score']:.1f} â†’ {trends['latest_score']:.1f})
ãƒ†ã‚¹ãƒˆåˆæ ¼ç‡å¤‰åŒ–: {trends['test_change']:+.1f}%
ã‚«ãƒãƒ¬ãƒƒã‚¸å¤‰åŒ–: {trends['coverage_change']:+.1f}%
æœŸé–“å¹³å‡ã‚¹ã‚³ã‚¢: {trends['average_score']:.1f}/100

ğŸš¨ å“è³ªåŠ£åŒ–æ¤œå‡º
---------------
"""
        
        if degradation and degradation.get('degradation_detected'):
            report += f"""âŒ å“è³ªåŠ£åŒ–ã‚’æ¤œå‡ºã—ã¾ã—ãŸï¼
é‡è¦åº¦: {degradation['severity']}
åŠ£åŒ–é‡: -{degradation['degradation_amount']:.1f}ãƒã‚¤ãƒ³ãƒˆ
æœ€æ–°ã‚¹ã‚³ã‚¢: {degradation['latest_score']:.1f}/100
å¹³å‡ã‚¹ã‚³ã‚¢: {degradation['average_score']:.1f}/100
æ¤œå‡ºæ™‚åˆ»: {degradation['detection_time']}
"""
        else:
            report += "âœ… å“è³ªåŠ£åŒ–ã¯æ¤œå‡ºã•ã‚Œã¦ã„ã¾ã›ã‚“\n"
        
        report += f"""
ğŸ“ˆ æœ€è¿‘ã®å“è³ªã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ
----------------------------
"""
        for i, snapshot in enumerate(recent_snapshots[:5]):
            timestamp = datetime.fromisoformat(snapshot.timestamp).strftime('%m-%d %H:%M')
            report += f"{i+1}. {timestamp} | ã‚¹ã‚³ã‚¢: {snapshot.total_score:.1f} | ãƒ†ã‚¹ãƒˆ: {snapshot.test_pass_rate:.1f}% | ãƒ–ãƒ©ãƒ³ãƒ: {snapshot.branch_name}\n"
        
        if len(recent_snapshots) > 5:
            report += f"... ä»–{len(recent_snapshots)-5}ä»¶\n"
        
        report += f"""
ğŸ“‹ çµ±è¨ˆã‚µãƒãƒªãƒ¼
--------------
è¨˜éŒ²ã•ã‚ŒãŸã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆæ•°: {len(recent_snapshots)}
åˆ†æå¯¾è±¡æœŸé–“: {days}æ—¥é–“
ãƒ‡ãƒ¼ã‚¿å“è³ª: {'è‰¯å¥½' if len(recent_snapshots) >= 5 else 'ä¸ååˆ†ï¼ˆ5ä»¶æœªæº€ï¼‰'}

ğŸ’¡ æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
----------------
"""
        
        if trends['trend'] == 'declining':
            report += "- ğŸ”´ å“è³ªä½ä¸‹ãƒˆãƒ¬ãƒ³ãƒ‰ã§ã™ã€‚åŸå› èª¿æŸ»ã¨æ”¹å–„æªç½®ãŒå¿…è¦ã§ã™\n"
        elif trends['trend'] == 'improving':
            report += "- ğŸŸ¢ å“è³ªå‘ä¸Šãƒˆãƒ¬ãƒ³ãƒ‰ã§ã™ã€‚ç¾åœ¨ã®é–‹ç™ºæ–¹é‡ã‚’ç¶™ç¶šã—ã¦ãã ã•ã„\n"
        else:
            report += "- ğŸŸ¡ å“è³ªã¯å®‰å®šã—ã¦ã„ã¾ã™ã€‚ç¶™ç¶šçš„ãªç›£è¦–ã‚’æ¨å¥¨ã—ã¾ã™\n"
        
        if degradation and degradation.get('degradation_detected'):
            report += "- ğŸš¨ ç·Šæ€¥: å“è³ªåŠ£åŒ–ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚å³åº§ã®å¯¾å¿œãŒå¿…è¦ã§ã™\n"
        
        return report
    
    def export_data(self, output_path: Path, format: str = "json", days: int = 30):
        """å“è³ªãƒ‡ãƒ¼ã‚¿ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        snapshots = self.get_recent_snapshots(days)
        
        if format == "json":
            data = {
                "export_date": datetime.now().isoformat(),
                "period_days": days,
                "snapshots": [asdict(s) for s in snapshots]
            }
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
        
        elif format == "csv":
            import csv
            with open(output_path, 'w', newline='', encoding='utf-8') as f:
                if snapshots:
                    writer = csv.DictWriter(f, fieldnames=list(asdict(snapshots[0]).keys()))
                    writer.writeheader()
                    for snapshot in snapshots:
                        writer.writerow(asdict(snapshot))


def main():
    parser = argparse.ArgumentParser(description="å“è³ªãƒˆãƒ¬ãƒ³ãƒ‰ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ")
    parser.add_argument('command', choices=['record', 'analyze', 'report', 'export'], 
                       help='å®Ÿè¡Œã™ã‚‹ã‚³ãƒãƒ³ãƒ‰')
    parser.add_argument('--metrics-file', type=Path, 
                       help='å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹JSONãƒ•ã‚¡ã‚¤ãƒ«')
    parser.add_argument('--commit', default="", help='ã‚³ãƒŸãƒƒãƒˆãƒãƒƒã‚·ãƒ¥')
    parser.add_argument('--branch', default="", help='ãƒ–ãƒ©ãƒ³ãƒå')
    parser.add_argument('--environment', default="local", help='å®Ÿè¡Œç’°å¢ƒ')
    parser.add_argument('--days', type=int, default=30, help='åˆ†ææœŸé–“ï¼ˆæ—¥æ•°ï¼‰')
    parser.add_argument('--output', type=Path, help='å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
    parser.add_argument('--format', choices=['json', 'csv'], default='json', 
                       help='ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå½¢å¼')
    
    args = parser.parse_args()
    
    tracker = QualityTrendTracker()
    
    if args.command == 'record':
        if not args.metrics_file or not args.metrics_file.exists():
            print("âŒ ã‚¨ãƒ©ãƒ¼: --metrics-file ãŒå¿…è¦ã§ã™")
            return
        
        with open(args.metrics_file, 'r', encoding='utf-8') as f:
            metrics_data = json.load(f)
        
        snapshot = QualitySnapshot.from_quality_metrics(
            metrics_data, args.commit, args.branch, args.environment
        )
        tracker.record_snapshot(snapshot, metrics_data)
        print(f"âœ… å“è³ªã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’è¨˜éŒ²ã—ã¾ã—ãŸ: {snapshot.total_score:.1f}/100")
    
    elif args.command == 'analyze':
        trends = tracker.analyze_trends(args.days, args.branch)
        print(f"ğŸ“Š å“è³ªãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ (ç›´è¿‘{args.days}æ—¥é–“)")
        print(f"ãƒˆãƒ¬ãƒ³ãƒ‰: {trends['trend_emoji']} {trends['trend']}")
        print(f"ã‚¹ã‚³ã‚¢å¤‰åŒ–: {trends['score_change']:+.1f}")
        print(f"å¹³å‡ã‚¹ã‚³ã‚¢: {trends['average_score']:.1f}/100")
    
    elif args.command == 'report':
        report = tracker.generate_trend_report(args.days, args.branch)
        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                f.write(report)
            print(f"ğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆã‚’å‡ºåŠ›ã—ã¾ã—ãŸ: {args.output}")
        else:
            print(report)
    
    elif args.command == 'export':
        if not args.output:
            print("âŒ ã‚¨ãƒ©ãƒ¼: --output ãŒå¿…è¦ã§ã™")
            return
        
        tracker.export_data(args.output, args.format, args.days)
        print(f"ğŸ“ ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ã¾ã—ãŸ: {args.output}")


if __name__ == "__main__":
    main()