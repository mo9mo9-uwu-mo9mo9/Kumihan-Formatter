#!/usr/bin/env python3
"""
è‡ªå‹•å“è³ªãƒ¬ãƒãƒ¼ãƒˆã‚·ã‚¹ãƒ†ãƒ 

PRã”ã¨ã®è©³ç´°å“è³ªåˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚„æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆã‚’è‡ªå‹•ç”Ÿæˆã™ã‚‹
"""

import json
import os
import tempfile
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import argparse
import subprocess

from quality_trend_tracker import QualityTrendTracker, QualitySnapshot


class QualityReportGenerator:
    """å“è³ªãƒ¬ãƒãƒ¼ãƒˆè‡ªå‹•ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, trend_tracker: QualityTrendTracker = None):
        self.trend_tracker = trend_tracker or QualityTrendTracker()
    
    def generate_pr_quality_report(self, current_metrics: Dict[str, Any],
                                 base_branch: str = "main",
                                 pr_number: str = "",
                                 pr_title: str = "") -> str:
        """PRã®å“è³ªãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        
        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã®æ¯”è¼ƒç”¨ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        baseline_snapshots = self.trend_tracker.get_recent_snapshots(days=7, branch=base_branch)
        baseline_snapshot = baseline_snapshots[0] if baseline_snapshots else None
        baseline_score = baseline_snapshot.total_score if baseline_snapshot else 90.0
        
        current_score = current_metrics.get('overall_quality_score', 0.0)
        score_diff = current_score - baseline_score
        
        # å“è³ªå¤‰åŒ–ã®è©•ä¾¡
        if score_diff >= 2.0:
            impact_emoji = "ğŸŸ¢"
            impact_text = "å“è³ªå‘ä¸Š"
            impact_color = "green"
        elif score_diff <= -2.0:
            impact_emoji = "ğŸ”´"
            impact_text = "å“è³ªä½ä¸‹"
            impact_color = "red"
        else:
            impact_emoji = "ğŸŸ¡"
            impact_text = "å“è³ªç¶­æŒ"
            impact_color = "yellow"
        
        # è©³ç´°åˆ†æ
        details = self._analyze_quality_details(current_metrics, baseline_snapshot)
        
        # ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æ
        coverage_analysis = self._analyze_coverage_changes(current_metrics)
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
        performance_analysis = self._analyze_performance(current_metrics)
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report = f"""## {impact_emoji} å“è³ªãƒ¬ãƒãƒ¼ãƒˆ - PR #{pr_number}

### ğŸ“Š å“è³ªã‚µãƒãƒªãƒ¼
- **ç·åˆã‚¹ã‚³ã‚¢**: {current_score:.1f}/100 ({score_diff:+.1f} vs {base_branch})
- **å“è³ªè©•ä¾¡**: {impact_text}
- **ãƒ†ã‚¹ãƒˆåˆæ ¼ç‡**: {current_metrics.get('test_pass_rate', 0.0):.1f}%
- **ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸**: {current_metrics.get('test_coverage', 0.0):.1f}%

### ğŸ” è©³ç´°åˆ†æ

#### è¨˜æ³•å“è³ª
- **åˆæ ¼ç‡**: {current_metrics.get('syntax_pass_rate', 0.0):.1f}%
- **ã‚¨ãƒ©ãƒ¼æ•°**: {current_metrics.get('syntax_errors', 0)}ä»¶
{details['syntax_details']}

#### ãƒ†ã‚¹ãƒˆå“è³ª
- **å®Ÿè¡Œãƒ†ã‚¹ãƒˆæ•°**: {current_metrics.get('passed_count', 0) + current_metrics.get('failed_count', 0)}ä»¶
- **æˆåŠŸ**: {current_metrics.get('passed_count', 0)}ä»¶
- **å¤±æ•—**: {current_metrics.get('failed_count', 0)}ä»¶
{details['test_details']}

#### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
- **å¹³å‡å¤‰æ›æ™‚é–“**: {current_metrics.get('conversion_time_avg', 0.0):.3f}ç§’
- **æœ€å¤§å¤‰æ›æ™‚é–“**: {current_metrics.get('conversion_time_max', 0.0):.3f}ç§’
{performance_analysis}

{coverage_analysis}

### ğŸ“ˆ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¯”è¼ƒ
| ãƒ¡ãƒˆãƒªã‚¯ã‚¹ | ç¾åœ¨ | ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ | å¤‰åŒ– |
|-----------|------|-------------|------|
| ç·åˆã‚¹ã‚³ã‚¢ | {current_score:.1f} | {baseline_score:.1f} | {score_diff:+.1f} |"""
        
        if baseline_snapshot:
            report += f"""
| ãƒ†ã‚¹ãƒˆåˆæ ¼ç‡ | {current_metrics.get('test_pass_rate', 0.0):.1f}% | {baseline_snapshot.test_pass_rate:.1f}% | {current_metrics.get('test_pass_rate', 0.0) - baseline_snapshot.test_pass_rate:+.1f}% |
| ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ | {current_metrics.get('test_coverage', 0.0):.1f}% | {baseline_snapshot.test_coverage:.1f}% | {current_metrics.get('test_coverage', 0.0) - baseline_snapshot.test_coverage:+.1f}% |"""
        else:
            report += f"""
| ãƒ†ã‚¹ãƒˆåˆæ ¼ç‡ | {current_metrics.get('test_pass_rate', 0.0):.1f}% | N/A | N/A |
| ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ | {current_metrics.get('test_coverage', 0.0):.1f}% | N/A | N/A |"""
        
        report += f"""

### ğŸ’¡ æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
{self._generate_recommendations(current_metrics, score_diff, details)}

---
ğŸ¤– Generated by Kumihan-Formatter Quality System
ç”Ÿæˆæ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
        
        return report
    
    def _analyze_quality_details(self, current_metrics: Dict[str, Any], 
                                baseline: Optional[QualitySnapshot]) -> Dict[str, str]:
        """å“è³ªè©³ç´°åˆ†æ"""
        details = {}
        
        # è¨˜æ³•å“è³ªè©³ç´°
        syntax_errors = current_metrics.get('syntax_errors', 0)
        if syntax_errors == 0:
            details['syntax_details'] = "- âœ… è¨˜æ³•ã‚¨ãƒ©ãƒ¼ãªã—"
        else:
            details['syntax_details'] = f"- âš ï¸ {syntax_errors}ä»¶ã®ã‚¨ãƒ©ãƒ¼ã‚’ä¿®æ­£ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™"
        
        # ãƒ†ã‚¹ãƒˆå“è³ªè©³ç´°
        failed_tests = current_metrics.get('failed_count', 0)
        if failed_tests == 0:
            details['test_details'] = "- âœ… å…¨ãƒ†ã‚¹ãƒˆæˆåŠŸ"
        else:
            details['test_details'] = f"- âŒ {failed_tests}ä»¶ã®ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ã¦ã„ã¾ã™"
        
        return details
    
    def _analyze_coverage_changes(self, current_metrics: Dict[str, Any]) -> str:
        """ã‚«ãƒãƒ¬ãƒƒã‚¸å¤‰åŒ–åˆ†æ"""
        coverage = current_metrics.get('test_coverage', 0.0)
        
        if coverage >= 90.0:
            return """#### ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸
- ğŸŒŸ **å„ªç§€**: 90%ä»¥ä¸Šã®ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚’é”æˆ
- é«˜å“è³ªãªãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ã§ã™"""
        elif coverage >= 80.0:
            return """#### ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸
- âœ… **è‰¯å¥½**: 80%ä»¥ä¸Šã®ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚’é”æˆ
- å“è³ªåŸºæº–ã‚’æº€ãŸã—ã¦ã„ã¾ã™"""
        elif coverage >= 70.0:
            return """#### ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸
- âš ï¸ **è¦æ³¨æ„**: ã‚«ãƒãƒ¬ãƒƒã‚¸ãŒ70%å°ã§ã™
- 80%ä»¥ä¸Šã‚’ç›®æ¨™ã«æ”¹å–„ã‚’æ¨å¥¨ã—ã¾ã™"""
        else:
            return """#### ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸
- ğŸ”´ **ä¸ååˆ†**: ã‚«ãƒãƒ¬ãƒƒã‚¸ãŒ70%æœªæº€ã§ã™
- æ—©æ€¥ãªãƒ†ã‚¹ãƒˆã®è¿½åŠ ãŒå¿…è¦ã§ã™"""
    
    def _analyze_performance(self, current_metrics: Dict[str, Any]) -> str:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ"""
        avg_time = current_metrics.get('conversion_time_avg', 0.0)
        max_time = current_metrics.get('conversion_time_max', 0.0)
        
        if avg_time <= 0.1:
            return "- ğŸš€ **é«˜é€Ÿ**: å¹³å‡0.1ç§’ä»¥ä¸‹ã®é«˜é€Ÿå¤‰æ›"
        elif avg_time <= 0.5:
            return "- âœ… **è‰¯å¥½**: å¹³å‡0.5ç§’ä»¥ä¸‹ã®å¤‰æ›æ™‚é–“"
        elif avg_time <= 1.0:
            return "- âš ï¸ **è¦æ³¨æ„**: å¹³å‡1ç§’ä»¥ä¸‹ã ãŒæ”¹å–„ä½™åœ°ã‚ã‚Š"
        else:
            return "- ğŸ”´ **é…ã„**: å¹³å‡1ç§’è¶…éã€æœ€é©åŒ–ãŒå¿…è¦"
    
    def _generate_recommendations(self, current_metrics: Dict[str, Any], 
                                score_diff: float, details: Dict[str, str]) -> str:
        """æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç”Ÿæˆ"""
        recommendations = []
        
        # ã‚¹ã‚³ã‚¢å¤‰åŒ–ã«åŸºã¥ãæ¨å¥¨
        if score_diff <= -5.0:
            recommendations.append("ğŸš¨ **ç·Šæ€¥**: å“è³ªãŒå¤§å¹…ã«ä½ä¸‹ã—ã¦ã„ã¾ã™ã€‚åŸå› èª¿æŸ»ã¨ä¿®æ­£ãŒå¿…è¦ã§ã™")
        elif score_diff <= -2.0:
            recommendations.append("âš ï¸ **æ³¨æ„**: å“è³ªä½ä¸‹ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚å½±éŸ¿ç¯„å›²ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
        elif score_diff >= 5.0:
            recommendations.append("ğŸ‰ **ç´ æ™´ã‚‰ã—ã„**: å“è³ªãŒå¤§å¹…ã«å‘ä¸Šã—ã¾ã—ãŸï¼")
        
        # ãƒ†ã‚¹ãƒˆå¤±æ•—ã«åŸºã¥ãæ¨å¥¨
        failed_tests = current_metrics.get('failed_count', 0)
        if failed_tests > 0:
            recommendations.append(f"ğŸ”§ {failed_tests}ä»¶ã®å¤±æ•—ãƒ†ã‚¹ãƒˆã‚’ä¿®æ­£ã—ã¦ãã ã•ã„")
        
        # ã‚«ãƒãƒ¬ãƒƒã‚¸ã«åŸºã¥ãæ¨å¥¨
        coverage = current_metrics.get('test_coverage', 0.0)
        if coverage < 80.0:
            recommendations.append(f"ğŸ“ ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ã‚’{coverage:.1f}% â†’ 80%+ã«å‘ä¸Šã•ã›ã¦ãã ã•ã„")
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«åŸºã¥ãæ¨å¥¨
        avg_time = current_metrics.get('conversion_time_avg', 0.0)
        if avg_time > 1.0:
            recommendations.append("âš¡ å¤‰æ›ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®æœ€é©åŒ–ã‚’æ¤œè¨ã—ã¦ãã ã•ã„")
        
        if not recommendations:
            recommendations.append("âœ… å“è³ªã¯è‰¯å¥½ã§ã™ã€‚ç¾åœ¨ã®é–‹ç™ºæ–¹é‡ã‚’ç¶™ç¶šã—ã¦ãã ã•ã„")
        
        return "\n".join(f"- {rec}" for rec in recommendations)
    
    def generate_comparison_report(self, before_metrics: Dict[str, Any],
                                 after_metrics: Dict[str, Any],
                                 change_description: str = "") -> str:
        """å¤‰æ›´å‰å¾Œã®æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆ"""
        
        before_score = before_metrics.get('overall_quality_score', 0.0)
        after_score = after_metrics.get('overall_quality_score', 0.0)
        score_change = after_score - before_score
        
        # å¤‰åŒ–ã®è©•ä¾¡
        if score_change >= 2.0:
            impact_emoji = "ğŸ“ˆ"
            impact_text = "æ”¹å–„"
        elif score_change <= -2.0:
            impact_emoji = "ğŸ“‰"
            impact_text = "æ‚ªåŒ–"
        else:
            impact_emoji = "â¡ï¸"
            impact_text = "ç¶­æŒ"
        
        report = f"""## å“è³ªæ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆ {impact_emoji}

### å¤‰æ›´å†…å®¹
{change_description or "å¤‰æ›´å†…å®¹ã®èª¬æ˜ãªã—"}

### ğŸ“Š å“è³ªå¤‰åŒ–ã‚µãƒãƒªãƒ¼
- **ç·åˆã‚¹ã‚³ã‚¢**: {before_score:.1f} â†’ {after_score:.1f} ({score_change:+.1f})
- **è©•ä¾¡**: {impact_text}

### ğŸ“‹ è©³ç´°æ¯”è¼ƒ

| ãƒ¡ãƒˆãƒªã‚¯ã‚¹ | å¤‰æ›´å‰ | å¤‰æ›´å¾Œ | å¤‰åŒ– |
|-----------|--------|--------|------|
| ç·åˆã‚¹ã‚³ã‚¢ | {before_score:.1f}/100 | {after_score:.1f}/100 | {score_change:+.1f} |
| è¨˜æ³•åˆæ ¼ç‡ | {before_metrics.get('syntax_pass_rate', 0.0):.1f}% | {after_metrics.get('syntax_pass_rate', 0.0):.1f}% | {after_metrics.get('syntax_pass_rate', 0.0) - before_metrics.get('syntax_pass_rate', 0.0):+.1f}% |
| ãƒ†ã‚¹ãƒˆåˆæ ¼ç‡ | {before_metrics.get('test_pass_rate', 0.0):.1f}% | {after_metrics.get('test_pass_rate', 0.0):.1f}% | {after_metrics.get('test_pass_rate', 0.0) - before_metrics.get('test_pass_rate', 0.0):+.1f}% |
| ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ | {before_metrics.get('test_coverage', 0.0):.1f}% | {after_metrics.get('test_coverage', 0.0):.1f}% | {after_metrics.get('test_coverage', 0.0) - before_metrics.get('test_coverage', 0.0):+.1f}% |
| å¹³å‡å¤‰æ›æ™‚é–“ | {before_metrics.get('conversion_time_avg', 0.0):.3f}ç§’ | {after_metrics.get('conversion_time_avg', 0.0):.3f}ç§’ | {after_metrics.get('conversion_time_avg', 0.0) - before_metrics.get('conversion_time_avg', 0.0):+.3f}ç§’ |

### ğŸ’¡ å½±éŸ¿è©•ä¾¡
{self._evaluate_impact(score_change, before_metrics, after_metrics)}

---
ğŸ¤– Generated by Kumihan-Formatter Quality System
"""
        
        return report
    
    def _evaluate_impact(self, score_change: float, 
                        before_metrics: Dict[str, Any], 
                        after_metrics: Dict[str, Any]) -> str:
        """å½±éŸ¿è©•ä¾¡"""
        impact_points = []
        
        if score_change >= 5.0:
            impact_points.append("ğŸŒŸ ç·åˆå“è³ªãŒå¤§å¹…ã«å‘ä¸Šã—ã¾ã—ãŸ")
        elif score_change >= 2.0:
            impact_points.append("âœ… ç·åˆå“è³ªãŒå‘ä¸Šã—ã¾ã—ãŸ")
        elif score_change <= -5.0:
            impact_points.append("ğŸš¨ ç·åˆå“è³ªãŒå¤§å¹…ã«æ‚ªåŒ–ã—ã¾ã—ãŸ")
        elif score_change <= -2.0:
            impact_points.append("âš ï¸ ç·åˆå“è³ªãŒæ‚ªåŒ–ã—ã¾ã—ãŸ")
        else:
            impact_points.append("â¡ï¸ ç·åˆå“è³ªã¯ç¶­æŒã•ã‚Œã¦ã„ã¾ã™")
        
        # ãƒ†ã‚¹ãƒˆåˆæ ¼ç‡ã®å¤‰åŒ–
        test_change = after_metrics.get('test_pass_rate', 0.0) - before_metrics.get('test_pass_rate', 0.0)
        if test_change >= 5.0:
            impact_points.append("ğŸ“ˆ ãƒ†ã‚¹ãƒˆåˆæ ¼ç‡ãŒå¤§å¹…ã«æ”¹å–„")
        elif test_change <= -5.0:
            impact_points.append("ğŸ“‰ ãƒ†ã‚¹ãƒˆåˆæ ¼ç‡ãŒæ‚ªåŒ–")
        
        # ã‚«ãƒãƒ¬ãƒƒã‚¸ã®å¤‰åŒ–
        coverage_change = after_metrics.get('test_coverage', 0.0) - before_metrics.get('test_coverage', 0.0)
        if coverage_change >= 5.0:
            impact_points.append("ğŸ“ ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ãŒæ”¹å–„")
        elif coverage_change <= -5.0:
            impact_points.append("ğŸ“ ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ãŒæ‚ªåŒ–")
        
        return "\n".join(f"- {point}" for point in impact_points)
    
    def generate_dashboard_data(self, days: int = 30) -> Dict[str, Any]:
        """å“è³ªãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”¨ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
        snapshots = self.trend_tracker.get_recent_snapshots(days)
        trends = self.trend_tracker.analyze_trends(days)
        degradation = self.trend_tracker.detect_quality_degradation()
        
        # æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿
        timeline_data = []
        for snapshot in reversed(snapshots[-30:]):  # æœ€æ–°30ä»¶
            timeline_data.append({
                "timestamp": snapshot.timestamp,
                "total_score": snapshot.total_score,
                "test_pass_rate": snapshot.test_pass_rate,
                "test_coverage": snapshot.test_coverage,
                "branch": snapshot.branch_name
            })
        
        # å“è³ªåˆ†å¸ƒãƒ‡ãƒ¼ã‚¿
        if snapshots:
            scores = [s.total_score for s in snapshots]
            quality_distribution = {
                "excellent": len([s for s in scores if s >= 90]),
                "good": len([s for s in scores if 80 <= s < 90]),
                "needs_improvement": len([s for s in scores if 70 <= s < 80]),
                "poor": len([s for s in scores if s < 70])
            }
        else:
            quality_distribution = {"excellent": 0, "good": 0, "needs_improvement": 0, "poor": 0}
        
        return {
            "generated_at": datetime.now().isoformat(),
            "period_days": days,
            "timeline_data": timeline_data,
            "trends": trends,
            "degradation_status": degradation,
            "quality_distribution": quality_distribution,
            "total_snapshots": len(snapshots),
            "latest_score": snapshots[0].total_score if snapshots else 0.0
        }


def main():
    parser = argparse.ArgumentParser(description="å“è³ªãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ")
    parser.add_argument('command', choices=['pr-report', 'compare', 'dashboard-data'], 
                       help='å®Ÿè¡Œã™ã‚‹ã‚³ãƒãƒ³ãƒ‰')
    parser.add_argument('--current-metrics', type=Path, required=True,
                       help='ç¾åœ¨ã®å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹JSONãƒ•ã‚¡ã‚¤ãƒ«')
    parser.add_argument('--baseline-metrics', type=Path,
                       help='ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹JSONãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆcompareç”¨ï¼‰')
    parser.add_argument('--pr-number', default="", help='ãƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆç•ªå·')
    parser.add_argument('--pr-title', default="", help='ãƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¿ã‚¤ãƒˆãƒ«')
    parser.add_argument('--base-branch', default="main", help='ãƒ™ãƒ¼ã‚¹ãƒ–ãƒ©ãƒ³ãƒ')
    parser.add_argument('--change-description', default="", help='å¤‰æ›´å†…å®¹ã®èª¬æ˜')
    parser.add_argument('--output', type=Path, help='å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
    parser.add_argument('--days', type=int, default=30, help='åˆ†ææœŸé–“ï¼ˆæ—¥æ•°ï¼‰')
    
    args = parser.parse_args()
    
    generator = QualityReportGenerator()
    
    # ç¾åœ¨ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’èª­ã¿è¾¼ã¿
    with open(args.current_metrics, 'r', encoding='utf-8') as f:
        current_metrics = json.load(f)
    
    if args.command == 'pr-report':
        report = generator.generate_pr_quality_report(
            current_metrics, args.base_branch, args.pr_number, args.pr_title
        )
    
    elif args.command == 'compare':
        if not args.baseline_metrics:
            print("âŒ ã‚¨ãƒ©ãƒ¼: --baseline-metrics ãŒå¿…è¦ã§ã™")
            return
        
        with open(args.baseline_metrics, 'r', encoding='utf-8') as f:
            baseline_metrics = json.load(f)
        
        report = generator.generate_comparison_report(
            baseline_metrics, current_metrics, args.change_description
        )
    
    elif args.command == 'dashboard-data':
        dashboard_data = generator.generate_dashboard_data(args.days)
        report = json.dumps(dashboard_data, indent=2, ensure_ascii=False)
    
    # å‡ºåŠ›
    if args.output:
        with open(args.output, 'w', encoding='utf-8') as f:
            f.write(report)
        print(f"ğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆã‚’å‡ºåŠ›ã—ã¾ã—ãŸ: {args.output}")
    else:
        print(report)


if __name__ == "__main__":
    main()