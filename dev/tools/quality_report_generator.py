#!/usr/bin/env python3
"""
自動品質レポートシステム

PRごとの詳細品質分析レポートや比較レポートを自動生成する
"""

import json
import os
import tempfile
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import argparse
import subprocess

from quality_trend_tracker import QualityTrendTracker, QualitySnapshot


class QualityReportGenerator:
    """品質レポート自動生成システム"""
    
    def __init__(self, trend_tracker: QualityTrendTracker = None):
        self.trend_tracker = trend_tracker or QualityTrendTracker()
    
    def generate_pr_quality_report(self, current_metrics: Dict[str, Any],
                                 base_branch: str = "main",
                                 pr_number: str = "",
                                 pr_title: str = "") -> str:
        """PRの品質レポートを生成"""
        
        # ベースラインとの比較用データを取得
        baseline_snapshots = self.trend_tracker.get_recent_snapshots(days=7, branch=base_branch)
        baseline_snapshot = baseline_snapshots[0] if baseline_snapshots else None
        baseline_score = baseline_snapshot.total_score if baseline_snapshot else 90.0
        
        current_score = current_metrics.get('overall_quality_score', 0.0)
        score_diff = current_score - baseline_score
        
        # 品質変化の評価
        if score_diff >= 2.0:
            impact_emoji = "🟢"
            impact_text = "品質向上"
            impact_color = "green"
        elif score_diff <= -2.0:
            impact_emoji = "🔴"
            impact_text = "品質低下"
            impact_color = "red"
        else:
            impact_emoji = "🟡"
            impact_text = "品質維持"
            impact_color = "yellow"
        
        # 詳細分析
        details = self._analyze_quality_details(current_metrics, baseline_snapshot)
        
        # テストカバレッジ分析
        coverage_analysis = self._analyze_coverage_changes(current_metrics)
        
        # パフォーマンス分析
        performance_analysis = self._analyze_performance(current_metrics)
        
        # レポート生成
        report = f"""## {impact_emoji} 品質レポート - PR #{pr_number}

### 📊 品質サマリー
- **総合スコア**: {current_score:.1f}/100 ({score_diff:+.1f} vs {base_branch})
- **品質評価**: {impact_text}
- **テスト合格率**: {current_metrics.get('test_pass_rate', 0.0):.1f}%
- **テストカバレッジ**: {current_metrics.get('test_coverage', 0.0):.1f}%

### 🔍 詳細分析

#### 記法品質
- **合格率**: {current_metrics.get('syntax_pass_rate', 0.0):.1f}%
- **エラー数**: {current_metrics.get('syntax_errors', 0)}件
{details['syntax_details']}

#### テスト品質
- **実行テスト数**: {current_metrics.get('passed_count', 0) + current_metrics.get('failed_count', 0)}件
- **成功**: {current_metrics.get('passed_count', 0)}件
- **失敗**: {current_metrics.get('failed_count', 0)}件
{details['test_details']}

#### パフォーマンス
- **平均変換時間**: {current_metrics.get('conversion_time_avg', 0.0):.3f}秒
- **最大変換時間**: {current_metrics.get('conversion_time_max', 0.0):.3f}秒
{performance_analysis}

{coverage_analysis}

### 📈 ベースライン比較
| メトリクス | 現在 | ベースライン | 変化 |
|-----------|------|-------------|------|
| 総合スコア | {current_score:.1f} | {baseline_score:.1f} | {score_diff:+.1f} |"""
        
        if baseline_snapshot:
            report += f"""
| テスト合格率 | {current_metrics.get('test_pass_rate', 0.0):.1f}% | {baseline_snapshot.test_pass_rate:.1f}% | {current_metrics.get('test_pass_rate', 0.0) - baseline_snapshot.test_pass_rate:+.1f}% |
| テストカバレッジ | {current_metrics.get('test_coverage', 0.0):.1f}% | {baseline_snapshot.test_coverage:.1f}% | {current_metrics.get('test_coverage', 0.0) - baseline_snapshot.test_coverage:+.1f}% |"""
        else:
            report += f"""
| テスト合格率 | {current_metrics.get('test_pass_rate', 0.0):.1f}% | N/A | N/A |
| テストカバレッジ | {current_metrics.get('test_coverage', 0.0):.1f}% | N/A | N/A |"""
        
        report += f"""

### 💡 推奨アクション
{self._generate_recommendations(current_metrics, score_diff, details)}

---
🤖 Generated by Kumihan-Formatter Quality System
生成時刻: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
        
        return report
    
    def _analyze_quality_details(self, current_metrics: Dict[str, Any], 
                                baseline: Optional[QualitySnapshot]) -> Dict[str, str]:
        """品質詳細分析"""
        details = {}
        
        # 記法品質詳細
        syntax_errors = current_metrics.get('syntax_errors', 0)
        if syntax_errors == 0:
            details['syntax_details'] = "- ✅ 記法エラーなし"
        else:
            details['syntax_details'] = f"- ⚠️ {syntax_errors}件のエラーを修正する必要があります"
        
        # テスト品質詳細
        failed_tests = current_metrics.get('failed_count', 0)
        if failed_tests == 0:
            details['test_details'] = "- ✅ 全テスト成功"
        else:
            details['test_details'] = f"- ❌ {failed_tests}件のテストが失敗しています"
        
        return details
    
    def _analyze_coverage_changes(self, current_metrics: Dict[str, Any]) -> str:
        """カバレッジ変化分析"""
        coverage = current_metrics.get('test_coverage', 0.0)
        
        if coverage >= 90.0:
            return """#### テストカバレッジ
- 🌟 **優秀**: 90%以上のカバレッジを達成
- 高品質なテストカバレッジです"""
        elif coverage >= 80.0:
            return """#### テストカバレッジ
- ✅ **良好**: 80%以上のカバレッジを達成
- 品質基準を満たしています"""
        elif coverage >= 70.0:
            return """#### テストカバレッジ
- ⚠️ **要注意**: カバレッジが70%台です
- 80%以上を目標に改善を推奨します"""
        else:
            return """#### テストカバレッジ
- 🔴 **不十分**: カバレッジが70%未満です
- 早急なテストの追加が必要です"""
    
    def _analyze_performance(self, current_metrics: Dict[str, Any]) -> str:
        """パフォーマンス分析"""
        avg_time = current_metrics.get('conversion_time_avg', 0.0)
        max_time = current_metrics.get('conversion_time_max', 0.0)
        
        if avg_time <= 0.1:
            return "- 🚀 **高速**: 平均0.1秒以下の高速変換"
        elif avg_time <= 0.5:
            return "- ✅ **良好**: 平均0.5秒以下の変換時間"
        elif avg_time <= 1.0:
            return "- ⚠️ **要注意**: 平均1秒以下だが改善余地あり"
        else:
            return "- 🔴 **遅い**: 平均1秒超過、最適化が必要"
    
    def _generate_recommendations(self, current_metrics: Dict[str, Any], 
                                score_diff: float, details: Dict[str, str]) -> str:
        """推奨アクション生成"""
        recommendations = []
        
        # スコア変化に基づく推奨
        if score_diff <= -5.0:
            recommendations.append("🚨 **緊急**: 品質が大幅に低下しています。原因調査と修正が必要です")
        elif score_diff <= -2.0:
            recommendations.append("⚠️ **注意**: 品質低下が検出されました。影響範囲を確認してください")
        elif score_diff >= 5.0:
            recommendations.append("🎉 **素晴らしい**: 品質が大幅に向上しました！")
        
        # テスト失敗に基づく推奨
        failed_tests = current_metrics.get('failed_count', 0)
        if failed_tests > 0:
            recommendations.append(f"🔧 {failed_tests}件の失敗テストを修正してください")
        
        # カバレッジに基づく推奨
        coverage = current_metrics.get('test_coverage', 0.0)
        if coverage < 80.0:
            recommendations.append(f"📝 テストカバレッジを{coverage:.1f}% → 80%+に向上させてください")
        
        # パフォーマンスに基づく推奨
        avg_time = current_metrics.get('conversion_time_avg', 0.0)
        if avg_time > 1.0:
            recommendations.append("⚡ 変換パフォーマンスの最適化を検討してください")
        
        if not recommendations:
            recommendations.append("✅ 品質は良好です。現在の開発方針を継続してください")
        
        return "\n".join(f"- {rec}" for rec in recommendations)
    
    def generate_comparison_report(self, before_metrics: Dict[str, Any],
                                 after_metrics: Dict[str, Any],
                                 change_description: str = "") -> str:
        """変更前後の比較レポート"""
        
        before_score = before_metrics.get('overall_quality_score', 0.0)
        after_score = after_metrics.get('overall_quality_score', 0.0)
        score_change = after_score - before_score
        
        # 変化の評価
        if score_change >= 2.0:
            impact_emoji = "📈"
            impact_text = "改善"
        elif score_change <= -2.0:
            impact_emoji = "📉"
            impact_text = "悪化"
        else:
            impact_emoji = "➡️"
            impact_text = "維持"
        
        report = f"""## 品質比較レポート {impact_emoji}

### 変更内容
{change_description or "変更内容の説明なし"}

### 📊 品質変化サマリー
- **総合スコア**: {before_score:.1f} → {after_score:.1f} ({score_change:+.1f})
- **評価**: {impact_text}

### 📋 詳細比較

| メトリクス | 変更前 | 変更後 | 変化 |
|-----------|--------|--------|------|
| 総合スコア | {before_score:.1f}/100 | {after_score:.1f}/100 | {score_change:+.1f} |
| 記法合格率 | {before_metrics.get('syntax_pass_rate', 0.0):.1f}% | {after_metrics.get('syntax_pass_rate', 0.0):.1f}% | {after_metrics.get('syntax_pass_rate', 0.0) - before_metrics.get('syntax_pass_rate', 0.0):+.1f}% |
| テスト合格率 | {before_metrics.get('test_pass_rate', 0.0):.1f}% | {after_metrics.get('test_pass_rate', 0.0):.1f}% | {after_metrics.get('test_pass_rate', 0.0) - before_metrics.get('test_pass_rate', 0.0):+.1f}% |
| テストカバレッジ | {before_metrics.get('test_coverage', 0.0):.1f}% | {after_metrics.get('test_coverage', 0.0):.1f}% | {after_metrics.get('test_coverage', 0.0) - before_metrics.get('test_coverage', 0.0):+.1f}% |
| 平均変換時間 | {before_metrics.get('conversion_time_avg', 0.0):.3f}秒 | {after_metrics.get('conversion_time_avg', 0.0):.3f}秒 | {after_metrics.get('conversion_time_avg', 0.0) - before_metrics.get('conversion_time_avg', 0.0):+.3f}秒 |

### 💡 影響評価
{self._evaluate_impact(score_change, before_metrics, after_metrics)}

---
🤖 Generated by Kumihan-Formatter Quality System
"""
        
        return report
    
    def _evaluate_impact(self, score_change: float, 
                        before_metrics: Dict[str, Any], 
                        after_metrics: Dict[str, Any]) -> str:
        """影響評価"""
        impact_points = []
        
        if score_change >= 5.0:
            impact_points.append("🌟 総合品質が大幅に向上しました")
        elif score_change >= 2.0:
            impact_points.append("✅ 総合品質が向上しました")
        elif score_change <= -5.0:
            impact_points.append("🚨 総合品質が大幅に悪化しました")
        elif score_change <= -2.0:
            impact_points.append("⚠️ 総合品質が悪化しました")
        else:
            impact_points.append("➡️ 総合品質は維持されています")
        
        # テスト合格率の変化
        test_change = after_metrics.get('test_pass_rate', 0.0) - before_metrics.get('test_pass_rate', 0.0)
        if test_change >= 5.0:
            impact_points.append("📈 テスト合格率が大幅に改善")
        elif test_change <= -5.0:
            impact_points.append("📉 テスト合格率が悪化")
        
        # カバレッジの変化
        coverage_change = after_metrics.get('test_coverage', 0.0) - before_metrics.get('test_coverage', 0.0)
        if coverage_change >= 5.0:
            impact_points.append("📝 テストカバレッジが改善")
        elif coverage_change <= -5.0:
            impact_points.append("📝 テストカバレッジが悪化")
        
        return "\n".join(f"- {point}" for point in impact_points)
    
    def generate_dashboard_data(self, days: int = 30) -> Dict[str, Any]:
        """品質ダッシュボード用データ生成"""
        snapshots = self.trend_tracker.get_recent_snapshots(days)
        trends = self.trend_tracker.analyze_trends(days)
        degradation = self.trend_tracker.detect_quality_degradation()
        
        # 時系列データ
        timeline_data = []
        for snapshot in reversed(snapshots[-30:]):  # 最新30件
            timeline_data.append({
                "timestamp": snapshot.timestamp,
                "total_score": snapshot.total_score,
                "test_pass_rate": snapshot.test_pass_rate,
                "test_coverage": snapshot.test_coverage,
                "branch": snapshot.branch_name
            })
        
        # 品質分布データ
        if snapshots:
            scores = [s.total_score for s in snapshots]
            quality_distribution = {
                "excellent": len([s for s in scores if s >= 90]),
                "good": len([s for s in scores if 80 <= s < 90]),
                "needs_improvement": len([s for s in scores if 70 <= s < 80]),
                "poor": len([s for s in scores if s < 70])
            }
        else:
            quality_distribution = {"excellent": 0, "good": 0, "needs_improvement": 0, "poor": 0}
        
        return {
            "generated_at": datetime.now().isoformat(),
            "period_days": days,
            "timeline_data": timeline_data,
            "trends": trends,
            "degradation_status": degradation,
            "quality_distribution": quality_distribution,
            "total_snapshots": len(snapshots),
            "latest_score": snapshots[0].total_score if snapshots else 0.0
        }


def main():
    parser = argparse.ArgumentParser(description="品質レポート生成システム")
    parser.add_argument('command', choices=['pr-report', 'compare', 'dashboard-data'], 
                       help='実行するコマンド')
    parser.add_argument('--current-metrics', type=Path, required=True,
                       help='現在の品質メトリクスJSONファイル')
    parser.add_argument('--baseline-metrics', type=Path,
                       help='ベースライン品質メトリクスJSONファイル（compare用）')
    parser.add_argument('--pr-number', default="", help='プルリクエスト番号')
    parser.add_argument('--pr-title', default="", help='プルリクエストタイトル')
    parser.add_argument('--base-branch', default="main", help='ベースブランチ')
    parser.add_argument('--change-description', default="", help='変更内容の説明')
    parser.add_argument('--output', type=Path, help='出力ファイルパス')
    parser.add_argument('--days', type=int, default=30, help='分析期間（日数）')
    
    args = parser.parse_args()
    
    generator = QualityReportGenerator()
    
    # 現在のメトリクスを読み込み
    with open(args.current_metrics, 'r', encoding='utf-8') as f:
        current_metrics = json.load(f)
    
    if args.command == 'pr-report':
        report = generator.generate_pr_quality_report(
            current_metrics, args.base_branch, args.pr_number, args.pr_title
        )
    
    elif args.command == 'compare':
        if not args.baseline_metrics:
            print("❌ エラー: --baseline-metrics が必要です")
            return
        
        with open(args.baseline_metrics, 'r', encoding='utf-8') as f:
            baseline_metrics = json.load(f)
        
        report = generator.generate_comparison_report(
            baseline_metrics, current_metrics, args.change_description
        )
    
    elif args.command == 'dashboard-data':
        dashboard_data = generator.generate_dashboard_data(args.days)
        report = json.dumps(dashboard_data, indent=2, ensure_ascii=False)
    
    # 出力
    if args.output:
        with open(args.output, 'w', encoding='utf-8') as f:
            f.write(report)
        print(f"📄 レポートを出力しました: {args.output}")
    else:
        print(report)


if __name__ == "__main__":
    main()