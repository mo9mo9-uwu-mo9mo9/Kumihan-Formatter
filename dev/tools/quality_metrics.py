#!/usr/bin/env python3
"""
å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ãƒ»åˆ†æãƒ„ãƒ¼ãƒ«

ä½¿ç”¨æ–¹æ³•:
    # åŸºæœ¬çš„ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
    python dev/tools/quality_metrics.py <directory>
    
    # JSONãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›
    python dev/tools/quality_metrics.py <directory> --format=json --output=metrics.json
    
    # å“è³ªã‚²ãƒ¼ãƒˆåˆ¤å®š
    python dev/tools/quality_metrics.py <directory> --check-gates
"""

import os
import sys
import json
import time
import argparse
import subprocess
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from datetime import datetime
try:
    import yaml
except ImportError:
    yaml = None


@dataclass
class QualityMetrics:
    """å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹æƒ…å ±"""
    timestamp: str
    project_name: str
    
    # ãƒ•ã‚¡ã‚¤ãƒ«çµ±è¨ˆ
    total_files: int
    txt_files: int
    total_lines: int
    total_size_bytes: int
    
    # è¨˜æ³•çµ±è¨ˆ
    syntax_errors: int
    syntax_pass_rate: float
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ
    conversion_time_avg: float
    conversion_time_max: float
    html_size_avg: float
    html_size_max: float
    
    # ãƒ†ã‚¹ãƒˆçµ±è¨ˆ
    test_pass_rate: float
    test_coverage: float
    
    # å“è³ªã‚¹ã‚³ã‚¢
    overall_quality_score: float
    
    # è©³ç´°æƒ…å ±
    file_details: List[Dict[str, Any]]
    error_details: List[Dict[str, Any]]


class QualityAnalyzer:
    """å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹åˆ†æã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, config_path: Path = None):
        self.config = self._load_config(config_path)
        self.project_root = Path.cwd()
    
    def _load_config(self, config_path: Path = None) -> Dict[str, Any]:
        """è¨­å®šã‚’èª­ã¿è¾¼ã¿"""
        default_config = {
            "quality_gates": {
                "syntax_pass_rate_min": 95.0,
                "conversion_time_max": 5.0,
                "html_size_growth_max": 1.2,
                "test_pass_rate_min": 100.0,
                "overall_quality_min": 80.0
            },
            "performance_thresholds": {
                "file_size_max": 1048576,  # 1MB
                "lines_max": 10000,
                "conversion_timeout": 30.0
            },
            "exclusions": {
                "directories": [".git", "__pycache__", ".venv", "node_modules", "dist", "build"],
                "file_patterns": ["*.pyc", "*.log", "*.tmp"]
            }
        }
        
        if config_path and config_path.exists() and yaml:
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    user_config = yaml.safe_load(f)
                    default_config.update(user_config)
            except Exception:
                pass
        
        return default_config
    
    def analyze_project(self, target_dir: Path) -> QualityMetrics:
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ã®å“è³ªåˆ†æ"""
        print("ğŸ” å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ã‚’é–‹å§‹...")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«çµ±è¨ˆåé›†
        file_stats = self._collect_file_stats(target_dir)
        print(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«çµ±è¨ˆ: {file_stats['total_files']}ãƒ•ã‚¡ã‚¤ãƒ«, {file_stats['txt_files']}ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«")
        
        # è¨˜æ³•ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ
        syntax_stats = self._run_syntax_validation(target_dir)
        print(f"ğŸ“ è¨˜æ³•ãƒã‚§ãƒƒã‚¯: {syntax_stats['pass_rate']:.1f}% åˆæ ¼")
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        perf_stats = self._run_performance_tests(target_dir)
        print(f"âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹: å¹³å‡{perf_stats['conversion_time_avg']:.2f}ç§’")
        
        # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        test_stats = self._run_tests()
        print(f"ğŸ§ª ãƒ†ã‚¹ãƒˆ: {test_stats['pass_rate']:.1f}% åˆæ ¼")
        
        # å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—
        quality_score = self._calculate_quality_score(syntax_stats, perf_stats, test_stats)
        print(f"ğŸ¯ ç·åˆå“è³ªã‚¹ã‚³ã‚¢: {quality_score:.1f}/100")
        
        return QualityMetrics(
            timestamp=datetime.now().isoformat(),
            project_name=self.project_root.name,
            total_files=file_stats['total_files'],
            txt_files=file_stats['txt_files'],
            total_lines=file_stats['total_lines'],
            total_size_bytes=file_stats['total_size'],
            syntax_errors=syntax_stats['error_count'],
            syntax_pass_rate=syntax_stats['pass_rate'],
            conversion_time_avg=perf_stats['conversion_time_avg'],
            conversion_time_max=perf_stats['conversion_time_max'],
            html_size_avg=perf_stats['html_size_avg'],
            html_size_max=perf_stats['html_size_max'],
            test_pass_rate=test_stats['pass_rate'],
            test_coverage=test_stats['coverage'],
            overall_quality_score=quality_score,
            file_details=file_stats['details'],
            error_details=syntax_stats['errors']
        )
    
    def _collect_file_stats(self, target_dir: Path) -> Dict[str, Any]:
        """ãƒ•ã‚¡ã‚¤ãƒ«çµ±è¨ˆã‚’åé›†"""
        total_files = 0
        txt_files = 0
        total_lines = 0
        total_size = 0
        file_details = []
        
        for root, dirs, files in os.walk(target_dir):
            # é™¤å¤–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã‚¹ã‚­ãƒƒãƒ—
            dirs[:] = [d for d in dirs if d not in self.config['exclusions']['directories']]
            
            for file in files:
                file_path = Path(root) / file
                
                # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ã‚¹ã‚­ãƒƒãƒ—
                if any(file_path.match(pattern) for pattern in self.config['exclusions']['file_patterns']):
                    continue
                
                total_files += 1
                file_size = file_path.stat().st_size
                total_size += file_size
                
                if file_path.suffix.lower() == '.txt':
                    txt_files += 1
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            lines = len(f.readlines())
                            total_lines += lines
                        
                        file_details.append({
                            'path': str(file_path.relative_to(self.project_root)),
                            'size': file_size,
                            'lines': lines
                        })
                    except Exception:
                        pass
        
        return {
            'total_files': total_files,
            'txt_files': txt_files,
            'total_lines': total_lines,
            'total_size': total_size,
            'details': file_details
        }
    
    def _run_syntax_validation(self, target_dir: Path) -> Dict[str, Any]:
        """è¨˜æ³•æ¤œè¨¼ã‚’å®Ÿè¡Œ"""
        try:
            # syntax_validatorã‚’å®Ÿè¡Œ
            cmd = [
                sys.executable, 
                'dev/tools/syntax_validator.py',
                '--mode=strict',
                '--quiet',
                str(target_dir / '**/*.txt')
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, shell=True)
            
            # çµæœã‚’è§£æ
            error_count = 0
            total_files = 0
            errors = []
            
            if result.returncode != 0:
                # ã‚¨ãƒ©ãƒ¼å‡ºåŠ›ã‚’è§£æ
                for line in result.stdout.split('\n'):
                    if '[ã‚¨ãƒ©ãƒ¼]' in line and 'ã‚¨ãƒ©ãƒ¼' in line:
                        parts = line.split(':')
                        if len(parts) >= 2:
                            error_count += int(parts[1].split()[0])
                    elif '[å®Œäº†]' in line and 'OK' in line:
                        total_files += 1
                    elif 'Line' in line:
                        errors.append({'message': line.strip()})
            
            # ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’æ•°ãˆã‚‹
            txt_files = len(list(target_dir.glob('**/*.txt')))
            total_files = max(total_files, txt_files)
            
            pass_rate = ((total_files - error_count) / total_files * 100) if total_files > 0 else 0
            
            return {
                'error_count': error_count,
                'total_files': total_files,
                'pass_rate': pass_rate,
                'errors': errors
            }
            
        except Exception as e:
            return {
                'error_count': 999,
                'total_files': 1,
                'pass_rate': 0.0,
                'errors': [{'message': f'æ¤œè¨¼å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}'}]
            }
    
    def _run_performance_tests(self, target_dir: Path) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
        conversion_times = []
        html_sizes = []
        
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
        sample_files = list(target_dir.glob('examples/*.txt'))[:5]  # æœ€å¤§5ãƒ•ã‚¡ã‚¤ãƒ«
        
        for sample_file in sample_files:
            try:
                start_time = time.time()
                
                # å¤‰æ›å®Ÿè¡Œ
                cmd = [
                    sys.executable, 
                    '-m', 'kumihan_formatter.cli',
                    str(sample_file),
                    '--no-preview',
                    '-o', '/tmp/quality_test'
                ]
                
                result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
                
                end_time = time.time()
                conversion_time = end_time - start_time
                conversion_times.append(conversion_time)
                
                # HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’ç¢ºèª
                html_file = Path(f'/tmp/quality_test/{sample_file.stem}.html')
                if html_file.exists():
                    html_size = html_file.stat().st_size
                    html_sizes.append(html_size)
                
            except Exception:
                conversion_times.append(999.0)  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå€¤
                html_sizes.append(0)
        
        return {
            'conversion_time_avg': sum(conversion_times) / len(conversion_times) if conversion_times else 0,
            'conversion_time_max': max(conversion_times) if conversion_times else 0,
            'html_size_avg': sum(html_sizes) / len(html_sizes) if html_sizes else 0,
            'html_size_max': max(html_sizes) if html_sizes else 0
        }
    
    def _run_tests(self) -> Dict[str, Any]:
        """ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
        try:
            # pytestå®Ÿè¡Œ
            cmd = [sys.executable, '-m', 'pytest', '--tb=no', '-q']
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            # çµæœè§£æ
            output = result.stdout + result.stderr
            
            # ãƒ†ã‚¹ãƒˆé€šéç‡ã‚’è§£æ
            if 'passed' in output:
                # "5 passed, 1 failed" ã®ã‚ˆã†ãªå½¢å¼ã‚’è§£æ
                import re
                matches = re.findall(r'(\d+) passed', output)
                passed = int(matches[0]) if matches else 0
                
                failed_matches = re.findall(r'(\d+) failed', output)
                failed = int(failed_matches[0]) if failed_matches else 0
                
                total = passed + failed
                pass_rate = (passed / total * 100) if total > 0 else 0
            else:
                pass_rate = 0.0
            
            # ã‚«ãƒãƒ¬ãƒƒã‚¸æƒ…å ±ï¼ˆç°¡æ˜“ç‰ˆï¼‰
            coverage = 80.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            
            return {
                'pass_rate': pass_rate,
                'coverage': coverage
            }
            
        except Exception:
            return {
                'pass_rate': 0.0,
                'coverage': 0.0
            }
    
    def _calculate_quality_score(self, syntax_stats, perf_stats, test_stats) -> float:
        """ç·åˆå“è³ªã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        weights = {
            'syntax': 0.4,      # è¨˜æ³•å“è³ª: 40%
            'performance': 0.3, # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹: 30%
            'tests': 0.3        # ãƒ†ã‚¹ãƒˆå“è³ª: 30%
        }
        
        # å„é …ç›®ã®ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆ0-100ï¼‰
        syntax_score = min(syntax_stats['pass_rate'], 100.0)
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¹ã‚³ã‚¢ï¼ˆå¤‰æ›æ™‚é–“ãƒ™ãƒ¼ã‚¹ï¼‰
        perf_score = max(0, 100 - (perf_stats['conversion_time_avg'] * 20))
        
        test_score = min(test_stats['pass_rate'], 100.0)
        
        # é‡ã¿ä»˜ãå¹³å‡
        quality_score = (
            syntax_score * weights['syntax'] +
            perf_score * weights['performance'] +
            test_score * weights['tests']
        )
        
        return round(quality_score, 1)
    
    def check_quality_gates(self, metrics: QualityMetrics) -> Dict[str, Any]:
        """å“è³ªã‚²ãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯"""
        gates = self.config['quality_gates']
        results = {}
        
        checks = [
            ('syntax_pass_rate', metrics.syntax_pass_rate >= gates['syntax_pass_rate_min']),
            ('conversion_time', metrics.conversion_time_max <= gates['conversion_time_max']),
            ('test_pass_rate', metrics.test_pass_rate >= gates['test_pass_rate_min']),
            ('overall_quality', metrics.overall_quality_score >= gates['overall_quality_min'])
        ]
        
        for check_name, passed in checks:
            results[check_name] = {
                'passed': passed,
                'threshold': gates.get(f'{check_name}_min', gates.get(f'{check_name}_max', 0)),
                'actual': getattr(metrics, check_name, 0)
            }
        
        results['all_passed'] = all(check[1] for check in checks)
        
        return results


def format_report(metrics: QualityMetrics, format_type: str = 'detailed') -> str:
    """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ¬ãƒãƒ¼ãƒˆã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    if format_type == 'json':
        return json.dumps(asdict(metrics), indent=2, ensure_ascii=False)
    
    elif format_type == 'summary':
        return f"""
ğŸ“Š å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ ã‚µãƒãƒªãƒ¼
========================
ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: {metrics.project_name}
æ™‚åˆ»: {metrics.timestamp}

ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«çµ±è¨ˆ: {metrics.total_files}ãƒ•ã‚¡ã‚¤ãƒ« ({metrics.txt_files}ãƒ†ã‚­ã‚¹ãƒˆ)
ğŸ“ è¨˜æ³•å“è³ª: {metrics.syntax_pass_rate:.1f}% ({metrics.syntax_errors}ã‚¨ãƒ©ãƒ¼)
âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹: å¹³å‡{metrics.conversion_time_avg:.2f}ç§’
ğŸ§ª ãƒ†ã‚¹ãƒˆå“è³ª: {metrics.test_pass_rate:.1f}%
ğŸ¯ ç·åˆã‚¹ã‚³ã‚¢: {metrics.overall_quality_score:.1f}/100
"""
    
    else:  # detailed
        return f"""
ğŸ“Š Kumihan-Formatter å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ
==============================================
ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: {metrics.project_name}
ç”Ÿæˆæ™‚åˆ»: {metrics.timestamp}

ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«çµ±è¨ˆ
--------------
ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {metrics.total_files:,}
ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«æ•°: {metrics.txt_files:,}
ç·è¡Œæ•°: {metrics.total_lines:,}
ç·ã‚µã‚¤ã‚º: {metrics.total_size_bytes:,} bytes ({metrics.total_size_bytes/1024/1024:.1f} MB)

ğŸ“ è¨˜æ³•å“è³ª
-----------
è¨˜æ³•ã‚¨ãƒ©ãƒ¼æ•°: {metrics.syntax_errors}
è¨˜æ³•åˆæ ¼ç‡: {metrics.syntax_pass_rate:.1f}%

âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
---------------
å¹³å‡å¤‰æ›æ™‚é–“: {metrics.conversion_time_avg:.2f}ç§’
æœ€å¤§å¤‰æ›æ™‚é–“: {metrics.conversion_time_max:.2f}ç§’
å¹³å‡HTMLã‚µã‚¤ã‚º: {metrics.html_size_avg:,.0f} bytes
æœ€å¤§HTMLã‚µã‚¤ã‚º: {metrics.html_size_max:,.0f} bytes

ğŸ§ª ãƒ†ã‚¹ãƒˆå“è³ª
-----------
ãƒ†ã‚¹ãƒˆåˆæ ¼ç‡: {metrics.test_pass_rate:.1f}%
ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸: {metrics.test_coverage:.1f}%

ğŸ¯ ç·åˆè©•ä¾¡
-----------
å“è³ªã‚¹ã‚³ã‚¢: {metrics.overall_quality_score:.1f}/100

{_get_quality_rating(metrics.overall_quality_score)}
"""


def _get_quality_rating(score: float) -> str:
    """å“è³ªè©•ä¾¡ã®åˆ¤å®š"""
    if score >= 90:
        return "ğŸŒŸ å„ªç§€ - æœ¬ç•ªãƒªãƒªãƒ¼ã‚¹å¯èƒ½"
    elif score >= 80:
        return "âœ… è‰¯å¥½ - è»½å¾®ãªæ”¹å–„æ¨å¥¨"
    elif score >= 70:
        return "âš ï¸ è¦æ”¹å–„ - å“è³ªå‘ä¸ŠãŒå¿…è¦"
    else:
        return "ğŸš¨ é‡å¤§ - å¤§å¹…ãªæ”¹å–„ãŒå¿…è¦"


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(
        description="Kumihan-Formatter å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ãƒ„ãƒ¼ãƒ«",
        epilog="ä¾‹: python dev/tools/quality_metrics.py . --format=json --output=metrics.json"
    )
    parser.add_argument(
        'directory',
        type=Path,
        help='åˆ†æå¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª'
    )
    parser.add_argument(
        '--format',
        choices=['detailed', 'summary', 'json'],
        default='detailed',
        help='ãƒ¬ãƒãƒ¼ãƒˆå½¢å¼'
    )
    parser.add_argument(
        '--output', '-o',
        type=Path,
        help='å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹'
    )
    parser.add_argument(
        '--config',
        type=Path,
        help='è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹'
    )
    parser.add_argument(
        '--check-gates',
        action='store_true',
        help='å“è³ªã‚²ãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œ'
    )
    
    args = parser.parse_args()
    
    # åˆ†æå®Ÿè¡Œ
    analyzer = QualityAnalyzer(config_path=args.config)
    metrics = analyzer.analyze_project(args.directory)
    
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report = format_report(metrics, args.format)
    
    # å‡ºåŠ›
    if args.output:
        with open(args.output, 'w', encoding='utf-8') as f:
            f.write(report)
        print(f"ğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆã‚’å‡ºåŠ›ã—ã¾ã—ãŸ: {args.output}")
    else:
        print(report)
    
    # å“è³ªã‚²ãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯
    if args.check_gates:
        gate_results = analyzer.check_quality_gates(metrics)
        print("\nğŸšª å“è³ªã‚²ãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯çµæœ:")
        print("=" * 30)
        
        for gate_name, result in gate_results.items():
            if gate_name == 'all_passed':
                continue
            
            status = "âœ… PASS" if result['passed'] else "âŒ FAIL"
            print(f"{gate_name}: {status} ({result['actual']:.1f} / {result['threshold']:.1f})")
        
        if gate_results['all_passed']:
            print("\nğŸ‰ å…¨ã¦ã®å“è³ªã‚²ãƒ¼ãƒˆã‚’é€šéã—ã¾ã—ãŸï¼")
            sys.exit(0)
        else:
            print("\nğŸš¨ å“è³ªã‚²ãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯ã«å¤±æ•—ã—ã¾ã—ãŸ")
            sys.exit(1)


if __name__ == "__main__":
    main()