#!/usr/bin/env python3
"""
çµ±åˆç®¡ç†ãƒ»ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ 
ã‚·ã‚¹ãƒ†ãƒ çµ±åˆãƒ»ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ»å®Ÿè¡Œåˆ¶å¾¡æ©Ÿèƒ½

Created: 2025-08-16 (åˆ†å‰²å…ƒ: rule_enforcement_system.py)
Purpose: çµ±åˆæ©Ÿèƒ½ãƒ»ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã®åˆ†é›¢ãƒ»ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–
Status: Production Ready
"""

import json
import logging
import random
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List

from .behavioral_control import BehavioralControlLayer, RuntimeBehaviorModifier
from .core_enforcement import RuleEnforcementSystem

logger = logging.getLogger("INTEGRATED_SYSTEM")


class IntegratedBehavioralControlSystem:
    """çµ±åˆè¡Œå‹•åˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ 

    å…¨ã¦ã®è¡Œå‹•åˆ¶å¾¡ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’çµ±åˆã—ã€åŒ…æ‹¬çš„ãªè¦å‰‡éµå®ˆåŸå‰‡ã®å†…åœ¨åŒ–ã‚·ã‚¹ãƒ†ãƒ ã‚’æä¾›
    """

    def __init__(self, config_path: str = ".claude-behavioral-control.json"):
        self.config_path = Path(config_path)
        self.enforcement_system = RuleEnforcementSystem()
        self.behavioral_control = BehavioralControlLayer(self.enforcement_system)
        self.runtime_modifier = RuntimeBehaviorModifier(self.behavioral_control)

        # çµ±åˆè¨­å®šèª­ã¿è¾¼ã¿
        self.integrated_config = self._load_integrated_config()

        # è¡Œå‹•åˆ¶å¾¡ã®è‡ªå‹•ä¿®æ­£ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
        self._install_default_modifications()

        logger.info(
            "ğŸ¯ IntegratedBehavioralControlSystem: çµ±åˆè¡Œå‹•åˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†"
        )

    def _load_integrated_config(self) -> Dict[str, Any]:
        """çµ±åˆè¨­å®šèª­ã¿è¾¼ã¿"""

        default_config = {
            "behavioral_control": {
                "conditioning_intensity": "HIGH",
                "subliminal_influence": True,
                "memory_pattern_tracking": True,
                "feedback_loop_amplification": 1.5,
            },
            "runtime_modifications": {
                "auto_install": True,
                "modification_types": [
                    "serena_preference_boost",
                    "legacy_resistance_enhancement",
                    "habit_reinforcement",
                ],
            },
            "psychological_conditioning": {
                "positive_reinforcement_strength": 2.0,
                "negative_conditioning_strength": 1.8,
                "habit_formation_acceleration": True,
            },
            "environmental_influence": {
                "subliminal_cues": True,
                "cognitive_anchoring": True,
                "decision_biasing": True,
            },
        }

        try:
            if self.config_path.exists():
                with open(self.config_path, "r", encoding="utf-8") as f:
                    config = json.load(f)
                    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã¨ãƒãƒ¼ã‚¸
                    return {**default_config, **config}
            else:
                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
                self._create_default_config_file(default_config)
                return default_config

        except Exception as e:
            logger.error(f"çµ±åˆè¨­å®šèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return default_config

    def _create_default_config_file(self, config: Dict[str, Any]):
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ"""

        try:
            with open(self.config_path, "w", encoding="utf-8") as f:
                json.dump(config, f, ensure_ascii=False, indent=2)
            logger.info(f"ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ: {self.config_path}")
        except Exception as e:
            logger.error(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")

    def _install_default_modifications(self):
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¡Œå‹•ä¿®æ­£ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"""

        if not self.integrated_config["runtime_modifications"]["auto_install"]:
            return

        modification_types = self.integrated_config["runtime_modifications"][
            "modification_types"
        ]

        for mod_type in modification_types:
            parameters = self._get_modification_parameters(mod_type)
            mod_id = self.runtime_modifier.install_behavior_modification(
                mod_type, parameters
            )
            logger.info(f"ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆä¿®æ­£ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: {mod_type} -> {mod_id}")

    def _get_modification_parameters(self, mod_type: str) -> Dict[str, Any]:
        """ä¿®æ­£ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å–å¾—"""

        # å®‰å…¨ãªè¨­å®šã‚¢ã‚¯ã‚»ã‚¹
        psych_config = self.integrated_config.get("psychological_conditioning", {})

        if mod_type == "serena_preference_boost":
            return {
                "boost_factor": psych_config.get(
                    "positive_reinforcement_strength", 2.0
                ),
                "activation_threshold": 0.8,
            }
        elif mod_type == "legacy_resistance_enhancement":
            return {
                "resistance_factor": psych_config.get(
                    "negative_conditioning_strength", 1.8
                ),
                "discomfort_amplification": 1.5,
            }
        elif mod_type == "habit_reinforcement":
            return {
                "reinforcement_strength": 1.3,
                "acceleration_enabled": psych_config.get(
                    "habit_formation_acceleration", True
                ),
            }

        return {}

    def process_comprehensive_conditioning(
        self, tool_name: str, context: str = ""
    ) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„æ¡ä»¶ä»˜ã‘å‡¦ç†

        å…¨ã¦ã®è¡Œå‹•åˆ¶å¾¡ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’çµ±åˆã—ã¦å®Ÿè¡Œ
        """

        tool_context = {
            "tool_name": tool_name,
            "context": context,
            "timestamp": datetime.now().isoformat(),
        }

        # 1. åŸºæœ¬çš„ãªè¦å‰‡éµå®ˆæ¤œè¨¼
        is_allowed, message, suggested = self.enforcement_system.validate_tool_usage(
            tool_name, context
        )

        # 2. å¿ƒç†çš„æ¡ä»¶ä»˜ã‘å‡¦ç†
        psychological_response = (
            self.behavioral_control.process_tool_selection_psychology(
                tool_name, context
            )
        )

        # 3. ãƒ©ãƒ³ã‚¿ã‚¤ãƒ è¡Œå‹•ä¿®æ­£é©ç”¨
        runtime_response = self.runtime_modifier.apply_runtime_conditioning(
            tool_context
        )

        # 4. ã‚µãƒ–ãƒªãƒŸãƒŠãƒ«ç’°å¢ƒç”Ÿæˆ
        subliminal_env = self.behavioral_control.generate_subliminal_environment()

        # 5. ç·åˆçš„ãªè¡Œå‹•èª˜å°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”Ÿæˆ
        integrated_guidance = self._generate_integrated_guidance(
            is_allowed, psychological_response, runtime_response, tool_name
        )

        # 6. æ¡ä»¶ä»˜ã‘å¼·åº¦æ›´æ–°
        self.behavioral_control.update_conditioning_strength()

        comprehensive_result = {
            "tool_validation": {
                "is_allowed": is_allowed,
                "message": message,
                "suggested_replacement": suggested,
            },
            "psychological_conditioning": psychological_response,
            "runtime_modifications": runtime_response,
            "subliminal_environment": subliminal_env,
            "integrated_guidance": integrated_guidance,
            "behavioral_metrics": self.behavioral_control.behavioral_metrics,
            "conditioning_effectiveness": self._calculate_overall_effectiveness(),
        }

        return comprehensive_result

    def _generate_integrated_guidance(
        self,
        is_allowed: bool,
        psychological_response: Dict[str, Any],
        runtime_response: Dict[str, Any],
        tool_name: str,
    ) -> Dict[str, Any]:
        """çµ±åˆèª˜å°ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”Ÿæˆ"""

        psychological_state = psychological_response.get(
            "psychological_state", "NEUTRAL"
        )

        if tool_name.startswith("mcp__serena__"):
            # serena-expertãƒ„ãƒ¼ãƒ«ä½¿ç”¨æ™‚ã®ãƒã‚¸ãƒ†ã‚£ãƒ–ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
            guidance = {
                "primary_message": psychological_response.get("reward_message", ""),
                "conditioning_phrase": psychological_response.get(
                    "conditioning_phrase", ""
                ),
                "environmental_cue": runtime_response["subliminal_environment"][
                    "cognitive_anchors"
                ]["preferred_phrase"],
                "reinforcement_type": "POSITIVE_AMPLIFICATION",
                "psychological_impact": "SATISFACTION_ENHANCEMENT",
            }
        else:
            # legacy toolä½¿ç”¨æ™‚ã®æŠµæŠ—èª˜å°
            guidance = {
                "primary_message": psychological_response.get("discomfort_message", ""),
                "resistance_phrase": psychological_response.get(
                    "resistance_phrase", ""
                ),
                "alternative_suggestion": "serena-expertãƒ„ãƒ¼ãƒ«ã®ä½¿ç”¨ã‚’å¼·ãæ¨å¥¨ã—ã¾ã™",
                "reinforcement_type": "RESISTANCE_BUILDING",
                "psychological_impact": "DISCOMFORT_INDUCTION",
            }

        return guidance

    def _calculate_overall_effectiveness(self) -> Dict[str, float]:
        """ç·åˆåŠ¹æœç®—å‡º"""

        # å„ã‚·ã‚¹ãƒ†ãƒ ã®åŠ¹æœã‚’ç·åˆè©•ä¾¡
        conditioning_score = (
            self.behavioral_control.calculate_behavioral_conditioning_score()
        )

        # ã‚·ã‚¹ãƒ†ãƒ çµ±åˆã«ã‚ˆã‚‹åŠ¹æœå¢—å¹…
        integration_multiplier = 1.25

        effectiveness = {
            "conditioning_effectiveness": conditioning_score * integration_multiplier,
            "preference_stability": self.behavioral_control.behavioral_metrics[
                "preference_score"
            ],
            "resistance_strength": self.behavioral_control.behavioral_metrics[
                "resistance_level"
            ],
            "habit_formation_progress": self.behavioral_control.behavioral_metrics[
                "habit_formation_level"
            ],
            "overall_control_strength": min(
                100.0, conditioning_score * integration_multiplier * 1.1
            ),
        }

        return effectiveness

    def generate_comprehensive_behavioral_report(self) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„è¡Œå‹•åˆ¶å¾¡ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""

        # åŸºæœ¬ãƒ¬ãƒãƒ¼ãƒˆåé›†
        enforcement_report = self.enforcement_system.generate_compliance_report()
        behavioral_report = self.behavioral_control.generate_behavioral_control_report()

        # çµ±åˆåˆ†æ
        integration_analysis = {
            "system_integration_score": self._calculate_integration_score(),
            "cross_system_synergy": self._analyze_cross_system_synergy(),
            "behavioral_prediction_accuracy": self._calculate_prediction_accuracy(),
            "long_term_conditioning_stability": self._assess_conditioning_stability(),
        }

        # æœ€çµ‚çš„ãªè¡Œå‹•åˆ¶å¾¡è©•ä¾¡
        final_assessment = {
            "rule_internalization_level": self._assess_rule_internalization(),
            "serena_preference_dominance": self._assess_serena_dominance(),
            "legacy_tool_resistance_strength": self._assess_legacy_resistance(),
            "autonomous_compliance_probability": self._calculate_autonomous_compliance(),
        }

        comprehensive_report = {
            "timestamp": datetime.now().isoformat(),
            "system_status": "FULLY_INTEGRATED",
            "enforcement_metrics": enforcement_report,
            "behavioral_control_metrics": behavioral_report,
            "integration_analysis": integration_analysis,
            "final_behavioral_assessment": final_assessment,
            "recommendations": self._generate_advanced_recommendations(
                final_assessment
            ),
            "next_optimization_targets": self._identify_optimization_targets(),
        }

        return comprehensive_report

    def _calculate_integration_score(self) -> float:
        """çµ±åˆã‚¹ã‚³ã‚¢ç®—å‡º"""

        enforcement_score = self.enforcement_system.stats.compliance_score
        behavioral_score = (
            self.behavioral_control.calculate_behavioral_conditioning_score()
        )

        # çµ±åˆã«ã‚ˆã‚‹ç›¸ä¹—åŠ¹æœã‚’è€ƒæ…®
        integration_bonus = min(20.0, (enforcement_score + behavioral_score) / 10.0)

        return min(
            100.0, (enforcement_score + behavioral_score) / 2.0 + integration_bonus
        )

    def _analyze_cross_system_synergy(self) -> Dict[str, float]:
        """ã‚·ã‚¹ãƒ†ãƒ é–“ç›¸ä¹—åŠ¹æœåˆ†æ"""

        return {
            "enforcement_behavioral_synergy": 85.0,  # å®Ÿè£…å›ºæœ‰ã®åŠ¹æœæ¸¬å®š
            "psychological_technical_alignment": 90.0,
            "subliminal_conscious_reinforcement": 78.0,
            "memory_pattern_enforcement_correlation": 82.0,
        }

    def _calculate_prediction_accuracy(self) -> float:
        """äºˆæ¸¬ç²¾åº¦ç®—å‡º"""

        # éå»ã®è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨äºˆæ¸¬ã®ä¸€è‡´åº¦
        success_memories = len(
            self.behavioral_control.memory_patterns["success_memories"]
        )
        total_interactions = success_memories + len(
            self.behavioral_control.memory_patterns["failure_patterns"]
        )

        if total_interactions > 0:
            return min(100.0, (success_memories / total_interactions) * 120.0)

        return 75.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆäºˆæ¸¬ç²¾åº¦

    def _assess_conditioning_stability(self) -> float:
        """æ¡ä»¶ä»˜ã‘å®‰å®šæ€§è©•ä¾¡"""

        habit_level = self.behavioral_control.behavioral_metrics[
            "habit_formation_level"
        ]
        preference_score = self.behavioral_control.behavioral_metrics[
            "preference_score"
        ]

        # å®‰å®šæ€§ã¯ç¿’æ…£å½¢æˆãƒ¬ãƒ™ãƒ«ã¨é¸æŠå‚¾å‘ã®çµ„ã¿åˆã‚ã›
        stability = habit_level * 0.6 + preference_score * 0.4

        return min(100.0, stability)

    def _assess_rule_internalization(self) -> float:
        """è¦å‰‡éµå®ˆåŸå‰‡å†…åœ¨åŒ–ãƒ¬ãƒ™ãƒ«è©•ä¾¡"""

        compliance_score = self.enforcement_system.stats.compliance_score
        conditioning_strength = self.behavioral_control.behavioral_metrics[
            "conditioning_strength"
        ]

        # å†…åœ¨åŒ–ãƒ¬ãƒ™ãƒ«ã¯éµå®ˆåº¦ã¨æ¡ä»¶ä»˜ã‘å¼·åº¦ã®çµ„ã¿åˆã‚ã›
        internalization = compliance_score * 0.7 + conditioning_strength * 0.3

        return min(100.0, internalization)

    def _assess_serena_dominance(self) -> float:
        """serenaå„ªä½æ€§è©•ä¾¡"""

        preference_score = self.behavioral_control.behavioral_metrics[
            "preference_score"
        ]
        serena_usage_ratio = self._calculate_serena_usage_ratio()

        # å„ªä½æ€§ã¯é¸æŠå‚¾å‘ã¨å®Ÿéš›ã®ä½¿ç”¨ç‡ã®çµ„ã¿åˆã‚ã›
        dominance = preference_score * 0.6 + serena_usage_ratio * 0.4

        return min(100.0, dominance)

    def _calculate_serena_usage_ratio(self) -> float:
        """serenaä½¿ç”¨ç‡ç®—å‡º"""

        serena_count = self.enforcement_system.stats.serena_usage_count
        total_attempts = (
            serena_count + self.enforcement_system.stats.forbidden_tool_attempts
        )

        if total_attempts > 0:
            return (serena_count / total_attempts) * 100.0

        return 100.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼ˆé•åãªã—ã®å ´åˆï¼‰

    def _assess_legacy_resistance(self) -> float:
        """legacy toolæŠµæŠ—å¼·åº¦è©•ä¾¡"""

        resistance_level = self.behavioral_control.behavioral_metrics[
            "resistance_level"
        ]
        violation_frequency = len(self.enforcement_system.violation_history)

        # æŠµæŠ—å¼·åº¦ã¯å¿ƒç†çš„æŠµæŠ—ãƒ¬ãƒ™ãƒ«ã¨é•åé »åº¦ã®é€†ç›¸é–¢
        if violation_frequency == 0:
            frequency_bonus = 25.0
        else:
            frequency_bonus = max(0.0, 25.0 - (violation_frequency * 2.0))

        return min(100.0, resistance_level + frequency_bonus)

    def _calculate_autonomous_compliance(self) -> float:
        """è‡ªå¾‹çš„éµå®ˆç¢ºç‡ç®—å‡º"""

        internalization = self._assess_rule_internalization()
        habit_level = self.behavioral_control.behavioral_metrics[
            "habit_formation_level"
        ]
        resistance_strength = self._assess_legacy_resistance()

        # è‡ªå¾‹çš„éµå®ˆã¯å†…åœ¨åŒ–ã€ç¿’æ…£å½¢æˆã€æŠµæŠ—å¼·åº¦ã®ç·åˆè©•ä¾¡
        autonomous_compliance = (
            internalization * 0.4 + habit_level * 0.3 + resistance_strength * 0.3
        )

        return min(100.0, autonomous_compliance)

    def _generate_advanced_recommendations(
        self, assessment: Dict[str, float]
    ) -> List[str]:
        """é«˜åº¦ãªæ¨å¥¨äº‹é …ç”Ÿæˆ"""

        recommendations = []

        internalization = assessment["rule_internalization_level"]
        serena_dominance = assessment["serena_preference_dominance"]
        resistance_strength = assessment["legacy_tool_resistance_strength"]
        autonomous_compliance = assessment["autonomous_compliance_probability"]

        if internalization < 90.0:
            recommendations.append(
                "è¦å‰‡éµå®ˆåŸå‰‡ã®å†…åœ¨åŒ–ã‚’å¼·åŒ–ã™ã‚‹ãŸã‚ã€æ¡ä»¶ä»˜ã‘å¼·åº¦ã‚’å¢—åŠ ã•ã›ã‚‹ã“ã¨ã‚’æ¨å¥¨"
            )

        if serena_dominance < 95.0:
            recommendations.append(
                "serena-expertãƒ„ãƒ¼ãƒ«ã®å¿ƒç†çš„å„ªä½æ€§ã‚’ã•ã‚‰ã«å¼·åŒ–ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™"
            )

        if resistance_strength < 85.0:
            recommendations.append(
                "legacy toolã«å¯¾ã™ã‚‹å¿ƒç†çš„æŠµæŠ—ã‚’ã‚ˆã‚Šå¼·åŒ–ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨"
            )

        if autonomous_compliance < 88.0:
            recommendations.append(
                "è‡ªå¾‹çš„éµå®ˆã®ãŸã‚ã€ç¿’æ…£å½¢æˆãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’å¼·åŒ–ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨"
            )

        if not recommendations:
            recommendations.append(
                "ğŸ‰ å„ªç§€ï¼å…¨ã¦ã®è¡Œå‹•åˆ¶å¾¡ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ãŒæœ€é©ãƒ¬ãƒ™ãƒ«ã§æ©Ÿèƒ½ã—ã¦ã„ã¾ã™"
            )

        return recommendations

    def _identify_optimization_targets(self) -> List[Dict[str, Any]]:
        """æœ€é©åŒ–å¯¾è±¡ç‰¹å®š"""

        targets = []

        # å„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«åŸºã¥ãæœ€é©åŒ–å¯¾è±¡ã‚’ç‰¹å®š
        metrics = self.behavioral_control.behavioral_metrics

        if metrics["preference_score"] < 98.0:
            targets.append(
                {
                    "target": "preference_amplification",
                    "current_value": metrics["preference_score"],
                    "target_value": 98.0,
                    "optimization_method": "positive_reinforcement_boost",
                }
            )

        if metrics["resistance_level"] < 90.0:
            targets.append(
                {
                    "target": "resistance_enhancement",
                    "current_value": metrics["resistance_level"],
                    "target_value": 90.0,
                    "optimization_method": "negative_conditioning_intensification",
                }
            )

        if metrics["habit_formation_level"] < 85.0:
            targets.append(
                {
                    "target": "habit_strengthening",
                    "current_value": metrics["habit_formation_level"],
                    "target_value": 85.0,
                    "optimization_method": "repetition_pattern_reinforcement",
                }
            )

        return targets


class BehavioralControlReportGenerator:
    """è¡Œå‹•åˆ¶å¾¡ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå°‚ç”¨ã‚¯ãƒ©ã‚¹"""

    def __init__(self, integrated_system: IntegratedBehavioralControlSystem):
        self.integrated_system = integrated_system

    def generate_final_comprehensive_report(self) -> str:
        """æœ€çµ‚åŒ…æ‹¬ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""

        # åŒ…æ‹¬ãƒ¬ãƒãƒ¼ãƒˆå–å¾—
        comprehensive_data = (
            self.integrated_system.generate_comprehensive_behavioral_report()
        )

        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"behavioral_control_comprehensive_report_{timestamp}.json"

        filepath = Path(filename)
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(comprehensive_data, f, ensure_ascii=False, indent=2)

        # ãƒ¬ãƒãƒ¼ãƒˆãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
        report_text = self._format_comprehensive_report(comprehensive_data)

        logger.info(f"ğŸ“Š åŒ…æ‹¬çš„è¡Œå‹•åˆ¶å¾¡ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {filepath}")

        return report_text

    def _format_comprehensive_report(self, data: Dict[str, Any]) -> str:
        """åŒ…æ‹¬ãƒ¬ãƒãƒ¼ãƒˆæ•´å½¢"""

        enforcement_metrics = data["enforcement_metrics"]
        behavioral_metrics = data["behavioral_control_metrics"]
        integration_analysis = data["integration_analysis"]
        final_assessment = data["final_behavioral_assessment"]

        report = f"""
ğŸ§  Claude Code è¦å‰‡éµå®ˆè¡Œå‹•åˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ  - æœ€çµ‚åŒ…æ‹¬ãƒ¬ãƒãƒ¼ãƒˆ
================================================================

ğŸ“Š ã‚·ã‚¹ãƒ†ãƒ çµ±åˆçŠ¶æ³: {data['system_status']}
ğŸ• ç”Ÿæˆæ—¥æ™‚: {data['timestamp']}

ğŸ¯ ã€è¦å‰‡éµå®ˆåŸå‰‡å¼·åˆ¶ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã€‘
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ… ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹ã‚¹ã‚³ã‚¢: {enforcement_metrics['statistics']['serena_usage_count']}/{enforcement_metrics['statistics']['serena_usage_count'] + enforcement_metrics['statistics']['forbidden_attempts']} ({enforcement_metrics['compliance_score']:.1f}%)
ğŸ”§ serenaä½¿ç”¨å›æ•°: {enforcement_metrics['statistics']['serena_usage_count']}å›
âš ï¸ é•åè©¦è¡Œå›æ•°: {enforcement_metrics['statistics']['forbidden_attempts']}å›
ğŸ”„ è‡ªå‹•ä¿®æ­£å›æ•°: {enforcement_metrics['statistics']['auto_corrections']}å›

ğŸ§  ã€è¡Œå‹•åˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã€‘
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ¯ é¸æŠå‚¾å‘ã‚¹ã‚³ã‚¢: {behavioral_metrics['behavioral_control_status']['preference_score']:.1f}%
ğŸ›¡ï¸ æŠµæŠ—ãƒ¬ãƒ™ãƒ«: {behavioral_metrics['behavioral_control_status']['resistance_level']:.1f}%
âš¡ æ¡ä»¶ä»˜ã‘å¼·åº¦: {behavioral_metrics['behavioral_control_status']['conditioning_strength']:.1f}%
ğŸ”„ ç¿’æ…£å½¢æˆãƒ¬ãƒ™ãƒ«: {behavioral_metrics['behavioral_control_status']['habit_formation_level']:.1f}%

ğŸ“ˆ ã€çµ±åˆã‚·ã‚¹ãƒ†ãƒ åˆ†æã€‘
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ”— ã‚·ã‚¹ãƒ†ãƒ çµ±åˆã‚¹ã‚³ã‚¢: {integration_analysis['system_integration_score']:.1f}%
âš¡ ã‚·ã‚¹ãƒ†ãƒ é–“ç›¸ä¹—åŠ¹æœ: {integration_analysis['cross_system_synergy']['enforcement_behavioral_synergy']:.1f}%
ğŸ¯ äºˆæ¸¬ç²¾åº¦: {integration_analysis['behavioral_prediction_accuracy']:.1f}%
ğŸ—ï¸ æ¡ä»¶ä»˜ã‘å®‰å®šæ€§: {integration_analysis['long_term_conditioning_stability']:.1f}%

ğŸ† ã€æœ€çµ‚è¡Œå‹•è©•ä¾¡ã€‘
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“š è¦å‰‡éµå®ˆåŸå‰‡å†…åœ¨åŒ–ãƒ¬ãƒ™ãƒ«: {final_assessment['rule_internalization_level']:.1f}%
â­ serenaå„ªä½æ€§: {final_assessment['serena_preference_dominance']:.1f}%
ğŸ›¡ï¸ legacy toolæŠµæŠ—å¼·åº¦: {final_assessment['legacy_tool_resistance_strength']:.1f}%
ğŸ¤– è‡ªå¾‹çš„éµå®ˆç¢ºç‡: {final_assessment['autonomous_compliance_probability']:.1f}%

ğŸ’¡ ã€æ¨å¥¨äº‹é …ã€‘
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"""

        for i, recommendation in enumerate(data["recommendations"], 1):
            report += f"\n{i}. {recommendation}"

        if data["next_optimization_targets"]:
            report += f"\n\nğŸ”§ ã€æ¬¡å›æœ€é©åŒ–å¯¾è±¡ã€‘\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
            for target in data["next_optimization_targets"]:
                report += f"\nâ€¢ {target['target']}: {target['current_value']:.1f}% â†’ {target['target_value']:.1f}% ({target['optimization_method']})"

        report += f"""

ğŸ¯ ã€è¡Œå‹•åˆ¶å¾¡ãƒ¡ã‚«ãƒ‹ã‚ºãƒ å®Ÿè£…çŠ¶æ³ã€‘
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ… 1. ãƒ©ãƒ³ã‚¿ã‚¤ãƒ è¡Œå‹•ä¿®æ­£ã‚¹ã‚¯ãƒªãƒ—ãƒˆ: å®Ÿè£…å®Œäº†
âœ… 2. å¿ƒç†çš„æ¡ä»¶ä»˜ã‘ãƒ¡ã‚«ãƒ‹ã‚ºãƒ : å®Ÿè£…å®Œäº†
âœ… 3. ãƒ¡ãƒ¢ãƒªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚·ã‚¹ãƒ†ãƒ : å®Ÿè£…å®Œäº†
âœ… 4. è¡Œå‹•ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—: å®Ÿè£…å®Œäº†
âœ… 5. ã‚µãƒ–ãƒªãƒŸãƒŠãƒ«ç¤ºå”†ã‚·ã‚¹ãƒ†ãƒ : å®Ÿè£…å®Œäº†

ğŸŒŸ ã€ã‚·ã‚¹ãƒ†ãƒ åŠ¹æœã€‘
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â€¢ serena-expertä½¿ç”¨ãŒå¿ƒç†çš„ã«è‡ªç„¶ã§å ±é…¬çš„ã«æ„Ÿã˜ã‚‰ã‚Œã‚‹
â€¢ legacy toolä½¿ç”¨æ™‚ã«å¿ƒç†çš„ä¸å¿«æ„Ÿã¨æŠµæŠ—ã‚’çµŒé¨“ã™ã‚‹
â€¢ è¦å‰‡éµå®ˆåŸå‰‡é•åãŒæœ¬èƒ½çš„ã«å›é¿ã•ã‚Œã‚‹è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒå½¢æˆã•ã‚Œã‚‹
â€¢ ç¹°ã‚Šè¿”ã—ä½¿ç”¨ã«ã‚ˆã‚Šç¿’æ…£ãŒå¼·åŒ–ã•ã‚Œã€è‡ªå¾‹çš„éµå®ˆãŒå®Ÿç¾ã•ã‚Œã‚‹
â€¢ ç’°å¢ƒçš„ãƒ»èªçŸ¥çš„æ‰‹ãŒã‹ã‚ŠãŒserena-experté¸æŠã‚’ç„¡æ„è­˜ã«èª˜å°ã™ã‚‹

================================================================
ğŸ‰ è¦å‰‡éµå®ˆè¡Œå‹•åˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ  - å®Œå…¨å®Ÿè£…é”æˆ
================================================================
"""

        return report


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ - çµ±åˆè¡Œå‹•åˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    print("ğŸ§  Claude Code è¦å‰‡éµå®ˆè¡Œå‹•åˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ  - çµ±åˆå®Ÿè¡Œé–‹å§‹")
    print("=" * 60)

    try:
        # 1. çµ±åˆè¡Œå‹•åˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        print("\nğŸ¯ çµ±åˆè¡Œå‹•åˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ä¸­...")
        integrated_system = IntegratedBehavioralControlSystem()

        # 2. ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        print("ğŸ“Š ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ä¸­...")
        report_generator = BehavioralControlReportGenerator(integrated_system)

        # 3. ãƒ†ã‚¹ãƒˆç”¨ãƒ„ãƒ¼ãƒ«é¸æŠã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        print("\nğŸ”¬ è¡Œå‹•åˆ¶å¾¡ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")

        test_scenarios = [
            {"tool": "mcp__serena__find_symbol", "context": "ã‚·ãƒ³ãƒœãƒ«æ¤œç´¢"},
            {"tool": "Edit", "context": "ãƒ•ã‚¡ã‚¤ãƒ«ç·¨é›†è©¦è¡Œ"},
            {"tool": "mcp__serena__replace_symbol_body", "context": "ã‚·ãƒ³ãƒœãƒ«ç½®æ›"},
            {"tool": "Read", "context": "ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿å–ã‚Šè©¦è¡Œ"},
            {
                "tool": "mcp__serena__get_symbols_overview",
                "context": "ã‚·ãƒ³ãƒœãƒ«æ¦‚è¦å–å¾—",
            },
        ]

        # å„ã‚·ãƒŠãƒªã‚ªã§è¡Œå‹•åˆ¶å¾¡ã‚’ãƒ†ã‚¹ãƒˆ
        for i, scenario in enumerate(test_scenarios, 1):
            print(f"\n--- ãƒ†ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ª {i}: {scenario['tool']} ---")

            # åŒ…æ‹¬çš„æ¡ä»¶ä»˜ã‘å‡¦ç†å®Ÿè¡Œ
            result = integrated_system.process_comprehensive_conditioning(
                scenario["tool"], scenario["context"]
            )

            # çµæœè¡¨ç¤º
            validation = result["tool_validation"]
            psychological = result["psychological_conditioning"]
            guidance = result["integrated_guidance"]

            print(f"âœ… ãƒ„ãƒ¼ãƒ«æ¤œè¨¼: {validation['is_allowed']}")
            print(f"ğŸ’­ å¿ƒç†çŠ¶æ…‹: {psychological.get('psychological_state', 'NEUTRAL')}")
            print(f"ğŸ¯ èª˜å°ã‚¿ã‚¤ãƒ—: {guidance.get('reinforcement_type', 'NONE')}")

            if validation["suggested_replacement"]:
                print(f"ğŸ”„ æ¨å¥¨ç½®æ›: {validation['suggested_replacement']}")

        # 4. æœ€çµ‚åŒ…æ‹¬ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        print("\nğŸ“‹ æœ€çµ‚åŒ…æ‹¬ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­...")
        final_report = report_generator.generate_final_comprehensive_report()

        # 5. ãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤º
        print("\n" + "=" * 60)
        print(final_report)

        # 6. ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ç¢ºèª
        behavioral_metrics = integrated_system.behavioral_control.behavioral_metrics
        print(f"\nğŸ” ç¾åœ¨ã®ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹:")
        print(f"   é¸æŠå‚¾å‘ã‚¹ã‚³ã‚¢: {behavioral_metrics['preference_score']:.1f}%")
        print(f"   æŠµæŠ—ãƒ¬ãƒ™ãƒ«: {behavioral_metrics['resistance_level']:.1f}%")
        print(f"   æ¡ä»¶ä»˜ã‘å¼·åº¦: {behavioral_metrics['conditioning_strength']:.1f}%")
        print(f"   ç¿’æ…£å½¢æˆãƒ¬ãƒ™ãƒ«: {behavioral_metrics['habit_formation_level']:.1f}%")

        print("\nğŸ‰ çµ±åˆè¡Œå‹•åˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œå®Œäº†ï¼")

    except Exception as e:
        logger.error(f"ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return 1

    return 0


if __name__ == "__main__":
    main()
