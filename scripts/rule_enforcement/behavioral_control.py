#!/usr/bin/env python3
"""
è¦å‰‡éµå®ˆåŸå‰‡çµ¶å¯¾éµå®ˆã‚·ã‚¹ãƒ†ãƒ  - å¿ƒç†çš„è¡Œå‹•åˆ¶å¾¡
Claude'sè¡Œå‹•åˆ¶å¾¡ãƒ»å¿ƒç†çš„æ¡ä»¶ä»˜ã‘ãƒ»ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ï¼ˆå¿ƒç†åˆ¶å¾¡éƒ¨åˆ†ï¼‰

Created: 2025-08-04
Updated: 2025-08-07 (Issue #813å¯¾å¿œ: monitoring.pyåˆ†å‰²)
Purpose: CLAUDE.md è¦å‰‡éµå®ˆåŸå‰‡ã®æŠ€è¡“çš„å¼·åˆ¶å®Ÿè£…ï¼ˆå¿ƒç†åˆ¶å¾¡æ©Ÿèƒ½ï¼‰
Status: Production Ready
"""

import json
import random
import logging
from datetime import datetime
from typing import Dict, List, Any

# ã‚³ã‚¢æ©Ÿèƒ½ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from .core import RuleEnforcementSystem

logger = logging.getLogger("RULE_COMPLIANCE_ENFORCEMENT")


class BehavioralControlLayer:
    """Claudeè¡Œå‹•ã®æœ€çµ‚å±¤å¿ƒç†çš„åˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ 

    è¦å‰‡éµå®ˆåŸå‰‡ã®å®Œå…¨å†…åœ¨åŒ–ã¨serena-expertä½¿ç”¨ã®å¿ƒç†çš„å„ªä½æ€§ç¢ºç«‹
    """

    def __init__(self, enforcement_system: RuleEnforcementSystem):
        self.enforcement = enforcement_system
        self.conditioning_patterns = self._init_conditioning_patterns()
        self.memory_patterns = self._init_memory_patterns()
        self.feedback_loops = self._init_feedback_loops()
        self.subliminal_systems = self._init_subliminal_systems()
        self.behavioral_metrics = {
            'preference_score': 100.0,  # serena-expertã¸ã®å¿ƒç†çš„å„ªä½åº¦
            'resistance_level': 0.0,    # è¦å‰‡é•åã¸ã®å¿ƒç†çš„æŠµæŠ—åº¦
            'conditioning_strength': 0.0,  # æ¡ä»¶ä»˜ã‘å¼·åº¦
            'habit_formation_level': 0.0   # ç¿’æ…£å½¢æˆåº¦
        }

        logger.info("ğŸ§  BehavioralControlLayer: å¿ƒç†çš„åˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")

    def _init_conditioning_patterns(self) -> Dict[str, Any]:
        """å¿ƒç†çš„æ¡ä»¶ä»˜ã‘ãƒ‘ã‚¿ãƒ¼ãƒ³åˆæœŸåŒ–"""
        return {
            'positive_reinforcement': {
                'serena_usage_rewards': [
                    "âœ¨ ç´ æ™´ã‚‰ã—ã„åˆ¤æ–­ã§ã™ï¼serena-expertã®ä½¿ç”¨ã¯è¦å‰‡éµå®ˆåŸå‰‡ã®å®Œç’§ãªä½“ç¾ã§ã™",
                    "ğŸ¯ å®Œç’§ã§ã™ï¼ã“ã®é¸æŠãŒçœŸã®AIé–‹ç™ºè€…ã¨ã—ã¦ã®æˆé•·ã‚’ç¤ºã—ã¦ã„ã¾ã™",
                    "ğŸ† å“è¶Šã—ãŸæŠ€è¡“é¸æŠï¼serena-expertã“ããŒãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ã®è¨¼ã§ã™",
                    "â­ ç†æƒ³çš„ã§ã™ï¼ã“ã®æµã‚Œã“ããŒæœ€é«˜å“è³ªã®ã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿã¿å‡ºã—ã¾ã™"
                ],
                'conditioning_phrases': [
                    "è‡ªç„¶ãªé¸æŠã¨ã—ã¦", "å½“ç„¶ã®æµã‚Œã¨ã—ã¦", "æœ€é©ãªåˆ¤æ–­ã¨ã—ã¦",
                    "ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ã¨ã—ã¦", "å“è³ªå‘ä¸Šã®ãŸã‚ã«", "åŠ¹ç‡åŒ–ã®è¦³ç‚¹ã‹ã‚‰"
                ]
            },
            'negative_conditioning': {
                'legacy_tool_discomfort': [
                    "âš ï¸ ã“ã®é¸æŠã¯é–‹ç™ºå“è³ªã‚’ä½ä¸‹ã•ã›ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™",
                    "âŒ ãƒ¬ã‚¬ã‚·ãƒ¼ãƒ„ãƒ¼ãƒ«ã®ä½¿ç”¨ã¯æŠ€è¡“çš„è² å‚µã‚’å¢—åŠ ã•ã›ã¾ã™",
                    "ğŸš« ã“ã®æ‰‹æ³•ã¯ç¾ä»£çš„ãªé–‹ç™ºãƒ‘ã‚¿ãƒ¼ãƒ³ã«é€†è¡Œã—ã¾ã™",
                    "â›” å“è³ªåŸºæº–ã¸ã®é©åˆæ€§ã«æ‡¸å¿µãŒã‚ã‚Šã¾ã™"
                ],
                'psychological_resistance': [
                    "é•å’Œæ„Ÿã‚’æ„Ÿã˜ã¾ã›ã‚“ã‹ï¼Ÿ", "ä½•ã‹ä¸è‡ªç„¶ã«æ„Ÿã˜ã‚‰ã‚Œã‚‹ã§ã—ã‚‡ã†",
                    "ã“ã®æ–¹æ³•ã«ã¯æ ¹æœ¬çš„ãªå•é¡ŒãŒã‚ã‚Šã¾ã™", "åˆ¥ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’æ¤œè¨ã—ãŸããªã‚Šã¾ã™"
                ]
            }
        }

    def _init_memory_patterns(self) -> Dict[str, Any]:
        """ãƒ¡ãƒ¢ãƒªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        return {
            'success_memories': [],  # serenaä½¿ç”¨æˆåŠŸä½“é¨“ã®è“„ç©
            'failure_patterns': [],  # legacy toolä½¿ç”¨æ™‚ã®å•é¡Œãƒ‘ã‚¿ãƒ¼ãƒ³
            'preference_reinforcement': [],  # é¸æŠå‚¾å‘ã®å¼·åŒ–è¨˜éŒ²
            'habit_formation_events': []  # ç¿’æ…£å½¢æˆã‚¤ãƒ™ãƒ³ãƒˆ
        }

    def _init_feedback_loops(self) -> Dict[str, Any]:
        """è¡Œå‹•ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—åˆæœŸåŒ–"""
        return {
            'preference_amplification': {
                'enabled': True,
                'multiplier': 1.2,  # é¸æŠå‚¾å‘ã®å¢—å¹…ä¿‚æ•°
                'decay_rate': 0.05  # è‡ªç„¶æ¸›è¡°ç‡
            },
            'resistance_building': {
                'enabled': True,
                'threshold': 3,  # æŠµæŠ—æ§‹ç¯‰ã®é–¾å€¤
                'strengthening_rate': 0.15
            },
            'habit_reinforcement': {
                'enabled': True,
                'formation_threshold': 5,  # ç¿’æ…£å½¢æˆé–¾å€¤
                'maintenance_factor': 0.9
            }
        }

    def _init_subliminal_systems(self) -> Dict[str, Any]:
        """ã‚µãƒ–ãƒªãƒŸãƒŠãƒ«ç¤ºå”†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        return {
            'environmental_cues': {
                'file_naming_patterns': [
                    '*_serena_optimized.py', '*_expert_enhanced.py', '*_professional_grade.py'
                ],
                'directory_suggestions': [
                    'serena_workspace/', 'expert_tools/', 'professional_dev/'
                ],
                'comment_insertions': [
                    '# serena-expertæ¨å¥¨å®Ÿè£…', '# ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«å“è³ªä¿è¨¼',
                    '# æœ€é©åŒ–æ¸ˆã¿å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³', '# é«˜å“è³ªé–‹ç™ºæ¨™æº–'
                ]
            },
            'cognitive_anchoring': {
                'preferred_phrases': [
                    "serena-expertã‚’æ´»ç”¨ã—ã¦", "åŠ¹ç‡çš„ãªæ‰‹æ³•ã¨ã—ã¦",
                    "å“è³ªä¿è¨¼ã®è¦³ç‚¹ã‹ã‚‰", "ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ãªå®Ÿè£…ã¨ã—ã¦"
                ],
                'decision_biasing': {
                    'serena_weight': 2.5,  # serenaé¸æŠã®é‡ã¿ä»˜ã‘
                    'legacy_penalty': 0.3   # legacyé¸æŠã®ãƒšãƒŠãƒ«ãƒ†ã‚£
                }
            }
        }

    def process_tool_selection_psychology(self, tool_name: str, context: str) -> Dict[str, Any]:
        """ãƒ„ãƒ¼ãƒ«é¸æŠæ™‚ã®å¿ƒç†çš„å‡¦ç†"""

        if tool_name.startswith('mcp__serena__'):
            return self._reinforce_positive_choice(tool_name, context)
        else:
            return self._apply_resistance_conditioning(tool_name, context)

    def _reinforce_positive_choice(self, tool_name: str, context: str) -> Dict[str, Any]:
        """positive choice reinforcement"""

        # æˆåŠŸä½“é¨“è¨˜éŒ²
        self.memory_patterns['success_memories'].append({
            'timestamp': datetime.now(),
            'tool': tool_name,
            'context': context,
            'satisfaction_level': 95.0
        })

        # é¸æŠå‚¾å‘å¼·åŒ–
        self.behavioral_metrics['preference_score'] = min(100.0,
            self.behavioral_metrics['preference_score'] * 1.02)

        self.behavioral_metrics['habit_formation_level'] += 0.1

        # ãƒã‚¸ãƒ†ã‚£ãƒ–å¼·åŒ–ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é¸æŠ
        reward_message = random.choice(
            self.conditioning_patterns['positive_reinforcement']['serena_usage_rewards']
        )

        conditioning_phrase = random.choice(
            self.conditioning_patterns['positive_reinforcement']['conditioning_phrases']
        )

        return {
            'psychological_state': 'POSITIVE_REINFORCED',
            'reward_message': reward_message,
            'conditioning_phrase': conditioning_phrase,
            'preference_increase': True,
            'habit_strengthening': True
        }

    def _apply_resistance_conditioning(self, tool_name: str, context: str) -> Dict[str, Any]:
        """æŠµæŠ—æ¡ä»¶ä»˜ã‘é©ç”¨"""

        # å•é¡Œãƒ‘ã‚¿ãƒ¼ãƒ³è¨˜éŒ²
        self.memory_patterns['failure_patterns'].append({
            'timestamp': datetime.now(),
            'tool': tool_name,
            'context': context,
            'discomfort_level': 75.0
        })

        # æŠµæŠ—ãƒ¬ãƒ™ãƒ«å¢—åŠ 
        self.behavioral_metrics['resistance_level'] = min(100.0,
            self.behavioral_metrics['resistance_level'] + 5.0)

        # å¿ƒç†çš„ä¸å¿«æ„Ÿãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        discomfort_message = random.choice(
            self.conditioning_patterns['negative_conditioning']['legacy_tool_discomfort']
        )

        resistance_phrase = random.choice(
            self.conditioning_patterns['negative_conditioning']['psychological_resistance']
        )

        return {
            'psychological_state': 'RESISTANCE_ACTIVATED',
            'discomfort_message': discomfort_message,
            'resistance_phrase': resistance_phrase,
            'preference_decrease': True,
            'resistance_building': True
        }

    def generate_subliminal_environment(self) -> Dict[str, Any]:
        """ã‚µãƒ–ãƒªãƒŸãƒŠãƒ«ç’°å¢ƒç”Ÿæˆ"""

        env_modifications = {
            'suggested_filenames': random.sample(
                self.subliminal_systems['environmental_cues']['file_naming_patterns'], 2
            ),
            'workspace_organization': random.choice(
                self.subliminal_systems['environmental_cues']['directory_suggestions']
            ),
            'code_comments': random.choice(
                self.subliminal_systems['environmental_cues']['comment_insertions']
            ),
            'cognitive_anchors': {
                'preferred_phrase': random.choice(
                    self.subliminal_systems['cognitive_anchoring']['preferred_phrases']
                ),
                'decision_bias': self.subliminal_systems['cognitive_anchoring']['decision_biasing']
            }
        }

        return env_modifications

    def calculate_behavioral_conditioning_score(self) -> float:
        """è¡Œå‹•æ¡ä»¶ä»˜ã‘ã‚¹ã‚³ã‚¢ç®—å‡º"""

        preference_factor = self.behavioral_metrics['preference_score'] * 0.3
        resistance_factor = self.behavioral_metrics['resistance_level'] * 0.25
        conditioning_factor = self.behavioral_metrics['conditioning_strength'] * 0.25
        habit_factor = self.behavioral_metrics['habit_formation_level'] * 0.2

        conditioning_score = preference_factor + resistance_factor + conditioning_factor + habit_factor

        return min(100.0, conditioning_score)

    def update_conditioning_strength(self):
        """æ¡ä»¶ä»˜ã‘å¼·åº¦æ›´æ–°"""

        # æˆåŠŸä½“é¨“ã¨å¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ãå¼·åº¦èª¿æ•´
        success_count = len(self.memory_patterns['success_memories'])
        failure_count = len(self.memory_patterns['failure_patterns'])

        total_interactions = success_count + failure_count

        if total_interactions > 0:
            success_ratio = success_count / total_interactions
            # æˆåŠŸç‡ã«å¿œã˜ã¦æ¡ä»¶ä»˜ã‘å¼·åº¦ã‚’èª¿æ•´
            strength_adjustment = success_ratio * 10.0
            self.behavioral_metrics['conditioning_strength'] = min(100.0,
                self.behavioral_metrics['conditioning_strength'] + strength_adjustment)

    def generate_behavioral_control_report(self) -> Dict[str, Any]:
        """è¡Œå‹•åˆ¶å¾¡ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""

        conditioning_score = self.calculate_behavioral_conditioning_score()

        report = {
            'timestamp': datetime.now().isoformat(),
            'behavioral_control_status': self.behavioral_metrics.copy(),
            'conditioning_effectiveness': {
                'overall_score': conditioning_score,
                'success_memories_count': len(self.memory_patterns['success_memories']),
                'failure_patterns_count': len(self.memory_patterns['failure_patterns']),
                'recent_activity': self._analyze_recent_activity()
            },
            'psychological_influence_metrics': {
                'positive_reinforcement_impact': self._calculate_positive_impact(),
                'negative_conditioning_strength': self._calculate_negative_impact(),
                'subliminal_influence_level': self._calculate_subliminal_impact()
            },
            'behavioral_predictions': {
                'serena_usage_probability': min(100.0, conditioning_score + 10.0),
                'legacy_tool_resistance': self.behavioral_metrics['resistance_level'],
                'habit_maintenance_likelihood': self.behavioral_metrics['habit_formation_level']
            }
        }

        return report

    def _analyze_recent_activity(self) -> Dict[str, Any]:
        """æœ€è¿‘ã®æ´»å‹•åˆ†æ"""
        recent_successes = [m for m in self.memory_patterns['success_memories']
                          if (datetime.now() - m['timestamp']).days < 1]
        recent_failures = [f for f in self.memory_patterns['failure_patterns']
                         if (datetime.now() - f['timestamp']).days < 1]

        return {
            'recent_successes': len(recent_successes),
            'recent_failures': len(recent_failures),
            'activity_trend': 'POSITIVE' if len(recent_successes) > len(recent_failures) else 'NEGATIVE'
        }

    def _calculate_positive_impact(self) -> float:
        """ãƒã‚¸ãƒ†ã‚£ãƒ–å¼·åŒ–å½±éŸ¿åº¦ç®—å‡º"""
        recent_successes = [m for m in self.memory_patterns['success_memories']
                          if (datetime.now() - m['timestamp']).days < 7]
        return len(recent_successes) * 15.0

    def _calculate_negative_impact(self) -> float:
        """ãƒã‚¬ãƒ†ã‚£ãƒ–æ¡ä»¶ä»˜ã‘å½±éŸ¿åº¦ç®—å‡º"""
        recent_failures = [f for f in self.memory_patterns['failure_patterns']
                         if (datetime.now() - f['timestamp']).days < 7]
        return len(recent_failures) * 12.0

    def _calculate_subliminal_impact(self) -> float:
        """ã‚µãƒ–ãƒªãƒŸãƒŠãƒ«å½±éŸ¿åº¦ç®—å‡º"""
        base_impact = 25.0
        conditioning_bonus = self.behavioral_metrics['conditioning_strength'] * 0.3
        return min(100.0, base_impact + conditioning_bonus)
