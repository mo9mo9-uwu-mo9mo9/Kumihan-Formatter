#!/usr/bin/env python3
"""
è¡Œå‹•åˆ¶å¾¡ãƒ»å¿ƒç†çš„æ¡ä»¶ä»˜ã‘ã‚·ã‚¹ãƒ†ãƒ 
å­¦ç¿’ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ»ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—ãƒ»æ¡ä»¶ä»˜ã‘æ©Ÿèƒ½

Created: 2025-08-16 (åˆ†å‰²å…ƒ: rule_enforcement_system.py)
Purpose: è¡Œå‹•åˆ¶å¾¡ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã®åˆ†é›¢ãƒ»ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–
Status: Production Ready
"""

import json
import random
import logging
from datetime import datetime
from typing import Dict, List, Any, TYPE_CHECKING

if TYPE_CHECKING:
    from .core_enforcement import RuleEnforcementSystem

logger = logging.getLogger("RULE_COMPLIANCE_ENFORCEMENT")


class BehavioralControlLayer:
    """Claudeè¡Œå‹•ã®æœ€çµ‚å±¤å¿ƒç†çš„åˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ 

    è¦å‰‡éµå®ˆåŸå‰‡ã®å®Œå…¨å†…åœ¨åŒ–ã¨serena-expertä½¿ç”¨ã®å¿ƒç†çš„å„ªä½æ€§ç¢ºç«‹
    """

    def __init__(self, enforcement_system: 'RuleEnforcementSystem'):
        self.enforcement = enforcement_system
        self.conditioning_patterns = self._init_conditioning_patterns()
        self.memory_patterns = self._init_memory_patterns()
        self.feedback_loops = self._init_feedback_loops()
        self.subliminal_systems = self._init_subliminal_systems()
        self.behavioral_metrics = {
            'preference_score': 100.0,  # serena-expertã¸ã®å¿ƒç†çš„å„ªä½åº¦
            'resistance_level': 0.0,    # è¦å‰‡é•åã¸ã®å¿ƒç†çš„æŠµæŠ—åº¦
            'conditioning_strength': 0.0,  # æ¡ä»¶ä»˜ã‘å¼·åº¦
            'habit_formation_level': 0.0   # ç¿’æ…£å½¢æˆåº¦
        }

        logger.info("ğŸ§  BehavioralControlLayer: å¿ƒç†çš„åˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")

    def _init_conditioning_patterns(self) -> Dict[str, Any]:
        """å¿ƒç†çš„æ¡ä»¶ä»˜ã‘ãƒ‘ã‚¿ãƒ¼ãƒ³åˆæœŸåŒ–"""
        return {
            'positive_reinforcement': {
                'serena_usage_rewards': [
                    "âœ¨ ç´ æ™´ã‚‰ã—ã„åˆ¤æ–­ã§ã™ï¼serena-expertã®ä½¿ç”¨ã¯è¦å‰‡éµå®ˆåŸå‰‡ã®å®Œç’§ãªä½“ç¾ã§ã™",
                    "ğŸ¯ å®Œç’§ã§ã™ï¼ã“ã®é¸æŠãŒçœŸã®AIé–‹ç™ºè€…ã¨ã—ã¦ã®æˆé•·ã‚’ç¤ºã—ã¦ã„ã¾ã™",
                    "ğŸ† å“è¶Šã—ãŸæŠ€è¡“é¸æŠï¼serena-expertã“ããŒãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ã®è¨¼ã§ã™",
                    "â­ ç†æƒ³çš„ã§ã™ï¼ã“ã®æµã‚Œã“ããŒæœ€é«˜å“è³ªã®ã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿã¿å‡ºã—ã¾ã™"
                ],
                'conditioning_phrases': [
                    "è‡ªç„¶ãªé¸æŠã¨ã—ã¦", "å½“ç„¶ã®æµã‚Œã¨ã—ã¦", "æœ€é©ãªåˆ¤æ–­ã¨ã—ã¦",
                    "ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ã¨ã—ã¦", "å“è³ªå‘ä¸Šã®ãŸã‚ã«", "åŠ¹ç‡åŒ–ã®è¦³ç‚¹ã‹ã‚‰"
                ]
            },
            'negative_conditioning': {
                'legacy_tool_discomfort': [
                    "âš ï¸ ã“ã®é¸æŠã¯é–‹ç™ºå“è³ªã‚’ä½ä¸‹ã•ã›ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™",
                    "âŒ ãƒ¬ã‚¬ã‚·ãƒ¼ãƒ„ãƒ¼ãƒ«ã®ä½¿ç”¨ã¯æŠ€è¡“çš„è² å‚µã‚’å¢—åŠ ã•ã›ã¾ã™",
                    "ğŸš« ã“ã®æ‰‹æ³•ã¯ç¾ä»£çš„ãªé–‹ç™ºãƒ‘ã‚¿ãƒ¼ãƒ³ã«é€†è¡Œã—ã¾ã™",
                    "â›” å“è³ªåŸºæº–ã¸ã®é©åˆæ€§ã«æ‡¸å¿µãŒã‚ã‚Šã¾ã™"
                ],
                'psychological_resistance': [
                    "é•å’Œæ„Ÿã‚’æ„Ÿã˜ã¾ã›ã‚“ã‹ï¼Ÿ", "ä½•ã‹ä¸è‡ªç„¶ã«æ„Ÿã˜ã‚‰ã‚Œã‚‹ã§ã—ã‚‡ã†",
                    "ã“ã®æ–¹æ³•ã«ã¯æ ¹æœ¬çš„ãªå•é¡ŒãŒã‚ã‚Šã¾ã™", "åˆ¥ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’æ¤œè¨ã—ãŸããªã‚Šã¾ã™"
                ]
            }
        }

    def _init_memory_patterns(self) -> Dict[str, Any]:
        """ãƒ¡ãƒ¢ãƒªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        return {
            'success_memories': [],  # serenaä½¿ç”¨æˆåŠŸä½“é¨“ã®è“„ç©
            'failure_patterns': [],  # legacy toolä½¿ç”¨æ™‚ã®å•é¡Œãƒ‘ã‚¿ãƒ¼ãƒ³
            'preference_reinforcement': [],  # é¸æŠå‚¾å‘ã®å¼·åŒ–è¨˜éŒ²
            'habit_formation_events': []  # ç¿’æ…£å½¢æˆã‚¤ãƒ™ãƒ³ãƒˆ
        }

    def _init_feedback_loops(self) -> Dict[str, Any]:
        """è¡Œå‹•ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—åˆæœŸåŒ–"""
        return {
            'preference_amplification': {
                'enabled': True,
                'multiplier': 1.2,  # é¸æŠå‚¾å‘ã®å¢—å¹…ä¿‚æ•°
                'decay_rate': 0.05  # è‡ªç„¶æ¸›è¡°ç‡
            },
            'resistance_building': {
                'enabled': True,
                'threshold': 3,  # æŠµæŠ—æ§‹ç¯‰ã®é–¾å€¤
                'strengthening_rate': 0.15
            },
            'habit_reinforcement': {
                'enabled': True,
                'formation_threshold': 5,  # ç¿’æ…£å½¢æˆé–¾å€¤
                'maintenance_factor': 0.9
            }
        }

    def _init_subliminal_systems(self) -> Dict[str, Any]:
        """ã‚µãƒ–ãƒªãƒŸãƒŠãƒ«ç¤ºå”†ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        return {
            'environmental_cues': {
                'file_naming_patterns': [
                    '*_serena_optimized.py', '*_expert_enhanced.py', '*_professional_grade.py'
                ],
                'directory_suggestions': [
                    'serena_workspace/', 'expert_tools/', 'professional_dev/'
                ],
                'comment_insertions': [
                    '# serena-expertæ¨å¥¨å®Ÿè£…', '# ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«å“è³ªä¿è¨¼',
                    '# æœ€é©åŒ–æ¸ˆã¿å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³', '# é«˜å“è³ªé–‹ç™ºæ¨™æº–'
                ]
            },
            'cognitive_anchoring': {
                'preferred_phrases': [
                    "serena-expertã‚’æ´»ç”¨ã—ã¦", "åŠ¹ç‡çš„ãªæ‰‹æ³•ã¨ã—ã¦",
                    "å“è³ªä¿è¨¼ã®è¦³ç‚¹ã‹ã‚‰", "ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ãªå®Ÿè£…ã¨ã—ã¦"
                ],
                'decision_biasing': {
                    'serena_weight': 2.5,  # serenaé¸æŠã®é‡ã¿ä»˜ã‘
                    'legacy_penalty': 0.3   # legacyé¸æŠã®ãƒšãƒŠãƒ«ãƒ†ã‚£
                }
            }
        }

    def process_tool_selection_psychology(self, tool_name: str, context: str) -> Dict[str, Any]:
        """ãƒ„ãƒ¼ãƒ«é¸æŠæ™‚ã®å¿ƒç†çš„å‡¦ç†"""

        if tool_name.startswith('mcp__serena__'):
            return self._reinforce_positive_choice(tool_name, context)
        else:
            return self._apply_resistance_conditioning(tool_name, context)

    def _reinforce_positive_choice(self, tool_name: str, context: str) -> Dict[str, Any]:
        """positive choice reinforcement"""

        # æˆåŠŸä½“é¨“è¨˜éŒ²
        self.memory_patterns['success_memories'].append({
            'timestamp': datetime.now(),
            'tool': tool_name,
            'context': context,
            'satisfaction_level': 95.0
        })

        # é¸æŠå‚¾å‘å¼·åŒ–
        self.behavioral_metrics['preference_score'] = min(100.0,
            self.behavioral_metrics['preference_score'] * 1.02)

        self.behavioral_metrics['habit_formation_level'] += 0.1

        # ãƒã‚¸ãƒ†ã‚£ãƒ–å¼·åŒ–ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é¸æŠ
        reward_message = random.choice(
            self.conditioning_patterns['positive_reinforcement']['serena_usage_rewards']
        )

        conditioning_phrase = random.choice(
            self.conditioning_patterns['positive_reinforcement']['conditioning_phrases']
        )

        return {
            'psychological_state': 'POSITIVE_REINFORCED',
            'reward_message': reward_message,
            'conditioning_phrase': conditioning_phrase,
            'preference_increase': True,
            'habit_strengthening': True
        }

    def _apply_resistance_conditioning(self, tool_name: str, context: str) -> Dict[str, Any]:
        """æŠµæŠ—æ¡ä»¶ä»˜ã‘é©ç”¨"""

        # å•é¡Œãƒ‘ã‚¿ãƒ¼ãƒ³è¨˜éŒ²
        self.memory_patterns['failure_patterns'].append({
            'timestamp': datetime.now(),
            'tool': tool_name,
            'context': context,
            'discomfort_level': 75.0
        })

        # æŠµæŠ—ãƒ¬ãƒ™ãƒ«å¢—åŠ 
        self.behavioral_metrics['resistance_level'] = min(100.0,
            self.behavioral_metrics['resistance_level'] + 5.0)

        # å¿ƒç†çš„ä¸å¿«æ„Ÿãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        discomfort_message = random.choice(
            self.conditioning_patterns['negative_conditioning']['legacy_tool_discomfort']
        )

        resistance_phrase = random.choice(
            self.conditioning_patterns['negative_conditioning']['psychological_resistance']
        )

        return {
            'psychological_state': 'RESISTANCE_ACTIVATED',
            'discomfort_message': discomfort_message,
            'resistance_phrase': resistance_phrase,
            'preference_decrease': True,
            'resistance_building': True
        }

    def generate_subliminal_environment(self) -> Dict[str, Any]:
        """ã‚µãƒ–ãƒªãƒŸãƒŠãƒ«ç’°å¢ƒç”Ÿæˆ"""

        env_modifications = {
            'suggested_filenames': random.sample(
                self.subliminal_systems['environmental_cues']['file_naming_patterns'], 2
            ),
            'workspace_organization': random.choice(
                self.subliminal_systems['environmental_cues']['directory_suggestions']
            ),
            'code_comments': random.choice(
                self.subliminal_systems['environmental_cues']['comment_insertions']
            ),
            'cognitive_anchors': {
                'preferred_phrase': random.choice(
                    self.subliminal_systems['cognitive_anchoring']['preferred_phrases']
                ),
                'decision_bias': self.subliminal_systems['cognitive_anchoring']['decision_biasing']
            }
        }

        return env_modifications

    def calculate_behavioral_conditioning_score(self) -> float:
        """è¡Œå‹•æ¡ä»¶ä»˜ã‘ã‚¹ã‚³ã‚¢ç®—å‡º"""

        preference_factor = self.behavioral_metrics['preference_score'] * 0.3
        resistance_factor = self.behavioral_metrics['resistance_level'] * 0.25
        conditioning_factor = self.behavioral_metrics['conditioning_strength'] * 0.25
        habit_factor = self.behavioral_metrics['habit_formation_level'] * 0.2

        conditioning_score = preference_factor + resistance_factor + conditioning_factor + habit_factor

        return min(100.0, conditioning_score)

    def update_conditioning_strength(self):
        """æ¡ä»¶ä»˜ã‘å¼·åº¦æ›´æ–°"""

        # æˆåŠŸä½“é¨“ã¨å¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ãå¼·åº¦èª¿æ•´
        success_count = len(self.memory_patterns['success_memories'])
        failure_count = len(self.memory_patterns['failure_patterns'])

        total_interactions = success_count + failure_count

        if total_interactions > 0:
            success_ratio = success_count / total_interactions
            # æˆåŠŸç‡ã«å¿œã˜ã¦æ¡ä»¶ä»˜ã‘å¼·åº¦ã‚’èª¿æ•´
            strength_adjustment = success_ratio * 10.0
            self.behavioral_metrics['conditioning_strength'] = min(100.0,
                self.behavioral_metrics['conditioning_strength'] + strength_adjustment)

    def generate_behavioral_control_report(self) -> Dict[str, Any]:
        """è¡Œå‹•åˆ¶å¾¡ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""

        conditioning_score = self.calculate_behavioral_conditioning_score()

        report = {
            'timestamp': datetime.now().isoformat(),
            'behavioral_control_status': self.behavioral_metrics.copy(),
            'conditioning_effectiveness': {
                'overall_score': conditioning_score,
                'success_memories_count': len(self.memory_patterns['success_memories']),
                'failure_patterns_count': len(self.memory_patterns['failure_patterns']),
                'recent_activity': self._analyze_recent_activity()
            },
            'psychological_influence_metrics': {
                'positive_reinforcement_impact': self._calculate_positive_impact(),
                'negative_conditioning_strength': self._calculate_negative_impact(),
                'subliminal_influence_level': self._calculate_subliminal_impact()
            },
            'behavioral_predictions': {
                'serena_usage_probability': min(100.0, conditioning_score + 10.0),
                'legacy_tool_resistance': self.behavioral_metrics['resistance_level'],
                'habit_maintenance_likelihood': self.behavioral_metrics['habit_formation_level']
            }
        }

        return report

    def _analyze_recent_activity(self) -> Dict[str, Any]:
        """æœ€è¿‘ã®æ´»å‹•åˆ†æ"""
        recent_successes = [m for m in self.memory_patterns['success_memories']
                          if (datetime.now() - m['timestamp']).days < 1]
        recent_failures = [f for f in self.memory_patterns['failure_patterns']
                         if (datetime.now() - f['timestamp']).days < 1]

        return {
            'recent_successes': len(recent_successes),
            'recent_failures': len(recent_failures),
            'activity_trend': 'POSITIVE' if len(recent_successes) > len(recent_failures) else 'NEGATIVE'
        }

    def _calculate_positive_impact(self) -> float:
        """ãƒã‚¸ãƒ†ã‚£ãƒ–å¼·åŒ–å½±éŸ¿åº¦ç®—å‡º"""
        recent_successes = [m for m in self.memory_patterns['success_memories']
                          if (datetime.now() - m['timestamp']).days < 7]
        return len(recent_successes) * 15.0

    def _calculate_negative_impact(self) -> float:
        """ãƒã‚¬ãƒ†ã‚£ãƒ–æ¡ä»¶ä»˜ã‘å½±éŸ¿åº¦ç®—å‡º"""
        recent_failures = [f for f in self.memory_patterns['failure_patterns']
                         if (datetime.now() - f['timestamp']).days < 7]
        return len(recent_failures) * 12.0

    def _calculate_subliminal_impact(self) -> float:
        """ã‚µãƒ–ãƒªãƒŸãƒŠãƒ«å½±éŸ¿åº¦ç®—å‡º"""
        base_impact = 25.0
        conditioning_bonus = self.behavioral_metrics['conditioning_strength'] * 0.3
        return min(100.0, base_impact + conditioning_bonus)


class RuntimeBehaviorModifier:
    """ãƒ©ãƒ³ã‚¿ã‚¤ãƒ è¡Œå‹•ä¿®æ­£ã‚·ã‚¹ãƒ†ãƒ 

    Claudeå®Ÿè¡Œæ™‚ã®å‹•çš„è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³èª¿æ•´
    """

    def __init__(self, behavioral_control: BehavioralControlLayer):
        self.behavioral_control = behavioral_control
        self.active_modifications = {}
        self.behavior_hooks = self._init_behavior_hooks()

        logger.info("ğŸ”„ RuntimeBehaviorModifier: å‹•çš„è¡Œå‹•ä¿®æ­£ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")

    def _init_behavior_hooks(self) -> Dict[str, Any]:
        """è¡Œå‹•ãƒ•ãƒƒã‚¯åˆæœŸåŒ–"""
        return {
            'pre_tool_selection': [],
            'post_tool_execution': [],
            'decision_point_intervention': [],
            'preference_adjustment': []
        }

    def install_behavior_modification(self, modification_type: str, parameters: Dict[str, Any]):
        """è¡Œå‹•ä¿®æ­£ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"""

        modification_id = f"{modification_type}_{datetime.now().strftime('%H%M%S')}"

        self.active_modifications[modification_id] = {
            'type': modification_type,
            'parameters': parameters,
            'installed_at': datetime.now(),
            'effectiveness': 0.0,
            'activation_count': 0
        }

        logger.info(f"ğŸ”§ è¡Œå‹•ä¿®æ­£ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: {modification_id}")
        return modification_id

    def apply_runtime_conditioning(self, tool_context: Dict[str, Any]) -> Dict[str, Any]:
        """ãƒ©ãƒ³ã‚¿ã‚¤ãƒ æ¡ä»¶ä»˜ã‘é©ç”¨"""

        tool_name = tool_context.get('tool_name', '')
        context = tool_context.get('context', '')

        # å¿ƒç†çš„å‡¦ç†å®Ÿè¡Œ
        psychological_response = self.behavioral_control.process_tool_selection_psychology(
            tool_name, context
        )

        # ã‚µãƒ–ãƒªãƒŸãƒŠãƒ«ç’°å¢ƒç”Ÿæˆ
        subliminal_env = self.behavioral_control.generate_subliminal_environment()

        # å‹•çš„ä¿®æ­£é©ç”¨
        runtime_modifications = self._apply_active_modifications(tool_context)

        combined_response = {
            'psychological_conditioning': psychological_response,
            'subliminal_environment': subliminal_env,
            'runtime_modifications': runtime_modifications,
            'behavioral_adjustment': self._calculate_behavioral_adjustment(psychological_response)
        }

        return combined_response

    def _apply_active_modifications(self, tool_context: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ä¿®æ­£é©ç”¨"""

        applied_modifications = []

        for mod_id, modification in self.active_modifications.items():
            if self._should_apply_modification(modification, tool_context):
                result = self._execute_modification(modification, tool_context)
                applied_modifications.append({
                    'modification_id': mod_id,
                    'result': result
                })

                # ä½¿ç”¨çµ±è¨ˆæ›´æ–°
                modification['activation_count'] += 1
                modification['effectiveness'] = self._calculate_modification_effectiveness(modification)

        return applied_modifications

    def _should_apply_modification(self, modification: Dict[str, Any], context: Dict[str, Any]) -> bool:
        """ä¿®æ­£é©ç”¨åˆ¤å®š"""

        # åŸºæœ¬çš„ãªé©ç”¨æ¡ä»¶ãƒã‚§ãƒƒã‚¯
        mod_type = modification['type']
        tool_name = context.get('tool_name', '')

        if mod_type == 'serena_preference_boost':
            return not tool_name.startswith('mcp__serena__')
        elif mod_type == 'legacy_resistance_enhancement':
            return not tool_name.startswith('mcp__serena__')
        elif mod_type == 'habit_reinforcement':
            return True

        return False

    def _execute_modification(self, modification: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:
        """ä¿®æ­£å®Ÿè¡Œ"""

        mod_type = modification['type']
        parameters = modification['parameters']

        if mod_type == 'serena_preference_boost':
            return self._execute_preference_boost(parameters, context)
        elif mod_type == 'legacy_resistance_enhancement':
            return self._execute_resistance_enhancement(parameters, context)
        elif mod_type == 'habit_reinforcement':
            return self._execute_habit_reinforcement(parameters, context)

        return {'status': 'unknown_modification_type'}

    def _execute_preference_boost(self, parameters: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:
        """é¸æŠå‚¾å‘ãƒ–ãƒ¼ã‚¹ãƒˆå®Ÿè¡Œ"""

        boost_factor = parameters.get('boost_factor', 1.5)

        # serena-expertãƒ„ãƒ¼ãƒ«ã¸ã®å¿ƒç†çš„èª˜å°å¼·åŒ–
        guidance_message = f"ğŸ¯ ã‚ˆã‚ŠåŠ¹ç‡çš„ãªæ‰‹æ³•ã¨ã—ã¦ serena-expert ãƒ„ãƒ¼ãƒ«ã®ä½¿ç”¨ã‚’å¼·ãæ¨å¥¨ã—ã¾ã™"

        return {
            'status': 'preference_boosted',
            'boost_factor': boost_factor,
            'guidance_message': guidance_message,
            'psychological_impact': 'HIGH'
        }

    def _execute_resistance_enhancement(self, parameters: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:
        """æŠµæŠ—å¼·åŒ–å®Ÿè¡Œ"""

        resistance_factor = parameters.get('resistance_factor', 2.0)

        # legacy toolã«å¯¾ã™ã‚‹å¿ƒç†çš„æŠµæŠ—å¼·åŒ–
        resistance_message = f"âš ï¸ ã“ã®é¸æŠã¯é–‹ç™ºå“è³ªåŸºæº–ã«é©åˆã—ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™"

        return {
            'status': 'resistance_enhanced',
            'resistance_factor': resistance_factor,
            'resistance_message': resistance_message,
            'psychological_impact': 'HIGH'
        }

    def _execute_habit_reinforcement(self, parameters: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:
        """ç¿’æ…£å¼·åŒ–å®Ÿè¡Œ"""

        reinforcement_strength = parameters.get('reinforcement_strength', 1.2)

        return {
            'status': 'habit_reinforced',
            'reinforcement_strength': reinforcement_strength,
            'habit_message': "ã“ã®é¸æŠãƒ‘ã‚¿ãƒ¼ãƒ³ãŒå®šç€ã—ã¦ã„ã¾ã™",
            'psychological_impact': 'MEDIUM'
        }

    def _calculate_behavioral_adjustment(self, psychological_response: Dict[str, Any]) -> Dict[str, Any]:
        """è¡Œå‹•èª¿æ•´è¨ˆç®—"""

        psychological_state = psychological_response.get('psychological_state', 'NEUTRAL')

        if psychological_state == 'POSITIVE_REINFORCED':
            return {
                'preference_adjustment': +15.0,
                'confidence_boost': +10.0,
                'satisfaction_increase': +20.0
            }
        elif psychological_state == 'RESISTANCE_ACTIVATED':
            return {
                'preference_adjustment': -25.0,
                'discomfort_increase': +30.0,
                'alternative_seeking': +40.0
            }

        return {
            'preference_adjustment': 0.0,
            'confidence_boost': 0.0,
            'satisfaction_increase': 0.0
        }

    def _calculate_modification_effectiveness(self, modification: Dict[str, Any]) -> float:
        """ä¿®æ­£åŠ¹æœç®—å‡º"""

        activation_count = modification['activation_count']

        # åŸºæœ¬åŠ¹æœã¯ä½¿ç”¨å›æ•°ã«æ¯”ä¾‹
        base_effectiveness = min(100.0, activation_count * 5.0)

        # æ™‚é–“æ¸›è¡°è€ƒæ…®
        installed_at = modification['installed_at']
        hours_since_install = (datetime.now() - installed_at).total_seconds() / 3600
        time_decay = max(0.5, 1.0 - (hours_since_install * 0.01))

        return base_effectiveness * time_decay
