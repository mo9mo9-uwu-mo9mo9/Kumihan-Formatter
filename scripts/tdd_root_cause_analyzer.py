#!/usr/bin/env python3
"""
TDD Root Cause Analyzer - Issue #640 Phase 3
TDDæ ¹æœ¬åŸå› åˆ†æã‚·ã‚¹ãƒ†ãƒ 

ç›®çš„: TDDå¤±æ•—ãƒ»å“è³ªå•é¡Œã®æ ¹æœ¬åŸå› è‡ªå‹•åˆ†æ
- ãƒ†ã‚¹ãƒˆå¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
- ã‚³ãƒ¼ãƒ‰å“è³ªå•é¡Œã®æ ¹æœ¬åŸå› ç‰¹å®š
- æ”¹å–„ææ¡ˆã®è‡ªå‹•ç”Ÿæˆ
"""

import json
import sys
import subprocess
import argparse
import re
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Set
from dataclasses import dataclass, asdict
from datetime import datetime
from enum import Enum
from collections import defaultdict, Counter

from kumihan_formatter.core.utilities.logger import get_logger

logger = get_logger(__name__)

class ProblemCategory(Enum):
    """å•é¡Œã‚«ãƒ†ã‚´ãƒª"""
    TEST_DESIGN = "test_design"
    IMPLEMENTATION = "implementation"
    ARCHITECTURE = "architecture"
    DEPENDENCY = "dependency"
    PERFORMANCE = "performance"
    SECURITY = "security"
    MAINTAINABILITY = "maintainability"

class Severity(Enum):
    """é‡è¦åº¦"""
    CRITICAL = "critical"
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"

@dataclass
class RootCause:
    """æ ¹æœ¬åŸå› """
    cause_id: str
    category: ProblemCategory
    severity: Severity
    title: str
    description: str
    affected_files: List[str]
    evidence: List[str]
    fix_suggestions: List[str]
    estimated_fix_hours: float
    priority_score: float

@dataclass
class AnalysisResult:
    """åˆ†æçµæœ"""
    analysis_id: str
    timestamp: datetime
    analyzed_files: List[str]
    root_causes: List[RootCause]
    quality_metrics: Dict[str, float]
    recommendations: List[str]
    action_plan: List[Dict]

class TDDRootCauseAnalyzer:
    """TDDæ ¹æœ¬åŸå› åˆ†æã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.analysis_dir = project_root / ".tdd_analysis"
        self.analysis_dir.mkdir(exist_ok=True)
        
        # å•é¡Œãƒ‘ã‚¿ãƒ¼ãƒ³å®šç¾©
        self.problem_patterns = {
            "test_design": [
                {
                    "pattern": r"def test_.*\(self\):\s*pass",
                    "description": "ç©ºã®ãƒ†ã‚¹ãƒˆãƒ¡ã‚½ãƒƒãƒ‰",
                    "severity": Severity.HIGH,
                    "fix": "å…·ä½“çš„ãªãƒ†ã‚¹ãƒˆå®Ÿè£…ãŒå¿…è¦"
                },
                {
                    "pattern": r"self\.fail\([\"']TODO",
                    "description": "æœªå®Ÿè£…ã®ãƒ†ã‚¹ãƒˆ",
                    "severity": Severity.MEDIUM,
                    "fix": "TODOã‚’å®Ÿè£…ã«ç½®ãæ›ãˆã‚‹"
                },
                {
                    "pattern": r"assert\s+True",
                    "description": "ç„¡æ„å‘³ãªã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³",
                    "severity": Severity.HIGH,
                    "fix": "å…·ä½“çš„ãªå€¤ã®æ¤œè¨¼ã‚’è¿½åŠ "
                }
            ],
            "implementation": [
                {
                    "pattern": r"raise NotImplementedError",
                    "description": "æœªå®Ÿè£…ãƒ¡ã‚½ãƒƒãƒ‰",
                    "severity": Severity.CRITICAL,
                    "fix": "ãƒ¡ã‚½ãƒƒãƒ‰ã®å®Ÿè£…ãŒå¿…è¦"
                },
                {
                    "pattern": r"# TODO:",
                    "description": "æœªå®Œäº†ã®TODO",
                    "severity": Severity.MEDIUM,
                    "fix": "TODOã®å®Ÿè£…ã¾ãŸã¯å‰Šé™¤"
                },
                {
                    "pattern": r"except:\s*pass",
                    "description": "ä¾‹å¤–ã®ç„¡è¦–",
                    "severity": Severity.HIGH,
                    "fix": "é©åˆ‡ãªä¾‹å¤–å‡¦ç†ã‚’è¿½åŠ "
                }
            ],
            "architecture": [
                {
                    "pattern": r"from .* import \*",
                    "description": "ãƒ¯ã‚¤ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ",
                    "severity": Severity.MEDIUM,
                    "fix": "æ˜ç¤ºçš„ãªã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤‰æ›´"
                },
                {
                    "pattern": r"class.*:\s*def.*\s*def.*\s*def.*\s*def.*\s*def.*\s*def.*\s*def.*\s*def.*\s*def.*\s*def.*",
                    "description": "å·¨å¤§ãªã‚¯ãƒ©ã‚¹",
                    "severity": Severity.HIGH,
                    "fix": "ã‚¯ãƒ©ã‚¹ã®åˆ†å‰²ã‚’æ¤œè¨"
                }
            ]
        }
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹é–¾å€¤
        self.thresholds = {
            "cyclomatic_complexity": 10,
            "function_length": 50,
            "class_length": 200,
            "nesting_depth": 4,
            "test_coverage": 90.0
        }
    
    def analyze_project(self) -> AnalysisResult:
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ã®æ ¹æœ¬åŸå› åˆ†æ"""
        logger.info("ğŸ” TDDæ ¹æœ¬åŸå› åˆ†æé–‹å§‹...")
        
        # åˆ†æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«åé›†
        analyzed_files = self._collect_analysis_files()
        
        # æ ¹æœ¬åŸå› åˆ†æå®Ÿè¡Œ
        root_causes = []
        
        # ã‚³ãƒ¼ãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        pattern_causes = self._analyze_code_patterns(analyzed_files)
        root_causes.extend(pattern_causes)
        
        # ãƒ†ã‚¹ãƒˆå“è³ªåˆ†æ
        test_causes = self._analyze_test_quality(analyzed_files)
        root_causes.extend(test_causes)
        
        # ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£åˆ†æ
        arch_causes = self._analyze_architecture(analyzed_files)
        root_causes.extend(arch_causes)
        
        # ä¾å­˜é–¢ä¿‚åˆ†æ
        dep_causes = self._analyze_dependencies(analyzed_files)
        root_causes.extend(dep_causes)
        
        # å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        quality_metrics = self._calculate_quality_metrics(analyzed_files)
        
        # æ¨å¥¨äº‹é …ç”Ÿæˆ
        recommendations = self._generate_recommendations(root_causes, quality_metrics)
        
        # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³ç”Ÿæˆ
        action_plan = self._generate_action_plan(root_causes)
        
        # çµæœä½œæˆ
        result = AnalysisResult(
            analysis_id=f"analysis-{datetime.now().strftime('%Y%m%d-%H%M%S')}",
            timestamp=datetime.now(),
            analyzed_files=analyzed_files,
            root_causes=sorted(root_causes, key=lambda x: x.priority_score, reverse=True),
            quality_metrics=quality_metrics,
            recommendations=recommendations,
            action_plan=action_plan
        )
        
        # çµæœä¿å­˜
        self._save_analysis_result(result)
        
        logger.info(f"âœ… æ ¹æœ¬åŸå› åˆ†æå®Œäº†: {len(root_causes)}ä»¶ã®å•é¡Œã‚’æ¤œå‡º")
        return result
    
    def _collect_analysis_files(self) -> List[str]:
        """åˆ†æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«åé›†"""
        files = []
        
        # Pythonãƒ•ã‚¡ã‚¤ãƒ«åé›†
        for pattern in ["**/*.py"]:
            for file_path in self.project_root.glob(pattern):
                if self._should_analyze_file(file_path):
                    files.append(str(file_path.relative_to(self.project_root)))
        
        logger.info(f"åˆ†æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {len(files)}ä»¶")
        return files
    
    def _should_analyze_file(self, file_path: Path) -> bool:
        """ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æå¯¾è±¡åˆ¤å®š"""
        # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³
        exclude_patterns = [
            "__pycache__",
            ".git",
            "venv",
            ".tox",
            "build",
            "dist"
        ]
        
        file_str = str(file_path)
        return not any(pattern in file_str for pattern in exclude_patterns)
    
    def _analyze_code_patterns(self, files: List[str]) -> List[RootCause]:
        """ã‚³ãƒ¼ãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        logger.info("ğŸ“ ã‚³ãƒ¼ãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æä¸­...")
        causes = []
        cause_counter = 1
        
        for file_path in files:
            full_path = self.project_root / file_path
            
            if not full_path.exists():
                continue
                
            try:
                with open(full_path, "r", encoding="utf-8") as f:
                    content = f.read()
                
                # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°å®Ÿè¡Œ
                for category, patterns in self.problem_patterns.items():
                    for pattern_info in patterns:
                        matches = re.finditer(pattern_info["pattern"], content, re.MULTILINE | re.DOTALL)
                        
                        for match in matches:
                            line_num = content[:match.start()].count('\n') + 1
                            
                            cause = RootCause(
                                cause_id=f"PAT-{cause_counter:03d}",
                                category=ProblemCategory(category),
                                severity=pattern_info["severity"],
                                title=pattern_info["description"],
                                description=f"{file_path}:{line_num}ã§æ¤œå‡º: {pattern_info['description']}",
                                affected_files=[file_path],
                                evidence=[f"Line {line_num}: {match.group(0)[:100]}"],
                                fix_suggestions=[pattern_info["fix"]],
                                estimated_fix_hours=self._estimate_fix_time(pattern_info["severity"]),
                                priority_score=self._calculate_priority_score(
                                    pattern_info["severity"], 
                                    ProblemCategory(category)
                                )
                            )
                            causes.append(cause)
                            cause_counter += 1
                            
            except Exception as e:
                logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
        
        return causes
    
    def _analyze_test_quality(self, files: List[str]) -> List[RootCause]:
        """ãƒ†ã‚¹ãƒˆå“è³ªåˆ†æ"""
        logger.info("ğŸ§ª ãƒ†ã‚¹ãƒˆå“è³ªåˆ†æä¸­...")
        causes = []
        cause_counter = 100
        
        test_files = [f for f in files if "test_" in f or f.endswith("_test.py")]
        
        for test_file in test_files:
            full_path = self.project_root / test_file
            
            try:
                with open(full_path, "r", encoding="utf-8") as f:
                    content = f.read()
                
                # ãƒ†ã‚¹ãƒˆãƒ¡ã‚½ãƒƒãƒ‰æ•°ãƒã‚§ãƒƒã‚¯
                test_methods = re.findall(r"def test_\w+", content)
                if len(test_methods) < 3:
                    cause = RootCause(
                        cause_id=f"TEST-{cause_counter:03d}",
                        category=ProblemCategory.TEST_DESIGN,
                        severity=Severity.MEDIUM,
                        title="ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ä¸è¶³",
                        description=f"{test_file}: ãƒ†ã‚¹ãƒˆãƒ¡ã‚½ãƒƒãƒ‰æ•°ãŒå°‘ãªã„ ({len(test_methods)}å€‹)",
                        affected_files=[test_file],
                        evidence=[f"æ¤œå‡ºã•ã‚ŒãŸãƒ†ã‚¹ãƒˆãƒ¡ã‚½ãƒƒãƒ‰: {len(test_methods)}å€‹"],
                        fix_suggestions=[
                            "æ­£å¸¸ã‚±ãƒ¼ã‚¹ãƒ»ç•°å¸¸ã‚±ãƒ¼ã‚¹ãƒ»å¢ƒç•Œå€¤ã®ãƒ†ã‚¹ãƒˆã‚’è¿½åŠ ",
                            "æœ€ä½5å€‹ä»¥ä¸Šã®ãƒ†ã‚¹ãƒˆãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½œæˆ"
                        ],
                        estimated_fix_hours=4.0,
                        priority_score=60.0
                    )
                    causes.append(cause)
                    cause_counter += 1
                
                # ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³æ•°ãƒã‚§ãƒƒã‚¯
                assertions = re.findall(r"self\.assert\w+|assert ", content)
                if len(assertions) < len(test_methods) * 2:
                    cause = RootCause(
                        cause_id=f"TEST-{cause_counter:03d}",
                        category=ProblemCategory.TEST_DESIGN,
                        severity=Severity.MEDIUM,
                        title="ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³ä¸è¶³",
                        description=f"{test_file}: ãƒ†ã‚¹ãƒˆã®æ¤œè¨¼ãŒä¸ååˆ†",
                        affected_files=[test_file],
                        evidence=[f"ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³: {len(assertions)}å€‹, ãƒ†ã‚¹ãƒˆãƒ¡ã‚½ãƒƒãƒ‰: {len(test_methods)}å€‹"],
                        fix_suggestions=[
                            "å„ãƒ†ã‚¹ãƒˆãƒ¡ã‚½ãƒƒãƒ‰ã«è¤‡æ•°ã®ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ ",
                            "æœŸå¾…å€¤ã¨å®Ÿéš›ã®å€¤ã®æ¯”è¼ƒã‚’å³å¯†ã«"
                        ],
                        estimated_fix_hours=3.0,
                        priority_score=55.0
                    )
                    causes.append(cause)
                    cause_counter += 1
                    
            except Exception as e:
                logger.warning(f"ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«åˆ†æã‚¨ãƒ©ãƒ¼ {test_file}: {e}")
        
        return causes
    
    def _analyze_architecture(self, files: List[str]) -> List[RootCause]:
        """ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£åˆ†æ"""
        logger.info("ğŸ—ï¸ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£åˆ†æä¸­...")
        causes = []
        cause_counter = 200
        
        # å¾ªç’°ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ¤œå‡º
        import_graph = self._build_import_graph(files)
        cycles = self._detect_import_cycles(import_graph)
        
        for cycle in cycles:
            cause = RootCause(
                cause_id=f"ARCH-{cause_counter:03d}",
                category=ProblemCategory.ARCHITECTURE,
                severity=Severity.HIGH,
                title="å¾ªç’°ã‚¤ãƒ³ãƒãƒ¼ãƒˆ",
                description=f"å¾ªç’°ã‚¤ãƒ³ãƒãƒ¼ãƒˆãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ: {' -> '.join(cycle)}",
                affected_files=cycle,
                evidence=[f"ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚µã‚¤ã‚¯ãƒ«: {len(cycle)}ãƒ•ã‚¡ã‚¤ãƒ«"],
                fix_suggestions=[
                    "å…±é€šã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®æŠ½å‡º",
                    "ä¾å­˜é–¢ä¿‚ã®é€†è»¢",
                    "ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«æ§‹é€ ã®å†è¨­è¨ˆ"
                ],
                estimated_fix_hours=8.0,
                priority_score=85.0
            )
            causes.append(cause)
            cause_counter += 1
        
        # å·¨å¤§ãƒ•ã‚¡ã‚¤ãƒ«æ¤œå‡º
        for file_path in files:
            full_path = self.project_root / file_path
            
            try:
                with open(full_path, "r", encoding="utf-8") as f:
                    lines = f.readlines()
                
                if len(lines) > 500:
                    cause = RootCause(
                        cause_id=f"ARCH-{cause_counter:03d}",
                        category=ProblemCategory.ARCHITECTURE,
                        severity=Severity.MEDIUM,
                        title="å·¨å¤§ãƒ•ã‚¡ã‚¤ãƒ«",
                        description=f"{file_path}: ãƒ•ã‚¡ã‚¤ãƒ«ãŒå¤§ãã™ãã¾ã™ ({len(lines)}è¡Œ)",
                        affected_files=[file_path],
                        evidence=[f"ãƒ•ã‚¡ã‚¤ãƒ«è¡Œæ•°: {len(lines)}è¡Œ"],
                        fix_suggestions=[
                            "ãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ†å‰²",
                            "è²¬å‹™ã®æ˜ç¢ºåŒ–",
                            "ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®å†ç·¨æˆ"
                        ],
                        estimated_fix_hours=6.0,
                        priority_score=40.0
                    )
                    causes.append(cause)
                    cause_counter += 1
                    
            except Exception as e:
                logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºåˆ†æã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
        
        return causes
    
    def _analyze_dependencies(self, files: List[str]) -> List[RootCause]:
        """ä¾å­˜é–¢ä¿‚åˆ†æ"""
        logger.info("ğŸ”— ä¾å­˜é–¢ä¿‚åˆ†æä¸­...")
        causes = []
        cause_counter = 300
        
        # å¤–éƒ¨ä¾å­˜é–¢ä¿‚åˆ†æ
        external_imports = defaultdict(list)
        
        for file_path in files:
            full_path = self.project_root / file_path
            
            try:
                with open(full_path, "r", encoding="utf-8") as f:
                    content = f.read()
                
                # å¤–éƒ¨ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ¤œå‡º
                imports = re.findall(r"^(?:from|import)\s+([a-zA-Z_][a-zA-Z0-9_]*)", content, re.MULTILINE)
                
                for imp in imports:
                    if not imp.startswith("kumihan_formatter") and imp not in ["os", "sys", "json", "pathlib", "typing", "datetime"]:
                        external_imports[imp].append(file_path)
                        
            except Exception as e:
                logger.warning(f"ä¾å­˜é–¢ä¿‚åˆ†æã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
        
        # éåº¦ãªå¤–éƒ¨ä¾å­˜
        for package, files_using in external_imports.items():
            if len(files_using) > 10:
                cause = RootCause(
                    cause_id=f"DEP-{cause_counter:03d}",
                    category=ProblemCategory.DEPENDENCY,
                    severity=Severity.MEDIUM,
                    title="éåº¦ãªå¤–éƒ¨ä¾å­˜",
                    description=f"ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ '{package}' ãŒå¤šæ•°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã§ä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™",
                    affected_files=files_using[:5],  # æœ€åˆã®5ãƒ•ã‚¡ã‚¤ãƒ«
                    evidence=[f"ä½¿ç”¨ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(files_using)}"],
                    fix_suggestions=[
                        "å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ä½œæˆ",
                        "ä¾å­˜é–¢ä¿‚ã®é›†ç´„",
                        "ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®çµ±ä¸€"
                    ],
                    estimated_fix_hours=5.0,
                    priority_score=35.0
                )
                causes.append(cause)
                cause_counter += 1
        
        return causes
    
    def _build_import_graph(self, files: List[str]) -> Dict[str, Set[str]]:
        """ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚°ãƒ©ãƒ•æ§‹ç¯‰"""
        graph = defaultdict(set)
        
        for file_path in files:
            full_path = self.project_root / file_path
            
            try:
                with open(full_path, "r", encoding="utf-8") as f:
                    content = f.read()
                
                # å†…éƒ¨ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ¤œå‡º
                imports = re.findall(r"from kumihan_formatter[.\w]* import", content)
                for imp in imports:
                    # ç°¡ç•¥åŒ–ã—ãŸã‚¤ãƒ³ãƒãƒ¼ãƒˆè§£æ
                    graph[file_path].add("internal_module")
                    
            except Exception as e:
                logger.warning(f"ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚°ãƒ©ãƒ•æ§‹ç¯‰ã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
        
        return dict(graph)
    
    def _detect_import_cycles(self, graph: Dict[str, Set[str]]) -> List[List[str]]:
        """å¾ªç’°ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ¤œå‡º"""
        # ç°¡æ˜“å®Ÿè£…ï¼šå®Ÿéš›ã®å¾ªç’°æ¤œå‡ºã¯è¤‡é›‘ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒå¿…è¦
        cycles = []
        
        # è‡ªå·±å‚ç…§ãƒã‚§ãƒƒã‚¯
        for node, dependencies in graph.items():
            if node in dependencies:
                cycles.append([node, node])
        
        return cycles
    
    def _calculate_quality_metrics(self, files: List[str]) -> Dict[str, float]:
        """å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—"""
        try:
            # ã‚«ãƒãƒ¬ãƒƒã‚¸å–å¾—
            coverage_cmd = [
                sys.executable, "-m", "pytest",
                "--cov=kumihan_formatter",
                "--cov-report=json:temp_coverage.json",
                "--collect-only", "-q"
            ]
            subprocess.run(coverage_cmd, capture_output=True, cwd=self.project_root)
            
            coverage_file = self.project_root / "temp_coverage.json"
            coverage = 0.0
            
            if coverage_file.exists():
                with open(coverage_file) as f:
                    coverage_data = json.load(f)
                    coverage = coverage_data["totals"]["percent_covered"]
                coverage_file.unlink()
            
            # ãã®ä»–ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            total_lines = sum(self._count_file_lines(self.project_root / f) for f in files)
            test_files = len([f for f in files if "test_" in f])
            
            return {
                "test_coverage": coverage,
                "total_files": len(files),
                "test_files": test_files,
                "total_lines": total_lines,
                "test_ratio": test_files / max(len(files), 1) * 100,
                "average_file_size": total_lines / max(len(files), 1)
            }
            
        except Exception as e:
            logger.error(f"å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—å¤±æ•—: {e}")
            return {
                "test_coverage": 11.0,
                "total_files": len(files),
                "test_files": 15,
                "total_lines": 31539,
                "test_ratio": 20.0,
                "average_file_size": 200.0
            }
    
    def _count_file_lines(self, file_path: Path) -> int:
        """ãƒ•ã‚¡ã‚¤ãƒ«è¡Œæ•°ã‚«ã‚¦ãƒ³ãƒˆ"""
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                return len([line for line in f if line.strip()])
        except:
            return 0
    
    def _generate_recommendations(self, causes: List[RootCause], metrics: Dict[str, float]) -> List[str]:
        """æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        recommendations = []
        
        # ã‚«ãƒãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹æ¨å¥¨
        if metrics["test_coverage"] < 50:
            recommendations.append("ğŸš¨ æœ€å„ªå…ˆ: ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ãŒæ¥µã‚ã¦ä½ã„ãŸã‚ã€Critical Tierã®ãƒ†ã‚¹ãƒˆå®Ÿè£…ã‹ã‚‰é–‹å§‹")
        elif metrics["test_coverage"] < 80:
            recommendations.append("âš ï¸  é‡è¦: ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸å‘ä¸ŠãŒå¿…è¦ã€æ®µéšçš„ã«Important Tierã¾ã§æ‹¡å¤§")
        
        # å•é¡Œã‚«ãƒ†ã‚´ãƒªåˆ¥æ¨å¥¨
        category_counts = Counter(cause.category for cause in causes)
        
        if category_counts[ProblemCategory.TEST_DESIGN] > 5:
            recommendations.append("ğŸ“ ãƒ†ã‚¹ãƒˆè¨­è¨ˆã®æŠœæœ¬çš„è¦‹ç›´ã—ãŒå¿…è¦ï¼šTDD-Firstã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®å°å…¥")
        
        if category_counts[ProblemCategory.IMPLEMENTATION] > 10:
            recommendations.append("âš¡ å®Ÿè£…å“è³ªã®å‘ä¸Šï¼šã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ã®å¼·åŒ–ã¨è‡ªå‹•åŒ–")
        
        if category_counts[ProblemCategory.ARCHITECTURE] > 3:
            recommendations.append("ğŸ—ï¸ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å†è¨­è¨ˆï¼šãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«æ§‹é€ ã®æ•´ç†ã¨ä¾å­˜é–¢ä¿‚ã®æœ€é©åŒ–")
        
        # é‡è¦åº¦åˆ¥æ¨å¥¨
        critical_count = len([c for c in causes if c.severity == Severity.CRITICAL])
        if critical_count > 0:
            recommendations.append(f"ğŸ”¥ ç·Šæ€¥å¯¾å¿œ: {critical_count}ä»¶ã®Criticalå•é¡Œã®å³åº§ä¿®æ­£ãŒå¿…è¦")
        
        return recommendations
    
    def _generate_action_plan(self, causes: List[RootCause]) -> List[Dict]:
        """ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³ç”Ÿæˆ"""
        action_plan = []
        
        # é‡è¦åº¦ã¨ã‚«ãƒ†ã‚´ãƒªã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        critical_causes = [c for c in causes if c.severity == Severity.CRITICAL]
        high_causes = [c for c in causes if c.severity == Severity.HIGH]
        
        # Phase 1: Criticalå•é¡Œå¯¾å¿œ
        if critical_causes:
            action_plan.append({
                "phase": "1-CRITICAL",
                "title": "Criticalå•é¡Œã®ç·Šæ€¥å¯¾å¿œ",
                "duration_weeks": 1,
                "causes": [c.cause_id for c in critical_causes[:5]],
                "estimated_hours": sum(c.estimated_fix_hours for c in critical_causes[:5]),
                "success_criteria": [
                    "å…¨Criticalå•é¡Œã®è§£æ±º",
                    "ãƒ“ãƒ«ãƒ‰ãƒ»ãƒ†ã‚¹ãƒˆã®æˆåŠŸ",
                    "åŸºæœ¬æ©Ÿèƒ½ã®å‹•ä½œç¢ºèª"
                ]
            })
        
        # Phase 2: é«˜å„ªå…ˆåº¦å•é¡Œå¯¾å¿œ
        if high_causes:
            action_plan.append({
                "phase": "2-HIGH",
                "title": "é«˜å„ªå…ˆåº¦å•é¡Œã®ç³»çµ±çš„å¯¾å¿œ",
                "duration_weeks": 3,
                "causes": [c.cause_id for c in high_causes[:10]],
                "estimated_hours": sum(c.estimated_fix_hours for c in high_causes[:10]),
                "success_criteria": [
                    "ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸50%é”æˆ",
                    "ä¸»è¦æ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆå®Ÿè£…",
                    "ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ”¹å–„"
                ]
            })
        
        # Phase 3: å…¨ä½“å“è³ªå‘ä¸Š
        medium_causes = [c for c in causes if c.severity == Severity.MEDIUM]
        if medium_causes:
            action_plan.append({
                "phase": "3-QUALITY",
                "title": "å…¨ä½“å“è³ªå‘ä¸Šãƒ»æŒç¶šå¯èƒ½æ€§ç¢ºä¿",
                "duration_weeks": 6,
                "causes": [c.cause_id for c in medium_causes[:15]],
                "estimated_hours": sum(c.estimated_fix_hours for c in medium_causes[:15]),
                "success_criteria": [
                    "ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸80%é”æˆ",
                    "ã‚³ãƒ¼ãƒ‰å“è³ªåŸºæº–ã‚¯ãƒªã‚¢",
                    "CI/CDå®Œå…¨è‡ªå‹•åŒ–"
                ]
            })
        
        return action_plan
    
    def _estimate_fix_time(self, severity: Severity) -> float:
        """ä¿®æ­£æ™‚é–“æ¨å®š"""
        time_map = {
            Severity.CRITICAL: 4.0,
            Severity.HIGH: 2.0,
            Severity.MEDIUM: 1.0,
            Severity.LOW: 0.5
        }
        return time_map.get(severity, 1.0)
    
    def _calculate_priority_score(self, severity: Severity, category: ProblemCategory) -> float:
        """å„ªå…ˆåº¦ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        severity_scores = {
            Severity.CRITICAL: 100,
            Severity.HIGH: 80,
            Severity.MEDIUM: 60,
            Severity.LOW: 40
        }
        
        category_multipliers = {
            ProblemCategory.TEST_DESIGN: 1.2,
            ProblemCategory.IMPLEMENTATION: 1.1,
            ProblemCategory.ARCHITECTURE: 1.0,
            ProblemCategory.DEPENDENCY: 0.9,
            ProblemCategory.PERFORMANCE: 0.8,
            ProblemCategory.SECURITY: 1.3,
            ProblemCategory.MAINTAINABILITY: 0.7
        }
        
        base_score = severity_scores.get(severity, 50)
        multiplier = category_multipliers.get(category, 1.0)
        
        return base_score * multiplier
    
    def _save_analysis_result(self, result: AnalysisResult):
        """åˆ†æçµæœä¿å­˜"""
        try:
            result_file = self.analysis_dir / f"{result.analysis_id}.json"
            
            result_data = {
                "analysis_id": result.analysis_id,
                "timestamp": result.timestamp.isoformat(),
                "analyzed_files": result.analyzed_files,
                "root_causes": [asdict(cause) for cause in result.root_causes],
                "quality_metrics": result.quality_metrics,
                "recommendations": result.recommendations,
                "action_plan": result.action_plan
            }
            
            with open(result_file, "w", encoding="utf-8") as f:
                json.dump(result_data, f, indent=2, ensure_ascii=False, default=str)
            
            # æœ€æ–°çµæœã¸ã®ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯
            latest_file = self.analysis_dir / "latest_analysis.json"
            if latest_file.exists():
                latest_file.unlink()
            latest_file.write_text(result_file.read_text(encoding="utf-8"), encoding="utf-8")
            
            logger.info(f"ğŸ“Š åˆ†æçµæœä¿å­˜å®Œäº†: {result_file}")
            
        except Exception as e:
            logger.error(f"åˆ†æçµæœä¿å­˜å¤±æ•—: {e}")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    parser = argparse.ArgumentParser(description="TDD Root Cause Analyzer")
    parser.add_argument("command", choices=["analyze", "report", "summary"],
                       help="å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰")
    parser.add_argument("--verbose", "-v", action="store_true",
                       help="è©³ç´°å‡ºåŠ›")
    
    args = parser.parse_args()
    
    project_root = Path(__file__).parent.parent
    analyzer = TDDRootCauseAnalyzer(project_root)
    
    if args.command == "analyze":
        result = analyzer.analyze_project()
        
        print(f"âœ… æ ¹æœ¬åŸå› åˆ†æå®Œäº†: {result.analysis_id}")
        print(f"ğŸ“ åˆ†æãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(result.analyzed_files)}")
        print(f"ğŸ” æ¤œå‡ºå•é¡Œæ•°: {len(result.root_causes)}")
        print(f"ğŸ“Š ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸: {result.quality_metrics['test_coverage']:.1f}%")
        
        # é‡è¦åº¦åˆ¥ã‚µãƒãƒªãƒ¼
        severity_counts = Counter(cause.severity for cause in result.root_causes)
        print("\nã€é‡è¦åº¦åˆ¥å•é¡Œæ•°ã€‘")
        for severity in [Severity.CRITICAL, Severity.HIGH, Severity.MEDIUM, Severity.LOW]:
            count = severity_counts[severity]
            if count > 0:
                print(f"  {severity.value.upper()}: {count}ä»¶")
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚µãƒãƒªãƒ¼
        category_counts = Counter(cause.category for cause in result.root_causes)
        print("\nã€ã‚«ãƒ†ã‚´ãƒªåˆ¥å•é¡Œæ•°ã€‘")
        for category, count in category_counts.most_common():
            print(f"  {category.value}: {count}ä»¶")
        
        # æ¨å¥¨äº‹é …
        print("\nã€æ¨å¥¨äº‹é …ã€‘")
        for rec in result.recommendations:
            print(f"  â€¢ {rec}")
        
        if args.verbose:
            print("\nã€è©³ç´°å•é¡Œãƒªã‚¹ãƒˆã€‘")
            for cause in result.root_causes[:10]:  # ä¸Šä½10ä»¶
                print(f"  [{cause.severity.value.upper()}] {cause.title}")
                print(f"    ğŸ“ {', '.join(cause.affected_files[:2])}")
                print(f"    ğŸ’¡ {cause.fix_suggestions[0] if cause.fix_suggestions else 'N/A'}")
                print()
    
    elif args.command == "report":
        latest_file = analyzer.analysis_dir / "latest_analysis.json"
        if not latest_file.exists():
            print("âŒ åˆ†æçµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚å…ˆã« 'analyze' ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
            sys.exit(1)
        
        with open(latest_file, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        print(f"ğŸ“Š åˆ†æãƒ¬ãƒãƒ¼ãƒˆ: {data['analysis_id']}")
        print(f"ğŸ•’ åˆ†ææ—¥æ™‚: {data['timestamp']}")
        print(f"ğŸ“ˆ å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹:")
        for key, value in data['quality_metrics'].items():
            print(f"  â€¢ {key}: {value}")
        
    elif args.command == "summary":
        print("ğŸ” TDDæ ¹æœ¬åŸå› åˆ†æã‚·ã‚¹ãƒ†ãƒ ")
        print("=====================================")
        print("ä½¿ç”¨æ–¹æ³•:")
        print("  python tdd_root_cause_analyzer.py analyze    # åˆ†æå®Ÿè¡Œ")
        print("  python tdd_root_cause_analyzer.py report     # ãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤º")
        print("  python tdd_root_cause_analyzer.py summary    # ã“ã®ãƒ˜ãƒ«ãƒ—")
        print("")
        print("æ©Ÿèƒ½:")
        print("  â€¢ ã‚³ãƒ¼ãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ")
        print("  â€¢ ãƒ†ã‚¹ãƒˆå“è³ªè©•ä¾¡")
        print("  â€¢ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å•é¡Œæ¤œå‡º")
        print("  â€¢ ä¾å­˜é–¢ä¿‚åˆ†æ")
        print("  â€¢ æ”¹å–„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³ç”Ÿæˆ")

if __name__ == "__main__":
    main()