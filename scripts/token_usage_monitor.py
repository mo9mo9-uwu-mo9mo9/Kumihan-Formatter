#!/usr/bin/env python3
"""Tokenä½¿ç”¨é‡ã®ç›£è¦–ãƒ»å¯è¦–åŒ–ã‚·ã‚¹ãƒ†ãƒ ."""

import ast
import json
import os
import re
import subprocess
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Union

from kumihan_formatter.core.utilities.logger import get_logger


class TokenUsageMonitor:
    """Tokenä½¿ç”¨é‡ã®ç›£è¦–ãƒ»è­¦å‘Š."""

    # Tokenä½¿ç”¨é‡é—¾å€¤
    WARNING_THRESHOLD = 1500
    ERROR_THRESHOLD = 2000

    def __init__(self) -> None:
        """åˆæœŸåŒ–."""
        self.logger = get_logger(__name__)
        self.usage_log = Path(".token_usage.json")
        self.enable_logging = os.environ.get("KUMIHAN_DEV_LOG", "").lower() == "true"
        self.json_logging = os.environ.get("KUMIHAN_DEV_LOG_JSON", "").lower() == "true"

    def estimate_token_usage(self, files: List[str]) -> int:
        """Tokenä½¿ç”¨é‡ã®æ¨å®š.

        Args:
            files: å¤‰æ›´ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆ

        Returns:
            æ¨å®šTokenæ•°
        """
        total_tokens = 0

        for file_path in files:
            if not Path(file_path).exists():
                continue

            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    content = f.read()

                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—åˆ¥ã®ç²¾å¯†Tokenæ¨å®š
                if file_path.endswith(".py"):
                    tokens = self._estimate_python_tokens_advanced(content)
                elif file_path.endswith((".md", ".txt")):
                    tokens = self._estimate_text_tokens(content)
                elif file_path.endswith((".yml", ".yaml")):
                    tokens = self._estimate_yaml_tokens(content)
                else:
                    tokens = len(content) // 4  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

                total_tokens += tokens

            except Exception as e:
                self.logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
                continue

        return total_tokens

    def _estimate_python_tokens_advanced(self, content: str) -> int:
        """Pythonãƒ•ã‚¡ã‚¤ãƒ«ã®Tokenæ•°ç²¾å¯†æ¨å®šï¼ˆASTè§£æä½¿ç”¨ï¼‰.

        Args:
            content: Pythonã‚³ãƒ¼ãƒ‰

        Returns:
            æ¨å®šTokenæ•°
        """
        try:
            # ASTè§£æã§ç²¾å¯†ãªãƒˆãƒ¼ã‚¯ãƒ³æ¨å®š
            tree = ast.parse(content)
            return self._count_ast_tokens(tree, content)
        except SyntaxError:
            # æ§‹æ–‡ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return self._estimate_python_tokens_fallback(content)

    def _count_ast_tokens(self, tree: ast.AST, content: str) -> int:
        """ASTãƒãƒ¼ãƒ‰ã‹ã‚‰Tokenæ•°ã‚’ç²¾å¯†ã«æ¨å®š.

        Args:
            tree: ASTãƒ«ãƒ¼ãƒˆãƒãƒ¼ãƒ‰
            content: å…ƒã®ã‚³ãƒ¼ãƒ‰

        Returns:
            Tokenæ•°
        """
        token_count = 0

        # ASTãƒãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—åˆ¥ã®Tokenæ•°æ¨å®š
        for node in ast.walk(tree):
            if isinstance(node, ast.Name):
                # å¤‰æ•°åã€é–¢æ•°åãªã©
                token_count += 1
            elif isinstance(node, ast.Constant):
                # å®šæ•°ï¼ˆæ–‡å­—åˆ—ã€æ•°å€¤ãªã©ï¼‰
                if isinstance(node.value, str):
                    # æ–‡å­—åˆ—ã¯å†…å®¹ã«ã‚ˆã£ã¦Tokenæ•°ãŒå¤‰ã‚ã‚‹
                    token_count += max(1, len(str(node.value)) // 4)
                else:
                    token_count += 1
            elif isinstance(node, ast.keyword):
                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¼•æ•°
                token_count += 2  # ã‚­ãƒ¼ + å€¤
            elif isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                # é–¢æ•°å®šç¾©: def/async def + åå‰ + ã‚³ãƒ­ãƒ³
                token_count += 3
            elif isinstance(node, ast.ClassDef):
                # ã‚¯ãƒ©ã‚¹å®šç¾©: class + åå‰ + ã‚³ãƒ­ãƒ³
                token_count += 3
            elif isinstance(node, ast.Import):
                # importæ–‡
                token_count += 1 + len(node.names)
            elif isinstance(node, ast.ImportFrom):
                # from importæ–‡
                token_count += 2 + len(node.names)  # from + module + import + names
            elif isinstance(node, (ast.If, ast.While, ast.For)):
                # åˆ¶å¾¡æ–‡: if/while/for + æ¡ä»¶ + ã‚³ãƒ­ãƒ³
                token_count += 2
            elif isinstance(node, ast.BinOp):
                # äºŒé …æ¼”ç®—å­
                token_count += 1  # æ¼”ç®—å­è‡ªä½“
            elif isinstance(node, ast.Call):
                # é–¢æ•°å‘¼ã³å‡ºã—: é–¢æ•°å + æ‹¬å¼§
                token_count += 2

        # ã‚³ãƒ¡ãƒ³ãƒˆã‚„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ–‡å­—åˆ—ã®å‡¦ç†
        comment_tokens = self._count_comments_and_docstrings(content)

        return token_count + comment_tokens

    def _count_comments_and_docstrings(self, content: str) -> int:
        """Pythonã‚³ãƒ¡ãƒ³ãƒˆã¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ–‡å­—åˆ—ã®Tokenæ•°.

        Args:
            content: Pythonã‚³ãƒ¼ãƒ‰

        Returns:
            ã‚³ãƒ¡ãƒ³ãƒˆç³»ã®Tokenæ•°
        """
        lines = content.split("\\n")
        comment_tokens = 0

        in_multiline_string = False
        multiline_delimiter = None

        for line in lines:
            stripped = line.strip()

            # å˜ä¸€è¡Œã‚³ãƒ¡ãƒ³ãƒˆ
            if "#" in line and not in_multiline_string:
                comment_part = line.split("#", 1)[1]
                comment_tokens += max(1, len(comment_part.strip()) // 5)

            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ–‡å­—åˆ—ã®æ¤œå‡º
            if '"""' in stripped or "'''" in stripped:
                if not in_multiline_string:
                    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ–‡å­—åˆ—é–‹å§‹
                    multiline_delimiter = '"""' if '"""' in stripped else "'''"
                    in_multiline_string = True
                    # åŒä¸€è¡Œã§çµ‚äº†ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                    if stripped.count(multiline_delimiter) >= 2:
                        in_multiline_string = False
                        comment_tokens += max(1, len(stripped) // 4)
                elif multiline_delimiter in stripped:
                    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ–‡å­—åˆ—çµ‚äº†
                    in_multiline_string = False
                    comment_tokens += max(1, len(stripped) // 4)
            elif in_multiline_string:
                # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ–‡å­—åˆ—å†…
                comment_tokens += max(1, len(stripped) // 4)

        return comment_tokens

    def _estimate_python_tokens_fallback(self, content: str) -> int:
        """ASTè§£æå¤±æ•—æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¨å®š.

        Args:
            content: Pythonã‚³ãƒ¼ãƒ‰

        Returns:
            æ¨å®šTokenæ•°
        """
        # æ—¢å­˜ã®ç°¡æ˜“æ¨å®šãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨
        lines = content.split("\\n")
        token_count = 0

        for line in lines:
            line = line.strip()
            if not line or line.startswith("#"):
                continue

            # æ–‡å­—åˆ—ãƒªãƒ†ãƒ©ãƒ«ã®æ¤œå‡º
            string_content = self._extract_string_literals(line)
            code_content = line

            # æ–‡å­—åˆ—éƒ¨åˆ†ã¯Tokenæ•°ãŒå¤šã„
            for string_literal in string_content:
                token_count += len(string_literal) // 3
                code_content = code_content.replace(string_literal, "", 1)

            # ã‚³ãƒ¼ãƒ‰éƒ¨åˆ†ã®ãƒˆãƒ¼ã‚¯ãƒ³åˆ†å‰²
            code_tokens = len(
                re.findall(r"\\b\\w+\\b|[+\\-*/=<>!&|(){}\\[\\],.;:]", code_content)
            )
            token_count += code_tokens

        return max(token_count, len(content) // 5)

    def _extract_string_literals(self, line: str) -> List[str]:
        """è¡Œã‹ã‚‰æ–‡å­—åˆ—ãƒªãƒ†ãƒ©ãƒ«ã‚’æŠ½å‡º.

        Args:
            line: Pythonã‚³ãƒ¼ãƒ‰ã®è¡Œ

        Returns:
            æ–‡å­—åˆ—ãƒªãƒ†ãƒ©ãƒ«ã®ãƒªã‚¹ãƒˆ
        """
        # ç°¡å˜ãªæ–‡å­—åˆ—æ¤œå‡ºï¼ˆã‚¨ã‚¹ã‚±ãƒ¼ãƒ—å‡¦ç†ã¯ç°¡ç•¥åŒ–ï¼‰
        strings = []
        # ãƒ€ãƒ–ãƒ«ã‚¯ã‚©ãƒ¼ãƒˆ
        strings.extend(re.findall(r'"([^"\\\\]|\\\\.)*"', line))
        # ã‚·ãƒ³ã‚°ãƒ«ã‚¯ã‚©ãƒ¼ãƒˆ
        strings.extend(re.findall(r"'([^'\\\\]|\\\\.)*'", line))
        # ãƒˆãƒªãƒ—ãƒ«ã‚¯ã‚©ãƒ¼ãƒˆã¯ç°¡ç•¥åŒ–
        return strings

    def _estimate_text_tokens(self, content: str) -> int:
        """Text/Markdownãƒ•ã‚¡ã‚¤ãƒ«ã®Tokenæ•°æ¨å®š.

        Args:
            content: ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹

        Returns:
            æ¨å®šTokenæ•°
        """
        # è‹±èªã¨æ—¥æœ¬èªã®æ··åœ¨ã‚’è€ƒæ…®
        words = re.findall(r"\\b\\w+\\b", content)
        japanese_chars = len(
            re.findall(r"[\\u3040-\\u309f\\u30a0-\\u30ff\\u4e00-\\u9faf]", content)
        )

        # è‹±èªå˜èªã¯1å˜èª=1Tokenã€æ—¥æœ¬èªã¯2æ–‡å­—=1Tokenç¨‹åº¦
        return len(words) + japanese_chars // 2

    def _estimate_yaml_tokens(self, content: str) -> int:
        """YAMLãƒ•ã‚¡ã‚¤ãƒ«ã®Tokenæ•°æ¨å®š.

        Args:
            content: YAMLå†…å®¹

        Returns:
            æ¨å®šTokenæ•°
        """
        # YAMLã®ã‚­ãƒ¼ã€å€¤ã€æ§‹é€ ã‚’è€ƒæ…®
        lines = [line.strip() for line in content.split("\\n") if line.strip()]
        token_count = 0

        for line in lines:
            if line.startswith("#"):
                continue  # ã‚³ãƒ¡ãƒ³ãƒˆã¯ç„¡è¦–

            # ã‚­ãƒ¼: å€¤ ã®å½¢å¼
            if ":" in line:
                key_value = line.split(":", 1)
                token_count += len(key_value[0].strip()) // 4  # ã‚­ãƒ¼
                if len(key_value) > 1 and key_value[1].strip():
                    token_count += len(key_value[1].strip()) // 4  # å€¤
            else:
                token_count += len(line) // 4

        return max(token_count, len(content) // 6)

    def analyze_pr_diff(self) -> Dict[str, int]:
        """PRå·®åˆ†ã®Tokenä½¿ç”¨é‡åˆ†æ.

        Returns:
            åˆ†æçµæœ
        """
        try:
            # PRå·®åˆ†ã®å–å¾—
            result = subprocess.run(
                ["git", "diff", "--name-only", "origin/main...HEAD"],
                capture_output=True,
                text=True,
                check=True,
            )
            changed_files = result.stdout.strip().split("\n")

            # Pythonãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å¯¾è±¡
            py_files = [f for f in changed_files if f.endswith(".py")]

            token_usage = self.estimate_token_usage(py_files)

            return {
                "total_tokens": token_usage,
                "file_count": len(py_files),
                "status": self._get_status(token_usage),
                "timestamp": datetime.now().isoformat(),
            }

        except subprocess.CalledProcessError as e:
            self.logger.error(f"Gitå·®åˆ†å–å¾—å¤±æ•—: {e}")
            return {"error": str(e)}

    def _get_status(self, tokens: int) -> str:
        """Tokenæ•°ã«åŸºã¥ãã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¤å®š.

        Args:
            tokens: Tokenæ•°

        Returns:
            ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
        """
        if tokens >= self.ERROR_THRESHOLD:
            return "error"
        elif tokens >= self.WARNING_THRESHOLD:
            return "warning"
        return "ok"

    def report_usage(self, analysis: Dict[str, int]) -> None:
        """ä½¿ç”¨é‡ãƒ¬ãƒãƒ¼ãƒˆ.

        Args:
            analysis: åˆ†æçµæœ
        """
        if not self.enable_logging:
            return

        if self.json_logging:
            # JSONå½¢å¼ã§å‡ºåŠ›
            print(json.dumps(analysis, indent=2, ensure_ascii=False))
        else:
            # ãƒ’ãƒ¥ãƒ¼ãƒãƒ³ãƒªãƒ¼ãƒ€ãƒ–ãƒ«å½¢å¼
            if "error" in analysis:
                print(f"\nâŒ Tokenä½¿ç”¨é‡åˆ†æã‚¨ãƒ©ãƒ¼: {analysis['error']}")
                return

            tokens = analysis["total_tokens"]
            status = analysis["status"]

            print("\n" + "=" * 60)
            print("ğŸ“Š Tokenä½¿ç”¨é‡ãƒ¬ãƒãƒ¼ãƒˆ")
            print("=" * 60)
            print(f"\u5408è¨ˆTokenæ•°: {tokens:,}")
            print(f"\u5909æ›´ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {analysis['file_count']}")

            if status == "error":
                print(
                    f"\nâŒ ã‚¨ãƒ©ãƒ¼: Tokenä½¿ç”¨é‡ãŒä¸Šé™({self.ERROR_THRESHOLD:,})ã‚’è¶…ãˆã¾ã—ãŸ"
                )
                print("ğŸ’¡ ãƒ•ã‚¡ã‚¤ãƒ«åˆ†å‰²ã‚’æ¤œè¨ã—ã¦ãã ã•ã„")
            elif status == "warning":
                print(
                    f"\nâš ï¸  è­¦å‘Š: Tokenä½¿ç”¨é‡ãŒè­¦å‘Šé—¾å€¤({self.WARNING_THRESHOLD:,})ã«è¿‘ã¥ã„ã¦ã„ã¾ã™"
                )
            else:
                print("\nâœ… Tokenä½¿ç”¨é‡ã¯é©åˆ‡ãªç¯„å›²å†…ã§ã™")

            print("=" * 60)

    def save_history(
        self, analysis: Dict[str, Union[int, str, Dict[str, int]]]
    ) -> None:
        """å±¥æ­´ä¿å­˜.

        Args:
            analysis: åˆ†æçµæœ
        """
        history = []
        if self.usage_log.exists():
            with open(self.usage_log, "r") as f:
                history = json.load(f)

        history.append(analysis)

        # æœ€æ–°30ä»¶ä¿æŒ
        if len(history) > 30:
            history = history[-30:]

        with open(self.usage_log, "w") as f:
            json.dump(history, f, indent=2, ensure_ascii=False)


def main() -> None:
    """ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ."""
    monitor = TokenUsageMonitor()
    analysis = monitor.analyze_pr_diff()
    monitor.report_usage(analysis)

    if "total_tokens" in analysis:
        monitor.save_history(analysis)

        # CI/CDç”¨ã®çµ‚äº†ã‚³ãƒ¼ãƒ‰
        if analysis["status"] == "error":
            exit(1)


if __name__ == "__main__":
    main()
