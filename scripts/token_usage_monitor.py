#!/usr/bin/env python3
"""Tokenä½¿ç”¨é‡ã®ç›£è¦–ãƒ»å¯è¦–åŒ–ã‚·ã‚¹ãƒ†ãƒ ."""

import json
import os
import subprocess
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional

from kumihan_formatter.core.utilities.logger import get_logger


class TokenUsageMonitor:
    """Tokenä½¿ç”¨é‡ã®ç›£è¦–ãƒ»è­¦å‘Š."""

    # Tokenä½¿ç”¨é‡é—¾å€¤
    WARNING_THRESHOLD = 1500
    ERROR_THRESHOLD = 2000

    def __init__(self) -> None:
        """åˆæœŸåŒ–."""
        self.logger = get_logger(__name__)
        self.usage_log = Path(".token_usage.json")
        self.enable_logging = os.environ.get("KUMIHAN_DEV_LOG", "").lower() == "true"
        self.json_logging = os.environ.get("KUMIHAN_DEV_LOG_JSON", "").lower() == "true"

    def estimate_token_usage(self, files: List[str]) -> int:
        """Tokenä½¿ç”¨é‡ã®æ¨å®š.

        Args:
            files: å¤‰æ›´ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆ

        Returns:
            æ¨å®šTokenæ•°
        """
        total_tokens = 0

        for file_path in files:
            if not Path(file_path).exists():
                continue

            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()
                # ç°¡æ˜“çš„ãªTokenæ¨å®šï¼ˆç´„4æ–‡å­— = 1 tokenï¼‰
                tokens = len(content) // 4
                total_tokens += tokens

        return total_tokens

    def analyze_pr_diff(self) -> Dict[str, int]:
        """PRå·®åˆ†ã®Tokenä½¿ç”¨é‡åˆ†æ.

        Returns:
            åˆ†æçµæœ
        """
        try:
            # PRå·®åˆ†ã®å–å¾—
            result = subprocess.run(
                ["git", "diff", "--name-only", "origin/main...HEAD"],
                capture_output=True,
                text=True,
                check=True,
            )
            changed_files = result.stdout.strip().split("\n")

            # Pythonãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å¯¾è±¡
            py_files = [f for f in changed_files if f.endswith(".py")]

            token_usage = self.estimate_token_usage(py_files)

            return {
                "total_tokens": token_usage,
                "file_count": len(py_files),
                "status": self._get_status(token_usage),
                "timestamp": datetime.now().isoformat(),
            }

        except subprocess.CalledProcessError as e:
            self.logger.error(f"Gitå·®åˆ†å–å¾—å¤±æ•—: {e}")
            return {"error": str(e)}

    def _get_status(self, tokens: int) -> str:
        """Tokenæ•°ã«åŸºã¥ãã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¤å®š.

        Args:
            tokens: Tokenæ•°

        Returns:
            ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
        """
        if tokens >= self.ERROR_THRESHOLD:
            return "error"
        elif tokens >= self.WARNING_THRESHOLD:
            return "warning"
        return "ok"

    def report_usage(self, analysis: Dict[str, int]) -> None:
        """ä½¿ç”¨é‡ãƒ¬ãƒãƒ¼ãƒˆ.

        Args:
            analysis: åˆ†æçµæœ
        """
        if not self.enable_logging:
            return

        if self.json_logging:
            # JSONå½¢å¼ã§å‡ºåŠ›
            print(json.dumps(analysis, indent=2, ensure_ascii=False))
        else:
            # ãƒ’ãƒ¥ãƒ¼ãƒãƒ³ãƒªãƒ¼ãƒ€ãƒ–ãƒ«å½¢å¼
            if "error" in analysis:
                print(f"\nâŒ Tokenä½¿ç”¨é‡åˆ†æã‚¨ãƒ©ãƒ¼: {analysis['error']}")
                return

            tokens = analysis["total_tokens"]
            status = analysis["status"]

            print("\n" + "=" * 60)
            print("ğŸ“Š Tokenä½¿ç”¨é‡ãƒ¬ãƒãƒ¼ãƒˆ")
            print("=" * 60)
            print(f"\u5408è¨ˆTokenæ•°: {tokens:,}")
            print(f"\u5909æ›´ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {analysis['file_count']}")

            if status == "error":
                print(
                    f"\nâŒ ã‚¨ãƒ©ãƒ¼: Tokenä½¿ç”¨é‡ãŒä¸Šé™({self.ERROR_THRESHOLD:,})ã‚’è¶…ãˆã¾ã—ãŸ"
                )
                print("ğŸ’¡ ãƒ•ã‚¡ã‚¤ãƒ«åˆ†å‰²ã‚’æ¤œè¨ã—ã¦ãã ã•ã„")
            elif status == "warning":
                print(
                    f"\nâš ï¸  è­¦å‘Š: Tokenä½¿ç”¨é‡ãŒè­¦å‘Šé—¾å€¤({self.WARNING_THRESHOLD:,})ã«è¿‘ã¥ã„ã¦ã„ã¾ã™"
                )
            else:
                print("\nâœ… Tokenä½¿ç”¨é‡ã¯é©åˆ‡ãªç¯„å›²å†…ã§ã™")

            print("=" * 60)

    def save_history(
        self, analysis: Dict[str, Union[int, str, Dict[str, int]]]
    ) -> None:
        """å±¥æ­´ä¿å­˜.

        Args:
            analysis: åˆ†æçµæœ
        """
        history = []
        if self.usage_log.exists():
            with open(self.usage_log, "r") as f:
                history = json.load(f)

        history.append(analysis)

        # æœ€æ–°30ä»¶ä¿æŒ
        if len(history) > 30:
            history = history[-30:]

        with open(self.usage_log, "w") as f:
            json.dump(history, f, indent=2, ensure_ascii=False)


def main() -> None:
    """ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ."""
    monitor = TokenUsageMonitor()
    analysis = monitor.analyze_pr_diff()
    monitor.report_usage(analysis)

    if "total_tokens" in analysis:
        monitor.save_history(analysis)

        # CI/CDç”¨ã®çµ‚äº†ã‚³ãƒ¼ãƒ‰
        if analysis["status"] == "error":
            exit(1)


if __name__ == "__main__":
    main()
