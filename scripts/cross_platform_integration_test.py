#!/usr/bin/env python3
"""
Cross-Platform Integration Test - Issue #640 Phase 3
ã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ çµ±åˆãƒ†ã‚¹ãƒˆã‚·ã‚¹ãƒ†ãƒ 

ç›®çš„: Windows/macOS/Linuxç’°å¢ƒã§ã®çµ±åˆãƒ†ã‚¹ãƒˆ
- OSå›ºæœ‰ã®å•é¡Œæ¤œå‡º
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å·®ç•°ç¢ºèª
- ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ äº’æ›æ€§ãƒ†ã‚¹ãƒˆ
- æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å•é¡Œæ¤œå‡º
"""

import os
import sys
import platform
import subprocess
import tempfile
import shutil
import re
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple, Any
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
import json
import threading
import time
import psutil

from scripts.tdd_system_base import TDDSystemBase, create_tdd_config, TDDSystemError
from scripts.secure_subprocess import secure_run
from kumihan_formatter.core.utilities.logger import get_logger

logger = get_logger(__name__)


class PlatformCompatibilityRisk(Enum):
    """ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ äº’æ›æ€§ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«"""

    CRITICAL = "critical"
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"
    SAFE = "safe"


@dataclass
class PlatformIssue:
    """ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ å›ºæœ‰å•é¡Œ"""

    issue_type: str
    platform: str
    severity: PlatformCompatibilityRisk
    description: str
    code_snippet: Optional[str]
    file_path: Optional[str]
    line_number: Optional[int]
    recommendation: str
    test_case: str
    error_details: Dict


@dataclass
class PlatformTestResult:
    """ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ãƒ†ã‚¹ãƒˆçµæœ"""

    platform_info: Dict[str, str]
    total_tests_run: int
    passed_tests: int
    failed_tests: int
    platform_issues: List[PlatformIssue]
    performance_metrics: Dict[str, float]
    filesystem_compatibility: Dict[str, bool]
    encoding_compatibility: Dict[str, bool]
    test_duration: float
    timestamp: datetime
    overall_risk: PlatformCompatibilityRisk
    recommendations: List[str]


class CrossPlatformIntegrationTester(TDDSystemBase):
    """ã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ çµ±åˆãƒ†ã‚¹ãƒˆã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, project_root: Path):
        config = create_tdd_config(project_root)
        super().__init__(config)

        # ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ æƒ…å ±å–å¾—
        self.platform_info = {
            "system": platform.system(),
            "machine": platform.machine(),
            "platform": platform.platform(),
            "python_version": platform.python_version(),
            "python_implementation": platform.python_implementation(),
            "processor": (
                platform.processor() if hasattr(platform, "processor") else "unknown"
            ),
        }

        # ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ å›ºæœ‰ã®å•é¡Œãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å¤–éƒ¨è¨­å®šã‹ã‚‰èª­ã¿è¾¼ã¿
        self.platform_specific_patterns = self._load_platform_patterns()

        # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹å®šç¾©
        self.test_cases = [
            self._test_file_operations,
            self._test_path_handling,
            self._test_process_execution,
            self._test_encoding_handling,
            self._test_performance_consistency,
            self._test_dependency_compatibility,
            self._test_concurrent_operations,
            self._test_large_file_handling,
        ]

        self.platform_issues = []
        self.test_results = {}
        self.performance_metrics = {}

        # å¤–éƒ¨è¨­å®šã‹ã‚‰ã®ãƒ‘ã‚¿ãƒ¼ãƒ³æƒ…å ±
        self.pattern_config = None

    def _load_platform_patterns(self) -> Dict[str, str]:
        """ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ å›ºæœ‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å¤–éƒ¨è¨­å®šã‹ã‚‰èª­ã¿è¾¼ã¿ï¼ˆçµ±ä¸€ãƒ­ãƒƒã‚¯æ©Ÿæ§‹ä½¿ç”¨ï¼‰"""
        config_path = self.project_root / "config" / "cross_platform_patterns.json"

        # çµ±ä¸€ãƒ­ãƒƒã‚¯æ©Ÿæ§‹ã‚’ä½¿ç”¨
        try:
            from kumihan_formatter.core.utilities.config_lock_manager import (
                safe_read_config,
            )

            USE_UNIFIED_LOCKING = True
        except ImportError:
            USE_UNIFIED_LOCKING = False

        try:
            if USE_UNIFIED_LOCKING:
                # çµ±ä¸€ãƒ­ãƒƒã‚¯æ©Ÿæ§‹ã§å®‰å…¨ã«èª­ã¿è¾¼ã¿
                config_data = safe_read_config(config_path, default={}, timeout=5.0)
                if config_data:
                    self.pattern_config = config_data

                    # ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨èª¬æ˜ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ
                    patterns = {}
                    for pattern_id, pattern_info in self.pattern_config.get(
                        "platform_specific_patterns", {}
                    ).items():
                        patterns[pattern_info["pattern"]] = pattern_info["description"]

                    logger.info(
                        f"çµ±ä¸€ãƒ­ãƒƒã‚¯æ©Ÿæ§‹ã§å¤–éƒ¨è¨­å®šã‹ã‚‰{len(patterns)}å€‹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’èª­ã¿è¾¼ã¿: {config_path}"
                    )
                    return patterns
                else:
                    logger.warning(
                        f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºã¾ãŸã¯ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã›ã‚“: {config_path}"
                    )
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
                if config_path.exists():
                    with open(config_path, "r", encoding="utf-8") as f:
                        self.pattern_config = json.load(f)

                    # ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨èª¬æ˜ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ
                    patterns = {}
                    for pattern_id, pattern_info in self.pattern_config[
                        "platform_specific_patterns"
                    ].items():
                        patterns[pattern_info["pattern"]] = pattern_info["description"]

                    logger.info(
                        f"å¤–éƒ¨è¨­å®šã‹ã‚‰{len(patterns)}å€‹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’èª­ã¿è¾¼ã¿: {config_path}"
                    )
                    return patterns
                else:
                    logger.warning(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {config_path}")

            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®åŸºæœ¬ãƒ‘ã‚¿ãƒ¼ãƒ³
            return {
                r"[\\/]": "Mixed path separators - use os.path.join() or pathlib.Path",
                r"os\.system\s*\(": "os.system() usage - platform dependent",
            }

        except Exception as e:
            logger.error(f"ãƒ‘ã‚¿ãƒ¼ãƒ³è¨­å®šèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®åŸºæœ¬ãƒ‘ã‚¿ãƒ¼ãƒ³
            return {
                r"[\\/]": "Mixed path separators - use os.path.join() or pathlib.Path",
                r"os\.system\s*\(": "os.system() usage - platform dependent",
            }

    def initialize(self) -> bool:
        """ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        logger.info("ğŸ” ã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ çµ±åˆãƒ†ã‚¹ãƒˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ä¸­...")

        # ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ æƒ…å ±è¡¨ç¤º
        logger.info(
            f"å®Ÿè¡Œç’°å¢ƒ: {self.platform_info['system']} {self.platform_info['platform']}"
        )
        logger.info(
            f"Python: {self.platform_info['python_version']} ({self.platform_info['python_implementation']})"
        )

        # äº‹å‰æ¡ä»¶ç¢ºèª
        issues = self.validate_preconditions()
        if issues:
            logger.error("åˆæœŸåŒ–å¤±æ•—:")
            for issue in issues:
                logger.error(f"  - {issue}")
            return False

        logger.info("âœ… ã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ çµ±åˆãƒ†ã‚¹ãƒˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        return True

    def execute_main_operation(self) -> PlatformTestResult:
        """ã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        logger.info("ğŸš€ ã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ çµ±åˆãƒ†ã‚¹ãƒˆé–‹å§‹...")

        start_time = datetime.now()

        try:
            # ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰é™çš„è§£æ
            self._analyze_source_code()

            # çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
            self._run_integration_tests()

            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
            self._run_performance_tests()

            # çµæœã‚’åˆ†æ
            result = self._analyze_results(start_time)

            # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            self._generate_compatibility_report(result)

            logger.info(
                f"âœ… ã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ çµ±åˆãƒ†ã‚¹ãƒˆå®Œäº†: {len(self.platform_issues)}ä»¶ã®å•é¡Œç™ºè¦‹"
            )
            return result

        except Exception as e:
            # ã‚»ã‚­ãƒ¥ã‚¢ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°é©ç”¨
            try:
                from kumihan_formatter.core.utilities.secure_error_handler import (
                    safe_handle_exception,
                    ExposureRisk,
                    ErrorSeverity,
                )

                sanitized_error = safe_handle_exception(
                    e,
                    context={
                        "operation": "cross_platform_integration_test",
                        "platform": self.platform_info["system"],
                    },
                    user_exposure=ExposureRisk.INTERNAL,
                    severity=ErrorSeverity.HIGH,
                )
                logger.error(
                    f"ã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼ [{sanitized_error.trace_id}]: {sanitized_error.user_message}"
                )
                raise TDDSystemError(
                    f"ãƒ†ã‚¹ãƒˆå®Ÿè¡Œå¤±æ•— [{sanitized_error.error_code}]: {sanitized_error.user_message}"
                )
            except ImportError:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
                logger.error(f"ã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
                raise TDDSystemError(f"ãƒ†ã‚¹ãƒˆå®Ÿè¡Œå¤±æ•—: {e}")

    def _analyze_source_code(self):
        """ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰é™çš„è§£æ"""
        logger.info("ğŸ“ ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰é™çš„è§£æé–‹å§‹...")

        for py_file in self.project_root.glob("**/*.py"):
            if self._should_scan_file(py_file):
                self._analyze_python_file(py_file)

    def _should_scan_file(self, file_path: Path) -> bool:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒ£ãƒ³ã™ã¹ãã‹ãƒã‚§ãƒƒã‚¯"""
        # pathlib.Pathã§ã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ å¯¾å¿œ
        relative_path = file_path.relative_to(self.project_root)
        path_parts = relative_path.parts

        # å¤–éƒ¨è¨­å®šã‹ã‚‰é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å–å¾—
        if self.pattern_config and "test_configuration" in self.pattern_config:
            exclude_patterns = self.pattern_config["test_configuration"].get(
                "exclude_directories", []
            )
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            exclude_patterns = [
                "tests",
                "venv",
                ".venv",
                "__pycache__",
                ".git",
                "build",
                "dist",
                ".tox",
                "node_modules",
            ]

        for pattern in exclude_patterns:
            if any(pattern in part for part in path_parts):
                return False

        return True

    def _analyze_python_file(self, file_path: Path):
        """Pythonãƒ•ã‚¡ã‚¤ãƒ«è§£æ"""
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()

            # ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ å›ºæœ‰å•é¡Œæ¤œå‡º
            self._detect_platform_issues(file_path, content)

        except Exception as e:
            logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«è§£æã‚¨ãƒ©ãƒ¼ {file_path}: {e}")

    def _detect_platform_issues(self, file_path: Path, content: str):
        """ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ å›ºæœ‰å•é¡Œæ¤œå‡º"""
        lines = content.split("\n")

        for pattern, description in self.platform_specific_patterns.items():
            for match in re.finditer(pattern, content, re.IGNORECASE):
                line_num = content[: match.start()].count("\n") + 1

                issue = PlatformIssue(
                    issue_type="static_analysis",
                    platform=self.platform_info["system"],
                    severity=self._assess_pattern_severity(pattern, match.group()),
                    description=description,
                    code_snippet=(
                        lines[line_num - 1].strip() if line_num <= len(lines) else ""
                    ),
                    file_path=str(file_path.relative_to(self.project_root)),
                    line_number=line_num,
                    recommendation=self._get_pattern_recommendation(pattern),
                    test_case="static_analysis",
                    error_details={"pattern": pattern, "match": match.group()},
                )
                self.platform_issues.append(issue)

    def _assess_pattern_severity(
        self, pattern: str, match: str
    ) -> PlatformCompatibilityRisk:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³é‡è¦åº¦è©•ä¾¡"""
        if self.pattern_config:
            # å¤–éƒ¨è¨­å®šã‹ã‚‰é‡è¦åº¦ã‚’å–å¾—
            for pattern_id, pattern_info in self.pattern_config[
                "platform_specific_patterns"
            ].items():
                if pattern_info["pattern"] == pattern:
                    severity_str = pattern_info.get("severity", "medium")
                    severity_mapping = {
                        "critical": PlatformCompatibilityRisk.CRITICAL,
                        "high": PlatformCompatibilityRisk.HIGH,
                        "medium": PlatformCompatibilityRisk.MEDIUM,
                        "low": PlatformCompatibilityRisk.LOW,
                    }
                    return severity_mapping.get(
                        severity_str, PlatformCompatibilityRisk.MEDIUM
                    )

        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ™ãƒ¼ã‚¹ã®åˆ¤å®š
        critical_patterns = [r"os\.system\s*\(", r'["\'][C-Z]:[\\\/]']
        high_patterns = [
            r"subprocess\.[^(]*\([^)]*shell\s*=\s*True",
            r"fcntl\.|msvcrt\.",
        ]

        for crit_pattern in critical_patterns:
            if re.search(crit_pattern, pattern):
                return PlatformCompatibilityRisk.CRITICAL

        for high_pattern in high_patterns:
            if re.search(high_pattern, pattern):
                return PlatformCompatibilityRisk.HIGH

        return PlatformCompatibilityRisk.MEDIUM

    def _get_pattern_recommendation(self, pattern: str) -> str:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥æ¨å¥¨äº‹é …"""
        if self.pattern_config:
            # å¤–éƒ¨è¨­å®šã‹ã‚‰æ¨å¥¨äº‹é …ã‚’å–å¾—
            for pattern_id, pattern_info in self.pattern_config[
                "platform_specific_patterns"
            ].items():
                if pattern_info["pattern"] == pattern:
                    return pattern_info.get(
                        "recommendation",
                        "ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ å›ºæœ‰ã®å®Ÿè£…ã‚’é¿ã‘ã€æ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®æŠ½è±¡åŒ–æ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„",
                    )

        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ™ãƒ¼ã‚¹ã®æ¨å¥¨äº‹é …
        recommendations = {
            r"[\\/]": "pathlib.Pathã¾ãŸã¯os.path.join()ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„",
            r'["\'][C-Z]:[\\\/]': "ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ‘ã‚¹ã‚’ç’°å¢ƒå¤‰æ•°ã‚„è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—ã—ã¦ãã ã•ã„",
            r"os\.system\s*\(": "subprocessãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ä½¿ç”¨ã‚’æ¤œè¨ã—ã¦ãã ã•ã„",
            r"subprocess\.[^(]*\([^)]*shell\s*=\s*True": "shell=Falseã‚’ä½¿ç”¨ã—ã€å¼•æ•°ã‚’é…åˆ—ã§æ¸¡ã—ã¦ãã ã•ã„",
            r"HOME|USERPROFILE": 'pathlib.Path.home()ã¾ãŸã¯os.path.expanduser("~")ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„',
        }

        for pat, rec in recommendations.items():
            if re.search(pat, pattern):
                return rec

        return "ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ å›ºæœ‰ã®å®Ÿè£…ã‚’é¿ã‘ã€æ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®æŠ½è±¡åŒ–æ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„"

    def _run_integration_tests(self):
        """çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        logger.info("ğŸ§ª çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")

        total_tests = len(self.test_cases)
        passed = 0
        failed = 0

        for i, test_case in enumerate(self.test_cases, 1):
            test_name = test_case.__name__
            logger.info(f"[{i}/{total_tests}] {test_name} å®Ÿè¡Œä¸­...")

            try:
                start_time = time.time()
                result = test_case()
                duration = time.time() - start_time

                self.test_results[test_name] = {
                    "passed": result["passed"],
                    "duration": duration,
                    "details": result.get("details", {}),
                    "metrics": result.get("metrics", {}),
                }

                if result["passed"]:
                    passed += 1
                    logger.debug(f"âœ… {test_name} æˆåŠŸ ({duration:.2f}s)")
                else:
                    failed += 1
                    logger.warning(
                        f"âŒ {test_name} å¤±æ•—: {result.get('error', 'Unknown error')}"
                    )

                    # å¤±æ•—æ™‚ã®å•é¡Œã‚’è¨˜éŒ²
                    issue = PlatformIssue(
                        issue_type="integration_test",
                        platform=self.platform_info["system"],
                        severity=PlatformCompatibilityRisk.HIGH,
                        description=f"çµ±åˆãƒ†ã‚¹ãƒˆå¤±æ•—: {test_name}",
                        code_snippet=None,
                        file_path=None,
                        line_number=None,
                        recommendation=result.get(
                            "recommendation", "ãƒ†ã‚¹ãƒˆå¤±æ•—ã®åŸå› ã‚’èª¿æŸ»ã—ã¦ãã ã•ã„"
                        ),
                        test_case=test_name,
                        error_details=result.get("details", {}),
                    )
                    self.platform_issues.append(issue)

            except Exception as e:
                failed += 1
                logger.error(f"âŒ {test_name} ä¾‹å¤–: {e}")

                self.test_results[test_name] = {
                    "passed": False,
                    "duration": 0,
                    "error": str(e),
                    "details": {},
                    "metrics": {},
                }

        logger.info(f"çµ±åˆãƒ†ã‚¹ãƒˆå®Œäº†: {passed}æˆåŠŸ, {failed}å¤±æ•—")

    def _test_file_operations(self) -> Dict:
        """ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œãƒ†ã‚¹ãƒˆ"""
        try:
            with tempfile.TemporaryDirectory() as temp_dir:
                temp_path = Path(temp_dir)

                # ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆãƒ»èª­ã¿æ›¸ããƒ†ã‚¹ãƒˆ
                test_file = temp_path / "test_file.txt"
                test_content = "ãƒ†ã‚¹ãƒˆå†…å®¹\nTest content\næ—¥æœ¬èªæ–‡å­—åˆ—"

                # æ›¸ãè¾¼ã¿
                with open(test_file, "w", encoding="utf-8") as f:
                    f.write(test_content)

                # èª­ã¿è¾¼ã¿
                with open(test_file, "r", encoding="utf-8") as f:
                    read_content = f.read()

                # å†…å®¹ç¢ºèª
                if read_content != test_content:
                    return {
                        "passed": False,
                        "error": "ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ä¸ä¸€è‡´",
                        "details": {"expected": test_content, "actual": read_content},
                    }

                # ãƒ‘ãƒ¼ãƒŸãƒƒã‚·ãƒ§ãƒ³æ“ä½œãƒ†ã‚¹ãƒˆï¼ˆUnixç³»ã®ã¿ï¼‰
                if os.name != "nt":
                    try:
                        os.chmod(test_file, 0o644)
                    except Exception as e:
                        return {
                            "passed": False,
                            "error": f"ãƒ‘ãƒ¼ãƒŸãƒƒã‚·ãƒ§ãƒ³è¨­å®šå¤±æ•—: {e}",
                        }

                return {
                    "passed": True,
                    "details": {"platform": self.platform_info["system"]},
                }

        except Exception as e:
            return {"passed": False, "error": str(e)}

    def _test_path_handling(self) -> Dict:
        """ãƒ‘ã‚¹å‡¦ç†ãƒ†ã‚¹ãƒˆ"""
        try:
            # å„ç¨®ãƒ‘ã‚¹æ“ä½œã®ãƒ†ã‚¹ãƒˆ
            test_paths = [
                "folder/subfolder/file.txt",
                "../parent/file.txt",
                "./current/file.txt",
                "file with spaces.txt",
                "æ—¥æœ¬èªãƒ•ã‚¡ã‚¤ãƒ«å.txt",
            ]

            for test_path in test_paths:
                # pathlib.Pathã§ã®å‡¦ç†
                path_obj = Path(test_path)

                # ãƒ‘ã‚¹åˆ†è§£
                parts = path_obj.parts
                parent = path_obj.parent
                name = path_obj.name
                stem = path_obj.stem
                suffix = path_obj.suffix

                # çµ¶å¯¾ãƒ‘ã‚¹å¤‰æ›
                abs_path = path_obj.resolve()

                # ãƒ‘ã‚¹çµåˆ
                joined = Path("base") / path_obj

            return {"passed": True, "metrics": {"tested_paths": len(test_paths)}}

        except Exception as e:
            return {"passed": False, "error": str(e)}

    def _test_process_execution(self) -> Dict:
        """ãƒ—ãƒ­ã‚»ã‚¹å®Ÿè¡Œãƒ†ã‚¹ãƒˆ"""
        try:
            # Pythonå®Ÿè¡Œãƒ†ã‚¹ãƒˆ
            result = subprocess.run(
                [
                    sys.executable,
                    "-c",
                    'import sys; print(f"Python {sys.version_info.major}.{sys.version_info.minor}")',
                ],
                capture_output=True,
                text=True,
                timeout=10,
            )

            if result.returncode != 0:
                return {
                    "passed": False,
                    "error": "Pythonå®Ÿè¡Œå¤±æ•—",
                    "details": {"stderr": result.stderr},
                }

            # ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ å›ºæœ‰ã‚³ãƒãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆ
            if self.platform_info["system"] == "Windows":
                cmd_result = subprocess.run(
                    ["echo", "test"], capture_output=True, text=True
                )
            else:
                cmd_result = subprocess.run(
                    ["echo", "test"], capture_output=True, text=True
                )

            return {
                "passed": True,
                "metrics": {
                    "python_version": result.stdout.strip(),
                    "echo_result": cmd_result.stdout.strip(),
                },
            }

        except subprocess.TimeoutExpired:
            return {"passed": False, "error": "ãƒ—ãƒ­ã‚»ã‚¹å®Ÿè¡Œã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ"}
        except Exception as e:
            return {"passed": False, "error": str(e)}

    def _test_encoding_handling(self) -> Dict:
        """ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å‡¦ç†ãƒ†ã‚¹ãƒˆ"""
        try:
            test_strings = [
                "ASCII text",
                "æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆ",
                "CafÃ© rÃ©sumÃ© naÃ¯ve",
                "ğŸš€ Emoji test ğŸ‰",
                "ĞšĞ¸Ñ€Ğ¸Ğ»Ğ»Ğ¸Ñ†Ğ°",
                "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©",
            ]

            results = {}

            for test_str in test_strings:
                # UTF-8ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒ»ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
                encoded = test_str.encode("utf-8")
                decoded = encoded.decode("utf-8")

                if decoded != test_str:
                    return {
                        "passed": False,
                        "error": f"ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å¤±æ•—: {test_str}",
                        "details": {"original": test_str, "decoded": decoded},
                    }

                results[test_str] = {
                    "encoded_length": len(encoded),
                    "original_length": len(test_str),
                }

            return {
                "passed": True,
                "metrics": {"tested_strings": len(test_strings)},
                "details": results,
            }

        except Exception as e:
            return {"passed": False, "error": str(e)}

    def _test_performance_consistency(self) -> Dict:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ä¸€è²«æ€§ãƒ†ã‚¹ãƒˆ"""
        try:
            # CPUé›†ç´„çš„ã‚¿ã‚¹ã‚¯
            start_time = time.time()
            result = sum(i * i for i in range(100000))
            cpu_duration = time.time() - start_time

            # I/Oé›†ç´„çš„ã‚¿ã‚¹ã‚¯
            with tempfile.NamedTemporaryFile(mode="w+", delete=False) as temp_file:
                temp_path = temp_file.name

                start_time = time.time()
                for i in range(1000):
                    temp_file.write(f"Line {i}\n")
                temp_file.flush()
                io_duration = time.time() - start_time

            os.unlink(temp_path)

            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¸¬å®š
            process = psutil.Process()
            memory_info = process.memory_info()

            metrics = {
                "cpu_task_duration": cpu_duration,
                "io_task_duration": io_duration,
                "memory_rss": memory_info.rss,
                "memory_vms": memory_info.vms,
                "cpu_percent": process.cpu_percent(),
            }

            self.performance_metrics.update(metrics)

            return {"passed": True, "metrics": metrics}

        except Exception as e:
            return {"passed": False, "error": str(e)}

    def _test_dependency_compatibility(self) -> Dict:
        """ä¾å­˜é–¢ä¿‚äº’æ›æ€§ãƒ†ã‚¹ãƒˆ"""
        try:
            # ä¸»è¦ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ†ã‚¹ãƒˆ
            critical_imports = [
                "pathlib",
                "json",
                "logging",
                "datetime",
                "subprocess",
                "tempfile",
            ]

            import_results = {}

            for module_name in critical_imports:
                try:
                    __import__(module_name)
                    import_results[module_name] = True
                except ImportError as e:
                    import_results[module_name] = False
                    logger.warning(f"ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¤±æ•—: {module_name} - {e}")

            failed_imports = [
                name for name, success in import_results.items() if not success
            ]

            return {
                "passed": len(failed_imports) == 0,
                "details": import_results,
                "error": (
                    f"ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¤±æ•—: {failed_imports}" if failed_imports else None
                ),
            }

        except Exception as e:
            return {"passed": False, "error": str(e)}

    def _test_concurrent_operations(self) -> Dict:
        """ä¸¦è¡Œå‡¦ç†ãƒ†ã‚¹ãƒˆ"""
        try:
            results = []
            errors = []

            def worker_task(task_id):
                try:
                    # ç°¡å˜ãªè¨ˆç®—ã‚¿ã‚¹ã‚¯
                    result = sum(i for i in range(1000))
                    results.append((task_id, result))
                except Exception as e:
                    errors.append((task_id, str(e)))

            # è¤‡æ•°ã‚¹ãƒ¬ãƒƒãƒ‰ã§ä¸¦è¡Œå®Ÿè¡Œ
            threads = []
            thread_count = 4

            for i in range(thread_count):
                thread = threading.Thread(target=worker_task, args=(i,))
                threads.append(thread)
                thread.start()

            # å…¨ã‚¹ãƒ¬ãƒƒãƒ‰å®Œäº†å¾…æ©Ÿ
            for thread in threads:
                thread.join(timeout=5.0)

            if errors:
                return {
                    "passed": False,
                    "error": f"ä¸¦è¡Œå‡¦ç†ã‚¨ãƒ©ãƒ¼: {errors}",
                    "details": {"results": results, "errors": errors},
                }

            return {
                "passed": len(results) == thread_count,
                "metrics": {
                    "thread_count": thread_count,
                    "completed_tasks": len(results),
                    "expected_result": 499500,
                    "all_results_match": all(result[1] == 499500 for result in results),
                },
            }

        except Exception as e:
            return {"passed": False, "error": str(e)}

    def _test_large_file_handling(self) -> Dict:
        """å¤§ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ãƒ†ã‚¹ãƒˆ"""
        try:
            # å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆãƒ»èª­ã¿å–ã‚Šãƒ†ã‚¹ãƒˆ
            with tempfile.NamedTemporaryFile(delete=False) as temp_file:
                temp_path = temp_file.name

                # 1MBã®ãƒ‡ãƒ¼ã‚¿ä½œæˆ
                chunk_size = 1024
                chunk_count = 1024  # 1MB total
                test_chunk = b"x" * chunk_size

                start_time = time.time()
                for _ in range(chunk_count):
                    temp_file.write(test_chunk)
                write_duration = time.time() - start_time

                temp_file.flush()
                file_size = temp_file.tell()

            # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿å–ã‚Š
            start_time = time.time()
            with open(temp_path, "rb") as f:
                read_data = f.read()
            read_duration = time.time() - start_time

            os.unlink(temp_path)

            return {
                "passed": len(read_data) == file_size,
                "metrics": {
                    "file_size": file_size,
                    "write_duration": write_duration,
                    "read_duration": read_duration,
                    "write_speed_mb_per_sec": (file_size / (1024 * 1024))
                    / write_duration,
                    "read_speed_mb_per_sec": (file_size / (1024 * 1024))
                    / read_duration,
                },
            }

        except Exception as e:
            return {"passed": False, "error": str(e)}

    def _run_performance_tests(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        logger.info("âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")

        # æ—¢ã«_test_performance_consistencyã§å®Ÿè¡Œæ¸ˆã¿
        logger.info("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå®Œäº†")

    def _analyze_results(self, start_time: datetime) -> PlatformTestResult:
        """çµæœåˆ†æ"""
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()

        # ãƒ†ã‚¹ãƒˆçµ±è¨ˆ
        total_tests = len(self.test_results)
        passed_tests = sum(
            1 for result in self.test_results.values() if result["passed"]
        )
        failed_tests = total_tests - passed_tests

        # å…¨ä½“ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«æ±ºå®š
        overall_risk = self._calculate_overall_compatibility_risk()

        # æ¨å¥¨äº‹é …ç”Ÿæˆ
        recommendations = self._generate_compatibility_recommendations()

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ äº’æ›æ€§
        filesystem_compatibility = {
            "unicode_filenames": True,  # ç¾ä»£ã®OSã¯ã™ã¹ã¦ã‚µãƒãƒ¼ãƒˆ
            "long_paths": self.platform_info["system"] != "Windows",  # Windowsåˆ¶é™ã‚ã‚Š
            "case_sensitivity": self.platform_info["system"] in ["Linux", "Darwin"],
        }

        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°äº’æ›æ€§
        encoding_compatibility = {
            "utf8_support": True,  # Python 3ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆUTF-8
            "locale_handling": True,
            "console_encoding": True,
        }

        return PlatformTestResult(
            platform_info=self.platform_info,
            total_tests_run=total_tests,
            passed_tests=passed_tests,
            failed_tests=failed_tests,
            platform_issues=self.platform_issues,
            performance_metrics=self.performance_metrics,
            filesystem_compatibility=filesystem_compatibility,
            encoding_compatibility=encoding_compatibility,
            test_duration=duration,
            timestamp=end_time,
            overall_risk=overall_risk,
            recommendations=recommendations,
        )

    def _calculate_overall_compatibility_risk(self) -> PlatformCompatibilityRisk:
        """å…¨ä½“äº’æ›æ€§ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«è¨ˆç®—"""
        if not self.platform_issues:
            return PlatformCompatibilityRisk.SAFE

        risk_counts = {}
        for issue in self.platform_issues:
            risk_counts[issue.severity] = risk_counts.get(issue.severity, 0) + 1

        if risk_counts.get(PlatformCompatibilityRisk.CRITICAL, 0) > 0:
            return PlatformCompatibilityRisk.CRITICAL
        elif risk_counts.get(PlatformCompatibilityRisk.HIGH, 0) > 0:
            return PlatformCompatibilityRisk.HIGH
        elif risk_counts.get(PlatformCompatibilityRisk.MEDIUM, 0) > 0:
            return PlatformCompatibilityRisk.MEDIUM
        else:
            return PlatformCompatibilityRisk.LOW

    def _generate_compatibility_recommendations(self) -> List[str]:
        """äº’æ›æ€§æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        recommendations = [
            "pathlib.Pathã‚’ä½¿ç”¨ã—ã¦ã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ å¯¾å¿œã®ãƒ‘ã‚¹å‡¦ç†ã‚’å®Ÿè£…ã™ã‚‹",
            "os.path.join()ã®ä»£ã‚ã‚Šã«pathlibæ¼”ç®—å­ï¼ˆ/ï¼‰ã‚’ä½¿ç”¨ã™ã‚‹",
            "ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ å›ºæœ‰ã®ã‚³ãƒ¼ãƒ‰ã‚’if platform.system()ã§åˆ†å²ã•ã›ã‚‹",
            "ç’°å¢ƒå¤‰æ•°ã‚¢ã‚¯ã‚»ã‚¹æ™‚ã¯os.environ.get()ã§ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’æŒ‡å®šã™ã‚‹",
            "ãƒ•ã‚¡ã‚¤ãƒ«I/Oæ™‚ã¯æ˜ç¤ºçš„ã«encoding='utf-8'ã‚’æŒ‡å®šã™ã‚‹",
            "subprocessã§shell=Trueã®ä½¿ç”¨ã‚’é¿ã‘ã‚‹",
            "å®šæœŸçš„ãªã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ",
        ]

        if self.platform_issues:
            recommendations.extend(
                [
                    "ç™ºè¦‹ã•ã‚ŒãŸãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ äº’æ›æ€§å•é¡Œã‚’å„ªå…ˆåº¦ã«å¿œã˜ã¦ä¿®æ­£ã™ã‚‹",
                    "CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«è¤‡æ•°OSç’°å¢ƒã§ã®ãƒ†ã‚¹ãƒˆã‚’è¿½åŠ ã™ã‚‹",
                    "ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ å›ºæœ‰ã®æ©Ÿèƒ½ä½¿ç”¨æ™‚ã¯é©åˆ‡ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’å®Ÿè£…ã™ã‚‹",
                ]
            )

        return recommendations

    def _generate_compatibility_report(self, result: PlatformTestResult):
        """äº’æ›æ€§ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        report_file = self.project_root / "cross_platform_compatibility_report.json"

        report_data = {
            "platform_info": result.platform_info,
            "test_summary": {
                "total_tests": result.total_tests_run,
                "passed_tests": result.passed_tests,
                "failed_tests": result.failed_tests,
                "success_rate": (
                    (result.passed_tests / result.total_tests_run * 100)
                    if result.total_tests_run > 0
                    else 0
                ),
                "overall_risk": result.overall_risk.value,
                "test_duration": result.test_duration,
                "timestamp": result.timestamp.isoformat(),
            },
            "platform_issues": [
                {
                    "type": issue.issue_type,
                    "platform": issue.platform,
                    "severity": issue.severity.value,
                    "description": issue.description,
                    "file": issue.file_path,
                    "line": issue.line_number,
                    "code": issue.code_snippet,
                    "recommendation": issue.recommendation,
                    "test_case": issue.test_case,
                    "error_details": issue.error_details,
                }
                for issue in result.platform_issues
            ],
            "test_results": self.test_results,
            "performance_metrics": result.performance_metrics,
            "compatibility": {
                "filesystem": result.filesystem_compatibility,
                "encoding": result.encoding_compatibility,
            },
            "recommendations": result.recommendations,
            "risk_distribution": self._get_risk_distribution(result.platform_issues),
        }

        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)

        logger.info(f"ğŸ“‹ ã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ äº’æ›æ€§ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ: {report_file}")

    def _get_risk_distribution(self, issues: List[PlatformIssue]) -> Dict[str, int]:
        """ãƒªã‚¹ã‚¯åˆ†å¸ƒå–å¾—"""
        distribution = {}
        for issue in issues:
            risk = issue.severity.value
            distribution[risk] = distribution.get(risk, 0) + 1
        return distribution


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    project_root = Path(__file__).parent.parent

    logger.info("ğŸš€ ã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ çµ±åˆãƒ†ã‚¹ãƒˆé–‹å§‹")

    try:
        with CrossPlatformIntegrationTester(project_root) as tester:
            result = tester.run()

            # çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º
            logger.info("ğŸ“Š ã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ çµ±åˆãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼:")
            logger.info(
                f"  å®Ÿè¡Œç’°å¢ƒ: {result.platform_info['system']} {result.platform_info['platform']}"
            )
            logger.info(f"  ç·ãƒ†ã‚¹ãƒˆæ•°: {result.total_tests_run}")
            logger.info(f"  æˆåŠŸ: {result.passed_tests}, å¤±æ•—: {result.failed_tests}")
            logger.info(
                f"  æˆåŠŸç‡: {(result.passed_tests / result.total_tests_run * 100):.1f}%"
            )
            logger.info(f"  ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ å•é¡Œ: {len(result.platform_issues)}ä»¶")
            logger.info(f"  å…¨ä½“ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«: {result.overall_risk.value}")
            logger.info(f"  å®Ÿè¡Œæ™‚é–“: {result.test_duration:.2f}ç§’")

            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±
            if result.performance_metrics:
                logger.info("âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹:")
                for metric, value in result.performance_metrics.items():
                    if isinstance(value, float):
                        logger.info(f"  {metric}: {value:.4f}")
                    else:
                        logger.info(f"  {metric}: {value}")

            # é‡è¦ãªå•é¡Œã‚’è¡¨ç¤º
            critical_issues = [
                i
                for i in result.platform_issues
                if i.severity
                in [PlatformCompatibilityRisk.CRITICAL, PlatformCompatibilityRisk.HIGH]
            ]

            if critical_issues:
                logger.warning(
                    f"ğŸš¨ é«˜ãƒªã‚¹ã‚¯ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ å•é¡Œ {len(critical_issues)}ä»¶:"
                )
                for issue in critical_issues[:5]:  # ä¸Šä½5ä»¶è¡¨ç¤º
                    location = (
                        f"{issue.file_path}:{issue.line_number}"
                        if issue.file_path
                        else issue.test_case
                    )
                    logger.warning(
                        f"  - {location} ({issue.severity.value}): {issue.description}"
                    )

            return (
                0
                if result.overall_risk
                in [PlatformCompatibilityRisk.SAFE, PlatformCompatibilityRisk.LOW]
                else 1
            )

    except Exception as e:
        logger.error(f"ğŸ’¥ ã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        return 1


if __name__ == "__main__":
    sys.exit(main())
