#!/usr/bin/env python3
"""
ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºåŠ¹ç‡æ€§åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ

300è¡Œåˆ¶é™ã«ã‚ˆã‚‹éåŠ¹ç‡ãªè¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡ºã—ã€
äººç‚ºçš„åˆ†å‰²ãƒ»è¨­è¨ˆæ­ªæ›²ãƒ»ä¿å®ˆæ€§æ‚ªåŒ–ã‚’ç‰¹å®šã—ã¾ã™ã€‚
"""

import os
import re
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Optional, Tuple


@dataclass
class FileAnalysis:
    """ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æçµæœ"""

    path: Path
    lines: int
    classes: int
    functions: int
    imports: int
    complexity_score: int
    efficiency_issues: List[str]
    refactor_suggestions: List[str]


class FileSizeEfficiencyAnalyzer:
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºåŠ¹ç‡æ€§åˆ†æã‚¯ãƒ©ã‚¹"""

    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.line_limit = 300
        self.analyses = []

    def analyze_size_related_inefficiencies(self) -> Dict:
        """ã‚µã‚¤ã‚ºåˆ¶é™é–¢é€£ã®éåŠ¹ç‡æ€§åˆ†æ"""

        # 1. 300è¡Œè¶…éãƒ•ã‚¡ã‚¤ãƒ«ã®ç‰¹å®š
        oversized_files = self._find_oversized_files()

        # 2. äººç‚ºçš„åˆ†å‰²ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º
        artificial_splits = self._detect_artificial_splits()

        # 3. è¨­è¨ˆæ­ªæ›²ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ
        design_distortions = self._analyze_design_distortions()

        # 4. ä¿å®ˆæ€§ã¸ã®å½±éŸ¿è©•ä¾¡
        maintainability_impact = self._assess_maintainability_impact()

        return {
            "summary": self._generate_efficiency_summary(
                oversized_files, artificial_splits
            ),
            "oversized_files": oversized_files,
            "artificial_splits": artificial_splits,
            "design_distortions": design_distortions,
            "maintainability_impact": maintainability_impact,
            "recommendations": self._generate_recommendations(),
        }

    def _find_oversized_files(self) -> List[FileAnalysis]:
        """300è¡Œè¶…éãƒ•ã‚¡ã‚¤ãƒ«ã®ç‰¹å®šã¨åˆ†æ"""
        oversized = []

        for py_file in self.project_root.rglob("kumihan_formatter/**/*.py"):
            try:
                content = py_file.read_text(encoding="utf-8")
                lines = len(content.splitlines())

                if lines > self.line_limit:
                    analysis = self._analyze_file_structure(py_file, content, lines)
                    oversized.append(analysis)

            except Exception:
                continue

        # è¡Œæ•°é †ã§ã‚½ãƒ¼ãƒˆ
        oversized.sort(key=lambda x: x.lines, reverse=True)
        return oversized

    def _analyze_file_structure(
        self, file_path: Path, content: str, lines: int
    ) -> FileAnalysis:
        """ãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ ã®è©³ç´°åˆ†æ"""

        # åŸºæœ¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        classes = len(re.findall(r"^class\s+\w+", content, re.MULTILINE))
        functions = len(re.findall(r"^def\s+\w+", content, re.MULTILINE))
        imports = len(re.findall(r"^(import|from)\s+", content, re.MULTILINE))

        # è¤‡é›‘åº¦æŒ‡æ¨™
        if_statements = len(re.findall(r"\bif\b", content))
        try_blocks = len(re.findall(r"\btry\b", content))
        loops = len(re.findall(r"\b(for|while)\b", content))

        complexity_score = (
            if_statements + try_blocks * 2 + loops + classes * 5 + functions * 2
        )

        # åŠ¹ç‡æ€§å•é¡Œã®æ¤œå‡º
        efficiency_issues = []
        refactor_suggestions = []

        # 1. å˜ä¸€è²¬ä»»åŸå‰‡é•åã®æ¤œå‡º
        if classes > 3 and lines > 400:
            efficiency_issues.append(
                "Multiple classes in oversized file (SRP violation)"
            )
            refactor_suggestions.append(
                "Split into separate files by class responsibility"
            )

        # 2. é•·å¤§ãªé–¢æ•°ã®æ¤œå‡º
        long_functions = self._detect_long_functions(content)
        if long_functions:
            efficiency_issues.append(
                f"Long functions detected: {len(long_functions)} functions > 50 lines"
            )
            refactor_suggestions.append("Extract methods from long functions")

        # 3. é‡è¤‡ã‚³ãƒ¼ãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º
        if self._has_code_duplication(content):
            efficiency_issues.append("Code duplication patterns detected")
            refactor_suggestions.append("Extract common functionality to utilities")

        # 4. äººç‚ºçš„åˆ†å‰²ã®å…†å€™
        if lines > 280 and lines < 320 and classes == 1:
            efficiency_issues.append(
                "Possible artificial size constraint (near 300-line limit)"
            )
            refactor_suggestions.append(
                "Consider natural module boundaries over line limits"
            )

        return FileAnalysis(
            path=file_path,
            lines=lines,
            classes=classes,
            functions=functions,
            imports=imports,
            complexity_score=complexity_score,
            efficiency_issues=efficiency_issues,
            refactor_suggestions=refactor_suggestions,
        )

    def _detect_long_functions(self, content: str) -> List[str]:
        """é•·å¤§ãªé–¢æ•°ã®æ¤œå‡º"""
        lines = content.split("\n")
        long_functions = []

        current_function = None
        function_start = 0
        indent_level = 0

        for i, line in enumerate(lines):
            stripped = line.strip()

            # é–¢æ•°å®šç¾©ã®æ¤œå‡º
            if stripped.startswith("def "):
                if current_function and (i - function_start) > 50:
                    long_functions.append(
                        f"{current_function} ({i - function_start} lines)"
                    )

                current_function = stripped.split("(")[0].replace("def ", "")
                function_start = i
                indent_level = len(line) - len(line.lstrip())

            # é–¢æ•°çµ‚äº†ã®æ¤œå‡ºï¼ˆã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆãƒ¬ãƒ™ãƒ«ã§åˆ¤å®šï¼‰
            elif (
                current_function
                and line.strip()
                and len(line) - len(line.lstrip()) <= indent_level
                and not line.startswith(" ")
            ):
                if (i - function_start) > 50:
                    long_functions.append(
                        f"{current_function} ({i - function_start} lines)"
                    )
                current_function = None

        return long_functions

    def _has_code_duplication(self, content: str) -> bool:
        """ã‚³ãƒ¼ãƒ‰é‡è¤‡ã®ç°¡æ˜“æ¤œå‡º"""
        lines = [
            line.strip()
            for line in content.split("\n")
            if line.strip() and not line.strip().startswith("#")
        ]

        # åŒã˜è¡ŒãŒ3å›ä»¥ä¸Šå‡ºç¾ï¼ˆé‡è¤‡ã®å…†å€™ï¼‰
        line_counts = {}
        for line in lines:
            if len(line) > 20:  # çŸ­ã™ãã‚‹è¡Œã¯é™¤å¤–
                line_counts[line] = line_counts.get(line, 0) + 1

        return any(count >= 3 for count in line_counts.values())

    def _detect_artificial_splits(self) -> List[Dict]:
        """äººç‚ºçš„åˆ†å‰²ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º"""
        artificial_splits = []

        # é–¢é€£æ€§ã®é«˜ã„ãƒ•ã‚¡ã‚¤ãƒ«ç¾¤ã‚’æ¤œç´¢
        related_files = self._find_related_file_groups()

        for group in related_files:
            if len(group["files"]) > 3:  # 3å€‹ä»¥ä¸Šã®é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«
                total_lines = sum(f["lines"] for f in group["files"])
                avg_lines = total_lines / len(group["files"])

                # å¹³å‡250-299è¡Œ = 300è¡Œåˆ¶é™å›é¿ã®ç–‘ã„
                if 250 <= avg_lines <= 299:
                    artificial_splits.append(
                        {
                            "pattern": group["pattern"],
                            "files": group["files"],
                            "total_lines": total_lines,
                            "avg_lines": avg_lines,
                            "suspicion_reason": "Consistent sizing near 300-line limit suggests artificial splitting",
                        }
                    )

        return artificial_splits

    def _find_related_file_groups(self) -> List[Dict]:
        """é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã‚°ãƒ«ãƒ¼ãƒ—ã®ç‰¹å®š"""
        groups = []

        # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ¥ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        patterns = {
            "performance": "kumihan_formatter/core/performance/",
            "error_handling": "kumihan_formatter/core/error_handling/",
            "utilities": "kumihan_formatter/core/utilities/",
            "rendering": "kumihan_formatter/core/rendering/",
            "caching": "kumihan_formatter/core/caching/",
        }

        for pattern_name, pattern_path in patterns.items():
            pattern_dir = self.project_root / pattern_path
            if pattern_dir.exists():
                files = []
                for py_file in pattern_dir.rglob("*.py"):
                    try:
                        lines = len(py_file.read_text(encoding="utf-8").splitlines())
                        files.append({"path": py_file, "lines": lines})
                    except:
                        continue

                if files:
                    groups.append({"pattern": pattern_name, "files": files})

        return groups

    def _analyze_design_distortions(self) -> List[Dict]:
        """è¨­è¨ˆæ­ªæ›²ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ"""
        distortions = []

        # 1. éåº¦ãªæŠ½è±¡åŒ–ã®æ¤œå‡º
        over_abstraction = self._detect_over_abstraction()
        if over_abstraction:
            distortions.extend(over_abstraction)

        # 2. ä¸è‡ªç„¶ãªè²¬ä»»åˆ†æ•£ã®æ¤œå‡º
        unnatural_splits = self._detect_unnatural_responsibility_splits()
        if unnatural_splits:
            distortions.extend(unnatural_splits)

        return distortions

    def _detect_over_abstraction(self) -> List[Dict]:
        """éåº¦ãªæŠ½è±¡åŒ–ã®æ¤œå‡º"""
        over_abstractions = []

        # Factory, Builder, Strategy ãƒ‘ã‚¿ãƒ¼ãƒ³ã®éå‰°ä½¿ç”¨ã‚’æ¤œå‡º
        pattern_files = list(
            self.project_root.rglob("kumihan_formatter/**/*factory*.py")
        )
        pattern_files.extend(
            self.project_root.rglob("kumihan_formatter/**/*builder*.py")
        )
        pattern_files.extend(
            self.project_root.rglob("kumihan_formatter/**/*strategy*.py")
        )

        for file_path in pattern_files:
            try:
                content = file_path.read_text(encoding="utf-8")
                lines = len(content.splitlines())

                # å°ã•ãªãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ< 100è¡Œï¼‰ã§è¤‡é›‘ãªãƒ‘ã‚¿ãƒ¼ãƒ³ = éåº¦ãªæŠ½è±¡åŒ–ã®ç–‘ã„
                if lines < 100:
                    classes = len(re.findall(r"^class\s+\w+", content, re.MULTILINE))
                    if classes >= 2:
                        over_abstractions.append(
                            {
                                "file": file_path,
                                "lines": lines,
                                "classes": classes,
                                "issue": "Small file with multiple classes - possible over-abstraction",
                                "suggestion": "Consider consolidating or questioning necessity of pattern",
                            }
                        )
            except:
                continue

        return over_abstractions

    def _detect_unnatural_responsibility_splits(self) -> List[Dict]:
        """ä¸è‡ªç„¶ãªè²¬ä»»åˆ†æ•£ã®æ¤œå‡º"""
        unnatural_splits = []

        # é«˜ã„çµåˆåº¦ã‚’æŒã¤ãƒ•ã‚¡ã‚¤ãƒ«ç¾¤ã‚’æ¤œå‡º
        import_dependencies = self._analyze_import_dependencies()

        for file_group in import_dependencies:
            if file_group["cross_dependencies"] > 5:  # ç›¸äº’ä¾å­˜ãŒå¤šã„
                unnatural_splits.append(
                    {
                        "files": file_group["files"],
                        "cross_dependencies": file_group["cross_dependencies"],
                        "issue": "High coupling between supposedly separate modules",
                        "suggestion": "Consider merging highly coupled modules",
                    }
                )

        return unnatural_splits

    def _analyze_import_dependencies(self) -> List[Dict]:
        """importä¾å­˜é–¢ä¿‚ã®åˆ†æ"""
        # ç°¡ç•¥åŒ–ã•ã‚ŒãŸå®Ÿè£…
        return []  # å®Ÿéš›ã®å®Ÿè£…ã§ã¯è©³ç´°ãªä¾å­˜é–¢ä¿‚åˆ†æã‚’è¡Œã†

    def _assess_maintainability_impact(self) -> Dict:
        """ä¿å®ˆæ€§ã¸ã®å½±éŸ¿è©•ä¾¡"""
        return {
            "cognitive_load": {
                "issue": "å¤šæ•°ã®å°ãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚ˆã‚‹èªçŸ¥è² è·å¢—å¤§",
                "evidence": "é–¢é€£æ©Ÿèƒ½ãŒè¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã«åˆ†æ•£",
                "impact": "Medium",
            },
            "navigation_overhead": {
                "issue": "ãƒ•ã‚¡ã‚¤ãƒ«é–“ç§»å‹•ã®é »åº¦å¢—åŠ ",
                "evidence": "å˜ä¸€æ©Ÿèƒ½ç†è§£ã«è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèªãŒå¿…è¦",
                "impact": "High",
            },
            "testing_complexity": {
                "issue": "çµ±åˆãƒ†ã‚¹ãƒˆã®è¤‡é›‘åŒ–",
                "evidence": "åˆ†æ•£ã•ã‚ŒãŸæ©Ÿèƒ½ã®çµåˆãƒ†ã‚¹ãƒˆå›°é›£",
                "impact": "High",
            },
        }

    def _generate_efficiency_summary(
        self, oversized_files: List[FileAnalysis], artificial_splits: List[Dict]
    ) -> Dict:
        """åŠ¹ç‡æ€§ã‚µãƒãƒªãƒ¼ç”Ÿæˆ"""
        return {
            "total_oversized": len(oversized_files),
            "max_lines": max((f.lines for f in oversized_files), default=0),
            "avg_oversized_lines": (
                sum(f.lines for f in oversized_files) / len(oversized_files)
                if oversized_files
                else 0
            ),
            "artificial_split_groups": len(artificial_splits),
            "efficiency_verdict": self._calculate_efficiency_verdict(
                oversized_files, artificial_splits
            ),
        }

    def _calculate_efficiency_verdict(
        self, oversized_files: List[FileAnalysis], artificial_splits: List[Dict]
    ) -> str:
        """åŠ¹ç‡æ€§åˆ¤å®š"""
        if len(oversized_files) > 20:
            return "High Inefficiency Risk"
        elif len(artificial_splits) > 5:
            return "Moderate Inefficiency Risk"
        elif any(f.lines > 500 for f in oversized_files):
            return "Some Inefficiency Issues"
        else:
            return "Generally Efficient"

    def _generate_recommendations(self) -> List[Dict]:
        """æ”¹å–„æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        return [
            {
                "priority": "High",
                "action": "ãƒ†ã‚£ã‚¢åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºåˆ¶é™ã®å°å…¥",
                "description": "Critical: 400è¡Œ, Important: 500è¡Œ, Others: åˆ¶é™ãªã—",
                "benefit": "è‡ªç„¶ãªè¨­è¨ˆå¢ƒç•Œã®å°Šé‡",
            },
            {
                "priority": "Medium",
                "action": "é–¢é€£æ©Ÿèƒ½ãƒ•ã‚¡ã‚¤ãƒ«ã®çµ±åˆæ¤œè¨",
                "description": "é«˜ã„çµåˆåº¦ã‚’æŒã¤ãƒ•ã‚¡ã‚¤ãƒ«ç¾¤ã®çµ±åˆ",
                "benefit": "ä¿å®ˆæ€§ãƒ»ç†è§£æ€§ã®å‘ä¸Š",
            },
            {
                "priority": "Low",
                "action": "éåº¦ãªæŠ½è±¡åŒ–ã®è¦‹ç›´ã—",
                "description": "ä¸å¿…è¦ãªãƒ‡ã‚¶ã‚¤ãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç°¡ç´ åŒ–",
                "benefit": "ã‚³ãƒ¼ãƒ‰è¤‡é›‘æ€§ã®å‰Šæ¸›",
            },
        ]


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    analyzer = FileSizeEfficiencyAnalyzer(Path("."))

    print("ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºåŠ¹ç‡æ€§åˆ†æ")
    print("=" * 60)

    analysis = analyzer.analyze_size_related_inefficiencies()

    # 1. ã‚µãƒãƒªãƒ¼
    summary = analysis["summary"]
    print(f"ğŸ“Š åŠ¹ç‡æ€§ã‚µãƒãƒªãƒ¼:")
    print(f"  300è¡Œè¶…éãƒ•ã‚¡ã‚¤ãƒ«: {summary['total_oversized']}å€‹")
    print(f"  æœ€å¤§è¡Œæ•°: {summary['max_lines']:,}è¡Œ")
    print(f"  å¹³å‡è¡Œæ•°ï¼ˆè¶…éåˆ†ï¼‰: {summary['avg_oversized_lines']:.0f}è¡Œ")
    print(f"  äººç‚ºçš„åˆ†å‰²ç–‘ã„: {summary['artificial_split_groups']}ã‚°ãƒ«ãƒ¼ãƒ—")
    print(f"  åŠ¹ç‡æ€§åˆ¤å®š: {summary['efficiency_verdict']}")

    # 2. æœ€å¤§ã®å•é¡Œãƒ•ã‚¡ã‚¤ãƒ«
    oversized = analysis["oversized_files"]
    if oversized:
        print(f"\nğŸš¨ æœ€å¤§ã®ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆTop 5ï¼‰:")
        for i, file_analysis in enumerate(oversized[:5], 1):
            print(f"  {i}. {file_analysis.path.name}: {file_analysis.lines}è¡Œ")
            print(
                f"     ã‚¯ãƒ©ã‚¹: {file_analysis.classes}, é–¢æ•°: {file_analysis.functions}"
            )
            if file_analysis.efficiency_issues:
                print(f"     å•é¡Œ: {file_analysis.efficiency_issues[0]}")

    # 3. äººç‚ºçš„åˆ†å‰²ã®æ¤œå‡º
    artificial = analysis["artificial_splits"]
    if artificial:
        print(f"\nğŸ” äººç‚ºçš„åˆ†å‰²ã®ç–‘ã„:")
        for split in artificial:
            print(f"  {split['pattern']}: {len(split['files'])}ãƒ•ã‚¡ã‚¤ãƒ«")
            print(f"    å¹³å‡è¡Œæ•°: {split['avg_lines']:.0f}è¡Œ")
            print(f"    ç–‘ã„ç†ç”±: {split['suspicion_reason']}")

    # 4. ä¿å®ˆæ€§ã¸ã®å½±éŸ¿
    maintainability = analysis["maintainability_impact"]
    print(f"\nâš ï¸  ä¿å®ˆæ€§ã¸ã®å½±éŸ¿:")
    for aspect, impact in maintainability.items():
        print(f"  {aspect}: {impact['impact']} impact")
        print(f"    å•é¡Œ: {impact['issue']}")

    # 5. æ¨å¥¨äº‹é …
    recommendations = analysis["recommendations"]
    print(f"\nğŸ’¡ æ”¹å–„æ¨å¥¨äº‹é …:")
    for rec in recommendations:
        print(f"  [{rec['priority']}] {rec['action']}")
        print(f"    å†…å®¹: {rec['description']}")
        print(f"    åŠ¹æœ: {rec['benefit']}")
        print()

    # 6. çµè«–
    print(f"ğŸ¯ çµè«–:")
    if summary["efficiency_verdict"] in [
        "High Inefficiency Risk",
        "Moderate Inefficiency Risk",
    ]:
        print(f"  âŒ 300è¡Œåˆ¶é™ã«ã‚ˆã‚‹éåŠ¹ç‡æ€§ãŒç¢ºèªã•ã‚Œã¾ã—ãŸ")
        print(f"  ğŸ’¡ ãƒ†ã‚£ã‚¢åˆ¥åˆ¶é™ã¸ã®å¤‰æ›´ã‚’å¼·ãæ¨å¥¨ã—ã¾ã™")
    else:
        print(f"  âœ… å…¨ä½“çš„ã«ã¯åŠ¹ç‡çš„ãªè¨­è¨ˆãŒä¿ãŸã‚Œã¦ã„ã¾ã™")
        print(f"  ğŸ’¡ ä¸€éƒ¨ã®æ”¹å–„ä½™åœ°ã¯ã‚ã‚Šã¾ã™ãŒæ·±åˆ»ã§ã¯ã‚ã‚Šã¾ã›ã‚“")


if __name__ == "__main__":
    main()
