#!/usr/bin/env python3
"""
TDD System Integration Test - Final Validation
TDDã‚·ã‚¹ãƒ†ãƒ çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ

ç›®çš„: å…¨TDDã‚·ã‚¹ãƒ†ãƒ ã®çµ±åˆãƒ†ã‚¹ãƒˆã¨æœ€çµ‚æ¤œè¨¼
- å…¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆé€£æºãƒ†ã‚¹ãƒˆ
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
- ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆ
- ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ
- å®Ÿç”¨æ€§ãƒ†ã‚¹ãƒˆ
"""

import sys
import time
import threading
import tempfile
import shutil
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from datetime import datetime
import unittest
from unittest.mock import Mock, patch
import json

# TDDã‚·ã‚¹ãƒ†ãƒ ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
sys.path.append(str(Path(__file__).parent))

from tdd_system_base import (
    TDDSystemBase,
    TDDSystemConfig,
    create_tdd_config,
    TDDSystemRegistry,
)
from secure_subprocess import secure_run, test_secure_subprocess
from resource_manager import memory_manager, test_memory_manager
from performance_optimizer import test_performance_optimizer
from kumihan_formatter.core.utilities.logger import get_logger

logger = get_logger(__name__)


@dataclass
class IntegrationTestResult:
    """çµ±åˆãƒ†ã‚¹ãƒˆçµæœ"""

    test_name: str
    success: bool
    execution_time: float
    error_message: Optional[str]
    details: Dict[str, Any]
    timestamp: datetime


class TDDSystemIntegrationTest:
    """TDDçµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ"""

    def __init__(self):
        self.project_root = Path(__file__).parent.parent
        self.test_results = []
        self.test_project_root = None

    def run_all_tests(self) -> Dict[str, Any]:
        """å…¨çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        logger.info("ğŸš€ TDDã‚·ã‚¹ãƒ†ãƒ çµ±åˆãƒ†ã‚¹ãƒˆé–‹å§‹")

        # ãƒ†ã‚¹ãƒˆç”¨ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ
        self._setup_test_environment()

        try:
            # ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆå®Ÿè¡Œ
            test_methods = [
                self._test_secure_subprocess_integration,
                self._test_memory_management_integration,
                self._test_performance_optimization_integration,
                self._test_system_base_integration,
                self._test_session_management_integration,
                self._test_quality_gate_integration,
                self._test_realtime_monitoring_integration,
                self._test_full_tdd_workflow,
                self._test_concurrent_operations,
                self._test_resource_cleanup,
                self._test_error_handling,
                self._test_scalability,
            ]

            for test_method in test_methods:
                self._run_test(test_method)

            # æœ€çµ‚çµæœç”Ÿæˆ
            return self._generate_final_report()

        finally:
            self._cleanup_test_environment()

    def _setup_test_environment(self):
        """ãƒ†ã‚¹ãƒˆç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        logger.info("ğŸ”§ ãƒ†ã‚¹ãƒˆç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­...")

        # ãƒ†ã‚¹ãƒˆç”¨ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        self.test_project_root = Path(tempfile.mkdtemp(prefix="tdd_integration_test_"))

        # å¿…è¦ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹æˆä½œæˆ
        test_dirs = [
            "scripts",
            ".tdd_logs",
            ".quality_data",
            "tests",
            "kumihan_formatter/core",
        ]

        for dir_name in test_dirs:
            (self.test_project_root / dir_name).mkdir(parents=True, exist_ok=True)

        # å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ã‚³ãƒ”ãƒ¼
        script_files = [
            "secure_subprocess.py",
            "resource_manager.py",
            "performance_optimizer.py",
            "tdd_system_base.py",
        ]

        for script_file in script_files:
            src_file = self.project_root / "scripts" / script_file
            if src_file.exists():
                shutil.copy2(src_file, self.test_project_root / "scripts")

        logger.info(f"âœ… ãƒ†ã‚¹ãƒˆç’°å¢ƒä½œæˆå®Œäº†: {self.test_project_root}")

    def _cleanup_test_environment(self):
        """ãƒ†ã‚¹ãƒˆç’°å¢ƒã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if self.test_project_root and self.test_project_root.exists():
            try:
                shutil.rmtree(self.test_project_root)
                logger.info("ğŸ§¹ ãƒ†ã‚¹ãƒˆç’°å¢ƒã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†")
            except Exception as e:
                logger.warning(f"ãƒ†ã‚¹ãƒˆç’°å¢ƒã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—è­¦å‘Š: {e}")

    def _run_test(self, test_method):
        """å€‹åˆ¥ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        test_name = test_method.__name__
        logger.info(f"ğŸ§ª {test_name} å®Ÿè¡Œä¸­...")

        start_time = time.time()

        try:
            details = test_method()
            execution_time = time.time() - start_time

            result = IntegrationTestResult(
                test_name=test_name,
                success=True,
                execution_time=execution_time,
                error_message=None,
                details=details or {},
                timestamp=datetime.now(),
            )

            logger.info(f"âœ… {test_name} æˆåŠŸ ({execution_time:.2f}s)")

        except Exception as e:
            execution_time = time.time() - start_time

            result = IntegrationTestResult(
                test_name=test_name,
                success=False,
                execution_time=execution_time,
                error_message=str(e),
                details={"exception_type": type(e).__name__},
                timestamp=datetime.now(),
            )

            logger.error(f"âŒ {test_name} å¤±æ•— ({execution_time:.2f}s): {e}")

        self.test_results.append(result)

    def _test_secure_subprocess_integration(self) -> Dict:
        """ã‚»ã‚­ãƒ¥ã‚¢ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹çµ±åˆãƒ†ã‚¹ãƒˆ"""
        # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆ
        test_secure_subprocess()

        # å®Ÿéš›ã®ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œãƒ†ã‚¹ãƒˆ
        result = secure_run(["python3", "--version"])
        assert result.success, "Python version command failed"
        assert "Python" in result.stdout, "Python version output invalid"

        return {
            "python_version": result.stdout.strip(),
            "execution_time": result.execution_time,
        }

    def _test_memory_management_integration(self) -> Dict:
        """ãƒ¡ãƒ¢ãƒªç®¡ç†çµ±åˆãƒ†ã‚¹ãƒˆ"""
        # ãƒ¡ãƒ¢ãƒªãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ãƒ†ã‚¹ãƒˆ
        test_memory_manager()

        # ãƒ¡ãƒ¢ãƒªç›£è¦–é–‹å§‹
        memory_manager.start_monitoring()

        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨ãƒ†ã‚¹ãƒˆ
        test_data = [list(range(1000)) for _ in range(10)]
        time.sleep(2)  # ç›£è¦–ãƒ‡ãƒ¼ã‚¿è“„ç©

        # ãƒ¬ãƒãƒ¼ãƒˆå–å¾—
        report = memory_manager.get_memory_report()
        memory_manager.stop_monitoring()

        assert "current_memory_mb" in report, "Memory report missing key data"

        return {"memory_report": report, "test_data_size": len(test_data)}

    def _test_performance_optimization_integration(self) -> Dict:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–çµ±åˆãƒ†ã‚¹ãƒˆ"""
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ãƒ†ã‚¹ãƒˆ
        test_performance_optimizer()

        from performance_optimizer import init_performance_optimizer

        optimizer = init_performance_optimizer(self.test_project_root)

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ†ã‚¹ãƒˆ
        @optimizer.cached_function(ttl_seconds=10)
        def test_function(x):
            time.sleep(0.01)  # è»½ã„å‡¦ç†
            return x * 2

        # åˆå›å®Ÿè¡Œï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹ï¼‰
        result1 = test_function(5)

        # 2å›ç›®å®Ÿè¡Œï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆï¼‰
        result2 = test_function(5)

        assert result1 == result2 == 10, "Cache function results mismatch"

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆå–å¾—
        perf_report = optimizer.get_performance_report()
        optimizer.shutdown()

        return {"performance_report": perf_report, "cache_test_result": result1}

    def _test_system_base_integration(self) -> Dict:
        """ã‚·ã‚¹ãƒ†ãƒ åŸºç›¤çµ±åˆãƒ†ã‚¹ãƒˆ"""
        config = create_tdd_config(self.test_project_root)

        class TestSystem(TDDSystemBase):
            def initialize(self) -> bool:
                return True

            def execute_main_operation(self) -> str:
                return "test_complete"

        # ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œãƒ†ã‚¹ãƒˆ
        system = TestSystem(config)
        result = system.run()

        assert result == "test_complete", "System execution failed"

        # çŠ¶æ³å–å¾—ãƒ†ã‚¹ãƒˆ
        status = system.get_status()
        assert status["initialized"], "System not initialized"

        return {"execution_result": result, "system_status": status}

    def _test_session_management_integration(self) -> Dict:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†çµ±åˆãƒ†ã‚¹ãƒˆ"""
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†ã®åŸºæœ¬æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ
        session_data = {
            "issue_number": "test-001",
            "issue_title": "ãƒ†ã‚¹ãƒˆç”¨Issue",
            "branch_name": "test-branch",
            "start_time": datetime.now().isoformat(),
            "current_phase": "not_started",
        }

        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆãƒ†ã‚¹ãƒˆ
        session_file = self.test_project_root / ".tdd_session.json"
        with open(session_file, "w", encoding="utf-8") as f:
            json.dump(session_data, f, indent=2, ensure_ascii=False)

        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ
        with open(session_file, "r", encoding="utf-8") as f:
            loaded_data = json.load(f)

        assert loaded_data["issue_number"] == "test-001", "Session data mismatch"

        return {
            "session_file_created": session_file.exists(),
            "session_data": loaded_data,
        }

    def _test_quality_gate_integration(self) -> Dict:
        """å“è³ªã‚²ãƒ¼ãƒˆçµ±åˆãƒ†ã‚¹ãƒˆ"""
        # å“è³ªã‚²ãƒ¼ãƒˆè¨­å®šãƒ†ã‚¹ãƒˆ
        quality_gates = {
            "critical_tier": {
                "modules": ["test.module"],
                "min_coverage": 95.0,
                "required_tests": ["unit", "integration"],
                "max_complexity": 10,
            }
        }

        gates_file = self.test_project_root / "scripts" / "quality_gates.json"
        gates_file.parent.mkdir(exist_ok=True)

        with open(gates_file, "w", encoding="utf-8") as f:
            json.dump(quality_gates, f, indent=2, ensure_ascii=False)

        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ
        with open(gates_file, "r", encoding="utf-8") as f:
            loaded_gates = json.load(f)

        assert "critical_tier" in loaded_gates, "Quality gates configuration invalid"

        return {
            "quality_gates_file_created": gates_file.exists(),
            "quality_gates_config": loaded_gates,
        }

    def _test_realtime_monitoring_integration(self) -> Dict:
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–çµ±åˆãƒ†ã‚¹ãƒˆ"""
        # ç›£è¦–çŠ¶æ…‹ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆãƒ†ã‚¹ãƒˆ
        monitoring_data = {
            "timestamp": datetime.now().isoformat(),
            "coverage": {"percentage": 85.5, "status": "good"},
            "tests": {"total": 100, "failed": 2, "passed": 98},
            "quality": {"score": 88.5, "violations": []},
            "tdd": {"phase": "green", "status": "active_green"},
        }

        vscode_status_file = (
            self.test_project_root / ".quality_data" / "vscode_status.json"
        )
        vscode_status_file.parent.mkdir(exist_ok=True)

        with open(vscode_status_file, "w", encoding="utf-8") as f:
            json.dump(monitoring_data, f, indent=2, ensure_ascii=False)

        # ç›£è¦–ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ
        with open(vscode_status_file, "r", encoding="utf-8") as f:
            loaded_data = json.load(f)

        assert loaded_data["coverage"]["percentage"] == 85.5, "Monitoring data invalid"

        return {
            "monitoring_file_created": vscode_status_file.exists(),
            "monitoring_data": loaded_data,
        }

    def _test_full_tdd_workflow(self) -> Dict:
        """å®Œå…¨TDDãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒ†ã‚¹ãƒˆ"""
        # TDDãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å…¨ä½“ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        workflow_steps = [
            "session_start",
            "spec_generation",
            "red_phase",
            "green_phase",
            "refactor_phase",
            "cycle_complete",
        ]

        completed_steps = []

        for step in workflow_steps:
            # å„ã‚¹ãƒ†ãƒƒãƒ—ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
            time.sleep(0.1)  # å‡¦ç†æ™‚é–“ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            completed_steps.append(
                {
                    "step": step,
                    "timestamp": datetime.now().isoformat(),
                    "status": "completed",
                }
            )

        assert len(completed_steps) == len(workflow_steps), "Workflow incomplete"

        return {
            "workflow_steps": workflow_steps,
            "completed_steps": completed_steps,
            "total_steps": len(completed_steps),
        }

    def _test_concurrent_operations(self) -> Dict:
        """ä¸¦è¡Œæ“ä½œãƒ†ã‚¹ãƒˆ"""
        results = []
        threads = []

        def worker_task(task_id: int):
            time.sleep(0.1)
            results.append(f"task_{task_id}_completed")

        # è¤‡æ•°ã‚¹ãƒ¬ãƒƒãƒ‰åŒæ™‚å®Ÿè¡Œ
        for i in range(5):
            thread = threading.Thread(target=worker_task, args=(i,))
            threads.append(thread)
            thread.start()

        # å…¨ã‚¹ãƒ¬ãƒƒãƒ‰å®Œäº†å¾…æ©Ÿ
        for thread in threads:
            thread.join(timeout=5)

        assert len(results) == 5, "Concurrent operations failed"

        return {
            "concurrent_tasks": 5,
            "completed_tasks": len(results),
            "results": results,
        }

    def _test_resource_cleanup(self) -> Dict:
        """ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãƒ†ã‚¹ãƒˆ"""
        # ãƒ†ã‚¹ãƒˆç”¨ãƒªã‚½ãƒ¼ã‚¹ä½œæˆ
        test_files = []

        for i in range(3):
            test_file = self.test_project_root / f"test_resource_{i}.tmp"
            test_file.write_text(f"test content {i}")
            test_files.append(test_file)

        # ãƒªã‚½ãƒ¼ã‚¹å­˜åœ¨ç¢ºèª
        existing_files = [f for f in test_files if f.exists()]
        assert len(existing_files) == 3, "Test resources not created"

        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œ
        for test_file in test_files:
            if test_file.exists():
                test_file.unlink()

        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ç¢ºèª
        remaining_files = [f for f in test_files if f.exists()]
        assert len(remaining_files) == 0, "Resource cleanup failed"

        return {
            "created_files": len(test_files),
            "cleaned_files": len(test_files) - len(remaining_files),
            "cleanup_success": len(remaining_files) == 0,
        }

    def _test_error_handling(self) -> Dict:
        """ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ"""
        error_scenarios = []

        # ç„¡åŠ¹ãªã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œãƒ†ã‚¹ãƒˆ
        try:
            result = secure_run(["nonexistent_command"])
            error_scenarios.append(
                {
                    "scenario": "invalid_command",
                    "handled": not result.success,
                    "error": result.stderr,
                }
            )
        except Exception as e:
            error_scenarios.append(
                {"scenario": "invalid_command", "handled": True, "error": str(e)}
            )

        # ç„¡åŠ¹ãªãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ã‚¯ã‚»ã‚¹ãƒ†ã‚¹ãƒˆ
        try:
            invalid_file = Path("/nonexistent/path/file.txt")
            content = invalid_file.read_text()
            error_scenarios.append(
                {
                    "scenario": "invalid_file_access",
                    "handled": False,
                    "error": "No error raised",
                }
            )
        except Exception as e:
            error_scenarios.append(
                {"scenario": "invalid_file_access", "handled": True, "error": str(e)}
            )

        # å…¨ã‚¨ãƒ©ãƒ¼ãŒé©åˆ‡ã«å‡¦ç†ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
        all_handled = all(scenario["handled"] for scenario in error_scenarios)
        assert all_handled, "Some errors were not properly handled"

        return {"error_scenarios": error_scenarios, "all_errors_handled": all_handled}

    def _test_scalability(self) -> Dict:
        """ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆ"""
        # å¤§é‡ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ†ã‚¹ãƒˆ
        data_sizes = [100, 500, 1000]
        processing_times = []

        for size in data_sizes:
            start_time = time.time()

            # ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
            test_data = list(range(size))
            processed_data = [x * 2 for x in test_data]

            processing_time = time.time() - start_time
            processing_times.append(
                {
                    "data_size": size,
                    "processing_time": processing_time,
                    "throughput": size / processing_time if processing_time > 0 else 0,
                }
            )

        # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£æ¤œè¨¼
        average_throughput = sum(pt["throughput"] for pt in processing_times) / len(
            processing_times
        )
        scalability_good = average_throughput > 1000  # 1000 items/sec ä»¥ä¸Š

        return {
            "processing_times": processing_times,
            "average_throughput": average_throughput,
            "scalability_assessment": (
                "good" if scalability_good else "needs_improvement"
            ),
        }

    def _generate_final_report(self) -> Dict[str, Any]:
        """æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        successful_tests = [r for r in self.test_results if r.success]
        failed_tests = [r for r in self.test_results if not r.success]

        total_execution_time = sum(r.execution_time for r in self.test_results)
        success_rate = len(successful_tests) / len(self.test_results) * 100

        report = {
            "test_summary": {
                "total_tests": len(self.test_results),
                "successful_tests": len(successful_tests),
                "failed_tests": len(failed_tests),
                "success_rate": success_rate,
                "total_execution_time": total_execution_time,
            },
            "test_results": [
                {
                    "test_name": r.test_name,
                    "success": r.success,
                    "execution_time": r.execution_time,
                    "error_message": r.error_message,
                    "timestamp": r.timestamp.isoformat(),
                }
                for r in self.test_results
            ],
            "failed_tests_details": [
                {
                    "test_name": r.test_name,
                    "error_message": r.error_message,
                    "details": r.details,
                }
                for r in failed_tests
            ],
            "overall_assessment": self._assess_overall_quality(success_rate),
            "recommendations": self._generate_recommendations(failed_tests),
            "report_timestamp": datetime.now().isoformat(),
        }

        return report

    def _assess_overall_quality(self, success_rate: float) -> str:
        """å…¨ä½“å“è³ªè©•ä¾¡"""
        if success_rate >= 95:
            return "excellent"
        elif success_rate >= 85:
            return "good"
        elif success_rate >= 70:
            return "acceptable"
        else:
            return "needs_improvement"

    def _generate_recommendations(
        self, failed_tests: List[IntegrationTestResult]
    ) -> List[str]:
        """æ”¹å–„æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        recommendations = []

        if failed_tests:
            recommendations.append("å¤±æ•—ã—ãŸãƒ†ã‚¹ãƒˆã®åŸå› ã‚’èª¿æŸ»ã—ã€ä¿®æ­£ã—ã¦ãã ã•ã„")
            recommendations.append("ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®å¼·åŒ–ã‚’æ¤œè¨ã—ã¦ãã ã•ã„")

        recommendations.extend(
            [
                "å®šæœŸçš„ãªçµ±åˆãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œã‚’ç¶™ç¶šã—ã¦ãã ã•ã„",
                "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚’ç¶™ç¶šã—ã¦ãã ã•ã„",
                "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒã‚§ãƒƒã‚¯ã‚’å®šæœŸçš„ã«å®Ÿè¡Œã—ã¦ãã ã•ã„",
            ]
        )

        return recommendations


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    integration_test = TDDSystemIntegrationTest()

    logger.info("ğŸ¯ TDDã‚·ã‚¹ãƒ†ãƒ çµ±åˆãƒ†ã‚¹ãƒˆé–‹å§‹")

    try:
        # çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        final_report = integration_test.run_all_tests()

        # ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›
        report_file = integration_test.project_root / "tdd_integration_test_report.json"
        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(final_report, f, indent=2, ensure_ascii=False)

        # çµæœè¡¨ç¤º
        success_rate = final_report["test_summary"]["success_rate"]
        assessment = final_report["overall_assessment"]

        logger.info(f"ğŸ“Š çµ±åˆãƒ†ã‚¹ãƒˆå®Œäº†")
        logger.info(f"âœ… æˆåŠŸç‡: {success_rate:.1f}%")
        logger.info(f"ğŸ† ç·åˆè©•ä¾¡: {assessment}")
        logger.info(f"ğŸ“‹ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ: {report_file}")

        # å…¨ã‚·ã‚¹ãƒ†ãƒ ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³
        TDDSystemRegistry.shutdown_all()

        if success_rate >= 85:
            logger.info("ğŸ‰ TDDã‚·ã‚¹ãƒ†ãƒ çµ±åˆãƒ†ã‚¹ãƒˆæˆåŠŸ!")
            return 0
        else:
            logger.error("âŒ TDDã‚·ã‚¹ãƒ†ãƒ çµ±åˆãƒ†ã‚¹ãƒˆå¤±æ•—")
            return 1

    except Exception as e:
        logger.error(f"ğŸ’¥ çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        return 1


if __name__ == "__main__":
    sys.exit(main())
