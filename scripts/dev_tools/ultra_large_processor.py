#!/usr/bin/env python3
"""
è¶…å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«å°‚ç”¨å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ 
200K-300Kè¡Œã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºå®Ÿã«å‡¦ç†ã™ã‚‹ãŸã‚ã®æœ€é©åŒ–å®Ÿè£…
"""

import sys
import time
import psutil
import gc
from pathlib import Path
from typing import Generator, Any, Dict, List

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

class UltraLargeProcessor:
    """è¶…å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«å°‚ç”¨ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼"""

    def __init__(self):
        self.chunk_size = 100  # éå¸¸ã«å°ã•ãªãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º
        self.max_memory_mb = 500  # ãƒ¡ãƒ¢ãƒªåˆ¶é™
        self.progress_interval = 1000  # é€²æ—è¡¨ç¤ºé–“éš”

    def process_ultra_large_file(self, file_path: Path) -> Dict[str, Any]:
        """è¶…å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†"""

        print(f"ğŸ”¥ è¶…å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–‹å§‹: {file_path.name}")

        if not file_path.exists():
            return {"error": f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {file_path}"}

        # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±
        file_size_mb = file_path.stat().st_size / 1024 / 1024

        print(f"ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {file_size_mb:.1f} MB")

        # ãƒ¡ãƒ¢ãƒªç›£è¦–é–‹å§‹
        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024

        start_time = time.time()

        try:
            # æ®µéšçš„å‡¦ç†å®Ÿè¡Œ
            result = self._process_in_stages(file_path)

            end_time = time.time()
            final_memory = process.memory_info().rss / 1024 / 1024

            result.update({
                "processing_time": end_time - start_time,
                "memory_used": final_memory - initial_memory,
                "file_size_mb": file_size_mb
            })

            return result

        except Exception as e:
            return {"error": str(e), "file_size_mb": file_size_mb}

    def _process_in_stages(self, file_path: Path) -> Dict[str, Any]:
        """æ®µéšçš„å‡¦ç†å®Ÿè¡Œ"""

        stages = [
            ("ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿", self._stage_file_reading),
            ("ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è§£æ", self._stage_streaming_parse),
            ("æœ€é©åŒ–å‡¦ç†", self._stage_optimized_processing),
            ("çµæœé›†ç´„", self._stage_result_aggregation)
        ]

        context = {"file_path": file_path}

        for stage_name, stage_func in stages:
            print(f"ğŸ”„ {stage_name}...")

            try:
                stage_result = stage_func(context)
                context.update(stage_result)

                # ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                gc.collect()

                # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒã‚§ãƒƒã‚¯
                current_memory = psutil.Process().memory_info().rss / 1024 / 1024
                if current_memory > self.max_memory_mb:
                    print(f"âš ï¸  ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡è­¦å‘Š: {current_memory:.1f}MB")

            except Exception as e:
                return {"error": f"{stage_name}ã§ã‚¨ãƒ©ãƒ¼: {str(e)}"}

        return context

    def _stage_file_reading(self, context: Dict) -> Dict[str, Any]:
        """ã‚¹ãƒ†ãƒ¼ã‚¸1: ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""

        file_path = context["file_path"]

        # ãƒ•ã‚¡ã‚¤ãƒ«ã®è¡Œæ•°ã‚«ã‚¦ãƒ³ãƒˆï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ï¼‰
        line_count = 0
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                line_count += 1
                if line_count % 10000 == 0:
                    print(f"  è¡Œæ•°ã‚«ã‚¦ãƒ³ãƒˆ: {line_count:,}")

        print(f"  ç·è¡Œæ•°: {line_count:,}")

        return {
            "total_lines": line_count,
            "file_reading_completed": True
        }

    def _stage_streaming_parse(self, context: Dict) -> Dict[str, Any]:
        """ã‚¹ãƒ†ãƒ¼ã‚¸2: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è§£æ"""

        try:
            from kumihan_formatter.parser import StreamingParser

            file_path = context["file_path"]
            total_lines = context["total_lines"]

            # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ‘ãƒ¼ã‚µãƒ¼åˆæœŸåŒ–
            parser = StreamingParser()

            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ä»˜ãã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†
            processed_nodes = 0
            parsed_chunks = []

            def progress_callback(info):
                nonlocal processed_nodes
                processed_nodes += 1
                if processed_nodes % 1000 == 0:
                    percent = (processed_nodes / total_lines) * 100
                    print(f"  è§£æé€²æ—: {processed_nodes:,}/{total_lines:,} ({percent:.1f}%)")

            # ãƒãƒ£ãƒ³ã‚¯å˜ä½ã§ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†
            chunk_count = 0
            with open(file_path, 'r', encoding='utf-8') as f:
                chunk_lines = []

                for line_num, line in enumerate(f, 1):
                    chunk_lines.append(line)

                    if len(chunk_lines) >= self.chunk_size or line_num == total_lines:
                        # ãƒãƒ£ãƒ³ã‚¯å‡¦ç†
                        chunk_text = ''.join(chunk_lines)

                        try:
                            chunk_nodes = list(parser.parse_streaming_from_text(chunk_text))
                            parsed_chunks.append({
                                "chunk_id": chunk_count,
                                "nodes_count": len(chunk_nodes),
                                "lines_count": len(chunk_lines)
                            })

                        except Exception as e:
                            print(f"    ãƒãƒ£ãƒ³ã‚¯{chunk_count}ã‚¨ãƒ©ãƒ¼: {str(e)}")

                        chunk_lines = []
                        chunk_count += 1

                        if chunk_count % 100 == 0:
                            percent = (line_num / total_lines) * 100
                            print(f"  ãƒãƒ£ãƒ³ã‚¯å‡¦ç†: {chunk_count} ({percent:.1f}%)")
                            gc.collect()  # å®šæœŸçš„ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—

            total_nodes = sum(chunk["nodes_count"] for chunk in parsed_chunks)

            print(f"  ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è§£æå®Œäº†: {total_nodes:,} ãƒãƒ¼ãƒ‰")

            return {
                "streaming_completed": True,
                "total_chunks": len(parsed_chunks),
                "total_nodes": total_nodes,
                "parsed_chunks": parsed_chunks
            }

        except Exception as e:
            return {"streaming_error": str(e)}

    def _stage_optimized_processing(self, context: Dict) -> Dict[str, Any]:
        """ã‚¹ãƒ†ãƒ¼ã‚¸3: æœ€é©åŒ–å‡¦ç†"""

        try:
            from kumihan_formatter.parser import Parser

            file_path = context["file_path"]

            # æœ€é©åŒ–ãƒ‘ãƒ¼ã‚µãƒ¼ã§ã‚µãƒ³ãƒ—ãƒ«å‡¦ç†
            sample_size = min(1000, context.get("total_lines", 1000))

            print(f"  æœ€é©åŒ–å‡¦ç†ãƒ†ã‚¹ãƒˆ: {sample_size}è¡Œã‚µãƒ³ãƒ—ãƒ«")

            # ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆèª­ã¿è¾¼ã¿
            sample_lines = []
            with open(file_path, 'r', encoding='utf-8') as f:
                for i, line in enumerate(f):
                    if i >= sample_size:
                        break
                    sample_lines.append(line)

            sample_text = ''.join(sample_lines)

            # æœ€é©åŒ–ãƒ‘ãƒ¼ã‚µãƒ¼ãƒ†ã‚¹ãƒˆ
            parser = Parser()
            nodes = parser.parse_optimized(sample_text)

            print(f"  æœ€é©åŒ–å‡¦ç†å®Œäº†: {len(nodes)} ãƒãƒ¼ãƒ‰")

            return {
                "optimized_completed": True,
                "sample_nodes": len(nodes),
                "sample_size": sample_size
            }

        except Exception as e:
            return {"optimized_error": str(e)}

    def _stage_result_aggregation(self, context: Dict) -> Dict[str, Any]:
        """ã‚¹ãƒ†ãƒ¼ã‚¸4: çµæœé›†ç´„"""

        print("  çµæœé›†ç´„ä¸­...")

        success_stages = 0
        if context.get("file_reading_completed"):
            success_stages += 1
        if context.get("streaming_completed"):
            success_stages += 1
        if context.get("optimized_completed"):
            success_stages += 1

        success_rate = (success_stages / 3) * 100

        print(f"  å‡¦ç†æˆåŠŸç‡: {success_rate:.1f}%")

        return {
            "aggregation_completed": True,
            "success_stages": success_stages,
            "success_rate": success_rate,
            "processing_successful": success_stages >= 2  # 2æ®µéšä»¥ä¸ŠæˆåŠŸã§æˆåŠŸã¨ã¿ãªã™
        }

def test_all_ultra_large_files():
    """å…¨è¶…å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ†ã‚¹ãƒˆ"""

    print("ğŸš€ å…¨è¶…å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ãƒ†ã‚¹ãƒˆé–‹å§‹")
    print("=" * 60)

    processor = UltraLargeProcessor()

    # ãƒ†ã‚¹ãƒˆå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«
    ultra_large_dir = project_root / "samples" / "ultra_large"
    test_files = [
        ultra_large_dir / "40_ultra_large_200k.txt",
        ultra_large_dir / "41_ultra_large_300k.txt"
    ]

    results = []

    for file_path in test_files:
        print(f"\nğŸ“Š {file_path.name} å‡¦ç†ãƒ†ã‚¹ãƒˆ")
        print("-" * 50)

        result = processor.process_ultra_large_file(file_path)
        result["file_name"] = file_path.name
        results.append(result)

        if "error" in result:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {result['error']}")
        else:
            print(f"âœ… å‡¦ç†å®Œäº†: {result.get('processing_time', 0):.1f}ç§’")
            if result.get('processing_successful'):
                print(f"ğŸ‰ å‡¦ç†æˆåŠŸ: {result.get('success_rate', 0):.1f}%")
            else:
                print(f"âš ï¸  éƒ¨åˆ†æˆåŠŸ: {result.get('success_rate', 0):.1f}%")

    # ç·åˆè©•ä¾¡
    print(f"\nğŸ¯ ç·åˆè©•ä¾¡")
    print("=" * 60)

    successful_files = [r for r in results if r.get('processing_successful', False)]

    print(f"âœ… æˆåŠŸãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(successful_files)}/{len(results)}")

    if len(successful_files) == len(results):
        print("ğŸ‰ å…¨è¶…å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†ã«æˆåŠŸã—ã¾ã—ãŸï¼")
        return True
    else:
        print("âš ï¸  ä¸€éƒ¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ã§èª²é¡ŒãŒæ®‹ã£ã¦ã„ã¾ã™")
        return False

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""

    success = test_all_ultra_large_files()

    print(f"\nğŸ è¶…å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ãƒ†ã‚¹ãƒˆå®Œäº†")

    return 0 if success else 1

if __name__ == "__main__":
    exit(main())
