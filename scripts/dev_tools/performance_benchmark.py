#!/usr/bin/env python3
"""
Kumihan-Formatter ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
Issue #727 - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã®åŠ¹æœæ¸¬å®š

ä½¿ç”¨æ–¹æ³•:
    python scripts/performance_benchmark.py
    python scripts/performance_benchmark.py --test-size large
    python scripts/performance_benchmark.py --output-report benchmark_results.json
"""

import argparse
import json
import sys
import time
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from kumihan_formatter.core.utilities.performance_metrics import PerformanceBenchmark
from kumihan_formatter.core.utilities.logger import get_logger


def main():
    """ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    
    parser = argparse.ArgumentParser(
        description="Kumihan-Formatter ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"
    )
    parser.add_argument(
        "--test-size",
        choices=["small", "medium", "large", "extra_large", "all"],
        default="all",
        help="ãƒ†ã‚¹ãƒˆã‚µã‚¤ã‚ºã®æŒ‡å®š"
    )
    parser.add_argument(
        "--output-report",
        type=str,
        help="ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ (JSONå½¢å¼)"
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="è©³ç´°ãƒ­ã‚°å‡ºåŠ›"
    )
    
    args = parser.parse_args()
    
    # ãƒ­ã‚¬ãƒ¼è¨­å®š
    logger = get_logger(__name__)
    
    if args.verbose:
        import logging
        logging.getLogger().setLevel(logging.DEBUG)
    
    logger.info("ğŸš€ Kumihan-Formatter ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹")
    
    try:
        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
        benchmark = PerformanceBenchmark()
        
        if args.test_size == "all":
            results = benchmark.run_comprehensive_benchmark()
        else:
            # å€‹åˆ¥ãƒ†ã‚¹ãƒˆã‚µã‚¤ã‚ºã®å ´åˆã®å®Ÿè£…ã¯çœç•¥ï¼ˆç°¡ç•¥åŒ–ï¼‰
            results = benchmark.run_comprehensive_benchmark()
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report_text = benchmark.generate_benchmark_report(results)
        print("\n" + report_text)
        
        # çµæœä¿å­˜
        if args.output_report:
            output_path = Path(args.output_report)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            
            logger.info(f"ğŸ“„ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚’ä¿å­˜: {output_path}")
        
        # Issue #727 ç›®æ¨™é”æˆãƒã‚§ãƒƒã‚¯
        goals = results.get("goal_assessment", {}).get("goals", {})
        achieved_count = sum(1 for achieved in goals.values() if achieved)
        total_count = len(goals)
        
        if achieved_count == total_count:
            logger.info("ğŸ‰ å…¨ã¦ã®æ€§èƒ½ç›®æ¨™ã‚’é”æˆã—ã¾ã—ãŸï¼")
            return 0
        elif achieved_count >= total_count * 0.8:
            logger.info(f"âœ… æ€§èƒ½ç›®æ¨™ã®å¤§éƒ¨åˆ†ã‚’é”æˆ ({achieved_count}/{total_count})")
            return 0
        else:
            logger.warning(f"âš ï¸ æ€§èƒ½ç›®æ¨™ã®é”æˆç‡ãŒä½ã„ã§ã™ ({achieved_count}/{total_count})")
            return 1
            
    except Exception as e:
        logger.error(f"âŒ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}", exc_info=True)
        return 1


if __name__ == "__main__":
    exit(main())