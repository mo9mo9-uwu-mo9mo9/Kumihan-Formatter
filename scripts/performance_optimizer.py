#!/usr/bin/env python3
"""
Performance Optimizer - Important Technical Debt Fix
ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ 

ç›®çš„: ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£æ”¹å–„ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
- ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°
- ä¸¦åˆ—å‡¦ç†æœ€é©åŒ–
- I/OåŠ¹ç‡åŒ–
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æœ€é©åŒ–
"""

import asyncio
import concurrent.futures
import functools
import hashlib
import os
import sqlite3
import threading
import time
from pathlib import Path
from typing import Dict, List, Optional, Any, Callable, Tuple, Union
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from collections import OrderedDict
import json
import pickle

from kumihan_formatter.core.utilities.logger import get_logger

logger = get_logger(__name__)

@dataclass
class CacheEntry:
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¨ãƒ³ãƒˆãƒª"""
    key: str
    value: Any
    created_at: datetime
    accessed_at: datetime
    access_count: int
    size_bytes: int
    ttl_seconds: Optional[int] = None

@dataclass
class PerformanceMetrics:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    operation_name: str
    execution_time: float
    memory_usage: int
    cache_hit: bool
    timestamp: datetime

class LRUCache:
    """LRUï¼ˆLeast Recently Usedï¼‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥"""
    
    def __init__(self, max_size: int = 1000, ttl_seconds: Optional[int] = None):
        self.max_size = max_size
        self.ttl_seconds = ttl_seconds
        self.cache = OrderedDict()
        self._lock = threading.RLock()
        self._stats = {
            'hits': 0,
            'misses': 0,
            'evictions': 0,
            'size': 0
        }
    
    def get(self, key: str) -> Optional[Any]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å€¤ã‚’å–å¾—"""
        with self._lock:
            if key not in self.cache:
                self._stats['misses'] += 1
                return None
            
            entry = self.cache[key]
            
            # TTLãƒã‚§ãƒƒã‚¯
            if self._is_expired(entry):
                del self.cache[key]
                self._stats['misses'] += 1
                return None
            
            # LRUã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ
            entry.accessed_at = datetime.now()
            entry.access_count += 1
            self.cache.move_to_end(key)
            
            self._stats['hits'] += 1
            return entry.value
    
    def put(self, key: str, value: Any, ttl_seconds: Optional[int] = None):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«å€¤ã‚’è¨­å®š"""
        with self._lock:
            size_bytes = self._calculate_size(value)
            
            entry = CacheEntry(
                key=key,
                value=value,
                created_at=datetime.now(),
                accessed_at=datetime.now(),
                access_count=1,
                size_bytes=size_bytes,
                ttl_seconds=ttl_seconds or self.ttl_seconds
            )
            
            # æ—¢å­˜ã‚¨ãƒ³ãƒˆãƒªã®æ›´æ–°
            if key in self.cache:
                self.cache[key] = entry
                self.cache.move_to_end(key)
            else:
                # æ–°è¦ã‚¨ãƒ³ãƒˆãƒªã®è¿½åŠ 
                self.cache[key] = entry
                
                # ã‚µã‚¤ã‚ºåˆ¶é™ãƒã‚§ãƒƒã‚¯
                while len(self.cache) > self.max_size:
                    oldest_key = next(iter(self.cache))
                    del self.cache[oldest_key]
                    self._stats['evictions'] += 1
            
            self._stats['size'] = len(self.cache)
    
    def clear(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢"""
        with self._lock:
            self.cache.clear()
            self._stats['size'] = 0
    
    def _is_expired(self, entry: CacheEntry) -> bool:
        """TTLæœŸé™åˆ‡ã‚Œãƒã‚§ãƒƒã‚¯"""
        if entry.ttl_seconds is None:
            return False
        
        elapsed = (datetime.now() - entry.created_at).total_seconds()
        return elapsed > entry.ttl_seconds
    
    def _calculate_size(self, value: Any) -> int:
        """å€¤ã®ã‚µã‚¤ã‚ºè¨ˆç®—"""
        try:
            return len(pickle.dumps(value))
        except:
            return len(str(value))
    
    def get_stats(self) -> Dict:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆå–å¾—"""
        with self._lock:
            total_requests = self._stats['hits'] + self._stats['misses']
            hit_rate = self._stats['hits'] / max(total_requests, 1) * 100
            
            return {
                **self._stats,
                'hit_rate': hit_rate,
                'total_requests': total_requests
            }

class PersistentCache:
    """æ°¸ç¶šåŒ–ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆSQLiteä½¿ç”¨ï¼‰"""
    
    def __init__(self, db_path: Path, table_name: str = "cache"):
        self.db_path = db_path
        self.table_name = table_name
        self._lock = threading.RLock()
        self._init_database()
    
    def _init_database(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–"""
        with sqlite3.connect(str(self.db_path)) as conn:
            conn.execute(f"""
                CREATE TABLE IF NOT EXISTS {self.table_name} (
                    key TEXT PRIMARY KEY,
                    value BLOB,
                    created_at TIMESTAMP,
                    accessed_at TIMESTAMP,
                    access_count INTEGER,
                    ttl_seconds INTEGER
                )
            """)
            conn.execute(f"""
                CREATE INDEX IF NOT EXISTS idx_{self.table_name}_accessed 
                ON {self.table_name}(accessed_at)
            """)
    
    def get(self, key: str) -> Optional[Any]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å€¤ã‚’å–å¾—"""
        with self._lock:
            try:
                with sqlite3.connect(str(self.db_path)) as conn:
                    cursor = conn.execute(f"""
                        SELECT value, created_at, ttl_seconds 
                        FROM {self.table_name} 
                        WHERE key = ?
                    """, (key,))
                    
                    row = cursor.fetchone()
                    if not row:
                        return None
                    
                    value_blob, created_at_str, ttl_seconds = row
                    created_at = datetime.fromisoformat(created_at_str)
                    
                    # TTLãƒã‚§ãƒƒã‚¯
                    if ttl_seconds and (datetime.now() - created_at).total_seconds() > ttl_seconds:
                        self.delete(key)
                        return None
                    
                    # ã‚¢ã‚¯ã‚»ã‚¹æƒ…å ±æ›´æ–°
                    conn.execute(f"""
                        UPDATE {self.table_name} 
                        SET accessed_at = ?, access_count = access_count + 1
                        WHERE key = ?
                    """, (datetime.now().isoformat(), key))
                    
                    return pickle.loads(value_blob)
                    
            except Exception as e:
                logger.error(f"æ°¸ç¶šã‚­ãƒ£ãƒƒã‚·ãƒ¥å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                return None
    
    def put(self, key: str, value: Any, ttl_seconds: Optional[int] = None):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«å€¤ã‚’è¨­å®š"""
        with self._lock:
            try:
                value_blob = pickle.dumps(value)
                now = datetime.now().isoformat()
                
                with sqlite3.connect(str(self.db_path)) as conn:
                    conn.execute(f"""
                        INSERT OR REPLACE INTO {self.table_name}
                        (key, value, created_at, accessed_at, access_count, ttl_seconds)
                        VALUES (?, ?, ?, ?, 1, ?)
                    """, (key, value_blob, now, now, ttl_seconds))
                    
            except Exception as e:
                logger.error(f"æ°¸ç¶šã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
    
    def delete(self, key: str):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å‰Šé™¤"""
        with self._lock:
            try:
                with sqlite3.connect(str(self.db_path)) as conn:
                    conn.execute(f"DELETE FROM {self.table_name} WHERE key = ?", (key,))
            except Exception as e:
                logger.error(f"æ°¸ç¶šã‚­ãƒ£ãƒƒã‚·ãƒ¥å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")
    
    def cleanup_expired(self):
        """æœŸé™åˆ‡ã‚Œã‚¨ãƒ³ãƒˆãƒªã®å‰Šé™¤"""
        with self._lock:
            try:
                now = datetime.now()
                with sqlite3.connect(str(self.db_path)) as conn:
                    cursor = conn.execute(f"""
                        SELECT key, created_at, ttl_seconds 
                        FROM {self.table_name} 
                        WHERE ttl_seconds IS NOT NULL
                    """)
                    
                    expired_keys = []
                    for key, created_at_str, ttl_seconds in cursor:
                        created_at = datetime.fromisoformat(created_at_str)
                        if (now - created_at).total_seconds() > ttl_seconds:
                            expired_keys.append(key)
                    
                    for key in expired_keys:
                        conn.execute(f"DELETE FROM {self.table_name} WHERE key = ?", (key,))
                    
                    logger.info(f"æœŸé™åˆ‡ã‚Œã‚­ãƒ£ãƒƒã‚·ãƒ¥å‰Šé™¤: {len(expired_keys)}ä»¶")
                    
            except Exception as e:
                logger.error(f"æœŸé™åˆ‡ã‚Œã‚­ãƒ£ãƒƒã‚·ãƒ¥å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")

class ParallelExecutor:
    """ä¸¦åˆ—å®Ÿè¡Œæœ€é©åŒ–ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, max_workers: Optional[int] = None):
        # CPUæ•°ã®å–å¾—ï¼ˆmacOSäº’æ›ï¼‰
        try:
            cpu_count = len(os.sched_getaffinity(0))
        except (AttributeError, OSError):
            cpu_count = os.cpu_count() or 4
        
        self.max_workers = max_workers or min(32, cpu_count + 4)
        self.thread_pool = concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers)
        self.process_pool = concurrent.futures.ProcessPoolExecutor(max_workers=self.max_workers)
    
    def execute_parallel(self, 
                        func: Callable, 
                        items: List[Any], 
                        use_processes: bool = False,
                        timeout: Optional[float] = None) -> List[Any]:
        """ä¸¦åˆ—å®Ÿè¡Œ"""
        executor = self.process_pool if use_processes else self.thread_pool
        
        try:
            futures = [executor.submit(func, item) for item in items]
            results = []
            
            for future in concurrent.futures.as_completed(futures, timeout=timeout):
                try:
                    result = future.result()
                    results.append(result)
                except Exception as e:
                    logger.error(f"ä¸¦åˆ—å®Ÿè¡Œã‚¿ã‚¹ã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
                    results.append(None)
            
            return results
            
        except concurrent.futures.TimeoutError:
            logger.error("ä¸¦åˆ—å®Ÿè¡Œã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
            return []
    
    async def execute_async(self, 
                          func: Callable, 
                          items: List[Any]) -> List[Any]:
        """éåŒæœŸä¸¦åˆ—å®Ÿè¡Œ"""
        semaphore = asyncio.Semaphore(self.max_workers)
        
        async def bounded_func(item):
            async with semaphore:
                return await asyncio.get_event_loop().run_in_executor(None, func, item)
        
        tasks = [bounded_func(item) for item in items]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        return [r for r in results if not isinstance(r, Exception)]
    
    def shutdown(self):
        """å®Ÿè¡Œãƒ—ãƒ¼ãƒ«çµ‚äº†"""
        self.thread_pool.shutdown(wait=True)
        self.process_pool.shutdown(wait=True)

class PerformanceOptimizer:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.cache_dir = project_root / ".performance_cache"
        self.cache_dir.mkdir(exist_ok=True)
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ 
        self.memory_cache = LRUCache(max_size=2000, ttl_seconds=3600)  # 1æ™‚é–“TTL
        self.persistent_cache = PersistentCache(
            self.cache_dir / "performance_cache.db"
        )
        
        # ä¸¦åˆ—å®Ÿè¡Œã‚·ã‚¹ãƒ†ãƒ 
        self.parallel_executor = ParallelExecutor()
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–
        self.metrics_history = []
        self._lock = threading.RLock()
    
    def cached_function(self, 
                       ttl_seconds: Optional[int] = None,
                       use_persistent: bool = False,
                       cache_key_func: Optional[Callable] = None):
        """é–¢æ•°ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""
        def decorator(func: Callable):
            @functools.wraps(func)
            def wrapper(*args, **kwargs):
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”Ÿæˆ
                if cache_key_func:
                    cache_key = cache_key_func(*args, **kwargs)
                else:
                    cache_key = self._generate_cache_key(func.__name__, args, kwargs)
                
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—è©¦è¡Œ
                cache = self.persistent_cache if use_persistent else self.memory_cache
                cached_value = cache.get(cache_key)
                
                start_time = time.time()
                
                if cached_value is not None:
                    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ
                    execution_time = time.time() - start_time
                    self._record_metrics(func.__name__, execution_time, 0, True)
                    return cached_value
                
                # é–¢æ•°å®Ÿè¡Œ
                import psutil
                process = psutil.Process()
                memory_before = process.memory_info().rss
                
                result = func(*args, **kwargs)
                
                memory_after = process.memory_info().rss
                execution_time = time.time() - start_time
                memory_usage = memory_after - memory_before
                
                # çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                cache.put(cache_key, result, ttl_seconds)
                
                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
                self._record_metrics(func.__name__, execution_time, memory_usage, False)
                
                return result
                
            return wrapper
        return decorator
    
    def batch_process(self, 
                     func: Callable, 
                     items: List[Any], 
                     batch_size: int = 100,
                     use_parallel: bool = True) -> List[Any]:
        """ãƒãƒƒãƒå‡¦ç†æœ€é©åŒ–"""
        if not items:
            return []
        
        results = []
        
        # ãƒãƒƒãƒã‚µã‚¤ã‚ºã«åˆ†å‰²
        for i in range(0, len(items), batch_size):
            batch = items[i:i+batch_size]
            
            if use_parallel and len(batch) > 1:
                # ä¸¦åˆ—å®Ÿè¡Œ
                batch_results = self.parallel_executor.execute_parallel(func, batch)
            else:
                # é€æ¬¡å®Ÿè¡Œ
                batch_results = [func(item) for item in batch]
            
            results.extend(batch_results)
        
        return results
    
    def optimize_file_operations(self, 
                                file_paths: List[Path], 
                                operation: Callable,
                                chunk_size: int = 8192) -> List[Any]:
        """ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œæœ€é©åŒ–"""
        def optimized_operation(file_path: Path):
            try:
                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
                if not file_path.exists():
                    return None
                
                file_size = file_path.stat().st_size
                
                # å°ã•ãªãƒ•ã‚¡ã‚¤ãƒ«ã¯é€šå¸¸å‡¦ç†
                if file_size < chunk_size:
                    return operation(file_path)
                
                # å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ã¯ãƒãƒ£ãƒ³ã‚¯èª­ã¿å–ã‚Š
                results = []
                with open(file_path, 'rb') as f:
                    while True:
                        chunk = f.read(chunk_size)
                        if not chunk:
                            break
                        
                        chunk_result = operation(chunk)
                        if chunk_result:
                            results.append(chunk_result)
                
                return results
                
            except Exception as e:
                logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œæœ€é©åŒ–ã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
                return None
        
        return self.batch_process(optimized_operation, file_paths)
    
    def _generate_cache_key(self, func_name: str, args: tuple, kwargs: dict) -> str:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”Ÿæˆ"""
        key_data = {
            'function': func_name,
            'args': str(args),
            'kwargs': str(sorted(kwargs.items()))
        }
        
        key_string = json.dumps(key_data, sort_keys=True)
        return hashlib.md5(key_string.encode()).hexdigest()
    
    def _record_metrics(self, operation_name: str, execution_time: float, 
                       memory_usage: int, cache_hit: bool):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²"""
        with self._lock:
            metric = PerformanceMetrics(
                operation_name=operation_name,
                execution_time=execution_time,
                memory_usage=memory_usage,
                cache_hit=cache_hit,
                timestamp=datetime.now()
            )
            
            self.metrics_history.append(metric)
            
            # å±¥æ­´ã‚µã‚¤ã‚ºåˆ¶é™
            if len(self.metrics_history) > 10000:
                self.metrics_history = self.metrics_history[-5000:]
    
    def get_performance_report(self) -> Dict:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        with self._lock:
            if not self.metrics_history:
                return {"error": "ãƒ¡ãƒˆãƒªã‚¯ã‚¹å±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“"}
            
            # çµ±è¨ˆè¨ˆç®—
            total_operations = len(self.metrics_history)
            cache_hits = sum(1 for m in self.metrics_history if m.cache_hit)
            cache_hit_rate = cache_hits / total_operations * 100
            
            execution_times = [m.execution_time for m in self.metrics_history]
            avg_execution_time = sum(execution_times) / len(execution_times)
            
            # é–¢æ•°åˆ¥çµ±è¨ˆ
            function_stats = {}
            for metric in self.metrics_history:
                func_name = metric.operation_name
                if func_name not in function_stats:
                    function_stats[func_name] = {
                        'count': 0,
                        'total_time': 0,
                        'cache_hits': 0
                    }
                
                function_stats[func_name]['count'] += 1
                function_stats[func_name]['total_time'] += metric.execution_time
                if metric.cache_hit:
                    function_stats[func_name]['cache_hits'] += 1
            
            # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            report = {
                "total_operations": total_operations,
                "cache_hit_rate": cache_hit_rate,
                "avg_execution_time": avg_execution_time,
                "memory_cache_stats": self.memory_cache.get_stats(),
                "function_statistics": {
                    func: {
                        **stats,
                        'avg_time': stats['total_time'] / stats['count'],
                        'cache_hit_rate': stats['cache_hits'] / stats['count'] * 100
                    } for func, stats in function_stats.items()
                },
                "timestamp": datetime.now().isoformat()
            }
            
            return report
    
    def cleanup_caches(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        logger.info("ğŸ§¹ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—é–‹å§‹...")
        
        # ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢
        self.memory_cache.clear()
        
        # æ°¸ç¶šã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æœŸé™åˆ‡ã‚Œå‰Šé™¤
        self.persistent_cache.cleanup_expired()
        
        logger.info("âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†")
    
    def shutdown(self):
        """æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ çµ‚äº†"""
        logger.info("âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ çµ‚äº†ä¸­...")
        
        # ä¸¦åˆ—å®Ÿè¡Œãƒ—ãƒ¼ãƒ«çµ‚äº†
        self.parallel_executor.shutdown()
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        self.cleanup_caches()
        
        logger.info("ğŸ‘‹ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ çµ‚äº†å®Œäº†")

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
performance_optimizer = None

def init_performance_optimizer(project_root: Path):
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
    global performance_optimizer
    performance_optimizer = PerformanceOptimizer(project_root)
    return performance_optimizer

def get_performance_optimizer() -> Optional[PerformanceOptimizer]:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ å–å¾—"""
    return performance_optimizer

# ä¾¿åˆ©ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿
def cached(ttl_seconds: Optional[int] = None, use_persistent: bool = False):
    """ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ä¾¿åˆ©ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""
    def decorator(func):
        if performance_optimizer:
            return performance_optimizer.cached_function(ttl_seconds, use_persistent)(func)
        return func
    return decorator

# ãƒ†ã‚¹ãƒˆé–¢æ•°
def test_performance_optimizer():
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ"""
    logger.info("ğŸ§ª ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    project_root = Path(__file__).parent.parent
    optimizer = init_performance_optimizer(project_root)
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ†ã‚¹ãƒˆ
    @optimizer.cached_function(ttl_seconds=10)
    def expensive_operation(n: int) -> int:
        time.sleep(0.1)  # é‡ã„å‡¦ç†ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        return n * n
    
    # åˆå›å®Ÿè¡Œï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹ï¼‰
    result1 = expensive_operation(10)
    assert result1 == 100
    
    # 2å›ç›®å®Ÿè¡Œï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆï¼‰
    result2 = expensive_operation(10)
    assert result2 == 100
    
    # ä¸¦åˆ—å‡¦ç†ãƒ†ã‚¹ãƒˆ
    items = list(range(10))
    results = optimizer.batch_process(expensive_operation, items, use_parallel=True)
    assert len(results) == 10
    
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report = optimizer.get_performance_report()
    logger.info(f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆ: {report}")
    
    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    optimizer.shutdown()
    
    logger.info("ğŸ¯ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆå®Œäº†")

if __name__ == "__main__":
    test_performance_optimizer()