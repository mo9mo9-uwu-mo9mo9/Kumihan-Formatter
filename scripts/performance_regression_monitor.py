#!/usr/bin/env python3
"""
Performance Regression Monitor - Issue #640 Phase 3
ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 

ç›®çš„: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°ã®è‡ªå‹•æ¤œå‡ºãƒ»ç›£è¦–
- ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°æ¤œå‡º
- ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯æ¤œå‡º
- CPUãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æ
"""

import os
import sys
import time
import psutil
import subprocess
import threading
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple, Any, Callable
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
import json
import statistics
import gc
import tracemalloc
import cProfile
import pstats
import io
import tempfile

from scripts.tdd_system_base import TDDSystemBase, create_tdd_config, TDDSystemError
from scripts.secure_subprocess import secure_run
from kumihan_formatter.core.utilities.logger import get_logger

logger = get_logger(__name__)

class PerformanceRisk(Enum):
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«"""
    CRITICAL = "critical"
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"
    SAFE = "safe"

@dataclass
class BenchmarkResult:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ"""
    test_name: str
    execution_time: float
    memory_usage: int
    cpu_usage: float
    iterations: int
    input_size: int
    throughput: float  # operations per second
    timestamp: datetime
    metadata: Dict[str, Any]

@dataclass
class PerformanceRegression:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°"""
    test_name: str
    metric_name: str
    baseline_value: float
    current_value: float
    regression_percentage: float
    severity: PerformanceRisk
    description: str
    recommendation: str
    detected_at: datetime
    confidence_level: float

@dataclass
class MemoryLeak:
    """ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯"""
    function_name: str
    leaked_memory: int
    leak_rate: float  # bytes per iteration
    iterations_tested: int
    confidence_level: float
    stack_trace: List[str]
    recommendation: str

@dataclass
class PerformanceTestResult:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆçµæœ"""
    total_benchmarks: int
    benchmark_results: List[BenchmarkResult]
    regressions: List[PerformanceRegression]
    memory_leaks: List[MemoryLeak]
    performance_baselines: Dict[str, Dict[str, float]]
    system_info: Dict[str, Any]
    test_duration: float
    timestamp: datetime
    overall_risk: PerformanceRisk
    recommendations: List[str]

class PerformanceRegressionMonitor(TDDSystemBase):
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, project_root: Path):
        config = create_tdd_config(project_root)
        super().__init__(config)
        
        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.benchmark_data_dir = project_root / ".benchmark_data"
        self.benchmark_data_dir.mkdir(exist_ok=True)
        
        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«
        self.baseline_file = self.benchmark_data_dir / "performance_baselines.json"
        self.history_file = self.benchmark_data_dir / "benchmark_history.json"
        
        # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±å–å¾—
        self.system_info = {
            "cpu_count": psutil.cpu_count(),
            "cpu_freq": psutil.cpu_freq()._asdict() if psutil.cpu_freq() else {},
            "memory_total": psutil.virtual_memory().total,
            "platform": sys.platform,
            "python_version": sys.version
        }
        
        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆå®šç¾©
        self.benchmark_tests = [
            self._benchmark_startup_time,
            self._benchmark_file_processing,
            self._benchmark_memory_usage,
            self._benchmark_concurrent_operations,
            self._benchmark_large_input_handling,
            self._benchmark_io_operations,
            self._benchmark_cpu_intensive_tasks
        ]
        
        # å›å¸°æ¤œå‡ºè¨­å®š
        self.regression_thresholds = {
            "execution_time": 0.15,    # 15%ä»¥ä¸Šã®åŠ£åŒ–
            "memory_usage": 0.20,      # 20%ä»¥ä¸Šã®å¢—åŠ 
            "cpu_usage": 0.25,         # 25%ä»¥ä¸Šã®å¢—åŠ 
            "throughput": -0.10        # 10%ä»¥ä¸Šã®æ¸›å°‘
        }
        
        self.benchmark_results = []
        self.regressions = []
        self.memory_leaks = []
        self.performance_baselines = {}
    
    def initialize(self) -> bool:
        """ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        logger.info("ğŸ” ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ä¸­...")
        
        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³èª­ã¿è¾¼ã¿
        self._load_performance_baselines()
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ç¢ºèª
        memory = psutil.virtual_memory()
        if memory.available < 512 * 1024 * 1024:  # 512MBæœªæº€
            logger.warning(f"ä½¿ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒªãŒå°‘ãªã„: {memory.available / (1024**2):.1f}MB")
        
        # äº‹å‰æ¡ä»¶ç¢ºèª
        issues = self.validate_preconditions()
        if issues:
            logger.error("åˆæœŸåŒ–å¤±æ•—:")
            for issue in issues:
                logger.error(f"  - {issue}")
            return False
        
        logger.info("âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
        return True
    
    def execute_main_operation(self) -> PerformanceTestResult:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°ç›£è¦–å®Ÿè¡Œ"""
        logger.info("ğŸš€ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°ç›£è¦–é–‹å§‹...")
        
        start_time = datetime.now()
        
        try:
            # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
            self._run_benchmark_tests()
            
            # å›å¸°æ¤œå‡º
            self._detect_performance_regressions()
            
            # ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯æ¤œå‡º
            self._detect_memory_leaks()
            
            # çµæœã‚’åˆ†æ
            result = self._analyze_results(start_time)
            
            # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ›´æ–°
            self._update_performance_baselines()
            
            # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            self._generate_performance_report(result)
            
            logger.info(f"âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°ç›£è¦–å®Œäº†: {len(self.regressions)}ä»¶ã®å›å¸°ã€{len(self.memory_leaks)}ä»¶ã®ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ç™ºè¦‹")
            return result
            
        except Exception as e:
            logger.error(f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            raise TDDSystemError(f"ç›£è¦–å®Ÿè¡Œå¤±æ•—: {e}")
    
    def _load_performance_baselines(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³èª­ã¿è¾¼ã¿"""
        if self.baseline_file.exists():
            try:
                with open(self.baseline_file, 'r', encoding='utf-8') as f:
                    self.performance_baselines = json.load(f)
                logger.info(f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³èª­ã¿è¾¼ã¿å®Œäº†: {len(self.performance_baselines)}ãƒ†ã‚¹ãƒˆ")
            except Exception as e:
                logger.warning(f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _run_benchmark_tests(self):
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        logger.info("âš¡ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
        
        total_tests = len(self.benchmark_tests)
        
        for i, benchmark_test in enumerate(self.benchmark_tests, 1):
            test_name = benchmark_test.__name__
            logger.info(f"[{i}/{total_tests}] {test_name} å®Ÿè¡Œä¸­...")
            
            try:
                # ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ
                gc.collect()
                
                # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
                start_time = time.time()
                result = benchmark_test()
                duration = time.time() - start_time
                
                if result:
                    result.execution_time = duration
                    result.timestamp = datetime.now()
                    self.benchmark_results.append(result)
                    
                    logger.debug(f"âœ… {test_name} å®Œäº†: {duration:.3f}s, {result.memory_usage/1024/1024:.1f}MB")
                else:
                    logger.warning(f"âŒ {test_name} å¤±æ•—")
                    
            except Exception as e:
                logger.error(f"âŒ {test_name} ä¾‹å¤–: {e}")
        
        logger.info(f"ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆå®Œäº†: {len(self.benchmark_results)}/{total_tests}")
    
    def _benchmark_startup_time(self) -> Optional[BenchmarkResult]:
        """èµ·å‹•æ™‚é–“ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        try:
            iterations = 5
            times = []
            
            for _ in range(iterations):
                start_time = time.time()
                # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ä¸»è¦ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
                result = subprocess.run([
                    sys.executable, '-c',
                    'import sys; sys.path.insert(0, "."); import kumihan_formatter; print("OK")'
                ], capture_output=True, text=True, cwd=self.project_root)
                
                if result.returncode == 0:
                    elapsed = time.time() - start_time
                    times.append(elapsed)
                else:
                    logger.warning(f"èµ·å‹•ãƒ†ã‚¹ãƒˆå¤±æ•—: {result.stderr}")
            
            if times:
                avg_time = statistics.mean(times)
                process = psutil.Process()
                memory_info = process.memory_info()
                
                return BenchmarkResult(
                    test_name="startup_time",
                    execution_time=avg_time,
                    memory_usage=memory_info.rss,
                    cpu_usage=process.cpu_percent(),
                    iterations=iterations,
                    input_size=0,
                    throughput=1.0 / avg_time,
                    timestamp=datetime.now(),
                    metadata={"times": times, "std_dev": statistics.stdev(times) if len(times) > 1 else 0}
                )
                
        except Exception as e:
            logger.error(f"èµ·å‹•æ™‚é–“ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
            
        return None
    
    def _benchmark_file_processing(self) -> Optional[BenchmarkResult]:
        """ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        try:
            # ãƒ†ã‚¹ãƒˆç”¨ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
            test_content = "ãƒ†ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„\n" * 1000
            iterations = 10
            
            with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8') as temp_file:
                temp_file.write(test_content)
                temp_path = Path(temp_file.name)
            
            try:
                times = []
                memory_usage_before = psutil.Process().memory_info().rss
                
                for _ in range(iterations):
                    start_time = time.time()
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿æ›¸ãå‡¦ç†
                    with open(temp_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    processed = content.upper()
                    
                    with open(temp_path, 'w', encoding='utf-8') as f:
                        f.write(processed)
                    
                    elapsed = time.time() - start_time
                    times.append(elapsed)
                
                memory_usage_after = psutil.Process().memory_info().rss
                memory_delta = memory_usage_after - memory_usage_before
                
                if times:
                    avg_time = statistics.mean(times)
                    file_size = temp_path.stat().st_size
                    
                    return BenchmarkResult(
                        test_name="file_processing",
                        execution_time=avg_time,
                        memory_usage=max(memory_delta, 0),
                        cpu_usage=psutil.Process().cpu_percent(),
                        iterations=iterations,
                        input_size=file_size,
                        throughput=file_size / avg_time,
                        timestamp=datetime.now(),
                        metadata={"file_size": file_size, "times": times}
                    )
                    
            finally:
                temp_path.unlink(missing_ok=True)
                
        except Exception as e:
            logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
            
        return None
    
    def _benchmark_memory_usage(self) -> Optional[BenchmarkResult]:
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        try:
            tracemalloc.start()
            initial_memory = psutil.Process().memory_info().rss
            
            # ãƒ¡ãƒ¢ãƒªé›†ç´„çš„ãªã‚¿ã‚¹ã‚¯
            data_structures = []
            iterations = 1000
            
            start_time = time.time()
            
            for i in range(iterations):
                # æ§˜ã€…ãªãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’ä½œæˆ
                data = {
                    'list': list(range(100)),
                    'dict': {f'key_{j}': f'value_{j}' for j in range(50)},
                    'string': 'x' * 100,
                    'tuple': tuple(range(50))
                }
                data_structures.append(data)
                
                # å®šæœŸçš„ã«ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒã‚§ãƒƒã‚¯
                if i % 100 == 0:
                    gc.collect()
            
            execution_time = time.time() - start_time
            
            # æœ€çµ‚ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
            final_memory = psutil.Process().memory_info().rss
            memory_delta = final_memory - initial_memory
            
            # ãƒ¡ãƒ¢ãƒªçµ±è¨ˆå–å¾—
            current_memory, peak_memory = tracemalloc.get_traced_memory()
            tracemalloc.stop()
            
            # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            data_structures.clear()
            gc.collect()
            
            return BenchmarkResult(
                test_name="memory_usage",
                execution_time=execution_time,
                memory_usage=memory_delta,
                cpu_usage=psutil.Process().cpu_percent(),
                iterations=iterations,
                input_size=iterations * 4,  # 4 data structures per iteration
                throughput=iterations / execution_time,
                timestamp=datetime.now(),
                metadata={
                    "peak_memory": peak_memory,
                    "current_memory": current_memory,
                    "initial_rss": initial_memory,
                    "final_rss": final_memory
                }
            )
            
        except Exception as e:
            logger.error(f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
            if tracemalloc.is_tracing():
                tracemalloc.stop()
                
        return None
    
    def _benchmark_concurrent_operations(self) -> Optional[BenchmarkResult]:
        """ä¸¦è¡Œå‡¦ç†ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        try:
            thread_count = 4
            operations_per_thread = 250
            results = []
            errors = []
            
            def worker_task(task_id):
                try:
                    start_time = time.time()
                    # CPUé›†ç´„çš„ã‚¿ã‚¹ã‚¯
                    result = sum(i * i for i in range(1000))
                    elapsed = time.time() - start_time
                    results.append((task_id, result, elapsed))
                except Exception as e:
                    errors.append((task_id, str(e)))
            
            # ä¸¦è¡Œå®Ÿè¡Œ
            start_time = time.time()
            initial_memory = psutil.Process().memory_info().rss
            
            threads = []
            for i in range(thread_count):
                thread = threading.Thread(target=worker_task, args=(i,))
                threads.append(thread)
                thread.start()
            
            # å…¨ã‚¹ãƒ¬ãƒƒãƒ‰å®Œäº†å¾…æ©Ÿ
            for thread in threads:
                thread.join()
            
            total_time = time.time() - start_time
            final_memory = psutil.Process().memory_info().rss
            
            if len(results) == thread_count and not errors:
                return BenchmarkResult(
                    test_name="concurrent_operations",
                    execution_time=total_time,
                    memory_usage=final_memory - initial_memory,
                    cpu_usage=psutil.Process().cpu_percent(),
                    iterations=thread_count,
                    input_size=operations_per_thread,
                    throughput=thread_count / total_time,
                    timestamp=datetime.now(),
                    metadata={
                        "thread_count": thread_count,
                        "operations_per_thread": operations_per_thread,
                        "worker_times": [r[2] for r in results],
                        "errors": errors
                    }
                )
            else:
                logger.warning(f"ä¸¦è¡Œå‡¦ç†ãƒ†ã‚¹ãƒˆå¤±æ•—: {len(errors)}ã‚¨ãƒ©ãƒ¼")
                
        except Exception as e:
            logger.error(f"ä¸¦è¡Œå‡¦ç†ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
            
        return None
    
    def _benchmark_large_input_handling(self) -> Optional[BenchmarkResult]:
        """å¤§å…¥åŠ›å‡¦ç†ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        try:
            # å¤§ããªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
            large_data = list(range(100000))
            iterations = 5
            
            times = []
            initial_memory = psutil.Process().memory_info().rss
            
            for _ in range(iterations):
                start_time = time.time()
                
                # ãƒ‡ãƒ¼ã‚¿å‡¦ç†æ“ä½œ
                filtered_data = [x for x in large_data if x % 2 == 0]
                sorted_data = sorted(filtered_data, reverse=True)
                result_sum = sum(sorted_data[:1000])
                
                elapsed = time.time() - start_time
                times.append(elapsed)
            
            final_memory = psutil.Process().memory_info().rss
            
            if times:
                avg_time = statistics.mean(times)
                
                return BenchmarkResult(
                    test_name="large_input_handling",
                    execution_time=avg_time,
                    memory_usage=final_memory - initial_memory,
                    cpu_usage=psutil.Process().cpu_percent(),
                    iterations=iterations,
                    input_size=len(large_data),
                    throughput=len(large_data) / avg_time,
                    timestamp=datetime.now(),
                    metadata={
                        "input_size": len(large_data),
                        "times": times,
                        "processed_items": len(large_data)
                    }
                )
                
        except Exception as e:
            logger.error(f"å¤§å…¥åŠ›å‡¦ç†ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
            
        return None
    
    def _benchmark_io_operations(self) -> Optional[BenchmarkResult]:
        """I/Oæ“ä½œãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        try:
            iterations = 20
            file_size = 1024 * 10  # 10KB
            test_data = b'x' * file_size
            
            times = []
            initial_memory = psutil.Process().memory_info().rss
            
            with tempfile.TemporaryDirectory() as temp_dir:
                temp_path = Path(temp_dir)
                
                for i in range(iterations):
                    test_file = temp_path / f"test_{i}.dat"
                    
                    start_time = time.time()
                    
                    # æ›¸ãè¾¼ã¿
                    with open(test_file, 'wb') as f:
                        f.write(test_data)
                    
                    # èª­ã¿è¾¼ã¿
                    with open(test_file, 'rb') as f:
                        read_data = f.read()
                    
                    # æ¤œè¨¼
                    assert len(read_data) == file_size
                    
                    elapsed = time.time() - start_time
                    times.append(elapsed)
            
            final_memory = psutil.Process().memory_info().rss
            
            if times:
                avg_time = statistics.mean(times)
                total_bytes = file_size * iterations * 2  # read + write
                
                return BenchmarkResult(
                    test_name="io_operations",
                    execution_time=avg_time,
                    memory_usage=final_memory - initial_memory,
                    cpu_usage=psutil.Process().cpu_percent(),
                    iterations=iterations,
                    input_size=file_size,
                    throughput=total_bytes / (avg_time * iterations),
                    timestamp=datetime.now(),
                    metadata={
                        "file_size": file_size,
                        "total_bytes": total_bytes,
                        "times": times
                    }
                )
                
        except Exception as e:
            logger.error(f"I/Oæ“ä½œãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
            
        return None
    
    def _benchmark_cpu_intensive_tasks(self) -> Optional[BenchmarkResult]:
        """CPUé›†ç´„çš„ã‚¿ã‚¹ã‚¯ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        try:
            iterations = 3
            times = []
            initial_memory = psutil.Process().memory_info().rss
            
            for _ in range(iterations):
                start_time = time.time()
                
                # CPUé›†ç´„çš„è¨ˆç®—
                result = 0
                for i in range(100000):
                    result += i ** 2
                    if i % 1000 == 0:
                        result = result % (10**9 + 7)  # ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼é˜²æ­¢
                
                elapsed = time.time() - start_time
                times.append(elapsed)
            
            final_memory = psutil.Process().memory_info().rss
            
            if times:
                avg_time = statistics.mean(times)
                operations = 100000
                
                return BenchmarkResult(
                    test_name="cpu_intensive_tasks",
                    execution_time=avg_time,
                    memory_usage=final_memory - initial_memory,
                    cpu_usage=psutil.Process().cpu_percent(),
                    iterations=iterations,
                    input_size=operations,
                    throughput=operations / avg_time,
                    timestamp=datetime.now(),
                    metadata={
                        "operations": operations,
                        "times": times,
                        "result": result
                    }
                )
                
        except Exception as e:
            logger.error(f"CPUé›†ç´„çš„ã‚¿ã‚¹ã‚¯ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
            
        return None
    
    def _detect_performance_regressions(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°æ¤œå‡º"""
        logger.info("ğŸ“‰ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°æ¤œå‡ºä¸­...")
        
        for result in self.benchmark_results:
            test_name = result.test_name
            
            if test_name in self.performance_baselines:
                baseline = self.performance_baselines[test_name]
                
                # å„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«ã¤ã„ã¦å›å¸°ãƒã‚§ãƒƒã‚¯
                metrics_to_check = {
                    "execution_time": (result.execution_time, baseline.get("execution_time")),
                    "memory_usage": (result.memory_usage, baseline.get("memory_usage")),
                    "cpu_usage": (result.cpu_usage, baseline.get("cpu_usage")),
                    "throughput": (result.throughput, baseline.get("throughput"))
                }
                
                for metric_name, (current_value, baseline_value) in metrics_to_check.items():
                    if baseline_value is not None and baseline_value > 0:
                        regression = self._calculate_regression(
                            metric_name, current_value, baseline_value, test_name
                        )
                        
                        if regression:
                            self.regressions.append(regression)
    
    def _calculate_regression(self, metric_name: str, current: float, baseline: float, test_name: str) -> Optional[PerformanceRegression]:
        """å›å¸°è¨ˆç®—"""
        threshold = self.regression_thresholds.get(metric_name, 0.15)
        
        if metric_name == "throughput":
            # ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã¯ä½ä¸‹ãŒå•é¡Œ
            regression_pct = (baseline - current) / baseline
            is_regression = regression_pct > -threshold  # è² ã®é–¾å€¤ãªã®ã§ä¸ç­‰å·é€†è»¢
        else:
            # ãã®ä»–ã¯å¢—åŠ ãŒå•é¡Œ
            regression_pct = (current - baseline) / baseline  
            is_regression = regression_pct > threshold
        
        if is_regression:
            severity = self._assess_regression_severity(abs(regression_pct))
            
            return PerformanceRegression(
                test_name=test_name,
                metric_name=metric_name,
                baseline_value=baseline,
                current_value=current,
                regression_percentage=regression_pct * 100,
                severity=severity,
                description=f"{test_name}ã®{metric_name}ãŒ{abs(regression_pct)*100:.1f}%æ‚ªåŒ–",
                recommendation=self._generate_regression_recommendation(metric_name, regression_pct),
                detected_at=datetime.now(),
                confidence_level=self._calculate_confidence_for_regression(test_name, metric_name)
            )
        
        return None
    
    def _calculate_confidence_for_regression(self, test_name: str, metric_name: str) -> float:
        """å›å¸°ã®ä¿¡é ¼åº¦ã‚’è¨ˆç®—"""
        # è©²å½“ãƒ†ã‚¹ãƒˆã®éå»ã®æ¸¬å®šå€¤ã‚’åé›†
        measurements = []
        for result in self.benchmark_results:
            if result.test_name == test_name:
                if metric_name == "execution_time":
                    measurements.append(result.execution_time)
                elif metric_name == "memory_usage":
                    measurements.append(result.memory_usage)
                elif metric_name == "cpu_usage":
                    measurements.append(result.cpu_usage)
                elif metric_name == "throughput":
                    measurements.append(result.throughput)
        
        if test_name in self.performance_baselines and metric_name in self.performance_baselines[test_name]:
            baseline = self.performance_baselines[test_name][metric_name]
            threshold = self.regression_thresholds.get(metric_name, 0.15)
            return calculate_regression_confidence(measurements, baseline, threshold)
        
        return 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
    
    def _assess_regression_severity(self, regression_pct: float) -> PerformanceRisk:
        """å›å¸°é‡è¦åº¦è©•ä¾¡"""
        if regression_pct >= 0.50:  # 50%ä»¥ä¸Š
            return PerformanceRisk.CRITICAL
        elif regression_pct >= 0.30:  # 30%ä»¥ä¸Š
            return PerformanceRisk.HIGH
        elif regression_pct >= 0.15:  # 15%ä»¥ä¸Š
            return PerformanceRisk.MEDIUM
        else:
            return PerformanceRisk.LOW
    
    def _generate_regression_recommendation(self, metric_name: str, regression_pct: float) -> str:
        """å›å¸°æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        recommendations = {
            "execution_time": "å®Ÿè¡Œæ™‚é–“ãŒå¢—åŠ ã—ã¦ã„ã¾ã™ã€‚ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã¦ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã‚’ç‰¹å®šã—ã¦ãã ã•ã„ã€‚",
            "memory_usage": "ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¢—åŠ ã—ã¦ã„ã¾ã™ã€‚ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ã®å¯èƒ½æ€§ã‚’èª¿æŸ»ã—ã¦ãã ã•ã„ã€‚",
            "cpu_usage": "CPUä½¿ç”¨ç‡ãŒå¢—åŠ ã—ã¦ã„ã¾ã™ã€‚ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æœ€é©åŒ–ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚",
            "throughput": "ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆãŒä½ä¸‹ã—ã¦ã„ã¾ã™ã€‚ä¸¦åˆ—å‡¦ç†ã‚„æœ€é©åŒ–ã®è¦‹ç›´ã—ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚"
        }
        
        base_rec = recommendations.get(metric_name, "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒæ‚ªåŒ–ã—ã¦ã„ã¾ã™ã€‚")
        
        if abs(regression_pct) > 0.3:
            base_rec += " é‡å¤§ãªå›å¸°ã®ãŸã‚ã€å³åº§ã®å¯¾å¿œãŒå¿…è¦ã§ã™ã€‚"
        
        return base_rec
    
    def _detect_memory_leaks(self):
        """ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯æ¤œå‡º"""
        logger.info("ğŸ” ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯æ¤œå‡ºä¸­...")
        
        # ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯æ¤œå‡ºãƒ†ã‚¹ãƒˆ
        leak_test_functions = [
            self._test_for_memory_leak_in_file_operations,
            self._test_for_memory_leak_in_data_structures
        ]
        
        for test_func in leak_test_functions:
            try:
                leak = test_func()
                if leak:
                    self.memory_leaks.append(leak)
            except Exception as e:
                logger.warning(f"ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯æ¤œå‡ºã‚¨ãƒ©ãƒ¼ {test_func.__name__}: {e}")
    
    def _test_for_memory_leak_in_file_operations(self) -> Optional[MemoryLeak]:
        """ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œã§ã®ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ"""
        try:
            tracemalloc.start()
            initial_memory = psutil.Process().memory_info().rss
            
            iterations = 100
            memory_samples = []
            
            for i in range(iterations):
                # ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œå®Ÿè¡Œ
                with tempfile.NamedTemporaryFile(mode='w', delete=True) as temp_file:
                    temp_file.write("test data " * 100)
                    temp_file.flush()
                
                # å®šæœŸçš„ã«ãƒ¡ãƒ¢ãƒªæ¸¬å®š
                if i % 10 == 0:
                    current_memory = psutil.Process().memory_info().rss
                    memory_samples.append((i, current_memory))
            
            final_memory = psutil.Process().memory_info().rss
            tracemalloc.stop()
            
            # ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯åˆ¤å®š
            memory_increase = final_memory - initial_memory
            leak_threshold = 1024 * 1024  # 1MB
            
            if memory_increase > leak_threshold:
                leak_rate = memory_increase / iterations
                
                return MemoryLeak(
                    function_name="file_operations",
                    leaked_memory=memory_increase,
                    leak_rate=leak_rate,
                    iterations_tested=iterations,
                    confidence_level=calculate_memory_leak_confidence(
                        memory_samples, iterations
                    ),
                    stack_trace=[],
                    recommendation="ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œå¾Œã®ãƒªã‚½ãƒ¼ã‚¹è§£æ”¾ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚withæ–‡ã®ä½¿ç”¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚"
                )
                
        except Exception as e:
            logger.debug(f"ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            if tracemalloc.is_tracing():
                tracemalloc.stop()
        
        return None
    
    def _test_for_memory_leak_in_data_structures(self) -> Optional[MemoryLeak]:
        """ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã§ã®ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ"""
        try:
            tracemalloc.start()
            initial_memory = psutil.Process().memory_info().rss
            
            iterations = 200
            data_holder = []
            
            for i in range(iterations):
                # ãƒ‡ãƒ¼ã‚¿æ§‹é€ ä½œæˆ
                data = {
                    'id': i,
                    'data': list(range(100)),
                    'metadata': {'created': datetime.now()}
                }
                data_holder.append(data)
                
                # å¤ã„ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ï¼ˆæœ¬æ¥ãªã‚‰ï¼‰
                if len(data_holder) > 50:
                    data_holder.pop(0)
            
            final_memory = psutil.Process().memory_info().rss
            tracemalloc.stop()
            
            # ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯åˆ¤å®š
            memory_increase = final_memory - initial_memory
            expected_memory = len(data_holder) * 1000  # æ¦‚ç®—
            
            if memory_increase > expected_memory * 2:  # æœŸå¾…å€¤ã®2å€ä»¥ä¸Š
                leak_rate = memory_increase / iterations
                
                return MemoryLeak(
                    function_name="data_structures",
                    leaked_memory=memory_increase,
                    leak_rate=leak_rate,
                    iterations_tested=iterations,
                    confidence_level=calculate_memory_leak_confidence(
                        [(i, m) for i, m in enumerate([initial_memory, final_memory])],
                        iterations
                    ),
                    stack_trace=[],
                    recommendation="å¾ªç’°å‚ç…§ã‚„ä¸è¦ãªã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå‚ç…§ãŒãªã„ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
                )
                
        except Exception as e:
            logger.debug(f"ãƒ‡ãƒ¼ã‚¿æ§‹é€ ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            if tracemalloc.is_tracing():
                tracemalloc.stop()
        
        return None
    
    def _analyze_results(self, start_time: datetime) -> PerformanceTestResult:
        """çµæœåˆ†æ"""
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        # å…¨ä½“ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«æ±ºå®š
        overall_risk = self._calculate_overall_performance_risk()
        
        # æ¨å¥¨äº‹é …ç”Ÿæˆ
        recommendations = self._generate_performance_recommendations()
        
        return PerformanceTestResult(
            total_benchmarks=len(self.benchmark_results),
            benchmark_results=self.benchmark_results,
            regressions=self.regressions,
            memory_leaks=self.memory_leaks,
            performance_baselines=self.performance_baselines,
            system_info=self.system_info,
            test_duration=duration,
            timestamp=end_time,
            overall_risk=overall_risk,
            recommendations=recommendations
        )
    
    def _calculate_overall_performance_risk(self) -> PerformanceRisk:
        """å…¨ä½“ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«è¨ˆç®—"""
        if not self.regressions and not self.memory_leaks:
            return PerformanceRisk.SAFE
        
        risk_items = []
        risk_items.extend([r.severity for r in self.regressions])
        
        # ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ã¯é«˜ãƒªã‚¹ã‚¯æ‰±ã„
        for leak in self.memory_leaks:
            if leak.leaked_memory > 10 * 1024 * 1024:  # 10MBä»¥ä¸Š
                risk_items.append(PerformanceRisk.HIGH)
            else:
                risk_items.append(PerformanceRisk.MEDIUM)
        
        if PerformanceRisk.CRITICAL in risk_items:
            return PerformanceRisk.CRITICAL
        elif PerformanceRisk.HIGH in risk_items:
            return PerformanceRisk.HIGH
        elif PerformanceRisk.MEDIUM in risk_items:
            return PerformanceRisk.MEDIUM
        else:
            return PerformanceRisk.LOW
    
    def _generate_performance_recommendations(self) -> List[str]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        recommendations = [
            "å®šæœŸçš„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®å®Ÿè¡Œ",
            "ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ãƒ„ãƒ¼ãƒ«ï¼ˆcProfile, memory_profilerï¼‰ã®æ´»ç”¨",
            "ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ç¶™ç¶šçš„ç›£è¦–",
            "ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç®‡æ‰€ã®ç‰¹å®šã¨æœ€é©åŒ–",
            "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã®è‡ªå‹•åŒ–",
            "CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°æ¤œå‡º",
        ]
        
        if self.regressions:
            recommendations.extend([
                "ç™ºè¦‹ã•ã‚ŒãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°ã‚’å„ªå…ˆåº¦ã«å¿œã˜ã¦ä¿®æ­£ã™ã‚‹",
                "å›å¸°ã®æ ¹æœ¬åŸå› ã‚’èª¿æŸ»ã—ã€å†ç™ºé˜²æ­¢ç­–ã‚’å®Ÿè£…ã™ã‚‹"
            ])
        
        if self.memory_leaks:
            recommendations.extend([
                "ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ã®ä¿®æ­£ã‚’æœ€å„ªå…ˆã§å®Ÿæ–½ã™ã‚‹",
                "ãƒªã‚½ãƒ¼ã‚¹ç®¡ç†ï¼ˆwithæ–‡ã€æ˜ç¤ºçš„ãªclose()ï¼‰ã®å¾¹åº•"
            ])
        
        return recommendations
    
    def _update_performance_baselines(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ›´æ–°"""
        logger.info("ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ›´æ–°ä¸­...")
        
        for result in self.benchmark_results:
            test_name = result.test_name
            
            baseline_data = {
                "execution_time": result.execution_time,
                "memory_usage": result.memory_usage,
                "cpu_usage": result.cpu_usage,
                "throughput": result.throughput,
                "last_updated": result.timestamp.isoformat(),
                "iterations": result.iterations,
                "input_size": result.input_size
            }
            
            if test_name in self.performance_baselines:
                # æ—¢å­˜ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’æ›´æ–°ï¼ˆç§»å‹•å¹³å‡ï¼‰
                old_baseline = self.performance_baselines[test_name]
                alpha = 0.2  # é‡ã¿
                
                for metric in ["execution_time", "memory_usage", "cpu_usage", "throughput"]:
                    if metric in old_baseline:
                        old_value = old_baseline[metric]
                        new_value = baseline_data[metric]
                        baseline_data[metric] = alpha * new_value + (1 - alpha) * old_value
            
            self.performance_baselines[test_name] = baseline_data
        
        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ä¿å­˜
        try:
            with open(self.baseline_file, 'w', encoding='utf-8') as f:
                json.dump(self.performance_baselines, f, indent=2, ensure_ascii=False)
            logger.info("âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ›´æ–°å®Œäº†")
        except Exception as e:
            logger.error(f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _generate_performance_report(self, result: PerformanceTestResult):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        report_file = self.project_root / "performance_regression_report.json"
        
        report_data = {
            "summary": {
                "total_benchmarks": result.total_benchmarks,
                "regressions_found": len(result.regressions),
                "memory_leaks_found": len(result.memory_leaks),
                "overall_risk": result.overall_risk.value,
                "test_duration": result.test_duration,
                "timestamp": result.timestamp.isoformat(),
                "system_info": result.system_info
            },
            "benchmark_results": [
                {
                    "test_name": r.test_name,
                    "execution_time": r.execution_time,
                    "memory_usage": r.memory_usage,
                    "cpu_usage": r.cpu_usage,
                    "throughput": r.throughput,
                    "iterations": r.iterations,
                    "input_size": r.input_size,
                    "timestamp": r.timestamp.isoformat(),
                    "metadata": r.metadata
                }
                for r in result.benchmark_results
            ],
            "performance_regressions": [
                {
                    "test_name": reg.test_name,
                    "metric_name": reg.metric_name,
                    "baseline_value": reg.baseline_value,
                    "current_value": reg.current_value,
                    "regression_percentage": reg.regression_percentage,
                    "severity": reg.severity.value,
                    "description": reg.description,
                    "recommendation": reg.recommendation,
                    "confidence_level": reg.confidence_level,
                    "detected_at": reg.detected_at.isoformat()
                }
                for reg in result.regressions
            ],
            "memory_leaks": [
                {
                    "function_name": leak.function_name,
                    "leaked_memory": leak.leaked_memory,
                    "leak_rate": leak.leak_rate,
                    "iterations_tested": leak.iterations_tested,
                    "confidence_level": leak.confidence_level,
                    "recommendation": leak.recommendation
                }
                for leak in result.memory_leaks
            ],
            "baselines": result.performance_baselines,
            "recommendations": result.recommendations,
            "risk_distribution": self._get_performance_risk_distribution(result.regressions)
        }
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ“‹ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ: {report_file}")
    
    def _get_performance_risk_distribution(self, regressions: List[PerformanceRegression]) -> Dict[str, int]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒªã‚¹ã‚¯åˆ†å¸ƒå–å¾—"""
        distribution = {}
        for regression in regressions:
            risk = regression.severity.value
            distribution[risk] = distribution.get(risk, 0) + 1
        return distribution

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    project_root = Path(__file__).parent.parent
    
    logger.info("ğŸš€ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°ç›£è¦–é–‹å§‹")
    
    try:
        with PerformanceRegressionMonitor(project_root) as monitor:
            result = monitor.run()
            
            # çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º
            logger.info("ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°ç›£è¦–çµæœã‚µãƒãƒªãƒ¼:")
            logger.info(f"  å®Ÿè¡Œãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ•°: {result.total_benchmarks}")
            logger.info(f"  æ¤œå‡ºã•ã‚ŒãŸå›å¸°: {len(result.regressions)}ä»¶")
            logger.info(f"  ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯: {len(result.memory_leaks)}ä»¶")
            logger.info(f"  å…¨ä½“ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«: {result.overall_risk.value}")
            logger.info(f"  å®Ÿè¡Œæ™‚é–“: {result.test_duration:.2f}ç§’")
            
            # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
            logger.info("ğŸ’» ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±:")
            logger.info(f"  CPU: {result.system_info['cpu_count']}ã‚³ã‚¢")
            logger.info(f"  ãƒ¡ãƒ¢ãƒª: {result.system_info['memory_total'] / (1024**3):.1f}GB")
            logger.info(f"  ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ : {result.system_info['platform']}")
            
            # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ
            if result.benchmark_results:
                logger.info("âš¡ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ:")
                for bench in result.benchmark_results:
                    logger.info(f"  {bench.test_name}: {bench.execution_time:.3f}s, {bench.memory_usage/1024/1024:.1f}MB")
            
            # é‡è¦ãªå›å¸°ã‚’è¡¨ç¤º
            critical_regressions = [r for r in result.regressions 
                                  if r.severity in [PerformanceRisk.CRITICAL, PerformanceRisk.HIGH]]
            
            if critical_regressions:
                logger.warning(f"ğŸš¨ é«˜ãƒªã‚¹ã‚¯ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸° {len(critical_regressions)}ä»¶:")
                for reg in critical_regressions[:5]:  # ä¸Šä½5ä»¶è¡¨ç¤º
                    logger.warning(f"  - {reg.test_name}.{reg.metric_name}: {reg.regression_percentage:+.1f}% ({reg.severity.value})")
            
            # ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯æƒ…å ±
            if result.memory_leaks:
                logger.warning(f"ğŸ” ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯æ¤œå‡º: {len(result.memory_leaks)}ä»¶")
                for leak in result.memory_leaks:
                    logger.warning(f"  - {leak.function_name}: {leak.leaked_memory/1024/1024:.1f}MB ãƒªãƒ¼ã‚¯")
            
            return 0 if result.overall_risk in [PerformanceRisk.SAFE, PerformanceRisk.LOW] else 1
            
    except Exception as e:
        logger.error(f"ğŸ’¥ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°ç›£è¦–å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main())