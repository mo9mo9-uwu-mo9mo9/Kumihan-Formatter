#!/usr/bin/env python3
"""
Context Splitter for Gemini Capability Enhancement
Flash 2.5ã®2000ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™å¯¾å¿œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²ã‚·ã‚¹ãƒ†ãƒ 
"""

import json
import os
import re
import ast
import hashlib
from typing import Dict, List, Any, Optional, Tuple, Set
from pathlib import Path
from dataclasses import dataclass, asdict
from enum import Enum

class SplitStrategy(Enum):
    """åˆ†å‰²æˆ¦ç•¥ã‚¿ã‚¤ãƒ—"""
    FUNCTION_BASED = "function_based"      # é–¢æ•°å˜ä½åˆ†å‰²
    CLASS_BASED = "class_based"            # ã‚¯ãƒ©ã‚¹å˜ä½åˆ†å‰²
    MODULE_BASED = "module_based"          # ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å˜ä½åˆ†å‰²
    DEPENDENCY_BASED = "dependency_based"  # ä¾å­˜é–¢ä¿‚åŸºæº–åˆ†å‰²
    TOKEN_BASED = "token_based"            # ãƒˆãƒ¼ã‚¯ãƒ³æ•°åŸºæº–åˆ†å‰²

@dataclass
class ContextChunk:
    """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ï¼ˆåˆ†å‰²å˜ä½ï¼‰"""
    chunk_id: str
    content: str
    estimated_tokens: int
    dependencies: List[str]
    priority: int
    chunk_type: str
    metadata: Dict[str, Any]
    execution_order: int = 0
    parent_task_id: str = ""
    
class DependencyAnalyzer:
    """ä¾å­˜é–¢ä¿‚è§£æå™¨"""
    
    def __init__(self):
        self.import_patterns = [
            r'from\s+(\w+(?:\.\w+)*)\s+import',
            r'import\s+(\w+(?:\.\w+)*)',
        ]
        self.call_patterns = [
            r'(\w+)\s*\(',  # é–¢æ•°å‘¼ã³å‡ºã—
            r'(\w+)\.(\w+)',  # ãƒ¡ã‚½ãƒƒãƒ‰å‘¼ã³å‡ºã—
            r'class\s+(\w+)\s*\(',  # ã‚¯ãƒ©ã‚¹ç¶™æ‰¿
        ]
    
    def analyze_code_dependencies(self, code: str, context: Dict[str, Any]) -> List[str]:
        """ã‚³ãƒ¼ãƒ‰ä¾å­˜é–¢ä¿‚åˆ†æ"""
        dependencies = set()
        
        try:
            # ASTè§£æã«ã‚ˆã‚‹ä¾å­˜é–¢ä¿‚æŠ½å‡º
            tree = ast.parse(code)
            
            for node in ast.walk(tree):
                if isinstance(node, ast.Import):
                    for alias in node.names:
                        dependencies.add(alias.name)
                elif isinstance(node, ast.ImportFrom):
                    if node.module:
                        dependencies.add(node.module)
                elif isinstance(node, ast.FunctionDef):
                    # é–¢æ•°å†…ã§ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹å¤–éƒ¨å‚ç…§ã‚’åˆ†æ
                    for child in ast.walk(node):
                        if isinstance(child, ast.Name):
                            dependencies.add(child.id)
                        elif isinstance(child, ast.Attribute):
                            if isinstance(child.value, ast.Name):
                                dependencies.add(child.value.id)
        
        except SyntaxError:
            # æ§‹æ–‡ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯æ­£è¦è¡¨ç¾ã§åŸºæœ¬çš„ãªä¾å­˜é–¢ä¿‚ã‚’æŠ½å‡º
            for pattern in self.import_patterns:
                matches = re.findall(pattern, code)
                dependencies.update(matches)
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼šPythonæ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚„æ˜ã‚‰ã‹ã«ä¸è¦ãªã‚‚ã®ã‚’é™¤å¤–
        filtered_deps = []
        builtin_modules = {'typing', 'os', 'sys', 'json', 're', 'datetime', 'pathlib'}
        
        for dep in dependencies:
            if dep not in builtin_modules and len(dep) > 1 and not dep.startswith('_'):
                filtered_deps.append(dep)
        
        return filtered_deps
    
    def analyze_task_dependencies(self, tasks: List[Dict[str, Any]]) -> Dict[str, List[str]]:
        """ã‚¿ã‚¹ã‚¯é–“ä¾å­˜é–¢ä¿‚åˆ†æ"""
        dependency_graph = {}
        
        for task in tasks:
            task_id = task.get('task_id', '')
            task_type = task.get('type', '')
            target_files = task.get('target_files', [])
            
            deps = set()
            
            # ãƒ•ã‚¡ã‚¤ãƒ«é–“ä¾å­˜é–¢ä¿‚
            for file_path in target_files:
                if os.path.exists(file_path):
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            content = f.read()
                        
                        file_deps = self.analyze_code_dependencies(content, task)
                        deps.update(file_deps)
                    
                    except Exception:
                        continue
            
            # ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—åŸºæº–ã®ä¾å­˜é–¢ä¿‚
            if task_type in ['hybrid_implementation', 'new_feature_development']:
                deps.add('base_implementation')
            
            dependency_graph[task_id] = list(deps)
        
        return dependency_graph
    
    def calculate_execution_order(self, dependency_graph: Dict[str, List[str]]) -> List[str]:
        """å®Ÿè¡Œé †åºè¨ˆç®—ï¼ˆãƒˆãƒãƒ­ã‚¸ã‚«ãƒ«ã‚½ãƒ¼ãƒˆï¼‰"""
        
        # ç°¡æ˜“ç‰ˆãƒˆãƒãƒ­ã‚¸ã‚«ãƒ«ã‚½ãƒ¼ãƒˆ
        in_degree = {}
        nodes = set(dependency_graph.keys())
        
        # åˆæœŸåŒ–
        for node in nodes:
            in_degree[node] = 0
        
        # å…¥æ¬¡æ•°è¨ˆç®—
        for node, deps in dependency_graph.items():
            for dep in deps:
                if dep in in_degree:
                    in_degree[dep] += 1
        
        # å®Ÿè¡Œé †åºæ±ºå®š
        execution_order = []
        queue = [node for node, degree in in_degree.items() if degree == 0]
        
        while queue:
            current = queue.pop(0)
            execution_order.append(current)
            
            # ä¾å­˜é–¢ä¿‚ã‚’æ›´æ–°
            if current in dependency_graph:
                for dep in dependency_graph[current]:
                    if dep in in_degree:
                        in_degree[dep] -= 1
                        if in_degree[dep] == 0:
                            queue.append(dep)
        
        return execution_order

class ContextSplitter:
    """
    ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²ã‚¨ãƒ³ã‚¸ãƒ³
    Flash 2.5ã®2000ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™ã«å¯¾å¿œã—ãŸè‡ªå‹•åˆ†å‰²ã‚·ã‚¹ãƒ†ãƒ 
    """
    
    def __init__(self, max_tokens_per_chunk: int = 1800):
        """
        åˆæœŸåŒ–
        
        Args:
            max_tokens_per_chunk: ãƒãƒ£ãƒ³ã‚¯å½“ãŸã‚Šã®æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆä½™è£•ã‚’æŒã£ã¦1800ï¼‰
        """
        self.max_tokens_per_chunk = max_tokens_per_chunk
        self.dependency_analyzer = DependencyAnalyzer()
        self.token_estimator = TokenEstimator()
        
        # åˆ†å‰²æˆ¦ç•¥ã®å„ªå…ˆé †ä½
        self.strategy_priority = [
            SplitStrategy.FUNCTION_BASED,
            SplitStrategy.CLASS_BASED,
            SplitStrategy.MODULE_BASED,
            SplitStrategy.DEPENDENCY_BASED,
            SplitStrategy.TOKEN_BASED
        ]
    
    def split_implementation_task(self, task_data: Dict[str, Any]) -> List[ContextChunk]:
        """
        æ–°è¦å®Ÿè£…ã‚¿ã‚¹ã‚¯ã®åˆ†å‰²
        
        Args:
            task_data: å®Ÿè£…ã‚¿ã‚¹ã‚¯ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            åˆ†å‰²ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ãƒªã‚¹ãƒˆ
        """
        
        task_type = task_data.get('type', '')
        implementation_spec = task_data.get('requirements', {}).get('implementation_spec', {})
        target_files = task_data.get('target_files', [])
        
        print(f"ğŸ”„ å®Ÿè£…ã‚¿ã‚¹ã‚¯åˆ†å‰²é–‹å§‹: {task_type}")
        print(f"ğŸ“ å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {len(target_files)}ä»¶")
        
        chunks = []
        
        # å®Ÿè£…ã‚¿ã‚¤ãƒ—åˆ¥åˆ†å‰²æˆ¦ç•¥
        if task_type == "new_implementation":
            chunks = self._split_new_implementation(task_data, implementation_spec)
        elif task_type == "hybrid_implementation":
            chunks = self._split_hybrid_implementation(task_data, implementation_spec)
        elif task_type == "new_feature_development":
            chunks = self._split_feature_development(task_data, implementation_spec)
        else:
            # æ±ç”¨åˆ†å‰²
            chunks = self._split_generic_task(task_data)
        
        # ä¾å­˜é–¢ä¿‚è§£æãƒ»å®Ÿè¡Œé †åºæ±ºå®š
        chunks = self._analyze_and_order_chunks(chunks, task_data)
        
        print(f"âœ… åˆ†å‰²å®Œäº†: {len(chunks)}ãƒãƒ£ãƒ³ã‚¯ç”Ÿæˆ")
        return chunks
    
    def split_modification_task(self, task_data: Dict[str, Any]) -> List[ContextChunk]:
        """
        ã‚³ãƒ¼ãƒ‰ä¿®æ­£ã‚¿ã‚¹ã‚¯ã®åˆ†å‰²
        
        Args:
            task_data: ä¿®æ­£ã‚¿ã‚¹ã‚¯ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            åˆ†å‰²ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ãƒªã‚¹ãƒˆ  
        """
        
        target_files = task_data.get('target_files', [])
        error_type = task_data.get('requirements', {}).get('error_type', '')
        
        print(f"ğŸ”„ ä¿®æ­£ã‚¿ã‚¹ã‚¯åˆ†å‰²é–‹å§‹: {error_type}")
        print(f"ğŸ“ å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {len(target_files)}ä»¶")
        
        chunks = []
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å˜ä½ã§åˆ†å‰²
        for i, file_path in enumerate(target_files):
            if not os.path.exists(file_path):
                continue
            
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    file_content = f.read()
                
                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã«ã‚ˆã‚‹åˆ†å‰²æˆ¦ç•¥æ±ºå®š
                estimated_tokens = self.token_estimator.estimate_tokens(file_content)
                
                if estimated_tokens <= self.max_tokens_per_chunk:
                    # å˜ä¸€ãƒãƒ£ãƒ³ã‚¯ã§å‡¦ç†å¯èƒ½
                    chunk = self._create_file_chunk(file_path, file_content, task_data, i)
                    chunks.append(chunk)
                else:
                    # é–¢æ•°ãƒ»ã‚¯ãƒ©ã‚¹å˜ä½ã§åˆ†å‰²
                    file_chunks = self._split_large_file(file_path, file_content, task_data, i)
                    chunks.extend(file_chunks)
            
            except Exception as e:
                print(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«åˆ†å‰²ã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
                continue
        
        # ä¾å­˜é–¢ä¿‚è§£æãƒ»å®Ÿè¡Œé †åºæ±ºå®š
        chunks = self._analyze_and_order_chunks(chunks, task_data)
        
        print(f"âœ… åˆ†å‰²å®Œäº†: {len(chunks)}ãƒãƒ£ãƒ³ã‚¯ç”Ÿæˆ")
        return chunks
    
    def _split_new_implementation(self, task_data: Dict[str, Any], 
                                implementation_spec: Dict[str, Any]) -> List[ContextChunk]:
        """æ–°è¦å®Ÿè£…ã®åˆ†å‰²"""
        
        chunks = []
        target_files = task_data.get('target_files', [])
        template_type = implementation_spec.get('template_type', 'class')
        
        for i, file_path in enumerate(target_files):
            # ãƒ•ã‚¡ã‚¤ãƒ«å˜ä½ã§ãƒãƒ£ãƒ³ã‚¯ä½œæˆ
            chunk_content = self._generate_implementation_context(
                file_path, template_type, implementation_spec
            )
            
            chunk = ContextChunk(
                chunk_id=f"impl_{i:03d}_{Path(file_path).stem}",
                content=chunk_content,
                estimated_tokens=self.token_estimator.estimate_tokens(chunk_content),
                dependencies=self._extract_implementation_dependencies(implementation_spec),
                priority=self._calculate_priority(file_path, template_type),
                chunk_type="new_implementation",
                metadata={
                    "file_path": file_path,
                    "template_type": template_type,
                    "implementation_spec": implementation_spec
                },
                parent_task_id=task_data.get('task_id', '')
            )
            
            chunks.append(chunk)
        
        return chunks
    
    def _split_hybrid_implementation(self, task_data: Dict[str, Any],
                                   implementation_spec: Dict[str, Any]) -> List[ContextChunk]:
        """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å®Ÿè£…ã®åˆ†å‰²"""
        
        chunks = []
        target_files = task_data.get('target_files', [])
        
        for i, file_path in enumerate(target_files):
            if os.path.exists(file_path):
                # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ä¿®æ­£
                chunk = self._create_modification_chunk(file_path, task_data, i, "hybrid_modify")
            else:
                # æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
                chunk = self._create_creation_chunk(file_path, implementation_spec, i, "hybrid_create")
            
            chunks.append(chunk)
        
        return chunks
    
    def _split_feature_development(self, task_data: Dict[str, Any],
                                 feature_spec: Dict[str, Any]) -> List[ContextChunk]:
        """æ–°æ©Ÿèƒ½é–‹ç™ºã®åˆ†å‰²"""
        
        chunks = []
        implementation_plan = feature_spec.get('implementation_plan', [])
        
        for i, step in enumerate(implementation_plan):
            step_type = step.get('type', 'implementation')
            step_files = step.get('files', [])
            
            chunk_content = self._generate_feature_context(step, feature_spec)
            
            chunk = ContextChunk(
                chunk_id=f"feat_{i:03d}_{step_type}",
                content=chunk_content,
                estimated_tokens=self.token_estimator.estimate_tokens(chunk_content),
                dependencies=self._extract_feature_dependencies(step, feature_spec),
                priority=step.get('priority', 5),
                chunk_type="feature_development",
                metadata={
                    "step_type": step_type,
                    "step_files": step_files,
                    "feature_spec": feature_spec
                },
                parent_task_id=task_data.get('task_id', '')
            )
            
            chunks.append(chunk)
        
        return chunks
    
    def _split_generic_task(self, task_data: Dict[str, Any]) -> List[ContextChunk]:
        """æ±ç”¨ã‚¿ã‚¹ã‚¯ã®åˆ†å‰²"""
        
        chunks = []
        target_files = task_data.get('target_files', [])
        
        if not target_files:
            # ãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã„å ´åˆã¯å˜ä¸€ãƒãƒ£ãƒ³ã‚¯ã¨ã—ã¦ä½œæˆ
            chunk = ContextChunk(
                chunk_id="generic_001",
                content=json.dumps(task_data, indent=2, ensure_ascii=False),
                estimated_tokens=500,
                dependencies=[],
                priority=5,
                chunk_type="generic",
                metadata=task_data,
                parent_task_id=task_data.get('task_id', '')
            )
            chunks.append(chunk)
        else:
            # ãƒ•ã‚¡ã‚¤ãƒ«å˜ä½ã§åˆ†å‰²
            for i, file_path in enumerate(target_files):
                if os.path.exists(file_path):
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    chunk = self._create_file_chunk(file_path, content, task_data, i)
                    chunks.append(chunk)
        
        return chunks
    
    def _split_large_file(self, file_path: str, content: str, 
                         task_data: Dict[str, Any], base_index: int) -> List[ContextChunk]:
        """å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ†å‰²"""
        
        chunks = []
        
        try:
            # ASTè§£æã§é–¢æ•°ãƒ»ã‚¯ãƒ©ã‚¹å˜ä½ã«åˆ†å‰²
            tree = ast.parse(content)
            
            functions = []
            classes = []
            imports = []
            other_code = []
            
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    func_start = node.lineno - 1
                    func_end = getattr(node, 'end_lineno', func_start + 10) - 1
                    func_content = '\n'.join(content.split('\n')[func_start:func_end + 1])
                    functions.append((node.name, func_content, func_start, func_end))
                
                elif isinstance(node, ast.ClassDef):
                    class_start = node.lineno - 1
                    class_end = getattr(node, 'end_lineno', class_start + 20) - 1
                    class_content = '\n'.join(content.split('\n')[class_start:class_end + 1])
                    classes.append((node.name, class_content, class_start, class_end))
                
                elif isinstance(node, (ast.Import, ast.ImportFrom)):
                    import_line = node.lineno - 1
                    import_content = content.split('\n')[import_line]
                    imports.append(import_content)
            
            # importæ–‡ã®çµ±åˆ
            import_chunk = '\n'.join(imports) if imports else ""
            
            # é–¢æ•°å˜ä½ã®ãƒãƒ£ãƒ³ã‚¯ä½œæˆ
            for i, (func_name, func_content, start, end) in enumerate(functions):
                chunk_content = import_chunk + '\n\n' + func_content
                
                if self.token_estimator.estimate_tokens(chunk_content) <= self.max_tokens_per_chunk:
                    chunk = ContextChunk(
                        chunk_id=f"func_{base_index:03d}_{i:03d}_{func_name}",
                        content=chunk_content,
                        estimated_tokens=self.token_estimator.estimate_tokens(chunk_content),
                        dependencies=self.dependency_analyzer.analyze_code_dependencies(
                            func_content, task_data
                        ),
                        priority=self._calculate_function_priority(func_name),
                        chunk_type="function_modification",
                        metadata={
                            "file_path": file_path,
                            "function_name": func_name,
                            "line_range": (start, end)
                        },
                        parent_task_id=task_data.get('task_id', '')
                    )
                    chunks.append(chunk)
            
            # ã‚¯ãƒ©ã‚¹å˜ä½ã®ãƒãƒ£ãƒ³ã‚¯ä½œæˆ
            for i, (class_name, class_content, start, end) in enumerate(classes):
                chunk_content = import_chunk + '\n\n' + class_content
                
                if self.token_estimator.estimate_tokens(chunk_content) <= self.max_tokens_per_chunk:
                    chunk = ContextChunk(
                        chunk_id=f"class_{base_index:03d}_{i:03d}_{class_name}",
                        content=chunk_content,
                        estimated_tokens=self.token_estimator.estimate_tokens(chunk_content),
                        dependencies=self.dependency_analyzer.analyze_code_dependencies(
                            class_content, task_data
                        ),
                        priority=self._calculate_class_priority(class_name),
                        chunk_type="class_modification",
                        metadata={
                            "file_path": file_path,
                            "class_name": class_name,
                            "line_range": (start, end)
                        },
                        parent_task_id=task_data.get('task_id', '')
                    )
                    chunks.append(chunk)
        
        except SyntaxError:
            # æ§‹æ–‡ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯è¡Œæ•°ãƒ™ãƒ¼ã‚¹ã§åˆ†å‰²
            lines = content.split('\n')
            chunk_size = self.max_tokens_per_chunk // 4  # 1è¡Œç´„4ãƒˆãƒ¼ã‚¯ãƒ³ã¨ä»®å®š
            
            for i in range(0, len(lines), chunk_size):
                chunk_lines = lines[i:i + chunk_size]
                chunk_content = '\n'.join(chunk_lines)
                
                chunk = ContextChunk(
                    chunk_id=f"lines_{base_index:03d}_{i:03d}",
                    content=chunk_content,
                    estimated_tokens=self.token_estimator.estimate_tokens(chunk_content),
                    dependencies=[],
                    priority=5,
                    chunk_type="line_based_modification",
                    metadata={
                        "file_path": file_path,
                        "line_range": (i, i + len(chunk_lines))
                    },
                    parent_task_id=task_data.get('task_id', '')
                )
                chunks.append(chunk)
        
        return chunks
    
    def _create_file_chunk(self, file_path: str, content: str, 
                          task_data: Dict[str, Any], index: int) -> ContextChunk:
        """ãƒ•ã‚¡ã‚¤ãƒ«å˜ä½ã®ãƒãƒ£ãƒ³ã‚¯ä½œæˆ"""
        
        return ContextChunk(
            chunk_id=f"file_{index:03d}_{Path(file_path).stem}",
            content=content,
            estimated_tokens=self.token_estimator.estimate_tokens(content),
            dependencies=self.dependency_analyzer.analyze_code_dependencies(content, task_data),
            priority=self._calculate_file_priority(file_path),
            chunk_type="file_modification",
            metadata={
                "file_path": file_path,
                "task_type": task_data.get('type', 'modification')
            },
            parent_task_id=task_data.get('task_id', '')
        )
    
    def _create_modification_chunk(self, file_path: str, task_data: Dict[str, Any],
                                 index: int, chunk_type: str) -> ContextChunk:
        """ä¿®æ­£ãƒãƒ£ãƒ³ã‚¯ä½œæˆ"""
        
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        return ContextChunk(
            chunk_id=f"modify_{index:03d}_{Path(file_path).stem}",
            content=content,
            estimated_tokens=self.token_estimator.estimate_tokens(content),
            dependencies=self.dependency_analyzer.analyze_code_dependencies(content, task_data),
            priority=self._calculate_file_priority(file_path),
            chunk_type=chunk_type,
            metadata={
                "file_path": file_path,
                "exists": True,
                "modification_type": "extend"
            },
            parent_task_id=task_data.get('task_id', '')
        )
    
    def _create_creation_chunk(self, file_path: str, implementation_spec: Dict[str, Any],
                             index: int, chunk_type: str) -> ContextChunk:
        """ä½œæˆãƒãƒ£ãƒ³ã‚¯ä½œæˆ"""
        
        template_content = self._generate_implementation_context(
            file_path, implementation_spec.get('template_type', 'class'), implementation_spec
        )
        
        return ContextChunk(
            chunk_id=f"create_{index:03d}_{Path(file_path).stem}",
            content=template_content,
            estimated_tokens=self.token_estimator.estimate_tokens(template_content),
            dependencies=self._extract_implementation_dependencies(implementation_spec),
            priority=self._calculate_file_priority(file_path),
            chunk_type=chunk_type,
            metadata={
                "file_path": file_path,
                "exists": False,
                "template_type": implementation_spec.get('template_type', 'class')
            },
            parent_task_id=implementation_spec.get('task_id', '')
        )
    
    def _generate_implementation_context(self, file_path: str, template_type: str,
                                       implementation_spec: Dict[str, Any]) -> str:
        """å®Ÿè£…ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        
        context = f"""
# æ–°è¦å®Ÿè£…ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ - {Path(file_path).name}

## å®Ÿè£…ä»•æ§˜
- ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: {file_path}
- ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚¿ã‚¤ãƒ—: {template_type}

## å®Ÿè£…è©³ç´°
{json.dumps(implementation_spec, indent=2, ensure_ascii=False)}

## Flash 2.5å®Ÿè¡ŒæŒ‡ç¤º
1. ä¸Šè¨˜ä»•æ§˜ã«åŸºã¥ã„ã¦{template_type}ã‚¿ã‚¤ãƒ—ã®å®Ÿè£…ã‚’ä½œæˆ
2. å…¨ã¦ã®é–¢æ•°ãƒ»ãƒ¡ã‚½ãƒƒãƒ‰ã«é©åˆ‡ãªå‹æ³¨é‡ˆã‚’è¿½åŠ 
3. é©åˆ‡ãªdocstringã‚’è¿½åŠ 
4. mypy strict mode ã«é©åˆã™ã‚‹å®Ÿè£…ã‚’ä½œæˆ
5. å®Ÿè£…å®Œäº†å¾Œã€æ§‹æ–‡ãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œ

## å“è³ªè¦ä»¶
- å‹æ³¨é‡ˆå¿…é ˆ (typing.Anyä½¿ç”¨å¯)
- docstringå¿…é ˆ
- PEP8æº–æ‹ 
- mypy strict modeé©åˆ
"""
        
        return context
    
    def _generate_feature_context(self, step: Dict[str, Any], 
                                feature_spec: Dict[str, Any]) -> str:
        """æ©Ÿèƒ½å®Ÿè£…ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        
        context = f"""
# æ©Ÿèƒ½é–‹ç™ºã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ - {feature_spec.get('name', 'new_feature')}

## é–‹ç™ºã‚¹ãƒ†ãƒƒãƒ—
{json.dumps(step, indent=2, ensure_ascii=False)}

## æ©Ÿèƒ½ä»•æ§˜
{json.dumps(feature_spec, indent=2, ensure_ascii=False)}

## Flash 2.5å®Ÿè¡ŒæŒ‡ç¤º
1. ä¸Šè¨˜ã‚¹ãƒ†ãƒƒãƒ—ã«åŸºã¥ã„ã¦æ©Ÿèƒ½å®Ÿè£…ã‚’å®Ÿè¡Œ
2. æ—¢å­˜ã‚³ãƒ¼ãƒ‰ã¨ã®æ•´åˆæ€§ã‚’ç¢ºä¿
3. é©åˆ‡ãªãƒ†ã‚¹ãƒˆä½œæˆï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
4. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ›´æ–°
5. å“è³ªãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ

## å“è³ªè¦ä»¶
- æ—¢å­˜æ©Ÿèƒ½ã¸ã®å½±éŸ¿æœ€å°åŒ–
- é©åˆ‡ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
- ååˆ†ãªãƒ†ã‚¹ãƒˆç¶²ç¾…ç‡
- ä¿å®ˆæ€§ã®é«˜ã„ã‚³ãƒ¼ãƒ‰è¨­è¨ˆ
"""
        
        return context
    
    def _extract_implementation_dependencies(self, implementation_spec: Dict[str, Any]) -> List[str]:
        """å®Ÿè£…ä»•æ§˜ã‹ã‚‰ä¾å­˜é–¢ä¿‚æŠ½å‡º"""
        
        deps = []
        
        # åŸºæœ¬çš„ãªä¾å­˜é–¢ä¿‚
        imports = implementation_spec.get('imports', [])
        for imp in imports:
            if 'import' in imp:
                # importæ–‡ã‹ã‚‰ä¾å­˜é–¢ä¿‚ã‚’æŠ½å‡º
                if 'from' in imp:
                    module = imp.split('from')[1].split('import')[0].strip()
                    deps.append(module)
                else:
                    module = imp.replace('import', '').strip()
                    deps.append(module)
        
        # åŸºåº•ã‚¯ãƒ©ã‚¹ã‹ã‚‰ä¾å­˜é–¢ä¿‚ã‚’æŠ½å‡º
        base_classes = implementation_spec.get('base_classes', [])
        deps.extend(base_classes)
        
        return deps
    
    def _extract_feature_dependencies(self, step: Dict[str, Any], 
                                    feature_spec: Dict[str, Any]) -> List[str]:
        """æ©Ÿèƒ½ã‚¹ãƒ†ãƒƒãƒ—ã‹ã‚‰ä¾å­˜é–¢ä¿‚æŠ½å‡º"""
        
        deps = []
        
        # ã‚¹ãƒ†ãƒƒãƒ—ã‚¿ã‚¤ãƒ—ã«åŸºã¥ãä¾å­˜é–¢ä¿‚
        step_type = step.get('type', '')
        if step_type == 'modify':
            deps.append('existing_implementation')
        elif step_type == 'create':
            deps.extend(step.get('dependencies', []))
        
        # æ©Ÿèƒ½ä»•æ§˜ã‹ã‚‰ä¾å­˜é–¢ä¿‚
        feature_deps = feature_spec.get('dependencies', [])
        deps.extend(feature_deps)
        
        return deps
    
    def _analyze_and_order_chunks(self, chunks: List[ContextChunk], 
                                task_data: Dict[str, Any]) -> List[ContextChunk]:
        """ãƒãƒ£ãƒ³ã‚¯ä¾å­˜é–¢ä¿‚è§£æãƒ»å®Ÿè¡Œé †åºæ±ºå®š"""
        
        # ä¾å­˜é–¢ä¿‚ã‚°ãƒ©ãƒ•æ§‹ç¯‰
        dependency_graph = {}
        for chunk in chunks:
            dependency_graph[chunk.chunk_id] = chunk.dependencies
        
        # å®Ÿè¡Œé †åºè¨ˆç®—
        execution_order = self.dependency_analyzer.calculate_execution_order(dependency_graph)
        
        # å®Ÿè¡Œé †åºã‚’ãƒãƒ£ãƒ³ã‚¯ã«è¨­å®š
        for i, chunk_id in enumerate(execution_order):
            for chunk in chunks:
                if chunk.chunk_id == chunk_id:
                    chunk.execution_order = i
                    break
        
        # å®Ÿè¡Œé †åºã§ã‚½ãƒ¼ãƒˆ
        chunks.sort(key=lambda x: (x.execution_order, x.priority, x.chunk_id))
        
        return chunks
    
    def _calculate_priority(self, file_path: str, template_type: str) -> int:
        """å„ªå…ˆåº¦è¨ˆç®—"""
        
        priority = 5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åã«ã‚ˆã‚‹å„ªå…ˆåº¦
        filename = Path(file_path).name.lower()
        if 'main' in filename or '__init__' in filename:
            priority = 1  # æœ€é«˜å„ªå…ˆåº¦
        elif 'config' in filename or 'setting' in filename:
            priority = 2
        elif 'util' in filename or 'helper' in filename:
            priority = 3
        elif 'test' in filename:
            priority = 8  # ä½å„ªå…ˆåº¦
        
        # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚¿ã‚¤ãƒ—ã«ã‚ˆã‚‹å„ªå…ˆåº¦èª¿æ•´
        if template_type == 'class':
            priority -= 1
        elif template_type == 'module':
            priority -= 0
        elif template_type == 'function':
            priority += 1
        
        return max(1, min(priority, 10))
    
    def _calculate_file_priority(self, file_path: str) -> int:
        """ãƒ•ã‚¡ã‚¤ãƒ«å„ªå…ˆåº¦è¨ˆç®—"""
        return self._calculate_priority(file_path, 'file')
    
    def _calculate_function_priority(self, func_name: str) -> int:
        """é–¢æ•°å„ªå…ˆåº¦è¨ˆç®—"""
        
        if func_name.startswith('__'):
            return 2  # ç‰¹æ®Šãƒ¡ã‚½ãƒƒãƒ‰ã¯é«˜å„ªå…ˆåº¦
        elif func_name.startswith('_'):
            return 7  # ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆé–¢æ•°ã¯ä½å„ªå…ˆåº¦
        elif func_name in ['main', 'init', 'setup']:
            return 1  # ä¸»è¦é–¢æ•°ã¯æœ€é«˜å„ªå…ˆåº¦
        else:
            return 5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    
    def _calculate_class_priority(self, class_name: str) -> int:
        """ã‚¯ãƒ©ã‚¹å„ªå…ˆåº¦è¨ˆç®—"""
        
        if 'Manager' in class_name or 'Controller' in class_name:
            return 2  # ç®¡ç†ã‚¯ãƒ©ã‚¹ã¯é«˜å„ªå…ˆåº¦
        elif 'Helper' in class_name or 'Util' in class_name:
            return 6  # ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã¯ä½å„ªå…ˆåº¦
        elif 'Test' in class_name:
            return 8  # ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹ã¯ä½å„ªå…ˆåº¦
        else:
            return 4  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    
    def save_chunks_to_files(self, chunks: List[ContextChunk], 
                           output_dir: str = "tmp/context_chunks") -> List[str]:
        """åˆ†å‰²ã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        saved_files = []
        
        for chunk in chunks:
            chunk_filename = f"{chunk.chunk_id}.json"
            chunk_filepath = output_path / chunk_filename
            
            # ãƒãƒ£ãƒ³ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’è¾æ›¸ã«å¤‰æ›
            chunk_data = asdict(chunk)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            with open(chunk_filepath, 'w', encoding='utf-8') as f:
                json.dump(chunk_data, f, indent=2, ensure_ascii=False)
            
            saved_files.append(str(chunk_filepath))
        
        print(f"ğŸ’¾ {len(chunks)}ãƒãƒ£ãƒ³ã‚¯ã‚’{output_dir}ã«ä¿å­˜ã—ã¾ã—ãŸ")
        return saved_files
    
    def load_chunks_from_files(self, chunk_dir: str) -> List[ContextChunk]:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒãƒ£ãƒ³ã‚¯ã‚’èª­ã¿è¾¼ã¿"""
        
        chunks = []
        chunk_path = Path(chunk_dir)
        
        if not chunk_path.exists():
            return chunks
        
        for chunk_file in chunk_path.glob("*.json"):
            try:
                with open(chunk_file, 'r', encoding='utf-8') as f:
                    chunk_data = json.load(f)
                
                chunk = ContextChunk(**chunk_data)
                chunks.append(chunk)
                
            except Exception as e:
                print(f"âš ï¸ ãƒãƒ£ãƒ³ã‚¯èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {chunk_file}: {e}")
                continue
        
        # å®Ÿè¡Œé †åºã§ã‚½ãƒ¼ãƒˆ
        chunks.sort(key=lambda x: (x.execution_order, x.priority, x.chunk_id))
        
        return chunks


class TokenEstimator:
    """ãƒˆãƒ¼ã‚¯ãƒ³æ•°æ¨å®šå™¨"""
    
    def __init__(self):
        # æ—¥æœ¬èªãƒ»è‹±èªæ··åœ¨ãƒ†ã‚­ã‚¹ãƒˆã®å¹³å‡çš„ãªãƒˆãƒ¼ã‚¯ãƒ³å¤‰æ›ç‡
        self.avg_chars_per_token = 3.5
        
        # ã‚³ãƒ¼ãƒ‰ã®ç‰¹æ€§ã‚’è€ƒæ…®ã—ãŸèª¿æ•´ä¿‚æ•°
        self.code_adjustment = 1.2  # ã‚³ãƒ¼ãƒ‰ã¯é€šå¸¸ã®ãƒ†ã‚­ã‚¹ãƒˆã‚ˆã‚Šè‹¥å¹²å¤šãã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½¿ç”¨
    
    def estimate_tokens(self, text: str) -> int:
        """ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’æ¨å®š"""
        
        if not text:
            return 0
        
        # åŸºæœ¬çš„ãªæ–‡å­—æ•°ãƒ™ãƒ¼ã‚¹ã®æ¨å®š
        base_estimate = len(text) / self.avg_chars_per_token
        
        # ã‚³ãƒ¼ãƒ‰ç‰¹æ€§ã®è€ƒæ…®
        code_factor = self._analyze_code_characteristics(text)
        adjusted_estimate = base_estimate * (1 + code_factor * 0.2)
        
        return int(adjusted_estimate * self.code_adjustment)
    
    def _analyze_code_characteristics(self, text: str) -> float:
        """ã‚³ãƒ¼ãƒ‰ç‰¹æ€§åˆ†æ"""
        
        # ã‚³ãƒ¼ãƒ‰çš„ç‰¹å¾´ã®ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
        code_indicators = [
            (r'def\s+\w+\s*\(', 0.1),    # é–¢æ•°å®šç¾©
            (r'class\s+\w+\s*\(', 0.1),  # ã‚¯ãƒ©ã‚¹å®šç¾©
            (r'import\s+\w+', 0.05),     # importæ–‡
            (r'from\s+\w+\s+import', 0.05), # from importæ–‡
            (r'[{}()\[\]]', 0.02),       # æ‹¬å¼§é¡
            (r'[=+\-*/]', 0.01),         # æ¼”ç®—å­
        ]
        
        score = 0.0
        for pattern, weight in code_indicators:
            matches = len(re.findall(pattern, text))
            score += matches * weight
        
        # æ­£è¦åŒ–ï¼ˆæœ€å¤§1.0ï¼‰
        return min(score / len(text.split('\n')), 1.0)


def main():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    
    splitter = ContextSplitter()
    
    # ãƒ†ã‚¹ãƒˆã‚¿ã‚¹ã‚¯ï¼šæ–°è¦å®Ÿè£…
    test_task = {
        'task_id': 'test_implementation_001',
        'type': 'new_implementation',
        'target_files': [
            'postbox/context/test_new_class.py',
            'postbox/context/test_utility.py'
        ],
        'requirements': {
            'implementation_spec': {
                'template_type': 'class',
                'class_name': 'TestContextManager',
                'methods': ['__init__', 'process', 'validate'],
                'imports': ['from typing import Dict, List, Any'],
                'description': 'ãƒ†ã‚¹ãƒˆç”¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç®¡ç†ã‚¯ãƒ©ã‚¹'
            }
        }
    }
    
    print("ğŸ§ª ContextSplitter ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
    
    # å®Ÿè£…ã‚¿ã‚¹ã‚¯åˆ†å‰²ãƒ†ã‚¹ãƒˆ
    chunks = splitter.split_implementation_task(test_task)
    
    print(f"\nğŸ“Š åˆ†å‰²çµæœ:")
    for i, chunk in enumerate(chunks):
        print(f"{i+1}. {chunk.chunk_id}")
        print(f"   ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {chunk.estimated_tokens}")
        print(f"   å„ªå…ˆåº¦: {chunk.priority}")
        print(f"   å®Ÿè¡Œé †åº: {chunk.execution_order}")
        print(f"   ä¾å­˜é–¢ä¿‚: {chunk.dependencies}")
        print()
    
    # ãƒãƒ£ãƒ³ã‚¯ã®ä¿å­˜ãƒ†ã‚¹ãƒˆ
    saved_files = splitter.save_chunks_to_files(chunks)
    print(f"ğŸ’¾ ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«: {len(saved_files)}ä»¶")
    
    # ãƒãƒ£ãƒ³ã‚¯ã®èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ
    loaded_chunks = splitter.load_chunks_from_files("tmp/context_chunks")
    print(f"ğŸ“– èª­ã¿è¾¼ã¾ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯: {len(loaded_chunks)}ä»¶")


if __name__ == "__main__":
    main()