#!/usr/bin/env python3
"""
CollaborationFlowOptimizer - Issue #860å¯¾å¿œ
GeminiÃ—Claudeå”æ¥­ãƒ•ãƒ­ãƒ¼ã®æœ€é©åŒ–ã‚¨ãƒ³ã‚¸ãƒ³

ã‚¿ã‚¹ã‚¯ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°æœ€é©åŒ–ãƒ»ä½œæ¥­åˆ†æ‹…åˆ¤å®šãƒ»è‡ªå‹•å®Ÿè¡Œãƒ‘ã‚¹é¸æŠã®ç²¾åº¦å‘ä¸Šã«ã‚ˆã‚Š
å”æ¥­å®‰å…¨æ€§ã‚’40/100ã‹ã‚‰75-80/100ã«å‘ä¸Šã•ã›ã‚‹
"""

import json
import datetime
import statistics
from typing import Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass, asdict
from enum import Enum
from pathlib import Path
import os
import re

from kumihan_formatter.core.utilities.logger import get_logger

logger = get_logger(__name__)


class TaskComplexity(Enum):
    """ã‚¿ã‚¹ã‚¯è¤‡é›‘åº¦"""
    SIMPLE = "simple"           # å˜ç´”ä½œæ¥­
    MODERATE = "moderate"       # ä¸­ç¨‹åº¦
    COMPLEX = "complex"         # è¤‡é›‘
    CRITICAL = "critical"       # é‡è¦ãƒ»é«˜ãƒªã‚¹ã‚¯


class AgentCapability(Enum):
    """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆèƒ½åŠ›"""
    GEMINI_OPTIMAL = "gemini_optimal"     # Geminiæœ€é©
    CLAUDE_OPTIMAL = "claude_optimal"     # Claudeæœ€é©
    HYBRID_REQUIRED = "hybrid_required"   # å”æ¥­å¿…é ˆ
    AUTO_DECISION = "auto_decision"       # è‡ªå‹•åˆ¤å®š


class OptimizationStrategy(Enum):
    """æœ€é©åŒ–æˆ¦ç•¥"""
    SPEED_FOCUSED = "speed_focused"       # é€Ÿåº¦é‡è¦–
    QUALITY_FOCUSED = "quality_focused"   # å“è³ªé‡è¦–
    COST_FOCUSED = "cost_focused"         # ã‚³ã‚¹ãƒˆé‡è¦–
    BALANCED = "balanced"                 # ãƒãƒ©ãƒ³ã‚¹é‡è¦–


@dataclass
class TaskRoutingDecision:
    """ã‚¿ã‚¹ã‚¯ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°æ±ºå®š"""
    task_id: str
    recommended_agent: AgentCapability
    confidence_score: float  # 0.0-1.0
    reasoning: List[str]
    estimated_success_rate: float
    estimated_processing_time: float
    risk_factors: List[str]
    optimization_strategy: OptimizationStrategy


@dataclass
class CollaborationPattern:
    """å”æ¥­ãƒ‘ã‚¿ãƒ¼ãƒ³"""
    pattern_id: str
    task_type: str
    success_rate: float
    average_processing_time: float
    failure_modes: List[str]
    optimization_tips: List[str]
    last_updated: str


@dataclass
class FlowOptimizationResult:
    """ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–çµæœ"""
    timestamp: str
    optimizations_applied: List[str]
    performance_improvement: Dict[str, float]
    new_routing_rules: List[Dict[str, Any]]
    updated_patterns: List[str]
    recommendations: List[str]


class CollaborationFlowOptimizer:
    """å”æ¥­ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–ã‚¨ãƒ³ã‚¸ãƒ³

    GeminiÃ—Claudeé–“ã®æœ€é©ãªã‚¿ã‚¹ã‚¯ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã€ä½œæ¥­åˆ†æ‹…åˆ¤å®šã€
    è‡ªå‹•å®Ÿè¡Œãƒ‘ã‚¹é¸æŠã‚’è¡Œã„ã€å”æ¥­åŠ¹ç‡ã‚’æœ€å¤§åŒ–ã™ã‚‹
    """

    def __init__(self, postbox_dir: str = "postbox"):
        self.postbox_dir = Path(postbox_dir)
        self.collaboration_dir = self.postbox_dir / "collaboration"
        self.monitoring_dir = self.postbox_dir / "monitoring"

        # æœ€é©åŒ–ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        self.collaboration_dir.mkdir(exist_ok=True)

        self.routing_rules_file = self.collaboration_dir / "routing_rules.json"
        self.patterns_file = self.collaboration_dir / "collaboration_patterns.json"
        self.optimization_history_file = self.collaboration_dir / "optimization_history.json"

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒ«ãƒ¼ãƒ«èª­ã¿è¾¼ã¿/ä½œæˆ
        self.routing_rules = self._load_or_create_routing_rules()

        # å”æ¥­ãƒ‘ã‚¿ãƒ¼ãƒ³èª­ã¿è¾¼ã¿/ä½œæˆ
        self.collaboration_patterns = self._load_or_create_patterns()

        logger.info("ğŸš€ CollaborationFlowOptimizer åˆæœŸåŒ–å®Œäº†")

    def optimize_task_routing(self, task_data: Dict[str, Any],
                            strategy: OptimizationStrategy = OptimizationStrategy.BALANCED) -> TaskRoutingDecision:
        """ã‚¿ã‚¹ã‚¯ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°æœ€é©åŒ–

        Args:
            task_data: ã‚¿ã‚¹ã‚¯è©³ç´°ãƒ‡ãƒ¼ã‚¿
            strategy: æœ€é©åŒ–æˆ¦ç•¥

        Returns:
            TaskRoutingDecision: ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°æ±ºå®š
        """

        logger.info(f"ğŸ¯ ã‚¿ã‚¹ã‚¯ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°æœ€é©åŒ–é–‹å§‹: {task_data.get('task_id', 'unknown')}")

        try:
            # ã‚¿ã‚¹ã‚¯è¤‡é›‘åº¦åˆ†æ
            complexity = self._analyze_task_complexity(task_data)

            # éå»ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
            matching_patterns = self._find_matching_patterns(task_data)

            # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆèƒ½åŠ›è©•ä¾¡
            agent_capability = self._evaluate_agent_capability(task_data, complexity, strategy)

            # æˆåŠŸç‡äºˆæ¸¬
            success_rate = self._predict_success_rate(task_data, agent_capability, matching_patterns)

            # å‡¦ç†æ™‚é–“äºˆæ¸¬
            processing_time = self._predict_processing_time(task_data, agent_capability, matching_patterns)

            # ãƒªã‚¹ã‚¯è¦å› åˆ†æ
            risk_factors = self._analyze_risk_factors(task_data, complexity)

            # æ¨è«–ç†ç”±ç”Ÿæˆ
            reasoning = self._generate_routing_reasoning(
                task_data, complexity, agent_capability, strategy
            )

            # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—
            confidence = self._calculate_confidence_score(
                matching_patterns, success_rate, risk_factors
            )

            decision = TaskRoutingDecision(
                task_id=task_data.get("task_id", f"task_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}"),
                recommended_agent=agent_capability,
                confidence_score=confidence,
                reasoning=reasoning,
                estimated_success_rate=success_rate,
                estimated_processing_time=processing_time,
                risk_factors=risk_factors,
                optimization_strategy=strategy
            )

            # ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°æ±ºå®šã‚’è¨˜éŒ²
            self._record_routing_decision(decision)

            logger.info(f"âœ… ã‚¿ã‚¹ã‚¯ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°æœ€é©åŒ–å®Œäº†: {agent_capability.value} (ä¿¡é ¼åº¦: {confidence:.3f})")

            return decision

        except Exception as e:
            logger.error(f"âŒ ã‚¿ã‚¹ã‚¯ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°æœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return self._create_fallback_routing_decision(task_data, strategy)

    def _analyze_task_complexity(self, task_data: Dict[str, Any]) -> TaskComplexity:
        """ã‚¿ã‚¹ã‚¯è¤‡é›‘åº¦åˆ†æ"""

        task_type = task_data.get("type", "unknown")
        description = task_data.get("description", "")
        requirements = task_data.get("requirements", {})
        target_files = task_data.get("target_files", [])

        complexity_score = 0

        # ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—åˆ¥è¤‡é›‘åº¦
        if task_type in ["new_implementation", "new_feature_development"]:
            complexity_score += 3
        elif task_type in ["hybrid_implementation", "code_modification"]:
            complexity_score += 2
        elif task_type in ["error_fixing", "simple_modification"]:
            complexity_score += 1

        # ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã«ã‚ˆã‚‹è¤‡é›‘åº¦
        file_count = len(target_files)
        if file_count > 10:
            complexity_score += 3
        elif file_count > 5:
            complexity_score += 2
        elif file_count > 1:
            complexity_score += 1

        # èª¬æ˜æ–‡ã®è¤‡é›‘ã•
        description_words = len(description.split()) if description else 0
        if description_words > 100:
            complexity_score += 2
        elif description_words > 50:
            complexity_score += 1

        # è¦ä»¶ã®è¤‡é›‘ã•
        if isinstance(requirements, dict):
            req_count = len(requirements)
            if req_count > 5:
                complexity_score += 2
            elif req_count > 2:
                complexity_score += 1

        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹è¤‡é›‘åº¦
        complex_keywords = [
            "architecture", "integration", "security", "performance",
            "database", "api", "complex", "critical", "refactor"
        ]

        description_lower = description.lower()
        for keyword in complex_keywords:
            if keyword in description_lower:
                complexity_score += 1

        # è¤‡é›‘åº¦åˆ¤å®š
        if complexity_score >= 8:
            return TaskComplexity.CRITICAL
        elif complexity_score >= 5:
            return TaskComplexity.COMPLEX
        elif complexity_score >= 2:
            return TaskComplexity.MODERATE
        else:
            return TaskComplexity.SIMPLE

    def _evaluate_agent_capability(self, task_data: Dict[str, Any],
                                 complexity: TaskComplexity,
                                 strategy: OptimizationStrategy) -> AgentCapability:
        """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆèƒ½åŠ›è©•ä¾¡"""

        task_type = task_data.get("type", "unknown")
        description = task_data.get("description", "")

        # Geminiæœ€é©ã‚¿ã‚¹ã‚¯
        gemini_optimal_types = [
            "error_fixing", "simple_modification", "type_annotation_fix",
            "syntax_fix", "format_fix", "import_fix"
        ]

        # Claudeæœ€é©ã‚¿ã‚¹ã‚¯
        claude_optimal_types = [
            "new_implementation", "architecture_design", "complex_refactor",
            "security_implementation", "code_review", "design_decision"
        ]

        # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å¿…é ˆã‚¿ã‚¹ã‚¯
        hybrid_required_types = [
            "hybrid_implementation", "new_feature_development",
            "integration_development", "system_integration"
        ]

        # ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ãƒ™ãƒ¼ã‚¹åˆ¤å®š
        if task_type in gemini_optimal_types:
            base_recommendation = AgentCapability.GEMINI_OPTIMAL
        elif task_type in claude_optimal_types:
            base_recommendation = AgentCapability.CLAUDE_OPTIMAL
        elif task_type in hybrid_required_types:
            base_recommendation = AgentCapability.HYBRID_REQUIRED
        else:
            base_recommendation = AgentCapability.AUTO_DECISION

        # è¤‡é›‘åº¦ã«ã‚ˆã‚‹èª¿æ•´
        if complexity == TaskComplexity.CRITICAL:
            # é‡è¦ã‚¿ã‚¹ã‚¯ã¯Claudeä¸»å°ã¾ãŸã¯ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰
            if base_recommendation == AgentCapability.GEMINI_OPTIMAL:
                base_recommendation = AgentCapability.HYBRID_REQUIRED
        elif complexity == TaskComplexity.SIMPLE:
            # å˜ç´”ã‚¿ã‚¹ã‚¯ã¯Geminiæœ€é©åŒ–
            if base_recommendation == AgentCapability.CLAUDE_OPTIMAL:
                base_recommendation = AgentCapability.GEMINI_OPTIMAL

        # æˆ¦ç•¥ã«ã‚ˆã‚‹èª¿æ•´
        if strategy == OptimizationStrategy.SPEED_FOCUSED:
            # é€Ÿåº¦é‡è¦–ãªã‚‰Geminiå„ªå…ˆ
            if base_recommendation == AgentCapability.AUTO_DECISION:
                base_recommendation = AgentCapability.GEMINI_OPTIMAL
        elif strategy == OptimizationStrategy.QUALITY_FOCUSED:
            # å“è³ªé‡è¦–ãªã‚‰Claudeå„ªå…ˆ
            if base_recommendation == AgentCapability.AUTO_DECISION:
                base_recommendation = AgentCapability.CLAUDE_OPTIMAL
        elif strategy == OptimizationStrategy.COST_FOCUSED:
            # ã‚³ã‚¹ãƒˆé‡è¦–ãªã‚‰Geminiå„ªå…ˆ
            if base_recommendation in [AgentCapability.AUTO_DECISION, AgentCapability.CLAUDE_OPTIMAL]:
                base_recommendation = AgentCapability.GEMINI_OPTIMAL

        # èª¬æ˜æ–‡ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ
        description_lower = description.lower()

        # Claudeæœ€é©ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        claude_keywords = [
            "design", "architecture", "strategy", "analysis", "review",
            "decision", "planning", "complex", "critical", "security"
        ]

        # Geminiæœ€é©ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        gemini_keywords = [
            "fix", "error", "type", "syntax", "format", "simple",
            "quick", "batch", "auto", "generate"
        ]

        claude_score = sum(1 for kw in claude_keywords if kw in description_lower)
        gemini_score = sum(1 for kw in gemini_keywords if kw in description_lower)

        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚¹ã‚³ã‚¢ã«ã‚ˆã‚‹å¾®èª¿æ•´
        if claude_score > gemini_score + 2:
            if base_recommendation == AgentCapability.GEMINI_OPTIMAL:
                base_recommendation = AgentCapability.HYBRID_REQUIRED
        elif gemini_score > claude_score + 2:
            if base_recommendation == AgentCapability.CLAUDE_OPTIMAL:
                base_recommendation = AgentCapability.HYBRID_REQUIRED

        return base_recommendation

    def _find_matching_patterns(self, task_data: Dict[str, Any]) -> List[CollaborationPattern]:
        """é¡ä¼¼ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œç´¢"""

        task_type = task_data.get("type", "unknown")
        description = task_data.get("description", "")

        matching_patterns = []

        for pattern in self.collaboration_patterns:
            # ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ãƒãƒƒãƒãƒ³ã‚°
            if pattern.task_type == task_type:
                matching_patterns.append(pattern)
                continue

            # èª¬æ˜æ–‡é¡ä¼¼åº¦ãƒã‚§ãƒƒã‚¯ï¼ˆç°¡æ˜“ï¼‰
            pattern_words = set(pattern.task_type.split("_"))
            description_words = set(description.lower().split())

            overlap = len(pattern_words & description_words)
            if overlap > 0:
                matching_patterns.append(pattern)

        # æˆåŠŸç‡é †ã§ã‚½ãƒ¼ãƒˆ
        matching_patterns.sort(key=lambda p: p.success_rate, reverse=True)

        return matching_patterns[:5]  # ä¸Šä½5ä»¶

    def _predict_success_rate(self, task_data: Dict[str, Any],
                            agent_capability: AgentCapability,
                            matching_patterns: List[CollaborationPattern]) -> float:
        """æˆåŠŸç‡äºˆæ¸¬"""

        # ãƒ™ãƒ¼ã‚¹æˆåŠŸç‡
        base_rates = {
            AgentCapability.GEMINI_OPTIMAL: 0.85,
            AgentCapability.CLAUDE_OPTIMAL: 0.80,
            AgentCapability.HYBRID_REQUIRED: 0.75,
            AgentCapability.AUTO_DECISION: 0.70
        }

        base_rate = base_rates.get(agent_capability, 0.70)

        # éå»ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ã‚ˆã‚‹èª¿æ•´
        if matching_patterns:
            pattern_rates = [p.success_rate for p in matching_patterns[:3]]
            pattern_avg = statistics.mean(pattern_rates)

            # éå»ãƒ‘ã‚¿ãƒ¼ãƒ³ã®é‡ã¿: 30%
            adjusted_rate = base_rate * 0.7 + pattern_avg * 0.3
        else:
            adjusted_rate = base_rate

        # ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã«ã‚ˆã‚‹èª¿æ•´
        target_files = task_data.get("target_files", [])
        file_count = len(target_files)

        if file_count > 10:
            adjusted_rate *= 0.9  # 10%æ¸›
        elif file_count > 5:
            adjusted_rate *= 0.95  # 5%æ¸›

        return min(adjusted_rate, 1.0)

    def _predict_processing_time(self, task_data: Dict[str, Any],
                               agent_capability: AgentCapability,
                               matching_patterns: List[CollaborationPattern]) -> float:
        """å‡¦ç†æ™‚é–“äºˆæ¸¬"""

        # ãƒ™ãƒ¼ã‚¹å‡¦ç†æ™‚é–“ï¼ˆç§’ï¼‰
        base_times = {
            AgentCapability.GEMINI_OPTIMAL: 1.0,
            AgentCapability.CLAUDE_OPTIMAL: 1.5,
            AgentCapability.HYBRID_REQUIRED: 2.0,
            AgentCapability.AUTO_DECISION: 1.8
        }

        base_time = base_times.get(agent_capability, 1.8)

        # éå»ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ã‚ˆã‚‹èª¿æ•´
        if matching_patterns:
            pattern_times = [p.average_processing_time for p in matching_patterns[:3]]
            pattern_avg = statistics.mean(pattern_times)

            # éå»ãƒ‘ã‚¿ãƒ¼ãƒ³ã®é‡ã¿: 40%
            adjusted_time = base_time * 0.6 + pattern_avg * 0.4
        else:
            adjusted_time = base_time

        # ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã«ã‚ˆã‚‹èª¿æ•´
        target_files = task_data.get("target_files", [])
        file_count = len(target_files)

        if file_count > 10:
            adjusted_time *= 2.0
        elif file_count > 5:
            adjusted_time *= 1.5
        elif file_count > 1:
            adjusted_time *= 1.2

        # èª¬æ˜æ–‡ã®é•·ã•ã«ã‚ˆã‚‹èª¿æ•´
        description = task_data.get("description", "")
        word_count = len(description.split()) if description else 0

        if word_count > 100:
            adjusted_time *= 1.3
        elif word_count > 50:
            adjusted_time *= 1.1

        return adjusted_time

    def _analyze_risk_factors(self, task_data: Dict[str, Any],
                            complexity: TaskComplexity) -> List[str]:
        """ãƒªã‚¹ã‚¯è¦å› åˆ†æ"""

        risk_factors = []

        # è¤‡é›‘åº¦ãƒªã‚¹ã‚¯
        if complexity == TaskComplexity.CRITICAL:
            risk_factors.append("ã‚¿ã‚¹ã‚¯è¤‡é›‘åº¦ãŒéå¸¸ã«é«˜ã„")
        elif complexity == TaskComplexity.COMPLEX:
            risk_factors.append("ã‚¿ã‚¹ã‚¯è¤‡é›‘åº¦ãŒé«˜ã„")

        # ãƒ•ã‚¡ã‚¤ãƒ«æ•°ãƒªã‚¹ã‚¯
        target_files = task_data.get("target_files", [])
        file_count = len(target_files)

        if file_count > 10:
            risk_factors.append("å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°ãŒå¤šã„ï¼ˆ10+ï¼‰")
        elif file_count > 5:
            risk_factors.append("å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°ãŒã‚„ã‚„å¤šã„ï¼ˆ5+ï¼‰")

        # ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ãƒªã‚¹ã‚¯
        high_risk_types = [
            "new_implementation", "architecture_design", "security_implementation",
            "database_modification", "api_integration"
        ]

        task_type = task_data.get("type", "unknown")
        if task_type in high_risk_types:
            risk_factors.append(f"é«˜ãƒªã‚¹ã‚¯ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—: {task_type}")

        # èª¬æ˜æ–‡ãƒªã‚¹ã‚¯è¦å› 
        description = task_data.get("description", "").lower()
        risk_keywords = [
            "critical", "urgent", "breaking", "security", "production",
            "database", "migration", "refactor", "legacy"
        ]

        for keyword in risk_keywords:
            if keyword in description:
                risk_factors.append(f"ãƒªã‚¹ã‚¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œå‡º: {keyword}")

        # è¦ä»¶ä¸æ˜ç¢ºãƒªã‚¹ã‚¯
        requirements = task_data.get("requirements", {})
        if not requirements or len(requirements) < 2:
            risk_factors.append("è¦ä»¶ãŒä¸æ˜ç¢ºã¾ãŸã¯ä¸è¶³")

        return risk_factors

    def _generate_routing_reasoning(self, task_data: Dict[str, Any],
                                  complexity: TaskComplexity,
                                  agent_capability: AgentCapability,
                                  strategy: OptimizationStrategy) -> List[str]:
        """ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°æ¨è«–ç†ç”±ç”Ÿæˆ"""

        reasoning = []

        # åŸºæœ¬æ¨è«–
        task_type = task_data.get("type", "unknown")
        reasoning.append(f"ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ— '{task_type}' ã‚’åˆ†æ")
        reasoning.append(f"è¤‡é›‘åº¦ãƒ¬ãƒ™ãƒ«: {complexity.value}")
        reasoning.append(f"æœ€é©åŒ–æˆ¦ç•¥: {strategy.value}")

        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé¸æŠç†ç”±
        if agent_capability == AgentCapability.GEMINI_OPTIMAL:
            reasoning.append("Geminiæœ€é©: åŠ¹ç‡çš„ãªè‡ªå‹•å‡¦ç†ãŒå¯èƒ½")
            reasoning.append("é«˜é€Ÿå‡¦ç†ã¨ã‚³ã‚¹ãƒˆåŠ¹ç‡ã‚’é‡è¦–")
        elif agent_capability == AgentCapability.CLAUDE_OPTIMAL:
            reasoning.append("Claudeæœ€é©: é«˜åº¦ãªåˆ¤æ–­åŠ›ã¨å“è³ªãŒå¿…è¦")
            reasoning.append("æˆ¦ç•¥çš„æ€è€ƒã¨è¤‡é›‘ãªå•é¡Œè§£æ±ºã‚’é‡è¦–")
        elif agent_capability == AgentCapability.HYBRID_REQUIRED:
            reasoning.append("ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å”æ¥­: ä¸¡ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å¼·ã¿ã‚’æ´»ç”¨")
            reasoning.append("é«˜å“è³ªã¨åŠ¹ç‡ã®ãƒãƒ©ãƒ³ã‚¹ã‚’é‡è¦–")
        else:
            reasoning.append("è‡ªå‹•åˆ¤å®š: å‹•çš„ãªæœ€é©åŒ–åˆ¤æ–­ã‚’å®Ÿæ–½")

        # æˆ¦ç•¥åˆ¥ç†ç”±
        if strategy == OptimizationStrategy.SPEED_FOCUSED:
            reasoning.append("é€Ÿåº¦é‡è¦–æˆ¦ç•¥ã«ã‚ˆã‚Šå‡¦ç†æ™‚é–“ã‚’æœ€å°åŒ–")
        elif strategy == OptimizationStrategy.QUALITY_FOCUSED:
            reasoning.append("å“è³ªé‡è¦–æˆ¦ç•¥ã«ã‚ˆã‚Šæœ€é«˜å“è³ªã‚’ç¢ºä¿")
        elif strategy == OptimizationStrategy.COST_FOCUSED:
            reasoning.append("ã‚³ã‚¹ãƒˆé‡è¦–æˆ¦ç•¥ã«ã‚ˆã‚ŠTokenä½¿ç”¨é‡ã‚’æœ€å°åŒ–")

        # ãƒ•ã‚¡ã‚¤ãƒ«æ•°è€ƒæ…®
        target_files = task_data.get("target_files", [])
        file_count = len(target_files)
        if file_count > 5:
            reasoning.append(f"å¤šæ•°ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ï¼ˆ{file_count}ä»¶ï¼‰ã‚’è€ƒæ…®")

        return reasoning

    def _calculate_confidence_score(self, matching_patterns: List[CollaborationPattern],
                                  success_rate: float, risk_factors: List[str]) -> float:
        """ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—"""

        # ãƒ™ãƒ¼ã‚¹ä¿¡é ¼åº¦
        base_confidence = 0.7

        # éå»ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ã‚ˆã‚‹èª¿æ•´
        if matching_patterns:
            pattern_count = len(matching_patterns)
            pattern_boost = min(pattern_count * 0.05, 0.15)  # æœ€å¤§15%å‘ä¸Š
            base_confidence += pattern_boost

        # æˆåŠŸç‡ã«ã‚ˆã‚‹èª¿æ•´
        success_boost = (success_rate - 0.7) * 0.5  # æˆåŠŸç‡70%ã‚’åŸºæº–
        base_confidence += success_boost

        # ãƒªã‚¹ã‚¯è¦å› ã«ã‚ˆã‚‹èª¿æ•´
        risk_penalty = len(risk_factors) * 0.03  # ãƒªã‚¹ã‚¯è¦å› 1ã¤ã«ã¤ã3%æ¸›
        base_confidence -= risk_penalty

        return max(min(base_confidence, 1.0), 0.1)  # 0.1-1.0ã®ç¯„å›²

    def _record_routing_decision(self, decision: TaskRoutingDecision) -> None:
        """ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°æ±ºå®šè¨˜éŒ²"""

        try:
            # æ—¢å­˜è¨˜éŒ²èª­ã¿è¾¼ã¿
            history_file = self.collaboration_dir / "routing_decisions.json"

            if history_file.exists():
                with open(history_file, 'r', encoding='utf-8') as f:
                    decisions = json.load(f)
            else:
                decisions = []

            # æ–°ã—ã„æ±ºå®šè¿½åŠ 
            decision_dict = asdict(decision)
            decision_dict["timestamp"] = datetime.datetime.now().isoformat()
            decisions.append(decision_dict)

            # æœ€æ–°100ä»¶ã®ã¿ä¿æŒ
            if len(decisions) > 100:
                decisions = decisions[-100:]

            # ä¿å­˜
            with open(history_file, 'w', encoding='utf-8') as f:
                json.dump(decisions, f, indent=2, ensure_ascii=False, default=str)

            logger.info(f"âœ… ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°æ±ºå®šè¨˜éŒ²å®Œäº†: {decision.task_id}")

        except Exception as e:
            logger.error(f"âŒ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°æ±ºå®šè¨˜éŒ²ã‚¨ãƒ©ãƒ¼: {e}")

    def _create_fallback_routing_decision(self, task_data: Dict[str, Any],
                                        strategy: OptimizationStrategy) -> TaskRoutingDecision:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°æ±ºå®š"""

        return TaskRoutingDecision(
            task_id=task_data.get("task_id", "fallback_task"),
            recommended_agent=AgentCapability.AUTO_DECISION,
            confidence_score=0.5,
            reasoning=["ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ±ºå®š: è©³ç´°åˆ†æã«å¤±æ•—"],
            estimated_success_rate=0.7,
            estimated_processing_time=2.0,
            risk_factors=["åˆ†æãƒ‡ãƒ¼ã‚¿ä¸è¶³"],
            optimization_strategy=strategy
        )

    def optimize_collaboration_flow(self) -> FlowOptimizationResult:
        """å”æ¥­ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–å®Ÿè¡Œ"""

        logger.info("ğŸ”„ å”æ¥­ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–é–‹å§‹")

        try:
            # éå»ã®å”æ¥­ãƒ‡ãƒ¼ã‚¿åˆ†æ
            performance_analysis = self._analyze_collaboration_performance()

            # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®š
            bottlenecks = self._identify_collaboration_bottlenecks(performance_analysis)

            # æœ€é©åŒ–ãƒ«ãƒ¼ãƒ«æ›´æ–°
            new_rules = self._update_routing_rules(bottlenecks)

            # ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’ãƒ»æ›´æ–°
            updated_patterns = self._update_collaboration_patterns(performance_analysis)

            # æœ€é©åŒ–æ¨å¥¨äº‹é …ç”Ÿæˆ
            recommendations = self._generate_optimization_recommendations(bottlenecks)

            # æ”¹å–„è¦‹ç©ã‚‚ã‚Š
            performance_improvement = self._estimate_performance_improvement(bottlenecks, new_rules)

            result = FlowOptimizationResult(
                timestamp=datetime.datetime.now().isoformat(),
                optimizations_applied=[
                    f"ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒ«ãƒ¼ãƒ«æ›´æ–°: {len(new_rules)}ä»¶",
                    f"å”æ¥­ãƒ‘ã‚¿ãƒ¼ãƒ³æ›´æ–°: {len(updated_patterns)}ä»¶",
                    f"ãƒœãƒˆãƒ«ãƒãƒƒã‚¯å¯¾ç­–: {len(bottlenecks)}ä»¶"
                ],
                performance_improvement=performance_improvement,
                new_routing_rules=new_rules,
                updated_patterns=updated_patterns,
                recommendations=recommendations
            )

            # æœ€é©åŒ–çµæœä¿å­˜
            self._save_optimization_result(result)

            logger.info("âœ… å”æ¥­ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–å®Œäº†")

            return result

        except Exception as e:
            logger.error(f"âŒ å”æ¥­ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return self._create_fallback_optimization_result()

    def _analyze_collaboration_performance(self) -> Dict[str, Any]:
        """å”æ¥­ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ"""

        analysis = {
            "total_tasks": 0,
            "success_rates": {},
            "processing_times": {},
            "failure_modes": [],
            "bottleneck_indicators": {}
        }

        # 3å±¤æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿åˆ†æ
        verification_file = self.monitoring_dir / "three_layer_verification_metrics.json"
        if verification_file.exists():
            try:
                with open(verification_file, 'r', encoding='utf-8') as f:
                    verification_data = json.load(f)

                if verification_data:
                    recent_data = verification_data[-10:]  # æœ€æ–°10ä»¶

                    # æˆåŠŸç‡åˆ†æ
                    for layer in ["layer1", "layer2", "layer3", "integration"]:
                        rates = [d["success_rates"].get(f"{layer}_success_rate", 0) for d in recent_data]
                        analysis["success_rates"][layer] = statistics.mean(rates) if rates else 0

                    # å‡¦ç†æ™‚é–“åˆ†æ
                    times = [d["execution_time"] for d in recent_data]
                    analysis["processing_times"]["average"] = statistics.mean(times) if times else 0
                    analysis["processing_times"]["max"] = max(times) if times else 0

                    analysis["total_tasks"] = len(verification_data)

            except Exception as e:
                logger.warning(f"âš ï¸ æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿åˆ†æã‚¨ãƒ©ãƒ¼: {e}")

        # ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ãƒ‡ãƒ¼ã‚¿åˆ†æ
        failsafe_file = self.monitoring_dir / "failsafe_usage.json"
        if failsafe_file.exists():
            try:
                with open(failsafe_file, 'r', encoding='utf-8') as f:
                    failsafe_data = json.load(f)

                if failsafe_data:
                    recent_failsafe = failsafe_data[-10:]

                    # å¤±æ•—ãƒ¢ãƒ¼ãƒ‰åˆ†æ
                    for entry in recent_failsafe:
                        for failsafe_type in entry.get("failsafe_types", []):
                            analysis["failure_modes"].append(failsafe_type)

                    # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æŒ‡æ¨™
                    total_failsafe = sum(entry["failsafe_count"] for entry in recent_failsafe)
                    total_files = sum(entry["total_files"] for entry in recent_failsafe)

                    if total_files > 0:
                        analysis["bottleneck_indicators"]["failsafe_rate"] = total_failsafe / total_files

            except Exception as e:
                logger.warning(f"âš ï¸ ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ãƒ‡ãƒ¼ã‚¿åˆ†æã‚¨ãƒ©ãƒ¼: {e}")

        return analysis

    def _identify_collaboration_bottlenecks(self, performance_analysis: Dict[str, Any]) -> List[str]:
        """å”æ¥­ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®š"""

        bottlenecks = []

        # æˆåŠŸç‡ãƒœãƒˆãƒ«ãƒãƒƒã‚¯
        success_rates = performance_analysis.get("success_rates", {})
        for layer, rate in success_rates.items():
            if rate < 0.8:
                bottlenecks.append(f"{layer}_low_success_rate")

        # å‡¦ç†æ™‚é–“ãƒœãƒˆãƒ«ãƒãƒƒã‚¯
        processing_times = performance_analysis.get("processing_times", {})
        avg_time = processing_times.get("average", 0)
        max_time = processing_times.get("max", 0)

        if avg_time > 2.0:
            bottlenecks.append("slow_average_processing")

        if max_time > 5.0:
            bottlenecks.append("extremely_slow_processing")

        # ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ãƒœãƒˆãƒ«ãƒãƒƒã‚¯
        bottleneck_indicators = performance_analysis.get("bottleneck_indicators", {})
        failsafe_rate = bottleneck_indicators.get("failsafe_rate", 0)

        if failsafe_rate > 0.3:
            bottlenecks.append("high_failsafe_usage")

        # å¤±æ•—ãƒ¢ãƒ¼ãƒ‰ãƒœãƒˆãƒ«ãƒãƒƒã‚¯
        failure_modes = performance_analysis.get("failure_modes", [])
        failure_counts = {}

        for mode in failure_modes:
            failure_counts[mode] = failure_counts.get(mode, 0) + 1

        for mode, count in failure_counts.items():
            if count > 3:  # 3å›ä»¥ä¸Šã®åŒã˜å¤±æ•—
                bottlenecks.append(f"repeated_{mode}")

        return bottlenecks

    def _update_routing_rules(self, bottlenecks: List[str]) -> List[Dict[str, Any]]:
        """ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒ«ãƒ¼ãƒ«æ›´æ–°"""

        new_rules = []

        # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ¥ãƒ«ãƒ¼ãƒ«ç”Ÿæˆ
        for bottleneck in bottlenecks:
            if "layer1" in bottleneck:
                new_rules.append({
                    "condition": "syntax_validation_required",
                    "action": "prefer_gemini_with_pre_validation",
                    "reason": f"Layer1ãƒœãƒˆãƒ«ãƒãƒƒã‚¯å¯¾ç­–: {bottleneck}"
                })

            elif "layer2" in bottleneck:
                new_rules.append({
                    "condition": "quality_check_critical",
                    "action": "require_claude_review",
                    "reason": f"Layer2å“è³ªãƒœãƒˆãƒ«ãƒãƒƒã‚¯å¯¾ç­–: {bottleneck}"
                })

            elif "layer3" in bottleneck:
                new_rules.append({
                    "condition": "claude_approval_needed",
                    "action": "optimize_approval_process",
                    "reason": f"Layer3æ‰¿èªãƒœãƒˆãƒ«ãƒãƒƒã‚¯å¯¾ç­–: {bottleneck}"
                })

            elif "failsafe" in bottleneck:
                new_rules.append({
                    "condition": "high_failsafe_risk",
                    "action": "enable_preventive_validation",
                    "reason": f"ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•éä½¿ç”¨å¯¾ç­–: {bottleneck}"
                })

            elif "slow" in bottleneck:
                new_rules.append({
                    "condition": "processing_time_critical",
                    "action": "prefer_parallel_processing",
                    "reason": f"å‡¦ç†æ™‚é–“ãƒœãƒˆãƒ«ãƒãƒƒã‚¯å¯¾ç­–: {bottleneck}"
                })

        # æ—¢å­˜ãƒ«ãƒ¼ãƒ«ã«è¿½åŠ 
        try:
            self.routing_rules.extend(new_rules)

            # ãƒ«ãƒ¼ãƒ«é‡è¤‡é™¤å»
            unique_rules = []
            seen_conditions = set()

            for rule in self.routing_rules:
                condition = rule.get("condition")
                if condition not in seen_conditions:
                    unique_rules.append(rule)
                    seen_conditions.add(condition)

            self.routing_rules = unique_rules

            # ä¿å­˜
            with open(self.routing_rules_file, 'w', encoding='utf-8') as f:
                json.dump(self.routing_rules, f, indent=2, ensure_ascii=False)

        except Exception as e:
            logger.error(f"âŒ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒ«ãƒ¼ãƒ«æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")

        return new_rules

    def _update_collaboration_patterns(self, performance_analysis: Dict[str, Any]) -> List[str]:
        """å”æ¥­ãƒ‘ã‚¿ãƒ¼ãƒ³æ›´æ–°"""

        updated_patterns = []

        try:
            # æ–°ã—ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±ã§ãƒ‘ã‚¿ãƒ¼ãƒ³æ›´æ–°
            for pattern in self.collaboration_patterns:
                old_success_rate = pattern.success_rate

                # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æã«åŸºã¥ãèª¿æ•´
                layer_rates = performance_analysis.get("success_rates", {})
                if layer_rates:
                    avg_success_rate = statistics.mean(layer_rates.values())
                    # åŠ é‡å¹³å‡ã§æ›´æ–° (æ—¢å­˜70%, æ–°ãƒ‡ãƒ¼ã‚¿30%)
                    pattern.success_rate = old_success_rate * 0.7 + avg_success_rate * 0.3

                # å‡¦ç†æ™‚é–“æ›´æ–°
                avg_time = performance_analysis.get("processing_times", {}).get("average", 0)
                if avg_time > 0:
                    pattern.average_processing_time = pattern.average_processing_time * 0.7 + avg_time * 0.3

                # æ›´æ–°æ™‚åˆ»
                pattern.last_updated = datetime.datetime.now().isoformat()

                if abs(pattern.success_rate - old_success_rate) > 0.05:  # 5%ä»¥ä¸Šã®å¤‰åŒ–
                    updated_patterns.append(pattern.pattern_id)

            # ãƒ‘ã‚¿ãƒ¼ãƒ³ä¿å­˜
            patterns_data = [asdict(p) for p in self.collaboration_patterns]
            with open(self.patterns_file, 'w', encoding='utf-8') as f:
                json.dump(patterns_data, f, indent=2, ensure_ascii=False)

        except Exception as e:
            logger.error(f"âŒ å”æ¥­ãƒ‘ã‚¿ãƒ¼ãƒ³æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")

        return updated_patterns

    def _generate_optimization_recommendations(self, bottlenecks: List[str]) -> List[str]:
        """æœ€é©åŒ–æ¨å¥¨äº‹é …ç”Ÿæˆ"""

        recommendations = []

        # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ¥æ¨å¥¨
        for bottleneck in bottlenecks:
            if "layer1" in bottleneck:
                recommendations.append("ğŸ“ Layer1æ§‹æ–‡æ¤œè¨¼ã®äº‹å‰ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å¼·åŒ–ã‚’å®Ÿæ–½")
            elif "layer2" in bottleneck:
                recommendations.append("ğŸ” Layer2å“è³ªãƒã‚§ãƒƒã‚¯ã®åŸºæº–è¦‹ç›´ã—ã¨æœ€é©åŒ–")
            elif "layer3" in bottleneck:
                recommendations.append("ğŸ‘¤ Layer3 Claudeæ‰¿èªãƒ—ãƒ­ã‚»ã‚¹ã®åŠ¹ç‡åŒ–")
            elif "failsafe" in bottleneck:
                recommendations.append("ğŸ›¡ï¸ ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ä½¿ç”¨ç‡å‰Šæ¸›ã®ãŸã‚äºˆé˜²çš„ãƒã‚§ãƒƒã‚¯å¼·åŒ–")
            elif "slow" in bottleneck:
                recommendations.append("âš¡ å‡¦ç†æ™‚é–“çŸ­ç¸®ã®ãŸã‚ä¸¦åˆ—å‡¦ç†æ©Ÿèƒ½ã®æ´»ç”¨")

        # ä¸€èˆ¬çš„ãªæ¨å¥¨
        if not bottlenecks:
            recommendations.append("âœ… å”æ¥­ãƒ•ãƒ­ãƒ¼ã¯è‰¯å¥½ã«å‹•ä½œã—ã¦ã„ã¾ã™")
            recommendations.append("ğŸ“Š ç¶™ç¶šçš„ãªç›£è¦–ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¿½è·¡ã‚’æ¨å¥¨")
        else:
            recommendations.append(f"ğŸ¯ {len(bottlenecks)}å€‹ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ãŒç‰¹å®šã•ã‚Œã¾ã—ãŸ")
            recommendations.append("ğŸ”„ æœ€é©åŒ–å¾Œã®åŠ¹æœæ¸¬å®šã‚’å®Ÿæ–½ã—ã¦ãã ã•ã„")

        # äºˆé˜²çš„æ¨å¥¨
        recommendations.append("ğŸ“ˆ å®šæœŸçš„ãªå”æ¥­ãƒ•ãƒ­ãƒ¼è¦‹ç›´ã—ã®å®Ÿæ–½")
        recommendations.append("ğŸ“ å”æ¥­ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç¶™ç¶šå­¦ç¿’ã¨æ”¹å–„")

        return recommendations

    def _estimate_performance_improvement(self, bottlenecks: List[str],
                                        new_rules: List[Dict[str, Any]]) -> Dict[str, float]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„è¦‹ç©ã‚‚ã‚Š"""

        improvement = {
            "safety_score_improvement": 0.0,
            "success_rate_improvement": 0.0,
            "processing_time_improvement": 0.0,
            "failsafe_usage_reduction": 0.0
        }

        # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯è§£æ±ºã«ã‚ˆã‚‹æ”¹å–„è¦‹ç©ã‚‚ã‚Š
        for bottleneck in bottlenecks:
            if "layer1" in bottleneck:
                improvement["success_rate_improvement"] += 0.05  # 5%æ”¹å–„
                improvement["safety_score_improvement"] += 2.0   # 2ç‚¹æ”¹å–„
            elif "layer2" in bottleneck:
                improvement["success_rate_improvement"] += 0.08  # 8%æ”¹å–„
                improvement["safety_score_improvement"] += 3.0   # 3ç‚¹æ”¹å–„
            elif "layer3" in bottleneck:
                improvement["success_rate_improvement"] += 0.06  # 6%æ”¹å–„
                improvement["safety_score_improvement"] += 2.5   # 2.5ç‚¹æ”¹å–„
            elif "failsafe" in bottleneck:
                improvement["failsafe_usage_reduction"] += 0.10  # 10%å‰Šæ¸›
                improvement["safety_score_improvement"] += 4.0   # 4ç‚¹æ”¹å–„
            elif "slow" in bottleneck:
                improvement["processing_time_improvement"] += 0.20  # 20%çŸ­ç¸®
                improvement["safety_score_improvement"] += 1.5     # 1.5ç‚¹æ”¹å–„

        # æ–°ãƒ«ãƒ¼ãƒ«ã«ã‚ˆã‚‹è¿½åŠ æ”¹å–„
        rule_count = len(new_rules)
        improvement["safety_score_improvement"] += rule_count * 0.5  # ãƒ«ãƒ¼ãƒ«1ã¤ã«ã¤ã0.5ç‚¹

        return improvement

    def _save_optimization_result(self, result: FlowOptimizationResult) -> None:
        """æœ€é©åŒ–çµæœä¿å­˜"""

        try:
            # æ—¢å­˜å±¥æ­´èª­ã¿è¾¼ã¿
            if self.optimization_history_file.exists():
                with open(self.optimization_history_file, 'r', encoding='utf-8') as f:
                    history = json.load(f)
            else:
                history = []

            # æ–°ã—ã„çµæœè¿½åŠ 
            history.append(asdict(result))

            # æœ€æ–°20ä»¶ã®ã¿ä¿æŒ
            if len(history) > 20:
                history = history[-20:]

            # ä¿å­˜
            with open(self.optimization_history_file, 'w', encoding='utf-8') as f:
                json.dump(history, f, indent=2, ensure_ascii=False)

            logger.info(f"âœ… æœ€é©åŒ–çµæœä¿å­˜å®Œäº†: {self.optimization_history_file}")

        except Exception as e:
            logger.error(f"âŒ æœ€é©åŒ–çµæœä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    def _create_fallback_optimization_result(self) -> FlowOptimizationResult:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æœ€é©åŒ–çµæœ"""

        return FlowOptimizationResult(
            timestamp=datetime.datetime.now().isoformat(),
            optimizations_applied=["ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æœ€é©åŒ–å®Ÿè¡Œ"],
            performance_improvement={
                "safety_score_improvement": 0.0,
                "success_rate_improvement": 0.0,
                "processing_time_improvement": 0.0,
                "failsafe_usage_reduction": 0.0
            },
            new_routing_rules=[],
            updated_patterns=[],
            recommendations=["ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã«ã‚ˆã‚Šæœ€é©åŒ–ãŒåˆ¶é™ã•ã‚Œã¾ã—ãŸ"]
        )

    def _load_or_create_routing_rules(self) -> List[Dict[str, Any]]:
        """ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒ«ãƒ¼ãƒ«èª­ã¿è¾¼ã¿/ä½œæˆ"""

        if self.routing_rules_file.exists():
            try:
                with open(self.routing_rules_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"âš ï¸ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒ«ãƒ¼ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ«ãƒ¼ãƒ«ä½œæˆ
        default_rules = [
            {
                "condition": "error_fixing",
                "action": "prefer_gemini",
                "reason": "ã‚¨ãƒ©ãƒ¼ä¿®æ­£ã¯GeminiãŒåŠ¹ç‡çš„"
            },
            {
                "condition": "new_implementation",
                "action": "prefer_claude",
                "reason": "æ–°è¦å®Ÿè£…ã¯Claudeåˆ¤æ–­ãŒé‡è¦"
            },
            {
                "condition": "complex_task",
                "action": "require_hybrid",
                "reason": "è¤‡é›‘ã‚¿ã‚¹ã‚¯ã¯å”æ¥­ãŒæœ€é©"
            }
        ]

        # ä¿å­˜
        try:
            with open(self.routing_rules_file, 'w', encoding='utf-8') as f:
                json.dump(default_rules, f, indent=2, ensure_ascii=False)
        except Exception as e:
            logger.error(f"âŒ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ«ãƒ¼ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

        return default_rules

    def _load_or_create_patterns(self) -> List[CollaborationPattern]:
        """å”æ¥­ãƒ‘ã‚¿ãƒ¼ãƒ³èª­ã¿è¾¼ã¿/ä½œæˆ"""

        if self.patterns_file.exists():
            try:
                with open(self.patterns_file, 'r', encoding='utf-8') as f:
                    patterns_data = json.load(f)

                patterns = []
                for data in patterns_data:
                    pattern = CollaborationPattern(**data)
                    patterns.append(pattern)

                return patterns

            except Exception as e:
                logger.warning(f"âš ï¸ å”æ¥­ãƒ‘ã‚¿ãƒ¼ãƒ³èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ä½œæˆ
        default_patterns = [
            CollaborationPattern(
                pattern_id="error_fixing_pattern",
                task_type="error_fixing",
                success_rate=0.85,
                average_processing_time=1.2,
                failure_modes=["syntax_error", "type_error"],
                optimization_tips=["äº‹å‰æ§‹æ–‡ãƒã‚§ãƒƒã‚¯", "å‹æ³¨é‡ˆç¢ºèª"],
                last_updated=datetime.datetime.now().isoformat()
            ),
            CollaborationPattern(
                pattern_id="new_implementation_pattern",
                task_type="new_implementation",
                success_rate=0.75,
                average_processing_time=2.5,
                failure_modes=["design_complexity", "integration_issues"],
                optimization_tips=["æ®µéšçš„å®Ÿè£…", "è©³ç´°è¨­è¨ˆæ›¸ä½œæˆ"],
                last_updated=datetime.datetime.now().isoformat()
            )
        ]

        # ä¿å­˜
        try:
            patterns_data = [asdict(p) for p in default_patterns]
            with open(self.patterns_file, 'w', encoding='utf-8') as f:
                json.dump(patterns_data, f, indent=2, ensure_ascii=False)
        except Exception as e:
            logger.error(f"âŒ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

        return default_patterns


def main() -> None:
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    optimizer = CollaborationFlowOptimizer()

    # ã‚µãƒ³ãƒ—ãƒ«ã‚¿ã‚¹ã‚¯ã§ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°æœ€é©åŒ–ãƒ†ã‚¹ãƒˆ
    sample_task = {
        "task_id": "test_task_001",
        "type": "error_fixing",
        "description": "å‹æ³¨é‡ˆã‚¨ãƒ©ãƒ¼ã®ä¿®æ­£",
        "target_files": ["file1.py", "file2.py"],
        "requirements": {"error_type": "type_annotation"}
    }

    # ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°æ±ºå®š
    decision = optimizer.optimize_task_routing(sample_task)
    print(f"æ¨å¥¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ: {decision.recommended_agent.value}")
    print(f"ä¿¡é ¼åº¦: {decision.confidence_score:.3f}")

    # ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–
    optimization = optimizer.optimize_collaboration_flow()
    print(f"æœ€é©åŒ–é©ç”¨: {len(optimization.optimizations_applied)}ä»¶")


if __name__ == "__main__":
    main()
