#!/usr/bin/env python3
"""
EnhancedCollaborationDashboard - Issue #860å¯¾å¿œ
å”æ¥­ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰å¼·åŒ–ã‚·ã‚¹ãƒ†ãƒ 

åŒ…æ‹¬çš„å¯è¦–åŒ–ãƒ»ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¢ãƒ©ãƒ¼ãƒˆãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã«ã‚ˆã‚Š
å”æ¥­çŠ¶æ³ã®é€æ˜æ€§å‘ä¸Šã¨å•é¡Œã®æ—©æœŸç™ºè¦‹ã‚’å®Ÿç¾
"""

import json
import datetime
import statistics
import os
from typing import Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass, asdict
from enum import Enum
from pathlib import Path

from kumihan_formatter.core.utilities.logger import get_logger

logger = get_logger(__name__)


class DashboardSection(Enum):
    """ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚»ã‚¯ã‚·ãƒ§ãƒ³"""
    OVERVIEW = "overview"
    COLLABORATION_METRICS = "collaboration_metrics"
    PERFORMANCE_TRENDS = "performance_trends"
    QUALITY_INDICATORS = "quality_indicators"
    ALERTS_NOTIFICATIONS = "alerts_notifications"
    OPTIMIZATION_INSIGHTS = "optimization_insights"
    HISTORICAL_ANALYSIS = "historical_analysis"


class AlertLevel(Enum):
    """ã‚¢ãƒ©ãƒ¼ãƒˆãƒ¬ãƒ™ãƒ«"""
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"


class TrendDirection(Enum):
    """ãƒˆãƒ¬ãƒ³ãƒ‰æ–¹å‘"""
    IMPROVING = "improving"
    STABLE = "stable"
    DECLINING = "declining"
    VOLATILE = "volatile"


@dataclass
class DashboardAlert:
    """ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚¢ãƒ©ãƒ¼ãƒˆ"""
    alert_id: str
    level: AlertLevel
    title: str
    message: str
    timestamp: str
    section: DashboardSection
    action_required: bool
    auto_resolve: bool
    resolution_status: str  # "active", "acknowledged", "resolved"


@dataclass
class MetricTrend:
    """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒˆãƒ¬ãƒ³ãƒ‰"""
    metric_name: str
    current_value: float
    previous_value: float
    change_percentage: float
    trend_direction: TrendDirection
    confidence_level: float  # 0.0-1.0
    data_points: List[float]
    time_period: str


@dataclass
class CollaborationInsight:
    """å”æ¥­ã‚¤ãƒ³ã‚µã‚¤ãƒˆ"""
    insight_id: str
    category: str
    title: str
    description: str
    impact_level: str  # "low", "medium", "high", "critical"
    recommended_actions: List[str]
    estimated_benefit: str
    implementation_difficulty: str


@dataclass
class DashboardData:
    """ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿"""
    timestamp: str
    overview_metrics: Dict[str, Any]
    collaboration_metrics: Dict[str, Any]
    performance_trends: List[MetricTrend]
    quality_indicators: Dict[str, Any]
    active_alerts: List[DashboardAlert]
    optimization_insights: List[CollaborationInsight]
    historical_summary: Dict[str, Any]


class EnhancedCollaborationDashboard:
    """å¼·åŒ–å”æ¥­ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚·ã‚¹ãƒ†ãƒ 

    GeminiÃ—Claudeå”æ¥­ã®åŒ…æ‹¬çš„å¯è¦–åŒ–ã¨åˆ†æã«ã‚ˆã‚Šã€
    å”æ¥­åŠ¹ç‡ã®é€æ˜æ€§å‘ä¸Šã¨æœ€é©åŒ–æ©Ÿä¼šã®ç‰¹å®šã‚’å®Ÿç¾
    """

    def __init__(self, postbox_dir: str = "postbox"):
        self.postbox_dir = Path(postbox_dir)
        self.collaboration_dir = self.postbox_dir / "collaboration"
        self.monitoring_dir = self.postbox_dir / "monitoring"
        self.dashboard_dir = self.collaboration_dir / "dashboard"

        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        self.dashboard_dir.mkdir(parents=True, exist_ok=True)

        self.dashboard_data_file = self.dashboard_dir / "dashboard_data.json"
        self.alerts_file = self.dashboard_dir / "alerts.json"
        self.insights_file = self.dashboard_dir / "insights.json"
        self.trends_cache_file = self.dashboard_dir / "trends_cache.json"

        # ã‚¢ãƒ©ãƒ¼ãƒˆç®¡ç†
        self.active_alerts = self._load_alerts()

        # ã‚¤ãƒ³ã‚µã‚¤ãƒˆå±¥æ­´
        self.insights_history = self._load_insights_history()

        # ãƒˆãƒ¬ãƒ³ãƒ‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.trends_cache = self._load_trends_cache()

        logger.info("ğŸ“Š EnhancedCollaborationDashboard åˆæœŸåŒ–å®Œäº†")

    def generate_dashboard_data(self) -> DashboardData:
        """ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ

        Returns:
            DashboardData: å®Œå…¨ãªãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿
        """

        logger.info("ğŸ“Š ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆé–‹å§‹")

        try:
            timestamp = datetime.datetime.now().isoformat()

            # å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
            overview = self._generate_overview_metrics()
            collaboration = self._generate_collaboration_metrics()
            trends = self._generate_performance_trends()
            quality = self._generate_quality_indicators()
            alerts = self._update_and_get_active_alerts()
            insights = self._generate_optimization_insights()
            historical = self._generate_historical_summary()

            dashboard_data = DashboardData(
                timestamp=timestamp,
                overview_metrics=overview,
                collaboration_metrics=collaboration,
                performance_trends=trends,
                quality_indicators=quality,
                active_alerts=alerts,
                optimization_insights=insights,
                historical_summary=historical
            )

            # ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ä¿å­˜
            self._save_dashboard_data(dashboard_data)

            logger.info("âœ… ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†")

            return dashboard_data

        except Exception as e:
            logger.error(f"âŒ ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return self._create_fallback_dashboard_data()

    def _generate_overview_metrics(self) -> Dict[str, Any]:
        """æ¦‚è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç”Ÿæˆ"""

        overview = {
            "collaboration_safety_score": 0.0,
            "current_safety_level": "unknown",
            "target_achievement": {
                "safety_score_progress": 0.0,
                "target_value": 75.0,
                "current_value": 40.0,
                "completion_percentage": 0.0
            },
            "system_status": {
                "operational": True,
                "last_update": datetime.datetime.now().isoformat(),
                "active_sessions": 1,
                "error_rate": 0.0
            },
            "quick_stats": {
                "total_tasks_today": 0,
                "success_rate_today": 0.0,
                "avg_processing_time": 0.0,
                "token_efficiency": 0.99
            }
        }

        try:
            # åŠ¹ç‡ç›£è¦–ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç¾åœ¨ã®å®‰å…¨æ€§ã‚¹ã‚³ã‚¢å–å¾—
            efficiency_file = self.collaboration_dir / "efficiency_metrics.json"
            if efficiency_file.exists():
                with open(efficiency_file, 'r', encoding='utf-8') as f:
                    efficiency_data = json.load(f)

                if efficiency_data:
                    latest_metric = efficiency_data[-1]
                    current_score = latest_metric.get("overall_safety_score", 40.0)
                    safety_level = latest_metric.get("safety_level", "low")

                    overview["collaboration_safety_score"] = current_score
                    overview["current_safety_level"] = safety_level
                    overview["target_achievement"]["current_value"] = current_score
                    overview["target_achievement"]["completion_percentage"] = (current_score / 75.0) * 100

            # 3å±¤æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ä»Šæ—¥ã®çµ±è¨ˆ
            verification_file = self.monitoring_dir / "three_layer_verification_metrics.json"
            if verification_file.exists():
                with open(verification_file, 'r', encoding='utf-8') as f:
                    verification_data = json.load(f)

                # ä»Šæ—¥ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                today = datetime.datetime.now().date()
                today_data = []

                for entry in verification_data:
                    try:
                        entry_date = datetime.datetime.fromisoformat(entry["timestamp"]).date()
                        if entry_date == today:
                            today_data.append(entry)
                    except Exception:
                        continue

                if today_data:
                    overview["quick_stats"]["total_tasks_today"] = len(today_data)

                    success_rates = [d["success_rates"]["overall_success_rate"] for d in today_data]
                    overview["quick_stats"]["success_rate_today"] = statistics.mean(success_rates)

                    exec_times = [d["execution_time"] for d in today_data]
                    overview["quick_stats"]["avg_processing_time"] = statistics.mean(exec_times)

        except Exception as e:
            logger.warning(f"âš ï¸ æ¦‚è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")

        return overview

    def _generate_collaboration_metrics(self) -> Dict[str, Any]:
        """å”æ¥­ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç”Ÿæˆ"""

        collaboration = {
            "agent_performance": {
                "gemini_efficiency": 0.85,
                "claude_effectiveness": 0.80,
                "collaboration_synergy": 0.75
            },
            "task_distribution": {
                "gemini_tasks": 0,
                "claude_tasks": 0,
                "hybrid_tasks": 0,
                "auto_routed_tasks": 0
            },
            "layer_performance": {
                "layer1_success_rate": 0.0,
                "layer2_success_rate": 0.0,
                "layer3_approval_rate": 0.0,
                "integration_success_rate": 0.0
            },
            "failsafe_metrics": {
                "current_usage_rate": 0.0,
                "target_usage_rate": 0.20,
                "reduction_progress": 0.0,
                "prevention_effectiveness": 0.0
            },
            "routing_accuracy": {
                "correct_routing_rate": 0.0,
                "optimization_efficiency": 0.0,
                "decision_confidence": 0.0
            }
        }

        try:
            # 3å±¤æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å±¤åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
            verification_file = self.monitoring_dir / "three_layer_verification_metrics.json"
            if verification_file.exists():
                with open(verification_file, 'r', encoding='utf-8') as f:
                    verification_data = json.load(f)

                if verification_data:
                    recent_data = verification_data[-10:]  # æœ€æ–°10ä»¶

                    layer1_rates = [d["success_rates"]["layer1_success_rate"] for d in recent_data]
                    layer2_rates = [d["success_rates"]["layer2_success_rate"] for d in recent_data]
                    layer3_rates = [d["success_rates"]["layer3_approval_rate"] for d in recent_data]
                    integration_rates = [d["success_rates"]["integration_success_rate"] for d in recent_data]

                    collaboration["layer_performance"] = {
                        "layer1_success_rate": statistics.mean(layer1_rates) if layer1_rates else 0.0,
                        "layer2_success_rate": statistics.mean(layer2_rates) if layer2_rates else 0.0,
                        "layer3_approval_rate": statistics.mean(layer3_rates) if layer3_rates else 0.0,
                        "integration_success_rate": statistics.mean(integration_rates) if integration_rates else 0.0
                    }

            # ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            failsafe_file = self.monitoring_dir / "failsafe_usage.json"
            if failsafe_file.exists():
                with open(failsafe_file, 'r', encoding='utf-8') as f:
                    failsafe_data = json.load(f)

                if failsafe_data:
                    recent_failsafe = failsafe_data[-5:]

                    total_failsafe = sum(entry["failsafe_count"] for entry in recent_failsafe)
                    total_files = sum(entry["total_files"] for entry in recent_failsafe)

                    if total_files > 0:
                        current_rate = total_failsafe / total_files
                        target_rate = 0.20

                        collaboration["failsafe_metrics"] = {
                            "current_usage_rate": current_rate,
                            "target_usage_rate": target_rate,
                            "reduction_progress": max(0, (0.5 - current_rate) / (0.5 - target_rate)),
                            "prevention_effectiveness": max(0, 1.0 - current_rate / 0.5)
                        }

            # ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ç²¾åº¦ï¼ˆãƒ•ãƒ­ãƒ¼æœ€é©åŒ–ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ï¼‰
            routing_file = self.collaboration_dir / "routing_decisions.json"
            if routing_file.exists():
                with open(routing_file, 'r', encoding='utf-8') as f:
                    routing_data = json.load(f)

                if routing_data:
                    recent_decisions = routing_data[-20:]  # æœ€æ–°20ä»¶

                    confidence_scores = [d["confidence_score"] for d in recent_decisions]

                    collaboration["routing_accuracy"] = {
                        "correct_routing_rate": 0.85,  # æ¨å®šå€¤
                        "optimization_efficiency": 0.80,  # æ¨å®šå€¤
                        "decision_confidence": statistics.mean(confidence_scores) if confidence_scores else 0.0
                    }

        except Exception as e:
            logger.warning(f"âš ï¸ å”æ¥­ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")

        return collaboration

    def _generate_performance_trends(self) -> List[MetricTrend]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒˆãƒ¬ãƒ³ãƒ‰ç”Ÿæˆ"""

        trends = []

        try:
            # åŠ¹ç‡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒˆãƒ¬ãƒ³ãƒ‰
            efficiency_file = self.collaboration_dir / "efficiency_metrics.json"
            if efficiency_file.exists():
                with open(efficiency_file, 'r', encoding='utf-8') as f:
                    efficiency_data = json.load(f)

                if len(efficiency_data) >= 2:
                    # å®‰å…¨æ€§ã‚¹ã‚³ã‚¢ãƒˆãƒ¬ãƒ³ãƒ‰
                    safety_scores = [d["overall_safety_score"] for d in efficiency_data[-10:]]
                    if len(safety_scores) >= 2:
                        trend = self._create_metric_trend(
                            "collaboration_safety_score",
                            safety_scores,
                            "7d"
                        )
                        trends.append(trend)

                    # æˆåŠŸç‡ãƒˆãƒ¬ãƒ³ãƒ‰
                    success_rates = [d["task_success_rate"] for d in efficiency_data[-10:]]
                    if len(success_rates) >= 2:
                        trend = self._create_metric_trend(
                            "task_success_rate",
                            success_rates,
                            "7d"
                        )
                        trends.append(trend)

            # ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ä½¿ç”¨ç‡ãƒˆãƒ¬ãƒ³ãƒ‰
            failsafe_file = self.monitoring_dir / "failsafe_usage.json"
            if failsafe_file.exists():
                with open(failsafe_file, 'r', encoding='utf-8') as f:
                    failsafe_data = json.load(f)

                if len(failsafe_data) >= 5:
                    # ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ç‡ã‚’æ™‚ç³»åˆ—ã§è¨ˆç®—
                    failsafe_rates = []
                    for entry in failsafe_data[-10:]:
                        if entry["total_files"] > 0:
                            rate = entry["failsafe_count"] / entry["total_files"]
                            failsafe_rates.append(rate)

                    if len(failsafe_rates) >= 2:
                        trend = self._create_metric_trend(
                            "failsafe_usage_rate",
                            failsafe_rates,
                            "7d"
                        )
                        trends.append(trend)

            # å‡¦ç†æ™‚é–“ãƒˆãƒ¬ãƒ³ãƒ‰
            verification_file = self.monitoring_dir / "three_layer_verification_metrics.json"
            if verification_file.exists():
                with open(verification_file, 'r', encoding='utf-8') as f:
                    verification_data = json.load(f)

                if len(verification_data) >= 5:
                    exec_times = [d["execution_time"] for d in verification_data[-10:]]
                    if len(exec_times) >= 2:
                        trend = self._create_metric_trend(
                            "processing_time",
                            exec_times,
                            "7d"
                        )
                        trends.append(trend)

        except Exception as e:
            logger.warning(f"âš ï¸ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒˆãƒ¬ãƒ³ãƒ‰ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")

        return trends

    def _create_metric_trend(self, metric_name: str, data_points: List[float],
                           time_period: str) -> MetricTrend:
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒˆãƒ¬ãƒ³ãƒ‰ä½œæˆ"""

        if len(data_points) < 2:
            return MetricTrend(
                metric_name=metric_name,
                current_value=data_points[0] if data_points else 0.0,
                previous_value=0.0,
                change_percentage=0.0,
                trend_direction=TrendDirection.STABLE,
                confidence_level=0.0,
                data_points=data_points,
                time_period=time_period
            )

        current_value = data_points[-1]
        previous_value = data_points[-2]

        # å¤‰åŒ–ç‡è¨ˆç®—
        if previous_value != 0:
            change_percentage = ((current_value - previous_value) / previous_value) * 100
        else:
            change_percentage = 0.0

        # ãƒˆãƒ¬ãƒ³ãƒ‰æ–¹å‘åˆ¤å®š
        if abs(change_percentage) < 5:
            trend_direction = TrendDirection.STABLE
        elif change_percentage > 5:
            trend_direction = TrendDirection.IMPROVING
        else:
            trend_direction = TrendDirection.DECLINING

        # ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒã‚§ãƒƒã‚¯
        if len(data_points) >= 3:
            volatility = statistics.stdev(data_points[-5:]) if len(data_points) >= 5 else statistics.stdev(data_points)
            if volatility > statistics.mean(data_points) * 0.2:  # 20%ä»¥ä¸Šã®å¤‰å‹•
                trend_direction = TrendDirection.VOLATILE

        # ä¿¡é ¼åº¦è¨ˆç®—
        confidence_level = min(len(data_points) / 10.0, 1.0)  # ãƒ‡ãƒ¼ã‚¿ç‚¹æ•°ã«åŸºã¥ã

        return MetricTrend(
            metric_name=metric_name,
            current_value=current_value,
            previous_value=previous_value,
            change_percentage=change_percentage,
            trend_direction=trend_direction,
            confidence_level=confidence_level,
            data_points=data_points,
            time_period=time_period
        )

    def _generate_quality_indicators(self) -> Dict[str, Any]:
        """å“è³ªæŒ‡æ¨™ç”Ÿæˆ"""

        quality = {
            "code_quality_score": 0.0,
            "type_safety_coverage": 0.0,
            "test_coverage_estimate": 0.0,
            "documentation_completeness": 0.0,
            "security_compliance": 0.0,
            "performance_efficiency": 0.0,
            "quality_trend": "stable",
            "improvement_areas": [],
            "quality_gates": {
                "syntax_validation": {"status": "unknown", "score": 0.0},
                "type_checking": {"status": "unknown", "score": 0.0},
                "integration_tests": {"status": "unknown", "score": 0.0},
                "claude_approval": {"status": "unknown", "score": 0.0}
            }
        }

        try:
            # äºˆé˜²çš„å“è³ªãƒã‚§ãƒƒã‚¯çµæœã‹ã‚‰å“è³ªæŒ‡æ¨™å–å¾—
            preventive_dir = self.postbox_dir / "quality" / "preventive_checks"
            if preventive_dir.exists():
                check_files = list(preventive_dir.glob("check_*.json"))

                if check_files:
                    # æœ€æ–°ã®ãƒã‚§ãƒƒã‚¯çµæœ
                    latest_files = sorted(check_files)[-5:]

                    quality_scores = []
                    risk_counts = []

                    for check_file in latest_files:
                        try:
                            with open(check_file, 'r', encoding='utf-8') as f:
                                check_data = json.load(f)

                            quality_scores.append(check_data.get("overall_quality_score", 0.0))
                            risk_counts.append(len(check_data.get("detected_risks", [])))

                        except Exception:
                            continue

                    if quality_scores:
                        quality["code_quality_score"] = statistics.mean(quality_scores)

                        # æ”¹å–„ã‚¨ãƒªã‚¢ç‰¹å®š
                        if statistics.mean(risk_counts) > 3:
                            quality["improvement_areas"].append("å¤šæ•°ã®å“è³ªãƒªã‚¹ã‚¯æ¤œå‡º")

                        if quality["code_quality_score"] < 0.7:
                            quality["improvement_areas"].append("å…¨ä½“çš„ãªå“è³ªå‘ä¸ŠãŒå¿…è¦")

            # 3å±¤æ¤œè¨¼ã‹ã‚‰å“è³ªã‚²ãƒ¼ãƒˆçŠ¶æ³
            verification_file = self.monitoring_dir / "three_layer_verification_metrics.json"
            if verification_file.exists():
                with open(verification_file, 'r', encoding='utf-8') as f:
                    verification_data = json.load(f)

                if verification_data:
                    latest_verification = verification_data[-1]
                    success_rates = latest_verification["success_rates"]

                    quality["quality_gates"] = {
                        "syntax_validation": {
                            "status": "pass" if success_rates["layer1_success_rate"] > 0.8 else "fail",
                            "score": success_rates["layer1_success_rate"]
                        },
                        "type_checking": {
                            "status": "pass" if success_rates["layer2_success_rate"] > 0.9 else "fail",
                            "score": success_rates["layer2_success_rate"]
                        },
                        "integration_tests": {
                            "status": "pass" if success_rates["integration_success_rate"] > 0.8 else "fail",
                            "score": success_rates["integration_success_rate"]
                        },
                        "claude_approval": {
                            "status": "pass" if success_rates["layer3_approval_rate"] > 0.7 else "fail",
                            "score": success_rates["layer3_approval_rate"]
                        }
                    }

        except Exception as e:
            logger.warning(f"âš ï¸ å“è³ªæŒ‡æ¨™ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")

        return quality

    def _update_and_get_active_alerts(self) -> List[DashboardAlert]:
        """ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¢ãƒ©ãƒ¼ãƒˆæ›´æ–°ãƒ»å–å¾—"""

        current_alerts = []

        try:
            # æ—¢å­˜ã‚¢ãƒ©ãƒ¼ãƒˆã‚’ãƒã‚§ãƒƒã‚¯ãƒ»æ›´æ–°
            self._update_existing_alerts()

            # æ–°ã—ã„ã‚¢ãƒ©ãƒ¼ãƒˆã‚’ç”Ÿæˆ
            new_alerts = self._generate_new_alerts()
            current_alerts.extend(new_alerts)

            # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¢ãƒ©ãƒ¼ãƒˆã®ã¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            active_alerts = [alert for alert in self.active_alerts + current_alerts
                           if alert.resolution_status == "active"]

            # é‡è¤‡é™¤å»
            unique_alerts = {}
            for alert in active_alerts:
                unique_alerts[alert.alert_id] = alert

            final_alerts = list(unique_alerts.values())

            # ã‚¢ãƒ©ãƒ¼ãƒˆä¿å­˜
            self._save_alerts(final_alerts)

            return final_alerts[:10]  # æœ€å¤§10ä»¶

        except Exception as e:
            logger.warning(f"âš ï¸ ã‚¢ãƒ©ãƒ¼ãƒˆæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def _update_existing_alerts(self) -> None:
        """æ—¢å­˜ã‚¢ãƒ©ãƒ¼ãƒˆæ›´æ–°"""

        for alert in self.active_alerts:
            # è‡ªå‹•è§£æ±ºã‚¢ãƒ©ãƒ¼ãƒˆã®ãƒã‚§ãƒƒã‚¯
            if alert.auto_resolve and alert.resolution_status == "active":
                if self._check_alert_auto_resolution(alert):
                    alert.resolution_status = "resolved"

    def _check_alert_auto_resolution(self, alert: DashboardAlert) -> bool:
        """ã‚¢ãƒ©ãƒ¼ãƒˆè‡ªå‹•è§£æ±ºãƒã‚§ãƒƒã‚¯"""

        # åŸºæœ¬çš„ãªè‡ªå‹•è§£æ±ºãƒ­ã‚¸ãƒƒã‚¯
        if "failsafe" in alert.title.lower():
            # ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•é–¢é€£ã‚¢ãƒ©ãƒ¼ãƒˆã¯ä½¿ç”¨ç‡æ”¹å–„ã§è§£æ±º
            try:
                current_rate = self._get_current_failsafe_rate()
                if current_rate < 0.3:  # 30%æœªæº€ã§è§£æ±º
                    return True
            except Exception:
                pass

        elif "success rate" in alert.title.lower():
            # æˆåŠŸç‡é–¢é€£ã‚¢ãƒ©ãƒ¼ãƒˆã¯æˆåŠŸç‡æ”¹å–„ã§è§£æ±º
            try:
                current_rate = self._get_current_success_rate()
                if current_rate > 0.7:  # 70%ä»¥ä¸Šã§è§£æ±º
                    return True
            except Exception:
                pass

        return False

    def _generate_new_alerts(self) -> List[DashboardAlert]:
        """æ–°ã—ã„ã‚¢ãƒ©ãƒ¼ãƒˆç”Ÿæˆ"""

        new_alerts = []

        try:
            # å”æ¥­å®‰å…¨æ€§ã‚¹ã‚³ã‚¢ã‚¢ãƒ©ãƒ¼ãƒˆ
            safety_score = self._get_current_safety_score()
            if safety_score < 60:
                alert = DashboardAlert(
                    alert_id=f"safety_score_{datetime.datetime.now().strftime('%Y%m%d_%H%M')}",
                    level=AlertLevel.CRITICAL if safety_score < 40 else AlertLevel.WARNING,
                    title="å”æ¥­å®‰å…¨æ€§ã‚¹ã‚³ã‚¢ä½ä¸‹",
                    message=f"ç¾åœ¨ã®å®‰å…¨æ€§ã‚¹ã‚³ã‚¢: {safety_score:.1f}/100 (ç›®æ¨™: 75+)",
                    timestamp=datetime.datetime.now().isoformat(),
                    section=DashboardSection.COLLABORATION_METRICS,
                    action_required=True,
                    auto_resolve=True,
                    resolution_status="active"
                )
                new_alerts.append(alert)

            # ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•éä½¿ç”¨ã‚¢ãƒ©ãƒ¼ãƒˆ
            failsafe_rate = self._get_current_failsafe_rate()
            if failsafe_rate > 0.4:
                alert = DashboardAlert(
                    alert_id=f"failsafe_overuse_{datetime.datetime.now().strftime('%Y%m%d_%H%M')}",
                    level=AlertLevel.WARNING,
                    title="ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•éä½¿ç”¨æ¤œå‡º",
                    message=f"ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ä½¿ç”¨ç‡: {failsafe_rate:.1%} (ç›®æ¨™: 20%ä»¥ä¸‹)",
                    timestamp=datetime.datetime.now().isoformat(),
                    section=DashboardSection.PERFORMANCE_TRENDS,
                    action_required=True,
                    auto_resolve=True,
                    resolution_status="active"
                )
                new_alerts.append(alert)

            # Layer1æˆåŠŸç‡ã‚¢ãƒ©ãƒ¼ãƒˆ
            layer1_rate = self._get_layer1_success_rate()
            if layer1_rate < 0.6:
                alert = DashboardAlert(
                    alert_id=f"layer1_failure_{datetime.datetime.now().strftime('%Y%m%d_%H%M')}",
                    level=AlertLevel.ERROR,
                    title="Layer1æ§‹æ–‡æ¤œè¨¼å¤±æ•—ç‡é«˜",
                    message=f"Layer1æˆåŠŸç‡: {layer1_rate:.1%} (ç›®æ¨™: 80%ä»¥ä¸Š)",
                    timestamp=datetime.datetime.now().isoformat(),
                    section=DashboardSection.QUALITY_INDICATORS,
                    action_required=True,
                    auto_resolve=True,
                    resolution_status="active"
                )
                new_alerts.append(alert)

        except Exception as e:
            logger.warning(f"âš ï¸ æ–°ã‚¢ãƒ©ãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")

        return new_alerts

    def _generate_optimization_insights(self) -> List[CollaborationInsight]:
        """æœ€é©åŒ–ã‚¤ãƒ³ã‚µã‚¤ãƒˆç”Ÿæˆ"""

        insights = []

        try:
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æã‹ã‚‰ã‚¤ãƒ³ã‚µã‚¤ãƒˆç”Ÿæˆ
            safety_score = self._get_current_safety_score()
            failsafe_rate = self._get_current_failsafe_rate()

            # å®‰å…¨æ€§ã‚¹ã‚³ã‚¢æ”¹å–„ã‚¤ãƒ³ã‚µã‚¤ãƒˆ
            if safety_score < 75:
                gap = 75 - safety_score
                if gap > 20:
                    impact_level = "critical"
                    benefit = "å”æ¥­å®‰å…¨æ€§ã®å¤§å¹…å‘ä¸Š"
                elif gap > 10:
                    impact_level = "high"
                    benefit = "å”æ¥­å®‰å…¨æ€§ã®é¡•è‘—ãªå‘ä¸Š"
                else:
                    impact_level = "medium"
                    benefit = "å”æ¥­å®‰å…¨æ€§ã®æ”¹å–„"

                insight = CollaborationInsight(
                    insight_id=f"safety_improvement_{datetime.datetime.now().strftime('%Y%m%d')}",
                    category="safety_optimization",
                    title="å”æ¥­å®‰å…¨æ€§ã‚¹ã‚³ã‚¢å‘ä¸Šæ©Ÿä¼š",
                    description=f"ç¾åœ¨{safety_score:.1f}ç‚¹ã®å®‰å…¨æ€§ã‚¹ã‚³ã‚¢ã‚’ç›®æ¨™ã®75ç‚¹ã«å‘ä¸Šã•ã›ã‚‹æ©Ÿä¼šãŒã‚ã‚Šã¾ã™",
                    impact_level=impact_level,
                    recommended_actions=[
                        "Layer1æ§‹æ–‡æ¤œè¨¼ã®å¼·åŒ–",
                        "äºˆé˜²çš„å“è³ªãƒã‚§ãƒƒã‚¯ã®å°å…¥",
                        "ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ä½¿ç”¨ç‡ã®å‰Šæ¸›"
                    ],
                    estimated_benefit=benefit,
                    implementation_difficulty="medium"
                )
                insights.append(insight)

            # ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•å‰Šæ¸›ã‚¤ãƒ³ã‚µã‚¤ãƒˆ
            if failsafe_rate > 0.25:
                insight = CollaborationInsight(
                    insight_id=f"failsafe_reduction_{datetime.datetime.now().strftime('%Y%m%d')}",
                    category="efficiency_optimization",
                    title="ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ä½¿ç”¨ç‡å‰Šæ¸›æ©Ÿä¼š",
                    description=f"ç¾åœ¨{failsafe_rate:.1%}ã®ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ä½¿ç”¨ç‡ã‚’20%ä»¥ä¸‹ã«å‰Šæ¸›å¯èƒ½",
                    impact_level="high",
                    recommended_actions=[
                        "äº‹å‰ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å¼·åŒ–",
                        "å‹æ³¨é‡ˆã‚«ãƒãƒ¬ãƒƒã‚¸å‘ä¸Š",
                        "è‡ªå‹•ä¿®æ­£æ©Ÿèƒ½æ´»ç”¨"
                    ],
                    estimated_benefit="å‡¦ç†åŠ¹ç‡20-30%å‘ä¸Š",
                    implementation_difficulty="low"
                )
                insights.append(insight)

            # å”æ¥­ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°æœ€é©åŒ–ã‚¤ãƒ³ã‚µã‚¤ãƒˆ
            routing_accuracy = self._get_routing_accuracy()
            if routing_accuracy < 0.9:
                insight = CollaborationInsight(
                    insight_id=f"routing_optimization_{datetime.datetime.now().strftime('%Y%m%d')}",
                    category="workflow_optimization",
                    title="ã‚¿ã‚¹ã‚¯ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ç²¾åº¦å‘ä¸Šæ©Ÿä¼š",
                    description=f"ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ç²¾åº¦{routing_accuracy:.1%}ã‚’90%ä»¥ä¸Šã«å‘ä¸Šå¯èƒ½",
                    impact_level="medium",
                    recommended_actions=[
                        "ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒ«ãƒ¼ãƒ«ã®æœ€é©åŒ–",
                        "æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹ç²¾åº¦å‘ä¸Š",
                        "ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—ã®å¼·åŒ–"
                    ],
                    estimated_benefit="ã‚¿ã‚¹ã‚¯å‡¦ç†åŠ¹ç‡10-15%å‘ä¸Š",
                    implementation_difficulty="medium"
                )
                insights.append(insight)

        except Exception as e:
            logger.warning(f"âš ï¸ æœ€é©åŒ–ã‚¤ãƒ³ã‚µã‚¤ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")

        return insights

    def _generate_historical_summary(self) -> Dict[str, Any]:
        """å±¥æ­´ã‚µãƒãƒªãƒ¼ç”Ÿæˆ"""

        summary = {
            "timeframe": "30d",
            "total_tasks_processed": 0,
            "average_safety_score": 0.0,
            "improvement_rate": 0.0,
            "major_milestones": [],
            "performance_comparison": {
                "last_30d": {"safety_score": 0.0, "success_rate": 0.0},
                "previous_30d": {"safety_score": 0.0, "success_rate": 0.0}
            },
            "optimization_impact": {
                "total_optimizations": 0,
                "estimated_efficiency_gain": 0.0,
                "cost_savings": 0.0
            }
        }

        try:
            # åŠ¹ç‡ãƒ¡ãƒˆãƒªã‚¯ã‚¹å±¥æ­´åˆ†æ
            efficiency_file = self.collaboration_dir / "efficiency_metrics.json"
            if efficiency_file.exists():
                with open(efficiency_file, 'r', encoding='utf-8') as f:
                    efficiency_data = json.load(f)

                if efficiency_data:
                    # éå»30æ—¥é–“ã®ãƒ‡ãƒ¼ã‚¿
                    cutoff_date = datetime.datetime.now() - datetime.timedelta(days=30)
                    recent_data = []

                    for entry in efficiency_data:
                        try:
                            entry_date = datetime.datetime.fromisoformat(entry["timestamp"])
                            if entry_date >= cutoff_date:
                                recent_data.append(entry)
                        except Exception:
                            continue

                    if recent_data:
                        summary["total_tasks_processed"] = len(recent_data)

                        safety_scores = [d["overall_safety_score"] for d in recent_data]
                        summary["average_safety_score"] = statistics.mean(safety_scores)

                        # æ”¹å–„ç‡è¨ˆç®—
                        if len(safety_scores) >= 2:
                            first_half = safety_scores[:len(safety_scores)//2]
                            second_half = safety_scores[len(safety_scores)//2:]

                            if first_half and second_half:
                                improvement = statistics.mean(second_half) - statistics.mean(first_half)
                                summary["improvement_rate"] = improvement

            # ãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³æ¤œå‡º
            if summary["average_safety_score"] >= 70:
                summary["major_milestones"].append("å”æ¥­å®‰å…¨æ€§ã‚¹ã‚³ã‚¢70ç‚¹çªç ´")

            if summary["improvement_rate"] > 5:
                summary["major_milestones"].append("é¡•è‘—ãªå“è³ªå‘ä¸Šãƒˆãƒ¬ãƒ³ãƒ‰")

        except Exception as e:
            logger.warning(f"âš ï¸ å±¥æ­´ã‚µãƒãƒªãƒ¼ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")

        return summary

    def _get_current_safety_score(self) -> float:
        """ç¾åœ¨ã®å®‰å…¨æ€§ã‚¹ã‚³ã‚¢å–å¾—"""

        try:
            efficiency_file = self.collaboration_dir / "efficiency_metrics.json"
            if efficiency_file.exists():
                with open(efficiency_file, 'r', encoding='utf-8') as f:
                    efficiency_data = json.load(f)

                if efficiency_data:
                    return efficiency_data[-1]["overall_safety_score"]
        except Exception:
            pass

        return 40.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤

    def _get_current_failsafe_rate(self) -> float:
        """ç¾åœ¨ã®ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ä½¿ç”¨ç‡å–å¾—"""

        try:
            failsafe_file = self.monitoring_dir / "failsafe_usage.json"
            if failsafe_file.exists():
                with open(failsafe_file, 'r', encoding='utf-8') as f:
                    failsafe_data = json.load(f)

                if failsafe_data:
                    recent_data = failsafe_data[-3:]
                    total_failsafe = sum(entry["failsafe_count"] for entry in recent_data)
                    total_files = sum(entry["total_files"] for entry in recent_data)

                    if total_files > 0:
                        return total_failsafe / total_files
        except Exception:
            pass

        return 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤

    def _get_current_success_rate(self) -> float:
        """ç¾åœ¨ã®æˆåŠŸç‡å–å¾—"""

        try:
            verification_file = self.monitoring_dir / "three_layer_verification_metrics.json"
            if verification_file.exists():
                with open(verification_file, 'r', encoding='utf-8') as f:
                    verification_data = json.load(f)

                if verification_data:
                    return verification_data[-1]["success_rates"]["overall_success_rate"]
        except Exception:
            pass

        return 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤

    def _get_layer1_success_rate(self) -> float:
        """Layer1æˆåŠŸç‡å–å¾—"""

        try:
            verification_file = self.monitoring_dir / "three_layer_verification_metrics.json"
            if verification_file.exists():
                with open(verification_file, 'r', encoding='utf-8') as f:
                    verification_data = json.load(f)

                if verification_data:
                    return verification_data[-1]["success_rates"]["layer1_success_rate"]
        except Exception:
            pass

        return 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤

    def _get_routing_accuracy(self) -> float:
        """ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ç²¾åº¦å–å¾—"""

        try:
            routing_file = self.collaboration_dir / "routing_decisions.json"
            if routing_file.exists():
                with open(routing_file, 'r', encoding='utf-8') as f:
                    routing_data = json.load(f)

                if routing_data:
                    recent_decisions = routing_data[-10:]
                    confidence_scores = [d["confidence_score"] for d in recent_decisions]

                    if confidence_scores:
                        return statistics.mean(confidence_scores)
        except Exception:
            pass

        return 0.8  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤

    def _save_dashboard_data(self, dashboard_data: DashboardData) -> None:
        """ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ä¿å­˜"""

        try:
            with open(self.dashboard_data_file, 'w', encoding='utf-8') as f:
                json.dump(asdict(dashboard_data), f, indent=2, ensure_ascii=False, default=str)

            logger.debug("âœ… ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†")

        except Exception as e:
            logger.error(f"âŒ ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    def _save_alerts(self, alerts: List[DashboardAlert]) -> None:
        """ã‚¢ãƒ©ãƒ¼ãƒˆä¿å­˜"""

        try:
            alerts_data = [asdict(alert) for alert in alerts]

            with open(self.alerts_file, 'w', encoding='utf-8') as f:
                json.dump(alerts_data, f, indent=2, ensure_ascii=False, default=str)

            self.active_alerts = alerts

        except Exception as e:
            logger.error(f"âŒ ã‚¢ãƒ©ãƒ¼ãƒˆä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    def _load_alerts(self) -> List[DashboardAlert]:
        """ã‚¢ãƒ©ãƒ¼ãƒˆèª­ã¿è¾¼ã¿"""

        if not self.alerts_file.exists():
            return []

        try:
            with open(self.alerts_file, 'r', encoding='utf-8') as f:
                alerts_data = json.load(f)

            alerts = []
            for data in alerts_data:
                alert = DashboardAlert(**data)
                alerts.append(alert)

            return alerts

        except Exception as e:
            logger.warning(f"âš ï¸ ã‚¢ãƒ©ãƒ¼ãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def _load_insights_history(self) -> List[CollaborationInsight]:
        """ã‚¤ãƒ³ã‚µã‚¤ãƒˆå±¥æ­´èª­ã¿è¾¼ã¿"""

        if not self.insights_file.exists():
            return []

        try:
            with open(self.insights_file, 'r', encoding='utf-8') as f:
                insights_data = json.load(f)

            insights = []
            for data in insights_data:
                insight = CollaborationInsight(**data)
                insights.append(insight)

            return insights[-20:]  # æœ€æ–°20ä»¶

        except Exception as e:
            logger.warning(f"âš ï¸ ã‚¤ãƒ³ã‚µã‚¤ãƒˆå±¥æ­´èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def _load_trends_cache(self) -> Dict[str, Any]:
        """ãƒˆãƒ¬ãƒ³ãƒ‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿"""

        if not self.trends_cache_file.exists():
            return {}

        try:
            with open(self.trends_cache_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.warning(f"âš ï¸ ãƒˆãƒ¬ãƒ³ãƒ‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return {}

    def _create_fallback_dashboard_data(self) -> DashboardData:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿"""

        return DashboardData(
            timestamp=datetime.datetime.now().isoformat(),
            overview_metrics={
                "collaboration_safety_score": 40.0,
                "current_safety_level": "low",
                "system_status": {"operational": True, "error_rate": 0.0}
            },
            collaboration_metrics={
                "layer_performance": {"layer1_success_rate": 0.5},
                "failsafe_metrics": {"current_usage_rate": 0.5}
            },
            performance_trends=[],
            quality_indicators={"code_quality_score": 0.5},
            active_alerts=[],
            optimization_insights=[],
            historical_summary={"total_tasks_processed": 0}
        )

    def export_dashboard_report(self, format_type: str = "json") -> str:
        """ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›

        Args:
            format_type: å‡ºåŠ›å½¢å¼ ("json", "html", "csv")

        Returns:
            str: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """

        logger.info(f"ğŸ“„ ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›é–‹å§‹: {format_type}")

        try:
            # æœ€æ–°ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
            dashboard_data = self.generate_dashboard_data()

            timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')

            if format_type == "json":
                output_file = self.dashboard_dir / f"dashboard_report_{timestamp}.json"

                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(asdict(dashboard_data), f, indent=2, ensure_ascii=False, default=str)

            elif format_type == "html":
                output_file = self.dashboard_dir / f"dashboard_report_{timestamp}.html"
                html_content = self._generate_html_report(dashboard_data)

                with open(output_file, 'w', encoding='utf-8') as f:
                    f.write(html_content)

            else:
                raise ValueError(f"æœªå¯¾å¿œã®å‡ºåŠ›å½¢å¼: {format_type}")

            logger.info(f"âœ… ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›å®Œäº†: {output_file}")

            return str(output_file)

        except Exception as e:
            logger.error(f"âŒ ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›ã‚¨ãƒ©ãƒ¼: {e}")
            return ""

    def _generate_html_report(self, dashboard_data: DashboardData) -> str:
        """HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""

        html_template = f"""
<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>å”æ¥­ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ¬ãƒãƒ¼ãƒˆ</title>
    <style>
        body {{ font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; margin: 0; padding: 20px; background-color: #f5f5f5; }}
        .container {{ max-width: 1200px; margin: 0 auto; background-color: white; padding: 30px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }}
        h1, h2, h3 {{ color: #333; }}
        .metric-card {{ background-color: #f8f9fa; padding: 20px; margin: 10px 0; border-radius: 8px; border-left: 4px solid #007bff; }}
        .alert-critical {{ border-left-color: #dc3545; }}
        .alert-warning {{ border-left-color: #ffc107; }}
        .alert-info {{ border-left-color: #17a2b8; }}
        .metric-value {{ font-size: 2em; font-weight: bold; color: #007bff; }}
        .trend-improving {{ color: #28a745; }}
        .trend-declining {{ color: #dc3545; }}
        .trend-stable {{ color: #6c757d; }}
        table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
        th, td {{ padding: 12px; text-align: left; border-bottom: 1px solid #ddd; }}
        th {{ background-color: #f8f9fa; font-weight: bold; }}
        .progress-bar {{ width: 100%; background-color: #e9ecef; border-radius: 4px; height: 20px; }}
        .progress-fill {{ height: 100%; background-color: #007bff; border-radius: 4px; transition: width 0.3s ease; }}
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸ¤– GeminiÃ—Claude å”æ¥­ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ¬ãƒãƒ¼ãƒˆ</h1>
        <p><strong>ç”Ÿæˆæ—¥æ™‚:</strong> {dashboard_data.timestamp}</p>

        <h2>ğŸ“Š æ¦‚è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹</h2>
        <div class="metric-card">
            <h3>å”æ¥­å®‰å…¨æ€§ã‚¹ã‚³ã‚¢</h3>
            <div class="metric-value">{dashboard_data.overview_metrics.get('collaboration_safety_score', 0):.1f}/100</div>
            <p>ç›®æ¨™: 75.0ä»¥ä¸Š | ç¾åœ¨ãƒ¬ãƒ™ãƒ«: {dashboard_data.overview_metrics.get('current_safety_level', 'unknown')}</p>
            <div class="progress-bar">
                <div class="progress-fill" style="width: {min(dashboard_data.overview_metrics.get('collaboration_safety_score', 0), 100)}%"></div>
            </div>
        </div>

        <h2>ğŸ¯ å”æ¥­ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹</h2>
        <table>
            <tr>
                <th>æŒ‡æ¨™</th>
                <th>ç¾åœ¨å€¤</th>
                <th>ç›®æ¨™å€¤</th>
                <th>çŠ¶æ…‹</th>
            </tr>
            <tr>
                <td>Layer1æˆåŠŸç‡</td>
                <td>{dashboard_data.collaboration_metrics.get('layer_performance', {}).get('layer1_success_rate', 0):.1%}</td>
                <td>80%ä»¥ä¸Š</td>
                <td>{'âœ…' if dashboard_data.collaboration_metrics.get('layer_performance', {}).get('layer1_success_rate', 0) >= 0.8 else 'âš ï¸'}</td>
            </tr>
            <tr>
                <td>ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ä½¿ç”¨ç‡</td>
                <td>{dashboard_data.collaboration_metrics.get('failsafe_metrics', {}).get('current_usage_rate', 0):.1%}</td>
                <td>20%ä»¥ä¸‹</td>
                <td>{'âœ…' if dashboard_data.collaboration_metrics.get('failsafe_metrics', {}).get('current_usage_rate', 0) <= 0.2 else 'âš ï¸'}</td>
            </tr>
        </table>

        <h2>ğŸ“ˆ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒˆãƒ¬ãƒ³ãƒ‰</h2>
        {self._generate_trends_html(dashboard_data.performance_trends)}

        <h2>ğŸš¨ ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¢ãƒ©ãƒ¼ãƒˆ</h2>
        {self._generate_alerts_html(dashboard_data.active_alerts)}

        <h2>ğŸ’¡ æœ€é©åŒ–ã‚¤ãƒ³ã‚µã‚¤ãƒˆ</h2>
        {self._generate_insights_html(dashboard_data.optimization_insights)}

        <p style="text-align: center; margin-top: 40px; color: #6c757d;">
            <em>ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯EnhancedCollaborationDashboardã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã‚Šè‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸ</em>
        </p>
    </div>
</body>
</html>
"""
        return html_template

    def _generate_trends_html(self, trends: List[MetricTrend]) -> str:
        """ãƒˆãƒ¬ãƒ³ãƒ‰HTMLç”Ÿæˆ"""

        if not trends:
            return "<p>ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“</p>"

        html = "<div>"
        for trend in trends:
            trend_class = f"trend-{trend.trend_direction.value}"
            direction_icon = {
                TrendDirection.IMPROVING: "ğŸ“ˆ",
                TrendDirection.DECLINING: "ğŸ“‰",
                TrendDirection.STABLE: "â¡ï¸",
                TrendDirection.VOLATILE: "ğŸ“Š"
            }.get(trend.trend_direction, "â¡ï¸")

            html += f"""
            <div class="metric-card">
                <h4>{direction_icon} {trend.metric_name}</h4>
                <p class="{trend_class}">
                    ç¾åœ¨å€¤: {trend.current_value:.3f} |
                    å¤‰åŒ–ç‡: {trend.change_percentage:+.1f}% |
                    ä¿¡é ¼åº¦: {trend.confidence_level:.1%}
                </p>
            </div>
            """

        html += "</div>"
        return html

    def _generate_alerts_html(self, alerts: List[DashboardAlert]) -> str:
        """ã‚¢ãƒ©ãƒ¼ãƒˆHTMLç”Ÿæˆ"""

        if not alerts:
            return "<p>ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªã‚¢ãƒ©ãƒ¼ãƒˆã¯ã‚ã‚Šã¾ã›ã‚“ âœ…</p>"

        html = "<div>"
        for alert in alerts:
            alert_class = f"alert-{alert.level.value}"
            level_icon = {
                AlertLevel.CRITICAL: "ğŸš¨",
                AlertLevel.ERROR: "âŒ",
                AlertLevel.WARNING: "âš ï¸",
                AlertLevel.INFO: "â„¹ï¸"
            }.get(alert.level, "â„¹ï¸")

            html += f"""
            <div class="metric-card {alert_class}">
                <h4>{level_icon} {alert.title}</h4>
                <p>{alert.message}</p>
                <small>ã‚»ã‚¯ã‚·ãƒ§ãƒ³: {alert.section.value} | æ™‚åˆ»: {alert.timestamp}</small>
            </div>
            """

        html += "</div>"
        return html

    def _generate_insights_html(self, insights: List[CollaborationInsight]) -> str:
        """ã‚¤ãƒ³ã‚µã‚¤ãƒˆHTMLç”Ÿæˆ"""

        if not insights:
            return "<p>æœ€é©åŒ–ã‚¤ãƒ³ã‚µã‚¤ãƒˆãŒã‚ã‚Šã¾ã›ã‚“</p>"

        html = "<div>"
        for insight in insights:
            impact_icon = {
                "critical": "ğŸš¨",
                "high": "ğŸ”¥",
                "medium": "âš¡",
                "low": "ğŸ’¡"
            }.get(insight.impact_level, "ğŸ’¡")

            html += f"""
            <div class="metric-card">
                <h4>{impact_icon} {insight.title}</h4>
                <p>{insight.description}</p>
                <p><strong>æœŸå¾…åŠ¹æœ:</strong> {insight.estimated_benefit}</p>
                <p><strong>æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:</strong></p>
                <ul>
            """

            for action in insight.recommended_actions:
                html += f"<li>{action}</li>"

            html += f"""
                </ul>
                <small>å®Ÿè£…é›£æ˜“åº¦: {insight.implementation_difficulty}</small>
            </div>
            """

        html += "</div>"
        return html


def main() -> None:
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    dashboard = EnhancedCollaborationDashboard()

    # ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    data = dashboard.generate_dashboard_data()
    print(f"å”æ¥­å®‰å…¨æ€§ã‚¹ã‚³ã‚¢: {data.overview_metrics['collaboration_safety_score']:.1f}")
    print(f"ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¢ãƒ©ãƒ¼ãƒˆ: {len(data.active_alerts)}ä»¶")
    print(f"æœ€é©åŒ–ã‚¤ãƒ³ã‚µã‚¤ãƒˆ: {len(data.optimization_insights)}ä»¶")

    # HTMLãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›
    report_file = dashboard.export_dashboard_report("html")
    if report_file:
        print(f"HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ: {report_file}")


if __name__ == "__main__":
    main()
