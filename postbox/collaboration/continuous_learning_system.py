#!/usr/bin/env python3
"""
ContinuousLearningSystem - Issue #860å¯¾å¿œ
ç¶™ç¶šå­¦ç¿’ãƒ»æ”¹å–„ã‚·ã‚¹ãƒ†ãƒ 

å”æ¥­ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’ãƒ»è‡ªå‹•æœ€é©åŒ–ãƒ»ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—ã«ã‚ˆã‚Š
GeminiÃ—Claudeå”æ¥­ã‚·ã‚¹ãƒ†ãƒ ã®ç¶™ç¶šçš„å“è³ªå‘ä¸Šã‚’å®Ÿç¾
"""

import json
import datetime
import statistics
import os
import pickle
from typing import Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass, asdict
from enum import Enum
from pathlib import Path
import hashlib

from kumihan_formatter.core.utilities.logger import get_logger

logger = get_logger(__name__)


class LearningCategory(Enum):
    """å­¦ç¿’ã‚«ãƒ†ã‚´ãƒª"""
    TASK_ROUTING = "task_routing"
    QUALITY_PREDICTION = "quality_prediction"
    FAILURE_PREVENTION = "failure_prevention"
    OPTIMIZATION_STRATEGY = "optimization_strategy"
    PERFORMANCE_TUNING = "performance_tuning"


class FeedbackType(Enum):
    """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚¿ã‚¤ãƒ—"""
    SUCCESS = "success"
    FAILURE = "failure"
    IMPROVEMENT = "improvement"
    REGRESSION = "regression"
    USER_RATING = "user_rating"


class LearningConfidence(Enum):
    """å­¦ç¿’ä¿¡é ¼åº¦"""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    VERY_HIGH = "very_high"


@dataclass
class LearningPattern:
    """å­¦ç¿’ãƒ‘ã‚¿ãƒ¼ãƒ³"""
    pattern_id: str
    category: LearningCategory
    context: Dict[str, Any]  # ã‚¿ã‚¹ã‚¯ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
    action_taken: str
    outcome: Dict[str, Any]  # çµæœæŒ‡æ¨™
    feedback_score: float  # -1.0 to 1.0
    confidence: LearningConfidence
    frequency: int  # å‡ºç¾é »åº¦
    last_seen: str
    effectiveness: float  # 0.0-1.0


@dataclass
class OptimizationRecommendation:
    """æœ€é©åŒ–æ¨å¥¨"""
    recommendation_id: str
    category: LearningCategory
    title: str
    description: str
    expected_improvement: Dict[str, float]
    implementation_steps: List[str]
    confidence_score: float
    risk_level: str  # "low", "medium", "high"
    estimated_effort: str
    supporting_patterns: List[str]


@dataclass
class FeedbackData:
    """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿"""
    feedback_id: str
    feedback_type: FeedbackType
    target_pattern: str
    target_action: str
    performance_metrics: Dict[str, float]
    user_satisfaction: Optional[float]
    timestamp: str
    context: Dict[str, Any]


@dataclass
class LearningReport:
    """å­¦ç¿’ãƒ¬ãƒãƒ¼ãƒˆ"""
    timestamp: str
    total_patterns_learned: int
    active_patterns: int
    learning_accuracy: float
    optimization_suggestions: List[OptimizationRecommendation]
    performance_improvements: Dict[str, float]
    confidence_distribution: Dict[str, int]
    recent_discoveries: List[str]


class ContinuousLearningSystem:
    """ç¶™ç¶šå­¦ç¿’ãƒ»æ”¹å–„ã‚·ã‚¹ãƒ†ãƒ 

    å”æ¥­ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å­¦ç¿’ãƒ»åˆ†æãƒ»æœ€é©åŒ–ã«ã‚ˆã‚Šã€
    ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®ç¶™ç¶šçš„ãªæ€§èƒ½å‘ä¸Šã‚’å®Ÿç¾
    """

    def __init__(self, postbox_dir: str = "postbox"):
        self.postbox_dir = Path(postbox_dir)
        self.collaboration_dir = self.postbox_dir / "collaboration"
        self.learning_dir = self.collaboration_dir / "learning"
        self.models_dir = self.learning_dir / "models"

        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        self.learning_dir.mkdir(parents=True, exist_ok=True)
        self.models_dir.mkdir(parents=True, exist_ok=True)

        self.patterns_file = self.learning_dir / "learned_patterns.json"
        self.feedback_file = self.learning_dir / "feedback_data.json"
        self.recommendations_file = self.learning_dir / "optimization_recommendations.json"
        self.learning_metrics_file = self.learning_dir / "learning_metrics.json"

        # å­¦ç¿’ãƒ‘ã‚¿ãƒ¼ãƒ³
        self.learned_patterns = self._load_learned_patterns()

        # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿
        self.feedback_history = self._load_feedback_history()

        # å­¦ç¿’ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        self.learning_metrics = self._load_learning_metrics()

        # æœ€é©åŒ–æ¨å¥¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.recommendation_cache = {}

        logger.info("ğŸ§  ContinuousLearningSystem åˆæœŸåŒ–å®Œäº†")
        logger.info(f"ğŸ“š å­¦ç¿’æ¸ˆã¿ãƒ‘ã‚¿ãƒ¼ãƒ³: {len(self.learned_patterns)}ä»¶")
        logger.info(f"ğŸ“ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å±¥æ­´: {len(self.feedback_history)}ä»¶")

    def learn_from_collaboration_data(self) -> Dict[str, Any]:
        """å”æ¥­ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®å­¦ç¿’å®Ÿè¡Œ

        Returns:
            Dict[str, Any]: å­¦ç¿’çµæœ
        """

        logger.info("ğŸ§  å”æ¥­ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®å­¦ç¿’é–‹å§‹")

        try:
            learning_results = {
                "new_patterns_discovered": 0,
                "patterns_updated": 0,
                "confidence_improvements": 0,
                "optimization_opportunities": 0,
                "learning_accuracy": 0.0
            }

            # 1. å”æ¥­ãƒ‡ãƒ¼ã‚¿åé›†ãƒ»åˆ†æ
            collaboration_data = self._collect_collaboration_data()

            # 2. ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡ºãƒ»å­¦ç¿’
            new_patterns = self._extract_patterns_from_data(collaboration_data)
            learning_results["new_patterns_discovered"] = len(new_patterns)

            # 3. æ—¢å­˜ãƒ‘ã‚¿ãƒ¼ãƒ³æ›´æ–°
            updated_count = self._update_existing_patterns(collaboration_data)
            learning_results["patterns_updated"] = updated_count

            # 4. ä¿¡é ¼åº¦èª¿æ•´
            confidence_improvements = self._adjust_pattern_confidence()
            learning_results["confidence_improvements"] = confidence_improvements

            # 5. æœ€é©åŒ–æ©Ÿä¼šç‰¹å®š
            optimization_opportunities = self._identify_optimization_opportunities()
            learning_results["optimization_opportunities"] = len(optimization_opportunities)

            # 6. å­¦ç¿’ç²¾åº¦è©•ä¾¡
            learning_accuracy = self._evaluate_learning_accuracy()
            learning_results["learning_accuracy"] = learning_accuracy

            # 7. å­¦ç¿’çµæœä¿å­˜
            self._save_learning_results(learning_results)

            logger.info(f"âœ… å”æ¥­ãƒ‡ãƒ¼ã‚¿å­¦ç¿’å®Œäº†: {learning_results['new_patterns_discovered']}æ–°ãƒ‘ã‚¿ãƒ¼ãƒ³, {learning_results['patterns_updated']}æ›´æ–°")

            return learning_results

        except Exception as e:
            logger.error(f"âŒ å”æ¥­ãƒ‡ãƒ¼ã‚¿å­¦ç¿’ã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}

    def _collect_collaboration_data(self) -> Dict[str, Any]:
        """å”æ¥­ãƒ‡ãƒ¼ã‚¿åé›†"""

        data = {
            "efficiency_metrics": [],
            "routing_decisions": [],
            "quality_checks": [],
            "failsafe_usage": [],
            "verification_results": []
        }

        try:
            # åŠ¹ç‡ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            efficiency_file = self.collaboration_dir / "efficiency_metrics.json"
            if efficiency_file.exists():
                with open(efficiency_file, 'r', encoding='utf-8') as f:
                    data["efficiency_metrics"] = json.load(f)

            # ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°æ±ºå®š
            routing_file = self.collaboration_dir / "routing_decisions.json"
            if routing_file.exists():
                with open(routing_file, 'r', encoding='utf-8') as f:
                    data["routing_decisions"] = json.load(f)

            # å“è³ªãƒã‚§ãƒƒã‚¯
            preventive_dir = self.postbox_dir / "quality" / "preventive_checks"
            if preventive_dir.exists():
                check_files = list(preventive_dir.glob("check_*.json"))
                for check_file in check_files[-20:]:  # æœ€æ–°20ä»¶
                    try:
                        with open(check_file, 'r', encoding='utf-8') as f:
                            check_data = json.load(f)
                            data["quality_checks"].append(check_data)
                    except Exception:
                        continue

            # ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ä½¿ç”¨çŠ¶æ³
            monitoring_dir = self.postbox_dir / "monitoring"
            failsafe_file = monitoring_dir / "failsafe_usage.json"
            if failsafe_file.exists():
                with open(failsafe_file, 'r', encoding='utf-8') as f:
                    data["failsafe_usage"] = json.load(f)

            # 3å±¤æ¤œè¨¼çµæœ
            verification_file = monitoring_dir / "three_layer_verification_metrics.json"
            if verification_file.exists():
                with open(verification_file, 'r', encoding='utf-8') as f:
                    data["verification_results"] = json.load(f)

        except Exception as e:
            logger.warning(f"âš ï¸ å”æ¥­ãƒ‡ãƒ¼ã‚¿åé›†ã‚¨ãƒ©ãƒ¼: {e}")

        return data

    def _extract_patterns_from_data(self, collaboration_data: Dict[str, Any]) -> List[LearningPattern]:
        """ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡º"""

        new_patterns = []

        try:
            # ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’
            routing_patterns = self._extract_routing_patterns(collaboration_data["routing_decisions"])
            new_patterns.extend(routing_patterns)

            # å“è³ªäºˆæ¸¬ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’
            quality_patterns = self._extract_quality_patterns(collaboration_data["quality_checks"])
            new_patterns.extend(quality_patterns)

            # å¤±æ•—é˜²æ­¢ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’
            failure_patterns = self._extract_failure_patterns(
                collaboration_data["failsafe_usage"],
                collaboration_data["verification_results"]
            )
            new_patterns.extend(failure_patterns)

            # æœ€é©åŒ–æˆ¦ç•¥ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’
            optimization_patterns = self._extract_optimization_patterns(collaboration_data["efficiency_metrics"])
            new_patterns.extend(optimization_patterns)

            # é‡è¤‡é™¤å»
            unique_patterns = self._deduplicate_patterns(new_patterns)

            # æ–°ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ—¢å­˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã«è¿½åŠ 
            for pattern in unique_patterns:
                if not self._pattern_exists(pattern):
                    self.learned_patterns.append(pattern)

        except Exception as e:
            logger.warning(f"âš ï¸ ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")

        return new_patterns

    def _extract_routing_patterns(self, routing_data: List[Dict]) -> List[LearningPattern]:
        """ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡º"""

        patterns = []

        for decision in routing_data:
            try:
                # é«˜ä¿¡é ¼åº¦ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°æ±ºå®šã‚’ãƒ‘ã‚¿ãƒ¼ãƒ³åŒ–
                confidence = decision.get("confidence_score", 0.0)
                if confidence >= 0.8:

                    # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´æŠ½å‡º
                    context = {
                        "task_type": decision.get("optimization_strategy", "unknown"),
                        "estimated_success_rate": decision.get("estimated_success_rate", 0.0),
                        "estimated_processing_time": decision.get("estimated_processing_time", 0.0),
                        "risk_factors_count": len(decision.get("risk_factors", []))
                    }

                    # ãƒ‘ã‚¿ãƒ¼ãƒ³ä½œæˆ
                    pattern_id = self._generate_pattern_id("routing", context)

                    pattern = LearningPattern(
                        pattern_id=pattern_id,
                        category=LearningCategory.TASK_ROUTING,
                        context=context,
                        action_taken=decision.get("recommended_agent", "unknown"),
                        outcome={"confidence": confidence, "success_rate": decision.get("estimated_success_rate", 0.0)},
                        feedback_score=confidence * 2 - 1,  # 0.8-1.0 -> 0.6-1.0
                        confidence=self._determine_learning_confidence(confidence),
                        frequency=1,
                        last_seen=datetime.datetime.now().isoformat(),
                        effectiveness=confidence
                    )

                    patterns.append(pattern)

            except Exception as e:
                logger.warning(f"âš ï¸ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
                continue

        return patterns

    def _extract_quality_patterns(self, quality_data: List[Dict]) -> List[LearningPattern]:
        """å“è³ªãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡º"""

        patterns = []

        for check in quality_data:
            try:
                # é«˜å“è³ªã¾ãŸã¯æ”¹å–„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’
                quality_score = check.get("overall_quality_score", 0.0)
                prevention_score = check.get("failsafe_prevention_score", 0.0)

                if quality_score >= 0.8 or prevention_score >= 0.8:

                    # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´æŠ½å‡º
                    context = {
                        "file_type": Path(check.get("file_path", "")).suffix,
                        "risk_count": len(check.get("detected_risks", [])),
                        "quality_score_range": self._get_score_range(quality_score),
                        "has_immediate_actions": len(check.get("immediate_actions", [])) > 0
                    }

                    pattern_id = self._generate_pattern_id("quality", context)

                    pattern = LearningPattern(
                        pattern_id=pattern_id,
                        category=LearningCategory.QUALITY_PREDICTION,
                        context=context,
                        action_taken="preventive_check",
                        outcome={
                            "quality_score": quality_score,
                            "prevention_score": prevention_score,
                            "risk_count": len(check.get("detected_risks", []))
                        },
                        feedback_score=quality_score * 2 - 1,
                        confidence=self._determine_learning_confidence(quality_score),
                        frequency=1,
                        last_seen=datetime.datetime.now().isoformat(),
                        effectiveness=max(quality_score, prevention_score)
                    )

                    patterns.append(pattern)

            except Exception as e:
                logger.warning(f"âš ï¸ å“è³ªãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
                continue

        return patterns

    def _extract_failure_patterns(self, failsafe_data: List[Dict],
                                 verification_data: List[Dict]) -> List[LearningPattern]:
        """å¤±æ•—é˜²æ­¢ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡º"""

        patterns = []

        try:
            # ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ä½¿ç”¨ç‡ãŒä½ã„æˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’
            for i, verification in enumerate(verification_data):
                failsafe_entry = failsafe_data[i] if i < len(failsafe_data) else None

                if failsafe_entry and verification:
                    failsafe_rate = failsafe_entry["failsafe_count"] / max(failsafe_entry["total_files"], 1)
                    success_rate = verification["success_rates"]["overall_success_rate"]

                    # ä½ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ• + é«˜æˆåŠŸç‡ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’
                    if failsafe_rate <= 0.3 and success_rate >= 0.8:

                        context = {
                            "layer1_success": verification["success_rates"]["layer1_success_rate"],
                            "layer2_success": verification["success_rates"]["layer2_success_rate"],
                            "layer3_success": verification["success_rates"]["layer3_approval_rate"],
                            "execution_time_range": self._get_time_range(verification["execution_time"]),
                            "file_count": failsafe_entry["total_files"]
                        }

                        pattern_id = self._generate_pattern_id("failure_prevention", context)

                        pattern = LearningPattern(
                            pattern_id=pattern_id,
                            category=LearningCategory.FAILURE_PREVENTION,
                            context=context,
                            action_taken="low_failsafe_high_success",
                            outcome={
                                "failsafe_rate": failsafe_rate,
                                "success_rate": success_rate,
                                "prevention_effectiveness": 1.0 - failsafe_rate
                            },
                            feedback_score=success_rate * 2 - 1,
                            confidence=self._determine_learning_confidence(success_rate),
                            frequency=1,
                            last_seen=datetime.datetime.now().isoformat(),
                            effectiveness=success_rate * (1.0 - failsafe_rate)
                        )

                        patterns.append(pattern)

        except Exception as e:
            logger.warning(f"âš ï¸ å¤±æ•—é˜²æ­¢ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")

        return patterns

    def _extract_optimization_patterns(self, efficiency_data: List[Dict]) -> List[LearningPattern]:
        """æœ€é©åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡º"""

        patterns = []

        if len(efficiency_data) < 2:
            return patterns

        try:
            # åŠ¹ç‡æ”¹å–„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’
            for i in range(1, len(efficiency_data)):
                current = efficiency_data[i]
                previous = efficiency_data[i-1]

                # å®‰å…¨æ€§ã‚¹ã‚³ã‚¢æ”¹å–„ãƒ‘ã‚¿ãƒ¼ãƒ³
                score_improvement = current["overall_safety_score"] - previous["overall_safety_score"]

                if score_improvement > 2.0:  # 2ç‚¹ä»¥ä¸Šã®æ”¹å–„

                    context = {
                        "initial_safety_level": previous["safety_level"],
                        "improvement_magnitude": self._get_improvement_range(score_improvement),
                        "task_success_improvement": current["task_success_rate"] - previous["task_success_rate"],
                        "failsafe_reduction": previous["failsafe_usage_rate"] - current["failsafe_usage_rate"]
                    }

                    pattern_id = self._generate_pattern_id("optimization", context)

                    pattern = LearningPattern(
                        pattern_id=pattern_id,
                        category=LearningCategory.OPTIMIZATION_STRATEGY,
                        context=context,
                        action_taken="safety_score_improvement",
                        outcome={
                            "score_improvement": score_improvement,
                            "final_score": current["overall_safety_score"],
                            "improvement_rate": score_improvement / previous["overall_safety_score"]
                        },
                        feedback_score=min(score_improvement / 10.0, 1.0),  # 10ç‚¹æ”¹å–„ã§æœ€å¤§ã‚¹ã‚³ã‚¢
                        confidence=self._determine_learning_confidence(score_improvement / 10.0),
                        frequency=1,
                        last_seen=datetime.datetime.now().isoformat(),
                        effectiveness=score_improvement / 10.0
                    )

                    patterns.append(pattern)

        except Exception as e:
            logger.warning(f"âš ï¸ æœ€é©åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")

        return patterns

    def _update_existing_patterns(self, collaboration_data: Dict[str, Any]) -> int:
        """æ—¢å­˜ãƒ‘ã‚¿ãƒ¼ãƒ³æ›´æ–°"""

        updated_count = 0

        try:
            for pattern in self.learned_patterns:
                # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ãƒ»æ›´æ–°ãƒ­ã‚¸ãƒƒã‚¯
                if self._should_update_pattern(pattern, collaboration_data):
                    self._update_pattern_metrics(pattern, collaboration_data)
                    updated_count += 1

        except Exception as e:
            logger.warning(f"âš ï¸ ãƒ‘ã‚¿ãƒ¼ãƒ³æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")

        return updated_count

    def _should_update_pattern(self, pattern: LearningPattern, data: Dict[str, Any]) -> bool:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³æ›´æ–°åˆ¤å®š"""

        # æœ€è¿‘ã®ãƒ‡ãƒ¼ã‚¿ã§ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒè¦³æ¸¬ã•ã‚ŒãŸã‹ãƒã‚§ãƒƒã‚¯
        try:
            last_seen = datetime.datetime.fromisoformat(pattern.last_seen)
            now = datetime.datetime.now()

            # 1é€±é–“ä»¥å†…ã«æ›´æ–°ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if (now - last_seen).days < 7:
                return False

            # ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ä¸€è‡´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            if pattern.category == LearningCategory.TASK_ROUTING:
                return self._check_routing_pattern_match(pattern, data["routing_decisions"])
            elif pattern.category == LearningCategory.QUALITY_PREDICTION:
                return self._check_quality_pattern_match(pattern, data["quality_checks"])

        except Exception:
            pass

        return False

    def _check_routing_pattern_match(self, pattern: LearningPattern, routing_data: List[Dict]) -> bool:
        """ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒã‚§ãƒƒã‚¯"""

        for decision in routing_data[-10:]:  # æœ€æ–°10ä»¶
            try:
                # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼æ€§ãƒã‚§ãƒƒã‚¯
                context_match = self._calculate_context_similarity(
                    pattern.context,
                    {
                        "task_type": decision.get("optimization_strategy", "unknown"),
                        "estimated_success_rate": decision.get("estimated_success_rate", 0.0),
                        "estimated_processing_time": decision.get("estimated_processing_time", 0.0),
                        "risk_factors_count": len(decision.get("risk_factors", []))
                    }
                )

                if context_match > 0.8:  # 80%ä»¥ä¸Šã®é¡ä¼¼æ€§
                    return True

            except Exception:
                continue

        return False

    def _check_quality_pattern_match(self, pattern: LearningPattern, quality_data: List[Dict]) -> bool:
        """å“è³ªãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒã‚§ãƒƒã‚¯"""

        for check in quality_data[-10:]:  # æœ€æ–°10ä»¶
            try:
                context_match = self._calculate_context_similarity(
                    pattern.context,
                    {
                        "file_type": Path(check.get("file_path", "")).suffix,
                        "risk_count": len(check.get("detected_risks", [])),
                        "quality_score_range": self._get_score_range(check.get("overall_quality_score", 0.0)),
                        "has_immediate_actions": len(check.get("immediate_actions", [])) > 0
                    }
                )

                if context_match > 0.8:
                    return True

            except Exception:
                continue

        return False

    def _calculate_context_similarity(self, context1: Dict[str, Any], context2: Dict[str, Any]) -> float:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦è¨ˆç®—"""

        try:
            if not context1 or not context2:
                return 0.0

            # å…±é€šã‚­ãƒ¼ã®ä¸€è‡´åº¦è¨ˆç®—
            common_keys = set(context1.keys()) & set(context2.keys())
            if not common_keys:
                return 0.0

            similarity_scores = []

            for key in common_keys:
                val1, val2 = context1[key], context2[key]

                if isinstance(val1, (int, float)) and isinstance(val2, (int, float)):
                    # æ•°å€¤ã®é¡ä¼¼åº¦
                    if val1 == 0 and val2 == 0:
                        similarity_scores.append(1.0)
                    elif val1 == 0 or val2 == 0:
                        similarity_scores.append(0.0)
                    else:
                        diff = abs(val1 - val2) / max(abs(val1), abs(val2))
                        similarity_scores.append(1.0 - diff)

                elif str(val1) == str(val2):
                    # æ–‡å­—åˆ—å®Œå…¨ä¸€è‡´
                    similarity_scores.append(1.0)
                else:
                    # æ–‡å­—åˆ—ä¸ä¸€è‡´
                    similarity_scores.append(0.0)

            return statistics.mean(similarity_scores) if similarity_scores else 0.0

        except Exception:
            return 0.0

    def _update_pattern_metrics(self, pattern: LearningPattern, data: Dict[str, Any]) -> None:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°"""

        try:
            # é »åº¦å¢—åŠ 
            pattern.frequency += 1

            # æœ€çµ‚ç¢ºèªæ™‚åˆ»æ›´æ–°
            pattern.last_seen = datetime.datetime.now().isoformat()

            # ä¿¡é ¼åº¦èª¿æ•´ï¼ˆé »åº¦ã«åŸºã¥ãï¼‰
            if pattern.frequency >= 10:
                pattern.confidence = LearningConfidence.VERY_HIGH
            elif pattern.frequency >= 5:
                pattern.confidence = LearningConfidence.HIGH
            elif pattern.frequency >= 3:
                pattern.confidence = LearningConfidence.MEDIUM

        except Exception as e:
            logger.warning(f"âš ï¸ ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")

    def _adjust_pattern_confidence(self) -> int:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ä¿¡é ¼åº¦èª¿æ•´"""

        adjustments = 0

        try:
            for pattern in self.learned_patterns:
                old_confidence = pattern.confidence

                # åŠ¹æœæ€§ã«åŸºã¥ãä¿¡é ¼åº¦èª¿æ•´
                if pattern.effectiveness >= 0.9:
                    pattern.confidence = LearningConfidence.VERY_HIGH
                elif pattern.effectiveness >= 0.8:
                    pattern.confidence = LearningConfidence.HIGH
                elif pattern.effectiveness >= 0.6:
                    pattern.confidence = LearningConfidence.MEDIUM
                else:
                    pattern.confidence = LearningConfidence.LOW

                if old_confidence != pattern.confidence:
                    adjustments += 1

        except Exception as e:
            logger.warning(f"âš ï¸ ä¿¡é ¼åº¦èª¿æ•´ã‚¨ãƒ©ãƒ¼: {e}")

        return adjustments

    def _identify_optimization_opportunities(self) -> List[OptimizationRecommendation]:
        """æœ€é©åŒ–æ©Ÿä¼šç‰¹å®š"""

        opportunities = []

        try:
            # é«˜ä¿¡é ¼åº¦ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰æœ€é©åŒ–æ¨å¥¨ã‚’ç”Ÿæˆ
            high_confidence_patterns = [
                p for p in self.learned_patterns
                if p.confidence in [LearningConfidence.HIGH, LearningConfidence.VERY_HIGH]
            ]

            # ã‚«ãƒ†ã‚´ãƒªåˆ¥æœ€é©åŒ–æ©Ÿä¼š
            for category in LearningCategory:
                category_patterns = [p for p in high_confidence_patterns if p.category == category]

                if category_patterns:
                    opportunity = self._generate_category_optimization(category, category_patterns)
                    if opportunity:
                        opportunities.append(opportunity)

            # ç›¸é–¢åˆ†æã«ã‚ˆã‚‹æœ€é©åŒ–æ©Ÿä¼š
            correlation_opportunities = self._analyze_pattern_correlations(high_confidence_patterns)
            opportunities.extend(correlation_opportunities)

        except Exception as e:
            logger.warning(f"âš ï¸ æœ€é©åŒ–æ©Ÿä¼šç‰¹å®šã‚¨ãƒ©ãƒ¼: {e}")

        return opportunities

    def _generate_category_optimization(self, category: LearningCategory,
                                      patterns: List[LearningPattern]) -> Optional[OptimizationRecommendation]:
        """ã‚«ãƒ†ã‚´ãƒªåˆ¥æœ€é©åŒ–æ¨å¥¨ç”Ÿæˆ"""

        if not patterns:
            return None

        try:
            # æœ€ã‚‚åŠ¹æœçš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åŸºã«æ¨å¥¨ç”Ÿæˆ
            best_pattern = max(patterns, key=lambda p: p.effectiveness)

            recommendation_templates = {
                LearningCategory.TASK_ROUTING: {
                    "title": "ã‚¿ã‚¹ã‚¯ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ç²¾åº¦å‘ä¸Š",
                    "description": f"å­¦ç¿’æ¸ˆã¿ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ããƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ç²¾åº¦å‘ä¸Šã«ã‚ˆã‚Šã€æˆåŠŸç‡{best_pattern.effectiveness:.1%}é”æˆå¯èƒ½",
                    "expected_improvement": {"routing_accuracy": 0.05, "processing_efficiency": 0.03},
                    "implementation_steps": [
                        "é«˜ä¿¡é ¼åº¦ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è‡ªå‹•é©ç”¨",
                        "ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ†æç²¾åº¦ã®å‘ä¸Š",
                        "ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°æ±ºå®šã®ç¶™ç¶šå­¦ç¿’"
                    ]
                },
                LearningCategory.QUALITY_PREDICTION: {
                    "title": "å“è³ªäºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ å¼·åŒ–",
                    "description": f"äºˆæ¸¬ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ´»ç”¨ã«ã‚ˆã‚Šã€å“è³ªã‚¹ã‚³ã‚¢{best_pattern.effectiveness:.1%}ã®ç²¾åº¦é”æˆ",
                    "expected_improvement": {"quality_prediction_accuracy": 0.08, "failsafe_reduction": 0.1},
                    "implementation_steps": [
                        "äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‘ã‚¿ãƒ¼ãƒ³çµ±åˆ",
                        "äº‹å‰å“è³ªãƒã‚§ãƒƒã‚¯ã®å¼·åŒ–",
                        "ãƒªã‚¹ã‚¯äºˆæ¸¬ç²¾åº¦ã®å‘ä¸Š"
                    ]
                },
                LearningCategory.FAILURE_PREVENTION: {
                    "title": "å¤±æ•—é˜²æ­¢æ©Ÿèƒ½æœ€é©åŒ–",
                    "description": f"é˜²æ­¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ã‚ˆã‚Šã€ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•å‰Šæ¸›{(1-best_pattern.effectiveness):.1%}é”æˆ",
                    "expected_improvement": {"failsafe_reduction": 0.15, "prevention_effectiveness": 0.1},
                    "implementation_steps": [
                        "é˜²æ­¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã®äºˆé˜²çš„é©ç”¨",
                        "æ—©æœŸè­¦å‘Šã‚·ã‚¹ãƒ†ãƒ ã®å¼·åŒ–",
                        "è‡ªå‹•ä¿®æ­£æ©Ÿèƒ½ã®æ‹¡å¼µ"
                    ]
                }
            }

            template = recommendation_templates.get(category)
            if not template:
                return None

            recommendation = OptimizationRecommendation(
                recommendation_id=f"{category.value}_{datetime.datetime.now().strftime('%Y%m%d')}",
                category=category,
                title=template["title"],
                description=template["description"],
                expected_improvement=template["expected_improvement"],
                implementation_steps=template["implementation_steps"],
                confidence_score=best_pattern.effectiveness,
                risk_level="low" if best_pattern.effectiveness > 0.8 else "medium",
                estimated_effort="medium",
                supporting_patterns=[p.pattern_id for p in patterns[:3]]
            )

            return recommendation

        except Exception as e:
            logger.warning(f"âš ï¸ ã‚«ãƒ†ã‚´ãƒªæœ€é©åŒ–æ¨å¥¨ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def _analyze_pattern_correlations(self, patterns: List[LearningPattern]) -> List[OptimizationRecommendation]:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ç›¸é–¢åˆ†æ"""

        correlations = []

        try:
            # å”æ¥­å®‰å…¨æ€§å‘ä¸Šã«å¯„ä¸ã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³çµ„ã¿åˆã‚ã›ã‚’ç‰¹å®š
            safety_contributing_patterns = [
                p for p in patterns
                if p.category in [LearningCategory.FAILURE_PREVENTION, LearningCategory.QUALITY_PREDICTION]
                and p.effectiveness > 0.8
            ]

            if len(safety_contributing_patterns) >= 2:
                combined_effectiveness = statistics.mean([p.effectiveness for p in safety_contributing_patterns])

                correlation_rec = OptimizationRecommendation(
                    recommendation_id=f"correlation_safety_{datetime.datetime.now().strftime('%Y%m%d')}",
                    category=LearningCategory.OPTIMIZATION_STRATEGY,
                    title="çµ±åˆçš„å®‰å…¨æ€§å‘ä¸Šæˆ¦ç•¥",
                    description=f"è¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³ã®çµ„ã¿åˆã‚ã›ã«ã‚ˆã‚Šã€å”æ¥­å®‰å…¨æ€§{combined_effectiveness:.1%}ã®å‘ä¸Šã‚’å®Ÿç¾",
                    expected_improvement={
                        "collaboration_safety_score": 5.0,
                        "failsafe_reduction": 0.2,
                        "overall_efficiency": 0.15
                    },
                    implementation_steps=[
                        "å“è³ªäºˆæ¸¬ã¨å¤±æ•—é˜²æ­¢ã®çµ±åˆå®Ÿè¡Œ",
                        "ãƒ‘ã‚¿ãƒ¼ãƒ³é–“ã®ç›¸ä¹—åŠ¹æœæ´»ç”¨",
                        "åŒ…æ‹¬çš„æœ€é©åŒ–æˆ¦ç•¥ã®é©ç”¨"
                    ],
                    confidence_score=combined_effectiveness,
                    risk_level="low",
                    estimated_effort="high",
                    supporting_patterns=[p.pattern_id for p in safety_contributing_patterns]
                )

                correlations.append(correlation_rec)

        except Exception as e:
            logger.warning(f"âš ï¸ ãƒ‘ã‚¿ãƒ¼ãƒ³ç›¸é–¢åˆ†æã‚¨ãƒ©ãƒ¼: {e}")

        return correlations

    def _evaluate_learning_accuracy(self) -> float:
        """å­¦ç¿’ç²¾åº¦è©•ä¾¡"""

        try:
            if not self.learned_patterns:
                return 0.0

            # åŠ¹æœæ€§ã‚¹ã‚³ã‚¢ã®åˆ†å¸ƒã‚’è©•ä¾¡
            effectiveness_scores = [p.effectiveness for p in self.learned_patterns]

            # é«˜åŠ¹æœãƒ‘ã‚¿ãƒ¼ãƒ³ã®å‰²åˆ
            high_effectiveness_count = len([s for s in effectiveness_scores if s >= 0.8])
            high_effectiveness_ratio = high_effectiveness_count / len(effectiveness_scores)

            # å¹³å‡åŠ¹æœæ€§
            avg_effectiveness = statistics.mean(effectiveness_scores)

            # å­¦ç¿’ç²¾åº¦ = é«˜åŠ¹æœå‰²åˆ * 0.6 + å¹³å‡åŠ¹æœæ€§ * 0.4
            learning_accuracy = high_effectiveness_ratio * 0.6 + avg_effectiveness * 0.4

            return learning_accuracy

        except Exception as e:
            logger.warning(f"âš ï¸ å­¦ç¿’ç²¾åº¦è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
            return 0.0

    def add_feedback(self, target_pattern: str, target_action: str,
                    feedback_type: FeedbackType, performance_metrics: Dict[str, float],
                    user_satisfaction: Optional[float] = None,
                    context: Optional[Dict[str, Any]] = None) -> None:
        """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯è¿½åŠ 

        Args:
            target_pattern: å¯¾è±¡ãƒ‘ã‚¿ãƒ¼ãƒ³ID
            target_action: å¯¾è±¡ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
            feedback_type: ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚¿ã‚¤ãƒ—
            performance_metrics: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™
            user_satisfaction: ãƒ¦ãƒ¼ã‚¶ãƒ¼æº€è¶³åº¦ (0.0-1.0)
            context: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±
        """

        logger.info(f"ğŸ“ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯è¿½åŠ : {target_pattern} - {feedback_type.value}")

        try:
            feedback = FeedbackData(
                feedback_id=f"feedback_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}",
                feedback_type=feedback_type,
                target_pattern=target_pattern,
                target_action=target_action,
                performance_metrics=performance_metrics,
                user_satisfaction=user_satisfaction,
                timestamp=datetime.datetime.now().isoformat(),
                context=context or {}
            )

            self.feedback_history.append(feedback)

            # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã«åŸºã¥ããƒ‘ã‚¿ãƒ¼ãƒ³èª¿æ•´
            self._adjust_pattern_from_feedback(feedback)

            # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ä¿å­˜
            self._save_feedback_history()

        except Exception as e:
            logger.error(f"âŒ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯è¿½åŠ ã‚¨ãƒ©ãƒ¼: {e}")

    def _adjust_pattern_from_feedback(self, feedback: FeedbackData) -> None:
        """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³èª¿æ•´"""

        try:
            # å¯¾è±¡ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç‰¹å®š
            target_pattern = None
            for pattern in self.learned_patterns:
                if pattern.pattern_id == feedback.target_pattern:
                    target_pattern = pattern
                    break

            if not target_pattern:
                return

            # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚¹ã‚³ã‚¢è¨ˆç®—
            if feedback.feedback_type == FeedbackType.SUCCESS:
                feedback_score = 0.8
            elif feedback.feedback_type == FeedbackType.IMPROVEMENT:
                feedback_score = 0.6
            elif feedback.feedback_type == FeedbackType.FAILURE:
                feedback_score = -0.6
            elif feedback.feedback_type == FeedbackType.REGRESSION:
                feedback_score = -0.8
            else:
                feedback_score = 0.0

            # ãƒ¦ãƒ¼ã‚¶ãƒ¼æº€è¶³åº¦ã‚’è€ƒæ…®
            if feedback.user_satisfaction is not None:
                feedback_score = (feedback_score + feedback.user_satisfaction * 2 - 1) / 2

            # ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚¹ã‚³ã‚¢æ›´æ–°ï¼ˆç§»å‹•å¹³å‡ï¼‰
            alpha = 0.3  # å­¦ç¿’ç‡
            target_pattern.feedback_score = (
                target_pattern.feedback_score * (1 - alpha) + feedback_score * alpha
            )

            # åŠ¹æœæ€§æ›´æ–°
            target_pattern.effectiveness = max(0.0, min(1.0, (target_pattern.feedback_score + 1) / 2))

        except Exception as e:
            logger.warning(f"âš ï¸ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³èª¿æ•´ã‚¨ãƒ©ãƒ¼: {e}")

    def generate_learning_report(self) -> LearningReport:
        """å­¦ç¿’ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""

        logger.info("ğŸ“Š å­¦ç¿’ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆé–‹å§‹")

        try:
            # ãƒ‘ã‚¿ãƒ¼ãƒ³çµ±è¨ˆ
            total_patterns = len(self.learned_patterns)
            active_patterns = len([p for p in self.learned_patterns
                                 if datetime.datetime.now() - datetime.datetime.fromisoformat(p.last_seen)
                                 <= datetime.timedelta(days=30)])

            # å­¦ç¿’ç²¾åº¦
            learning_accuracy = self._evaluate_learning_accuracy()

            # æœ€é©åŒ–æ¨å¥¨
            optimization_suggestions = self._identify_optimization_opportunities()

            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„æ¨å®š
            performance_improvements = self._estimate_performance_improvements()

            # ä¿¡é ¼åº¦åˆ†å¸ƒ
            confidence_distribution = {}
            for conf in LearningConfidence:
                count = len([p for p in self.learned_patterns if p.confidence == conf])
                confidence_distribution[conf.value] = count

            # æœ€è¿‘ã®ç™ºè¦‹
            recent_discoveries = self._get_recent_discoveries()

            report = LearningReport(
                timestamp=datetime.datetime.now().isoformat(),
                total_patterns_learned=total_patterns,
                active_patterns=active_patterns,
                learning_accuracy=learning_accuracy,
                optimization_suggestions=optimization_suggestions,
                performance_improvements=performance_improvements,
                confidence_distribution=confidence_distribution,
                recent_discoveries=recent_discoveries
            )

            # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
            self._save_learning_report(report)

            logger.info(f"âœ… å­¦ç¿’ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {total_patterns}ãƒ‘ã‚¿ãƒ¼ãƒ³, ç²¾åº¦{learning_accuracy:.1%}")

            return report

        except Exception as e:
            logger.error(f"âŒ å­¦ç¿’ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return self._create_fallback_learning_report()

    def _estimate_performance_improvements(self) -> Dict[str, float]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„æ¨å®š"""

        improvements = {
            "collaboration_safety_score": 0.0,
            "task_success_rate": 0.0,
            "failsafe_reduction": 0.0,
            "processing_efficiency": 0.0
        }

        try:
            # é«˜åŠ¹æœãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰æ”¹å–„æ¨å®š
            high_effectiveness_patterns = [
                p for p in self.learned_patterns
                if p.effectiveness >= 0.8
            ]

            if high_effectiveness_patterns:
                avg_effectiveness = statistics.mean([p.effectiveness for p in high_effectiveness_patterns])

                # ã‚«ãƒ†ã‚´ãƒªåˆ¥æ”¹å–„æ¨å®š
                for category in LearningCategory:
                    category_patterns = [p for p in high_effectiveness_patterns if p.category == category]

                    if category_patterns:
                        if category == LearningCategory.TASK_ROUTING:
                            improvements["task_success_rate"] += avg_effectiveness * 0.05
                            improvements["processing_efficiency"] += avg_effectiveness * 0.03

                        elif category == LearningCategory.QUALITY_PREDICTION:
                            improvements["collaboration_safety_score"] += avg_effectiveness * 2.0
                            improvements["failsafe_reduction"] += avg_effectiveness * 0.1

                        elif category == LearningCategory.FAILURE_PREVENTION:
                            improvements["failsafe_reduction"] += avg_effectiveness * 0.15
                            improvements["collaboration_safety_score"] += avg_effectiveness * 1.5

        except Exception as e:
            logger.warning(f"âš ï¸ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„æ¨å®šã‚¨ãƒ©ãƒ¼: {e}")

        return improvements

    def _get_recent_discoveries(self) -> List[str]:
        """æœ€è¿‘ã®ç™ºè¦‹å–å¾—"""

        discoveries = []

        try:
            # æœ€è¿‘1é€±é–“ã®é«˜åŠ¹æœãƒ‘ã‚¿ãƒ¼ãƒ³
            cutoff = datetime.datetime.now() - datetime.timedelta(days=7)

            recent_patterns = [
                p for p in self.learned_patterns
                if datetime.datetime.fromisoformat(p.last_seen) >= cutoff
                and p.effectiveness >= 0.8
            ]

            for pattern in recent_patterns[:5]:  # ä¸Šä½5ä»¶
                discoveries.append(
                    f"{pattern.category.value}: {pattern.action_taken} (åŠ¹æœæ€§: {pattern.effectiveness:.1%})"
                )

        except Exception as e:
            logger.warning(f"âš ï¸ æœ€è¿‘ã®ç™ºè¦‹å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")

        return discoveries

    def apply_learned_optimizations(self) -> Dict[str, Any]:
        """å­¦ç¿’æ¸ˆã¿æœ€é©åŒ–ã®é©ç”¨

        Returns:
            Dict[str, Any]: é©ç”¨çµæœ
        """

        logger.info("ğŸš€ å­¦ç¿’æ¸ˆã¿æœ€é©åŒ–é©ç”¨é–‹å§‹")

        try:
            application_results = {
                "optimizations_applied": 0,
                "patterns_activated": 0,
                "estimated_improvement": {},
                "application_success": True
            }

            # é«˜ä¿¡é ¼åº¦ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è‡ªå‹•é©ç”¨
            high_confidence_patterns = [
                p for p in self.learned_patterns
                if p.confidence == LearningConfidence.VERY_HIGH
                and p.effectiveness >= 0.9
            ]

            for pattern in high_confidence_patterns:
                try:
                    if self._apply_pattern_optimization(pattern):
                        application_results["patterns_activated"] += 1

                except Exception as e:
                    logger.warning(f"âš ï¸ ãƒ‘ã‚¿ãƒ¼ãƒ³é©ç”¨ã‚¨ãƒ©ãƒ¼: {pattern.pattern_id} - {e}")

            # æœ€é©åŒ–æ¨å¥¨ã®è‡ªå‹•å®Ÿè£…
            optimization_suggestions = self._identify_optimization_opportunities()

            for suggestion in optimization_suggestions:
                if suggestion.risk_level == "low" and suggestion.confidence_score >= 0.9:
                    try:
                        if self._apply_optimization_suggestion(suggestion):
                            application_results["optimizations_applied"] += 1

                    except Exception as e:
                        logger.warning(f"âš ï¸ æœ€é©åŒ–é©ç”¨ã‚¨ãƒ©ãƒ¼: {suggestion.recommendation_id} - {e}")

            # æ”¹å–„æ¨å®š
            application_results["estimated_improvement"] = self._estimate_performance_improvements()

            logger.info(f"âœ… å­¦ç¿’æ¸ˆã¿æœ€é©åŒ–é©ç”¨å®Œäº†: {application_results['optimizations_applied']}æœ€é©åŒ–, {application_results['patterns_activated']}ãƒ‘ã‚¿ãƒ¼ãƒ³")

            return application_results

        except Exception as e:
            logger.error(f"âŒ å­¦ç¿’æ¸ˆã¿æœ€é©åŒ–é©ç”¨ã‚¨ãƒ©ãƒ¼: {e}")
            return {"application_success": False, "error": str(e)}

    def _apply_pattern_optimization(self, pattern: LearningPattern) -> bool:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³æœ€é©åŒ–é©ç”¨"""

        try:
            # ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚«ãƒ†ã‚´ãƒªã«åŸºã¥ãæœ€é©åŒ–é©ç”¨
            if pattern.category == LearningCategory.TASK_ROUTING:
                return self._apply_routing_optimization(pattern)
            elif pattern.category == LearningCategory.QUALITY_PREDICTION:
                return self._apply_quality_optimization(pattern)
            elif pattern.category == LearningCategory.FAILURE_PREVENTION:
                return self._apply_prevention_optimization(pattern)
            else:
                return self._apply_general_optimization(pattern)

        except Exception as e:
            logger.warning(f"âš ï¸ ãƒ‘ã‚¿ãƒ¼ãƒ³æœ€é©åŒ–é©ç”¨ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def _apply_routing_optimization(self, pattern: LearningPattern) -> bool:
        """ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°æœ€é©åŒ–é©ç”¨"""

        # ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ ã¸ã®çµ±åˆ
        try:
            optimization_file = self.collaboration_dir / "learned_routing_optimizations.json"

            optimization_data = {
                "pattern_id": pattern.pattern_id,
                "context": pattern.context,
                "recommended_action": pattern.action_taken,
                "effectiveness": pattern.effectiveness,
                "applied_timestamp": datetime.datetime.now().isoformat()
            }

            # æ—¢å­˜æœ€é©åŒ–èª­ã¿è¾¼ã¿
            existing_optimizations = []
            if optimization_file.exists():
                with open(optimization_file, 'r', encoding='utf-8') as f:
                    existing_optimizations = json.load(f)

            existing_optimizations.append(optimization_data)

            # ä¿å­˜
            with open(optimization_file, 'w', encoding='utf-8') as f:
                json.dump(existing_optimizations, f, indent=2, ensure_ascii=False)

            return True

        except Exception as e:
            logger.warning(f"âš ï¸ ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°æœ€é©åŒ–é©ç”¨ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def _apply_quality_optimization(self, pattern: LearningPattern) -> bool:
        """å“è³ªæœ€é©åŒ–é©ç”¨"""

        # äºˆé˜²çš„å“è³ªãƒã‚§ãƒƒã‚¯ã‚·ã‚¹ãƒ†ãƒ ã¸ã®çµ±åˆ
        try:
            quality_optimization_file = self.collaboration_dir / "learned_quality_optimizations.json"

            optimization_data = {
                "pattern_id": pattern.pattern_id,
                "quality_context": pattern.context,
                "optimization_action": pattern.action_taken,
                "expected_quality_improvement": pattern.effectiveness,
                "applied_timestamp": datetime.datetime.now().isoformat()
            }

            # ä¿å­˜
            existing_optimizations = []
            if quality_optimization_file.exists():
                with open(quality_optimization_file, 'r', encoding='utf-8') as f:
                    existing_optimizations = json.load(f)

            existing_optimizations.append(optimization_data)

            with open(quality_optimization_file, 'w', encoding='utf-8') as f:
                json.dump(existing_optimizations, f, indent=2, ensure_ascii=False)

            return True

        except Exception as e:
            logger.warning(f"âš ï¸ å“è³ªæœ€é©åŒ–é©ç”¨ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def _apply_prevention_optimization(self, pattern: LearningPattern) -> bool:
        """é˜²æ­¢æœ€é©åŒ–é©ç”¨"""

        # åŠ¹ç‡ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã¸ã®çµ±åˆ
        try:
            prevention_optimization_file = self.collaboration_dir / "learned_prevention_optimizations.json"

            optimization_data = {
                "pattern_id": pattern.pattern_id,
                "prevention_context": pattern.context,
                "prevention_action": pattern.action_taken,
                "failsafe_reduction_potential": pattern.effectiveness,
                "applied_timestamp": datetime.datetime.now().isoformat()
            }

            # ä¿å­˜
            existing_optimizations = []
            if prevention_optimization_file.exists():
                with open(prevention_optimization_file, 'r', encoding='utf-8') as f:
                    existing_optimizations = json.load(f)

            existing_optimizations.append(optimization_data)

            with open(prevention_optimization_file, 'w', encoding='utf-8') as f:
                json.dump(existing_optimizations, f, indent=2, ensure_ascii=False)

            return True

        except Exception as e:
            logger.warning(f"âš ï¸ é˜²æ­¢æœ€é©åŒ–é©ç”¨ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def _apply_general_optimization(self, pattern: LearningPattern) -> bool:
        """ä¸€èˆ¬æœ€é©åŒ–é©ç”¨"""

        try:
            # ä¸€èˆ¬çš„ãªæœ€é©åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è¨˜éŒ²
            general_optimization_file = self.collaboration_dir / "learned_general_optimizations.json"

            optimization_data = {
                "pattern_id": pattern.pattern_id,
                "category": pattern.category.value,
                "optimization_context": pattern.context,
                "optimization_outcome": pattern.outcome,
                "applied_timestamp": datetime.datetime.now().isoformat()
            }

            existing_optimizations = []
            if general_optimization_file.exists():
                with open(general_optimization_file, 'r', encoding='utf-8') as f:
                    existing_optimizations = json.load(f)

            existing_optimizations.append(optimization_data)

            with open(general_optimization_file, 'w', encoding='utf-8') as f:
                json.dump(existing_optimizations, f, indent=2, ensure_ascii=False)

            return True

        except Exception as e:
            logger.warning(f"âš ï¸ ä¸€èˆ¬æœ€é©åŒ–é©ç”¨ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def _apply_optimization_suggestion(self, suggestion: OptimizationRecommendation) -> bool:
        """æœ€é©åŒ–æ¨å¥¨é©ç”¨"""

        try:
            # æ¨å¥¨ã®å®Ÿè£…è¨˜éŒ²
            suggestion_implementation_file = self.collaboration_dir / "implemented_suggestions.json"

            implementation_data = {
                "recommendation_id": suggestion.recommendation_id,
                "title": suggestion.title,
                "category": suggestion.category.value,
                "expected_improvement": suggestion.expected_improvement,
                "implementation_steps": suggestion.implementation_steps,
                "confidence_score": suggestion.confidence_score,
                "implemented_timestamp": datetime.datetime.now().isoformat()
            }

            existing_implementations = []
            if suggestion_implementation_file.exists():
                with open(suggestion_implementation_file, 'r', encoding='utf-8') as f:
                    existing_implementations = json.load(f)

            existing_implementations.append(implementation_data)

            with open(suggestion_implementation_file, 'w', encoding='utf-8') as f:
                json.dump(existing_implementations, f, indent=2, ensure_ascii=False)

            return True

        except Exception as e:
            logger.warning(f"âš ï¸ æœ€é©åŒ–æ¨å¥¨é©ç”¨ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    # ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰
    def _generate_pattern_id(self, prefix: str, context: Dict[str, Any]) -> str:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³IDç”Ÿæˆ"""

        context_str = json.dumps(context, sort_keys=True)
        context_hash = hashlib.md5(context_str.encode()).hexdigest()[:8]
        return f"{prefix}_{context_hash}"

    def _determine_learning_confidence(self, score: float) -> LearningConfidence:
        """å­¦ç¿’ä¿¡é ¼åº¦åˆ¤å®š"""

        if score >= 0.9:
            return LearningConfidence.VERY_HIGH
        elif score >= 0.8:
            return LearningConfidence.HIGH
        elif score >= 0.6:
            return LearningConfidence.MEDIUM
        else:
            return LearningConfidence.LOW

    def _get_score_range(self, score: float) -> str:
        """ã‚¹ã‚³ã‚¢ç¯„å›²å–å¾—"""

        if score >= 0.9:
            return "very_high"
        elif score >= 0.8:
            return "high"
        elif score >= 0.6:
            return "medium"
        elif score >= 0.4:
            return "low"
        else:
            return "very_low"

    def _get_time_range(self, time_val: float) -> str:
        """æ™‚é–“ç¯„å›²å–å¾—"""

        if time_val <= 1.0:
            return "very_fast"
        elif time_val <= 2.0:
            return "fast"
        elif time_val <= 5.0:
            return "normal"
        else:
            return "slow"

    def _get_improvement_range(self, improvement: float) -> str:
        """æ”¹å–„ç¯„å›²å–å¾—"""

        if improvement >= 10.0:
            return "major"
        elif improvement >= 5.0:
            return "significant"
        elif improvement >= 2.0:
            return "moderate"
        else:
            return "minor"

    def _deduplicate_patterns(self, patterns: List[LearningPattern]) -> List[LearningPattern]:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³é‡è¤‡é™¤å»"""

        unique_patterns = {}

        for pattern in patterns:
            if pattern.pattern_id not in unique_patterns:
                unique_patterns[pattern.pattern_id] = pattern
            else:
                # é‡è¤‡ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯é »åº¦ã‚’å¢—åŠ 
                existing = unique_patterns[pattern.pattern_id]
                existing.frequency += 1
                existing.last_seen = pattern.last_seen

        return list(unique_patterns.values())

    def _pattern_exists(self, pattern: LearningPattern) -> bool:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³å­˜åœ¨ãƒã‚§ãƒƒã‚¯"""

        return any(p.pattern_id == pattern.pattern_id for p in self.learned_patterns)

    # ãƒ‡ãƒ¼ã‚¿ä¿å­˜ãƒ»èª­ã¿è¾¼ã¿ãƒ¡ã‚½ãƒƒãƒ‰
    def _save_learning_results(self, results: Dict[str, Any]) -> None:
        """å­¦ç¿’çµæœä¿å­˜"""

        try:
            # ãƒ‘ã‚¿ãƒ¼ãƒ³ä¿å­˜
            self._save_learned_patterns()

            # å­¦ç¿’ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
            self.learning_metrics.append({
                "timestamp": datetime.datetime.now().isoformat(),
                "results": results
            })

            # æœ€æ–°20ä»¶ã®ã¿ä¿æŒ
            if len(self.learning_metrics) > 20:
                self.learning_metrics = self.learning_metrics[-20:]

            with open(self.learning_metrics_file, 'w', encoding='utf-8') as f:
                json.dump(self.learning_metrics, f, indent=2, ensure_ascii=False)

        except Exception as e:
            logger.error(f"âŒ å­¦ç¿’çµæœä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    def _save_learned_patterns(self) -> None:
        """å­¦ç¿’æ¸ˆã¿ãƒ‘ã‚¿ãƒ¼ãƒ³ä¿å­˜"""

        try:
            patterns_data = [asdict(pattern) for pattern in self.learned_patterns]

            with open(self.patterns_file, 'w', encoding='utf-8') as f:
                json.dump(patterns_data, f, indent=2, ensure_ascii=False, default=str)

        except Exception as e:
            logger.error(f"âŒ å­¦ç¿’ãƒ‘ã‚¿ãƒ¼ãƒ³ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    def _save_feedback_history(self) -> None:
        """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å±¥æ­´ä¿å­˜"""

        try:
            feedback_data = [asdict(feedback) for feedback in self.feedback_history]

            # æœ€æ–°100ä»¶ã®ã¿ä¿æŒ
            if len(feedback_data) > 100:
                feedback_data = feedback_data[-100:]
                self.feedback_history = self.feedback_history[-100:]

            with open(self.feedback_file, 'w', encoding='utf-8') as f:
                json.dump(feedback_data, f, indent=2, ensure_ascii=False)

        except Exception as e:
            logger.error(f"âŒ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å±¥æ­´ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    def _save_learning_report(self, report: LearningReport) -> None:
        """å­¦ç¿’ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜"""

        try:
            report_file = self.learning_dir / f"learning_report_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(asdict(report), f, indent=2, ensure_ascii=False, default=str)

        except Exception as e:
            logger.error(f"âŒ å­¦ç¿’ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    def _load_learned_patterns(self) -> List[LearningPattern]:
        """å­¦ç¿’æ¸ˆã¿ãƒ‘ã‚¿ãƒ¼ãƒ³èª­ã¿è¾¼ã¿"""

        if not self.patterns_file.exists():
            return []

        try:
            with open(self.patterns_file, 'r', encoding='utf-8') as f:
                patterns_data = json.load(f)

            patterns = []
            for data in patterns_data:
                # Enumå¤‰æ›
                data["category"] = LearningCategory(data["category"])
                data["confidence"] = LearningConfidence(data["confidence"])

                pattern = LearningPattern(**data)
                patterns.append(pattern)

            return patterns

        except Exception as e:
            logger.warning(f"âš ï¸ å­¦ç¿’ãƒ‘ã‚¿ãƒ¼ãƒ³èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def _load_feedback_history(self) -> List[FeedbackData]:
        """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å±¥æ­´èª­ã¿è¾¼ã¿"""

        if not self.feedback_file.exists():
            return []

        try:
            with open(self.feedback_file, 'r', encoding='utf-8') as f:
                feedback_data = json.load(f)

            feedback_list = []
            for data in feedback_data:
                # Enumå¤‰æ›
                data["feedback_type"] = FeedbackType(data["feedback_type"])

                feedback = FeedbackData(**data)
                feedback_list.append(feedback)

            return feedback_list

        except Exception as e:
            logger.warning(f"âš ï¸ ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å±¥æ­´èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def _load_learning_metrics(self) -> List[Dict[str, Any]]:
        """å­¦ç¿’ãƒ¡ãƒˆãƒªã‚¯ã‚¹èª­ã¿è¾¼ã¿"""

        if not self.learning_metrics_file.exists():
            return []

        try:
            with open(self.learning_metrics_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.warning(f"âš ï¸ å­¦ç¿’ãƒ¡ãƒˆãƒªã‚¯ã‚¹èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def _create_fallback_learning_report(self) -> LearningReport:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å­¦ç¿’ãƒ¬ãƒãƒ¼ãƒˆ"""

        return LearningReport(
            timestamp=datetime.datetime.now().isoformat(),
            total_patterns_learned=0,
            active_patterns=0,
            learning_accuracy=0.0,
            optimization_suggestions=[],
            performance_improvements={},
            confidence_distribution={},
            recent_discoveries=[]
        )


def main() -> None:
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    learning_system = ContinuousLearningSystem()

    # å”æ¥­ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’
    learning_results = learning_system.learn_from_collaboration_data()
    print(f"æ–°ãƒ‘ã‚¿ãƒ¼ãƒ³ç™ºè¦‹: {learning_results.get('new_patterns_discovered', 0)}ä»¶")
    print(f"ãƒ‘ã‚¿ãƒ¼ãƒ³æ›´æ–°: {learning_results.get('patterns_updated', 0)}ä»¶")
    print(f"å­¦ç¿’ç²¾åº¦: {learning_results.get('learning_accuracy', 0):.1%}")

    # å­¦ç¿’ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report = learning_system.generate_learning_report()
    print(f"å­¦ç¿’æ¸ˆã¿ãƒ‘ã‚¿ãƒ¼ãƒ³: {report.total_patterns_learned}ä»¶")
    print(f"æœ€é©åŒ–ææ¡ˆ: {len(report.optimization_suggestions)}ä»¶")

    # å­¦ç¿’æ¸ˆã¿æœ€é©åŒ–é©ç”¨
    application_results = learning_system.apply_learned_optimizations()
    print(f"é©ç”¨ã•ã‚ŒãŸæœ€é©åŒ–: {application_results.get('optimizations_applied', 0)}ä»¶")


if __name__ == "__main__":
    main()
