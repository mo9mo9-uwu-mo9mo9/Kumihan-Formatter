#!/usr/bin/env python3
"""
CollaborationEfficiencyMonitor - Issue #860å¯¾å¿œ
GeminiÃ—Claudeå”æ¥­åŠ¹ç‡ã®åŒ…æ‹¬çš„ç›£è¦–ãƒ»åˆ†æã‚·ã‚¹ãƒ†ãƒ 

å”æ¥­å®‰å…¨æ€§ã‚’40/100ã‹ã‚‰75-80/100ã«å‘ä¸Šã•ã›ã‚‹ãŸã‚ã®
ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–ãƒ»ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æ¤œå‡ºãƒ»æœ€é©åŒ–æ¨å¥¨ã‚·ã‚¹ãƒ†ãƒ 
"""

import json
import datetime
import statistics
from typing import Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass, asdict
from enum import Enum
from pathlib import Path
import os

from kumihan_formatter.core.utilities.logger import get_logger

logger = get_logger(__name__)


class CollaborationSafetyLevel(Enum):
    """å”æ¥­å®‰å…¨æ€§ãƒ¬ãƒ™ãƒ«"""
    CRITICAL = "critical"      # 0-30: ç·Šæ€¥å¯¾å¿œå¿…è¦
    LOW = "low"               # 31-50: æ”¹å–„å¿…è¦
    MODERATE = "moderate"     # 51-70: æ”¹å–„æ¨å¥¨
    GOOD = "good"             # 71-85: è‰¯å¥½
    EXCELLENT = "excellent"   # 86-100: å„ªç§€


class BottleneckType(Enum):
    """ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã‚¿ã‚¤ãƒ—"""
    LAYER1_SYNTAX = "layer1_syntax_validation"
    LAYER2_QUALITY = "layer2_quality_check"
    LAYER3_APPROVAL = "layer3_claude_approval"
    INTEGRATION = "integration_test"
    TASK_ROUTING = "task_routing_decision"
    FAILSAFE_OVERUSE = "failsafe_mechanism_overuse"
    PROCESSING_TIME = "processing_time_exceeded"
    TOKEN_EFFICIENCY = "token_efficiency_degraded"


@dataclass
class CollaborationMetrics:
    """å”æ¥­ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    timestamp: str
    overall_safety_score: float  # 0-100
    safety_level: CollaborationSafetyLevel
    task_success_rate: float  # 0.0-1.0
    layer_success_rates: Dict[str, float]
    failsafe_usage_rate: float  # 0.0-1.0
    average_processing_time: float  # seconds
    token_efficiency: float  # 0.0-1.0
    total_tasks_processed: int
    detected_bottlenecks: List[BottleneckType]
    recommendations: List[str]


@dataclass
class EfficiencyTrend:
    """åŠ¹ç‡ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ"""
    timeframe: str  # "1h", "24h", "7d", "30d"
    safety_score_trend: float  # positive=æ”¹å–„, negative=æ‚ªåŒ–
    success_rate_trend: float
    processing_time_trend: float
    failsafe_usage_trend: float
    critical_issues: List[str]
    improvement_opportunities: List[str]


class CollaborationEfficiencyMonitor:
    """å”æ¥­åŠ¹ç‡ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 

    GeminiÃ—Claudeå”æ¥­ã®åŒ…æ‹¬çš„ç›£è¦–ãƒ»åˆ†æãƒ»æœ€é©åŒ–æ¨å¥¨ã‚’è¡Œã†
    Issue #860: å”æ¥­å®‰å…¨æ€§40/100 â†’ 75-80/100é”æˆã®ãŸã‚
    """

    def __init__(self, postbox_dir: str = "postbox"):
        self.postbox_dir = Path(postbox_dir)
        self.monitoring_dir = self.postbox_dir / "monitoring"
        self.collaboration_dir = self.postbox_dir / "collaboration"

        # ç›£è¦–ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        self.collaboration_dir.mkdir(exist_ok=True)

        self.metrics_file = self.collaboration_dir / "efficiency_metrics.json"
        self.trends_file = self.collaboration_dir / "efficiency_trends.json"
        self.alerts_file = self.collaboration_dir / "efficiency_alerts.json"

        # ç›®æ¨™å€¤è¨­å®š (Issue #860è¦ä»¶)
        self.TARGET_SAFETY_SCORE = 75.0  # æœ€å°ç›®æ¨™
        self.TARGET_SUCCESS_RATE = 0.70  # 70%ä»¥ä¸Š
        self.TARGET_PROCESSING_TIME = 2.0  # 2ç§’ä»¥å†…
        self.TARGET_FAILSAFE_RATE = 0.30  # 30%ä»¥ä¸‹
        self.TARGET_TOKEN_EFFICIENCY = 0.99  # 99%ä»¥ä¸Š

        logger.info("ğŸ” CollaborationEfficiencyMonitor åˆæœŸåŒ–å®Œäº†")

    def collect_current_metrics(self) -> CollaborationMetrics:
        """ç¾åœ¨ã®å”æ¥­ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†"""

        logger.info("ğŸ“Š å”æ¥­ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†é–‹å§‹")

        try:
            # 3å±¤æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿åé›†
            verification_data = self._load_verification_metrics()

            # ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ãƒ‡ãƒ¼ã‚¿åé›†
            failsafe_data = self._load_failsafe_metrics()

            # ã‚³ã‚¹ãƒˆãƒ»ãƒˆãƒ¼ã‚¯ãƒ³ãƒ‡ãƒ¼ã‚¿åé›†
            cost_data = self._load_cost_metrics()

            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
            metrics = self._calculate_comprehensive_metrics(
                verification_data, failsafe_data, cost_data
            )

            # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æ¤œå‡º
            bottlenecks = self._detect_bottlenecks(metrics)
            metrics.detected_bottlenecks = bottlenecks

            # æ¨å¥¨äº‹é …ç”Ÿæˆ
            recommendations = self._generate_recommendations(metrics)
            metrics.recommendations = recommendations

            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¿å­˜
            self._save_metrics(metrics)

            logger.info(f"âœ… å”æ¥­ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†å®Œäº†: å®‰å…¨æ€§ã‚¹ã‚³ã‚¢ {metrics.overall_safety_score:.1f}/100")

            return metrics

        except Exception as e:
            logger.error(f"âŒ å”æ¥­ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            return self._create_fallback_metrics()

    def _load_verification_metrics(self) -> Dict[str, Any]:
        """3å±¤æ¤œè¨¼ãƒ¡ãƒˆãƒªã‚¯ã‚¹èª­ã¿è¾¼ã¿"""

        verification_file = self.monitoring_dir / "three_layer_verification_metrics.json"

        if not verification_file.exists():
            logger.warning("âš ï¸ 3å±¤æ¤œè¨¼ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return {}

        try:
            with open(verification_file, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã®ã¿ä½¿ç”¨ï¼ˆç›´è¿‘5ä»¶ï¼‰
            recent_data = data[-5:] if len(data) > 5 else data

            return {
                "recent_runs": recent_data,
                "total_runs": len(data),
                "data_available": True
            }

        except Exception as e:
            logger.error(f"âŒ 3å±¤æ¤œè¨¼ãƒ¡ãƒˆãƒªã‚¯ã‚¹èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return {"data_available": False}

    def _load_failsafe_metrics(self) -> Dict[str, Any]:
        """ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ãƒ¡ãƒˆãƒªã‚¯ã‚¹èª­ã¿è¾¼ã¿"""

        failsafe_file = self.monitoring_dir / "failsafe_usage.json"

        if not failsafe_file.exists():
            logger.warning("âš ï¸ ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return {}

        try:
            with open(failsafe_file, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã®ã¿ä½¿ç”¨
            recent_data = data[-10:] if len(data) > 10 else data

            return {
                "recent_usage": recent_data,
                "total_records": len(data),
                "data_available": True
            }

        except Exception as e:
            logger.error(f"âŒ ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ãƒ¡ãƒˆãƒªã‚¯ã‚¹èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return {"data_available": False}

    def _load_cost_metrics(self) -> Dict[str, Any]:
        """ã‚³ã‚¹ãƒˆãƒ»ãƒˆãƒ¼ã‚¯ãƒ³ãƒ¡ãƒˆãƒªã‚¯ã‚¹èª­ã¿è¾¼ã¿"""

        cost_file = self.monitoring_dir / "cost_tracking.json"

        if not cost_file.exists():
            logger.warning("âš ï¸ ã‚³ã‚¹ãƒˆãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return {}

        try:
            with open(cost_file, 'r', encoding='utf-8') as f:
                data = json.load(f)

            return {
                "total_cost": data.get("total_cost", 0.0),
                "tasks": data.get("tasks", []),
                "data_available": True
            }

        except Exception as e:
            logger.error(f"âŒ ã‚³ã‚¹ãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return {"data_available": False}

    def _calculate_comprehensive_metrics(self, verification_data: Dict[str, Any],
                                       failsafe_data: Dict[str, Any],
                                       cost_data: Dict[str, Any]) -> CollaborationMetrics:
        """åŒ…æ‹¬çš„ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—"""

        timestamp = datetime.datetime.now().isoformat()

        # 3å±¤æ¤œè¨¼æˆåŠŸç‡è¨ˆç®—
        layer_success_rates = self._calculate_layer_success_rates(verification_data)
        overall_success_rate = self._calculate_overall_success_rate(layer_success_rates)

        # ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ä½¿ç”¨ç‡è¨ˆç®—
        failsafe_usage_rate = self._calculate_failsafe_usage_rate(failsafe_data)

        # å‡¦ç†æ™‚é–“è¨ˆç®—
        avg_processing_time = self._calculate_processing_time(verification_data)

        # ãƒˆãƒ¼ã‚¯ãƒ³åŠ¹ç‡è¨ˆç®—
        token_efficiency = self._calculate_token_efficiency(cost_data)

        # å”æ¥­å®‰å…¨æ€§ã‚¹ã‚³ã‚¢è¨ˆç®— (Issue #860æ ¸å¿ƒæŒ‡æ¨™)
        safety_score = self._calculate_safety_score(
            overall_success_rate, failsafe_usage_rate,
            avg_processing_time, token_efficiency
        )

        # å®‰å…¨æ€§ãƒ¬ãƒ™ãƒ«åˆ¤å®š
        safety_level = self._determine_safety_level(safety_score)

        # ã‚¿ã‚¹ã‚¯å‡¦ç†æ•°
        total_tasks = self._count_total_tasks(verification_data, failsafe_data)

        return CollaborationMetrics(
            timestamp=timestamp,
            overall_safety_score=safety_score,
            safety_level=safety_level,
            task_success_rate=overall_success_rate,
            layer_success_rates=layer_success_rates,
            failsafe_usage_rate=failsafe_usage_rate,
            average_processing_time=avg_processing_time,
            token_efficiency=token_efficiency,
            total_tasks_processed=total_tasks,
            detected_bottlenecks=[],  # _detect_bottlenecksã§è¨­å®š
            recommendations=[]        # _generate_recommendationsã§è¨­å®š
        )

    def _calculate_layer_success_rates(self, verification_data: Dict[str, Any]) -> Dict[str, float]:
        """å„å±¤ã®æˆåŠŸç‡è¨ˆç®—"""

        if not verification_data.get("data_available", False):
            return {
                "layer1_success_rate": 0.0,
                "layer2_success_rate": 0.0,
                "layer3_success_rate": 0.0,
                "integration_success_rate": 0.0
            }

        recent_runs = verification_data.get("recent_runs", [])

        if not recent_runs:
            return {
                "layer1_success_rate": 0.0,
                "layer2_success_rate": 0.0,
                "layer3_success_rate": 0.0,
                "integration_success_rate": 0.0
            }

        # å„å±¤ã®æˆåŠŸç‡ã‚’å¹³å‡åŒ–
        layer1_rates = [run["success_rates"]["layer1_success_rate"] for run in recent_runs]
        layer2_rates = [run["success_rates"]["layer2_success_rate"] for run in recent_runs]
        layer3_rates = [run["success_rates"]["layer3_approval_rate"] for run in recent_runs]
        integration_rates = [run["success_rates"]["integration_success_rate"] for run in recent_runs]

        return {
            "layer1_success_rate": statistics.mean(layer1_rates) if layer1_rates else 0.0,
            "layer2_success_rate": statistics.mean(layer2_rates) if layer2_rates else 0.0,
            "layer3_success_rate": statistics.mean(layer3_rates) if layer3_rates else 0.0,
            "integration_success_rate": statistics.mean(integration_rates) if integration_rates else 0.0
        }

    def _calculate_overall_success_rate(self, layer_rates: Dict[str, float]) -> float:
        """å…¨ä½“æˆåŠŸç‡è¨ˆç®—"""

        # é‡ã¿ä»˜ãå¹³å‡ (Layer2å“è³ªãŒæœ€é‡è¦ã€Layer3 Claudeæ‰¿èªãŒæ¬¡é‡è¦)
        weights = {
            "layer1_success_rate": 0.2,
            "layer2_success_rate": 0.3,
            "layer3_success_rate": 0.3,
            "integration_success_rate": 0.2
        }

        weighted_sum = sum(layer_rates[key] * weights[key] for key in weights.keys())

        return min(weighted_sum, 1.0)  # 1.0ã§ä¸Šé™åˆ¶é™

    def _calculate_failsafe_usage_rate(self, failsafe_data: Dict[str, Any]) -> float:
        """ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ä½¿ç”¨ç‡è¨ˆç®—"""

        if not failsafe_data.get("data_available", False):
            return 0.0

        recent_usage = failsafe_data.get("recent_usage", [])

        if not recent_usage:
            return 0.0

        # ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ä½¿ç”¨é »åº¦ (ä»¶æ•° / ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°)
        total_failsafe_count = sum(usage["failsafe_count"] for usage in recent_usage)
        total_files_processed = sum(usage["total_files"] for usage in recent_usage)

        if total_files_processed == 0:
            return 0.0

        # ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ç‡ = ä½¿ç”¨å›æ•° / å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°
        usage_rate = total_failsafe_count / total_files_processed

        return min(usage_rate, 1.0)  # 1.0ã§ä¸Šé™åˆ¶é™

    def _calculate_processing_time(self, verification_data: Dict[str, Any]) -> float:
        """å¹³å‡å‡¦ç†æ™‚é–“è¨ˆç®—"""

        if not verification_data.get("data_available", False):
            return 0.0

        recent_runs = verification_data.get("recent_runs", [])

        if not recent_runs:
            return 0.0

        execution_times = [run["execution_time"] for run in recent_runs]

        return statistics.mean(execution_times) if execution_times else 0.0

    def _calculate_token_efficiency(self, cost_data: Dict[str, Any]) -> float:
        """ãƒˆãƒ¼ã‚¯ãƒ³åŠ¹ç‡è¨ˆç®—"""

        if not cost_data.get("data_available", False):
            return 0.99  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé«˜åŠ¹ç‡

        total_cost = cost_data.get("total_cost", 0.0)
        tasks = cost_data.get("tasks", [])

        if not tasks:
            return 0.99

        # å¹³å‡ãƒˆãƒ¼ã‚¯ãƒ³åŠ¹ç‡ã‚’æ¨å®š
        # ã‚³ã‚¹ãƒˆãŒä½ã„ = åŠ¹ç‡ãŒé«˜ã„ ã¨ä»®å®š
        if total_cost <= 0.02:  # $0.02ä»¥ä¸‹ã¯éå¸¸ã«åŠ¹ç‡çš„
            return 0.99
        elif total_cost <= 0.05:  # $0.05ä»¥ä¸‹ã¯åŠ¹ç‡çš„
            return 0.95
        elif total_cost <= 0.10:  # $0.10ä»¥ä¸‹ã¯æ™®é€š
            return 0.90
        else:
            return 0.85  # ãã‚Œä»¥ä¸Šã¯æ”¹å–„ä½™åœ°ã‚ã‚Š

    def _calculate_safety_score(self, success_rate: float, failsafe_rate: float,
                              processing_time: float, token_efficiency: float) -> float:
        """å”æ¥­å®‰å…¨æ€§ã‚¹ã‚³ã‚¢è¨ˆç®— (Issue #860æ ¸å¿ƒæŒ‡æ¨™)

        ç›®æ¨™: 40/100 â†’ 75-80/100
        """

        # æˆåŠŸç‡ã‚¹ã‚³ã‚¢ (40ç‚¹æº€ç‚¹)
        success_score = success_rate * 40

        # ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ã‚¹ã‚³ã‚¢ (25ç‚¹æº€ç‚¹) - ä½¿ç”¨ç‡ãŒä½ã„ã»ã©é«˜ã‚¹ã‚³ã‚¢
        failsafe_score = max(0, (1.0 - failsafe_rate) * 25)

        # å‡¦ç†æ™‚é–“ã‚¹ã‚³ã‚¢ (20ç‚¹æº€ç‚¹) - 2ç§’ä»¥å†…ã§æº€ç‚¹
        if processing_time <= 2.0:
            time_score = 20
        elif processing_time <= 5.0:
            time_score = 20 * (5.0 - processing_time) / 3.0
        else:
            time_score = 0

        # ãƒˆãƒ¼ã‚¯ãƒ³åŠ¹ç‡ã‚¹ã‚³ã‚¢ (15ç‚¹æº€ç‚¹)
        efficiency_score = token_efficiency * 15

        total_score = success_score + failsafe_score + time_score + efficiency_score

        return min(total_score, 100.0)  # 100ç‚¹æº€ç‚¹

    def _determine_safety_level(self, safety_score: float) -> CollaborationSafetyLevel:
        """å®‰å…¨æ€§ãƒ¬ãƒ™ãƒ«åˆ¤å®š"""

        if safety_score >= 86:
            return CollaborationSafetyLevel.EXCELLENT
        elif safety_score >= 71:
            return CollaborationSafetyLevel.GOOD
        elif safety_score >= 51:
            return CollaborationSafetyLevel.MODERATE
        elif safety_score >= 31:
            return CollaborationSafetyLevel.LOW
        else:
            return CollaborationSafetyLevel.CRITICAL

    def _count_total_tasks(self, verification_data: Dict[str, Any],
                          failsafe_data: Dict[str, Any]) -> int:
        """ç·ã‚¿ã‚¹ã‚¯å‡¦ç†æ•°è¨ˆç®—"""

        verification_count = 0
        if verification_data.get("data_available", False):
            verification_count = verification_data.get("total_runs", 0)

        failsafe_count = 0
        if failsafe_data.get("data_available", False):
            failsafe_count = failsafe_data.get("total_records", 0)

        return max(verification_count, failsafe_count)

    def _detect_bottlenecks(self, metrics: CollaborationMetrics) -> List[BottleneckType]:
        """å”æ¥­ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æ¤œå‡º"""

        bottlenecks = []

        # Layer1æ§‹æ–‡æ¤œè¨¼ãƒœãƒˆãƒ«ãƒãƒƒã‚¯
        if metrics.layer_success_rates["layer1_success_rate"] < 0.8:
            bottlenecks.append(BottleneckType.LAYER1_SYNTAX)

        # Layer2å“è³ªãƒã‚§ãƒƒã‚¯ãƒœãƒˆãƒ«ãƒãƒƒã‚¯
        if metrics.layer_success_rates["layer2_success_rate"] < 0.9:
            bottlenecks.append(BottleneckType.LAYER2_QUALITY)

        # Layer3 Claudeæ‰¿èªãƒœãƒˆãƒ«ãƒãƒƒã‚¯
        if metrics.layer_success_rates["layer3_success_rate"] < 0.7:
            bottlenecks.append(BottleneckType.LAYER3_APPROVAL)

        # çµ±åˆãƒ†ã‚¹ãƒˆãƒœãƒˆãƒ«ãƒãƒƒã‚¯
        if metrics.layer_success_rates["integration_success_rate"] < 0.8:
            bottlenecks.append(BottleneckType.INTEGRATION)

        # ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•éä½¿ç”¨ãƒœãƒˆãƒ«ãƒãƒƒã‚¯
        if metrics.failsafe_usage_rate > self.TARGET_FAILSAFE_RATE:
            bottlenecks.append(BottleneckType.FAILSAFE_OVERUSE)

        # å‡¦ç†æ™‚é–“ãƒœãƒˆãƒ«ãƒãƒƒã‚¯
        if metrics.average_processing_time > self.TARGET_PROCESSING_TIME:
            bottlenecks.append(BottleneckType.PROCESSING_TIME)

        # ãƒˆãƒ¼ã‚¯ãƒ³åŠ¹ç‡ãƒœãƒˆãƒ«ãƒãƒƒã‚¯
        if metrics.token_efficiency < self.TARGET_TOKEN_EFFICIENCY:
            bottlenecks.append(BottleneckType.TOKEN_EFFICIENCY)

        return bottlenecks

    def _generate_recommendations(self, metrics: CollaborationMetrics) -> List[str]:
        """æœ€é©åŒ–æ¨å¥¨äº‹é …ç”Ÿæˆ"""

        recommendations = []

        # å®‰å…¨æ€§ã‚¹ã‚³ã‚¢åˆ¥æ¨å¥¨
        if metrics.overall_safety_score < 50:
            recommendations.append("ğŸš¨ ç·Šæ€¥: å”æ¥­å®‰å…¨æ€§ãŒå±é™ºãƒ¬ãƒ™ãƒ«ã§ã™ã€‚å³åº§ã®æ”¹å–„ãŒå¿…è¦ã§ã™")
            recommendations.append("3å±¤æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ ã®å…¨é¢è¦‹ç›´ã—ã‚’å®Ÿæ–½ã—ã¦ãã ã•ã„")
        elif metrics.overall_safety_score < self.TARGET_SAFETY_SCORE:
            recommendations.append("âš ï¸ å”æ¥­å®‰å…¨æ€§ãŒç›®æ¨™å€¤æœªé”ã§ã™ã€‚ã‚·ã‚¹ãƒ†ãƒ æœ€é©åŒ–ã‚’æ¨å¥¨ã—ã¾ã™")

        # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ¥æ¨å¥¨
        for bottleneck in metrics.detected_bottlenecks:
            if bottleneck == BottleneckType.LAYER1_SYNTAX:
                recommendations.append("ğŸ“ Layer1æ§‹æ–‡æ¤œè¨¼ã®æ”¹å–„: äº‹å‰ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å¼·åŒ–ã‚’æ¨å¥¨")
            elif bottleneck == BottleneckType.LAYER2_QUALITY:
                recommendations.append("ğŸ” Layer2å“è³ªãƒã‚§ãƒƒã‚¯ã®æ”¹å–„: å“è³ªåŸºæº–è¦‹ç›´ã—ã‚’æ¨å¥¨")
            elif bottleneck == BottleneckType.LAYER3_APPROVAL:
                recommendations.append("ğŸ‘¤ Layer3 Claudeæ‰¿èªã®æ”¹å–„: æ‰¿èªãƒ—ãƒ­ã‚»ã‚¹æœ€é©åŒ–ã‚’æ¨å¥¨")
            elif bottleneck == BottleneckType.FAILSAFE_OVERUSE:
                recommendations.append("ğŸ›¡ï¸ ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•éä½¿ç”¨: äºˆé˜²çš„å“è³ªãƒã‚§ãƒƒã‚¯å¼·åŒ–ã‚’æ¨å¥¨")
            elif bottleneck == BottleneckType.PROCESSING_TIME:
                recommendations.append("â±ï¸ å‡¦ç†æ™‚é–“æ”¹å–„: ä¸¦åˆ—å‡¦ç†ãƒ»æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å°å…¥ã‚’æ¨å¥¨")

        # æˆåŠŸç‡åˆ¥æ¨å¥¨
        if metrics.task_success_rate < self.TARGET_SUCCESS_RATE:
            recommendations.append(f"ğŸ“ˆ ã‚¿ã‚¹ã‚¯æˆåŠŸç‡å‘ä¸Š: ç¾åœ¨{metrics.task_success_rate:.1%} â†’ ç›®æ¨™70%+")

        # ãƒã‚¸ãƒ†ã‚£ãƒ–ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
        if metrics.overall_safety_score >= self.TARGET_SAFETY_SCORE:
            recommendations.append("âœ… å”æ¥­å®‰å…¨æ€§ãŒç›®æ¨™å€¤ã‚’é”æˆã—ã¦ã„ã¾ã™ï¼")

        if metrics.token_efficiency >= self.TARGET_TOKEN_EFFICIENCY:
            recommendations.append("ğŸ’° ãƒˆãƒ¼ã‚¯ãƒ³åŠ¹ç‡ãŒå„ªç§€ã§ã™ï¼ã‚³ã‚¹ãƒˆæœ€é©åŒ–ãŒæˆåŠŸã—ã¦ã„ã¾ã™")

        return recommendations

    def _save_metrics(self, metrics: CollaborationMetrics) -> None:
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¿å­˜"""

        try:
            # æ—¢å­˜ãƒ¡ãƒˆãƒªã‚¯ã‚¹èª­ã¿è¾¼ã¿
            if self.metrics_file.exists():
                with open(self.metrics_file, 'r', encoding='utf-8') as f:
                    all_metrics = json.load(f)
            else:
                all_metrics = []

            # æ–°ã—ã„ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¿½åŠ 
            all_metrics.append(asdict(metrics))

            # æœ€æ–°100ä»¶ã®ã¿ä¿æŒ
            if len(all_metrics) > 100:
                all_metrics = all_metrics[-100:]

            # ä¿å­˜
            with open(self.metrics_file, 'w', encoding='utf-8') as f:
                json.dump(all_metrics, f, indent=2, ensure_ascii=False, default=str)

            logger.info(f"âœ… å”æ¥­ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¿å­˜å®Œäº†: {self.metrics_file}")

        except Exception as e:
            logger.error(f"âŒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    def _create_fallback_metrics(self) -> CollaborationMetrics:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä½œæˆ"""

        return CollaborationMetrics(
            timestamp=datetime.datetime.now().isoformat(),
            overall_safety_score=40.0,  # ç¾çŠ¶ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³
            safety_level=CollaborationSafetyLevel.LOW,
            task_success_rate=0.5,
            layer_success_rates={
                "layer1_success_rate": 0.5,
                "layer2_success_rate": 0.8,
                "layer3_success_rate": 0.6,
                "integration_success_rate": 0.7
            },
            failsafe_usage_rate=0.6,
            average_processing_time=1.5,
            token_efficiency=0.99,
            total_tasks_processed=0,
            detected_bottlenecks=[BottleneckType.LAYER1_SYNTAX, BottleneckType.FAILSAFE_OVERUSE],
            recommendations=["ğŸ“Š ãƒ‡ãƒ¼ã‚¿ä¸è¶³: å”æ¥­ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ã‚’ç¶™ç¶šã—ã¦ãã ã•ã„"]
        )

    def analyze_efficiency_trends(self, timeframe: str = "24h") -> EfficiencyTrend:
        """åŠ¹ç‡ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ"""

        logger.info(f"ğŸ“ˆ åŠ¹ç‡ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æé–‹å§‹: {timeframe}")

        try:
            # éå»ãƒ¡ãƒˆãƒªã‚¯ã‚¹èª­ã¿è¾¼ã¿
            if not self.metrics_file.exists():
                logger.warning("âš ï¸ éå»ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
                return self._create_fallback_trend(timeframe)

            with open(self.metrics_file, 'r', encoding='utf-8') as f:
                all_metrics = json.load(f)

            if len(all_metrics) < 2:
                logger.warning("âš ï¸ ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã«ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                return self._create_fallback_trend(timeframe)

            # æ™‚é–“ç¯„å›²ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            filtered_metrics = self._filter_metrics_by_timeframe(all_metrics, timeframe)

            if len(filtered_metrics) < 2:
                logger.warning(f"âš ï¸ {timeframe}ã®ç¯„å›²ã«ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                return self._create_fallback_trend(timeframe)

            # ãƒˆãƒ¬ãƒ³ãƒ‰è¨ˆç®—
            trend = self._calculate_trends(filtered_metrics, timeframe)

            # ãƒˆãƒ¬ãƒ³ãƒ‰ä¿å­˜
            self._save_trend(trend)

            logger.info(f"âœ… åŠ¹ç‡ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æå®Œäº†: {timeframe}")

            return trend

        except Exception as e:
            logger.error(f"âŒ åŠ¹ç‡ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return self._create_fallback_trend(timeframe)

    def _filter_metrics_by_timeframe(self, all_metrics: List[Dict], timeframe: str) -> List[Dict]:
        """æ™‚é–“ç¯„å›²ã§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""

        now = datetime.datetime.now()

        if timeframe == "1h":
            cutoff = now - datetime.timedelta(hours=1)
        elif timeframe == "24h":
            cutoff = now - datetime.timedelta(hours=24)
        elif timeframe == "7d":
            cutoff = now - datetime.timedelta(days=7)
        elif timeframe == "30d":
            cutoff = now - datetime.timedelta(days=30)
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ24æ™‚é–“
            cutoff = now - datetime.timedelta(hours=24)

        filtered = []
        for metric in all_metrics:
            try:
                metric_time = datetime.datetime.fromisoformat(metric["timestamp"])
                if metric_time >= cutoff:
                    filtered.append(metric)
            except Exception:
                continue  # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—è§£æã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–

        return filtered

    def _calculate_trends(self, metrics: List[Dict], timeframe: str) -> EfficiencyTrend:
        """ãƒˆãƒ¬ãƒ³ãƒ‰è¨ˆç®—"""

        # æœ€åˆã¨æœ€å¾Œã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã§æ¯”è¼ƒ
        first_metric = metrics[0]
        last_metric = metrics[-1]

        # å„æŒ‡æ¨™ã®ãƒˆãƒ¬ãƒ³ãƒ‰è¨ˆç®— (æ”¹å–„=positive, æ‚ªåŒ–=negative)
        safety_trend = last_metric["overall_safety_score"] - first_metric["overall_safety_score"]
        success_trend = last_metric["task_success_rate"] - first_metric["task_success_rate"]
        processing_trend = first_metric["average_processing_time"] - last_metric["average_processing_time"]  # æ™‚é–“çŸ­ç¸®=positive
        failsafe_trend = first_metric["failsafe_usage_rate"] - last_metric["failsafe_usage_rate"]  # ä½¿ç”¨ç‡æ¸›å°‘=positive

        # é‡è¦ãªå•é¡Œæ¤œå‡º
        critical_issues = []
        improvement_opportunities = []

        # å®‰å…¨æ€§ã‚¹ã‚³ã‚¢ãŒæ‚ªåŒ–ã—ã¦ã„ã‚‹å ´åˆ
        if safety_trend < -5:
            critical_issues.append("å”æ¥­å®‰å…¨æ€§ã‚¹ã‚³ã‚¢ãŒå¤§å¹…ã«æ‚ªåŒ–ã—ã¦ã„ã¾ã™")

        # æˆåŠŸç‡ãŒæ‚ªåŒ–ã—ã¦ã„ã‚‹å ´åˆ
        if success_trend < -0.1:
            critical_issues.append("ã‚¿ã‚¹ã‚¯æˆåŠŸç‡ãŒæ‚ªåŒ–å‚¾å‘ã«ã‚ã‚Šã¾ã™")

        # ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ä½¿ç”¨ç‡ãŒå¢—åŠ ã—ã¦ã„ã‚‹å ´åˆ
        if failsafe_trend < -0.1:
            critical_issues.append("ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ä½¿ç”¨ç‡ãŒå¢—åŠ ã—ã¦ã„ã¾ã™")

        # æ”¹å–„æ©Ÿä¼šã®ç‰¹å®š
        if safety_trend > 0:
            improvement_opportunities.append("å”æ¥­å®‰å…¨æ€§ãŒæ”¹å–„å‚¾å‘ã«ã‚ã‚Šã¾ã™")

        if success_trend > 0:
            improvement_opportunities.append("ã‚¿ã‚¹ã‚¯æˆåŠŸç‡ãŒå‘ä¸Šã—ã¦ã„ã¾ã™")

        if processing_trend > 0:
            improvement_opportunities.append("å‡¦ç†æ™‚é–“ãŒçŸ­ç¸®ã•ã‚Œã¦ã„ã¾ã™")

        return EfficiencyTrend(
            timeframe=timeframe,
            safety_score_trend=safety_trend,
            success_rate_trend=success_trend,
            processing_time_trend=processing_trend,
            failsafe_usage_trend=failsafe_trend,
            critical_issues=critical_issues,
            improvement_opportunities=improvement_opportunities
        )

    def _create_fallback_trend(self, timeframe: str) -> EfficiencyTrend:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒˆãƒ¬ãƒ³ãƒ‰ä½œæˆ"""

        return EfficiencyTrend(
            timeframe=timeframe,
            safety_score_trend=0.0,
            success_rate_trend=0.0,
            processing_time_trend=0.0,
            failsafe_usage_trend=0.0,
            critical_issues=["ãƒ‡ãƒ¼ã‚¿ä¸è¶³: ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã«ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“"],
            improvement_opportunities=["ç¶™ç¶šçš„ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ã§ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æãŒå¯èƒ½ã«ãªã‚Šã¾ã™"]
        )

    def _save_trend(self, trend: EfficiencyTrend) -> None:
        """ãƒˆãƒ¬ãƒ³ãƒ‰ä¿å­˜"""

        try:
            # æ—¢å­˜ãƒˆãƒ¬ãƒ³ãƒ‰èª­ã¿è¾¼ã¿
            if self.trends_file.exists():
                with open(self.trends_file, 'r', encoding='utf-8') as f:
                    all_trends = json.load(f)
            else:
                all_trends = []

            # æ–°ã—ã„ãƒˆãƒ¬ãƒ³ãƒ‰è¿½åŠ 
            trend_dict = asdict(trend)
            trend_dict["timestamp"] = datetime.datetime.now().isoformat()
            all_trends.append(trend_dict)

            # æœ€æ–°50ä»¶ã®ã¿ä¿æŒ
            if len(all_trends) > 50:
                all_trends = all_trends[-50:]

            # ä¿å­˜
            with open(self.trends_file, 'w', encoding='utf-8') as f:
                json.dump(all_trends, f, indent=2, ensure_ascii=False)

            logger.info(f"âœ… åŠ¹ç‡ãƒˆãƒ¬ãƒ³ãƒ‰ä¿å­˜å®Œäº†: {self.trends_file}")

        except Exception as e:
            logger.error(f"âŒ ãƒˆãƒ¬ãƒ³ãƒ‰ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    def generate_efficiency_report(self) -> Dict[str, Any]:
        """å”æ¥­åŠ¹ç‡ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""

        logger.info("ğŸ“„ å”æ¥­åŠ¹ç‡ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆé–‹å§‹")

        try:
            # ç¾åœ¨ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—
            current_metrics = self.collect_current_metrics()

            # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
            trends_24h = self.analyze_efficiency_trends("24h")
            trends_7d = self.analyze_efficiency_trends("7d")

            # åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
            report = {
                "report_timestamp": datetime.datetime.now().isoformat(),
                "collaboration_status": {
                    "current_safety_score": current_metrics.overall_safety_score,
                    "safety_level": current_metrics.safety_level.value,
                    "target_achievement": {
                        "safety_score_target": self.TARGET_SAFETY_SCORE,
                        "target_achieved": current_metrics.overall_safety_score >= self.TARGET_SAFETY_SCORE,
                        "gap_to_target": self.TARGET_SAFETY_SCORE - current_metrics.overall_safety_score
                    }
                },
                "performance_metrics": {
                    "task_success_rate": current_metrics.task_success_rate,
                    "layer_performance": current_metrics.layer_success_rates,
                    "failsafe_usage_rate": current_metrics.failsafe_usage_rate,
                    "average_processing_time": current_metrics.average_processing_time,
                    "token_efficiency": current_metrics.token_efficiency,
                    "total_tasks_processed": current_metrics.total_tasks_processed
                },
                "bottleneck_analysis": {
                    "detected_bottlenecks": [b.value for b in current_metrics.detected_bottlenecks],
                    "bottleneck_count": len(current_metrics.detected_bottlenecks),
                    "critical_bottlenecks": [
                        b.value for b in current_metrics.detected_bottlenecks
                        if b in [BottleneckType.LAYER1_SYNTAX, BottleneckType.FAILSAFE_OVERUSE]
                    ]
                },
                "trend_analysis": {
                    "24h_trends": asdict(trends_24h),
                    "7d_trends": asdict(trends_7d),
                    "trend_summary": self._summarize_trends(trends_24h, trends_7d)
                },
                "recommendations": current_metrics.recommendations,
                "next_actions": self._generate_next_actions(current_metrics, trends_24h)
            }

            # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
            report_file = self.collaboration_dir / f"efficiency_report_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)

            logger.info(f"âœ… å”æ¥­åŠ¹ç‡ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {report_file}")

            return report

        except Exception as e:
            logger.error(f"âŒ å”æ¥­åŠ¹ç‡ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return {
                "error": str(e),
                "timestamp": datetime.datetime.now().isoformat()
            }

    def _summarize_trends(self, trends_24h: EfficiencyTrend, trends_7d: EfficiencyTrend) -> Dict[str, Any]:
        """ãƒˆãƒ¬ãƒ³ãƒ‰ã‚µãƒãƒªãƒ¼"""

        summary = {
            "overall_direction": "stable",
            "key_improvements": [],
            "key_concerns": [],
            "trend_confidence": "low"  # ãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„é–“ã¯ä½ã„
        }

        # 24æ™‚é–“ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
        if trends_24h.safety_score_trend > 5:
            summary["key_improvements"].append("24æ™‚é–“ã§å”æ¥­å®‰å…¨æ€§ãŒå¤§å¹…æ”¹å–„")
            summary["overall_direction"] = "improving"
        elif trends_24h.safety_score_trend < -5:
            summary["key_concerns"].append("24æ™‚é–“ã§å”æ¥­å®‰å…¨æ€§ãŒæ‚ªåŒ–")
            summary["overall_direction"] = "declining"

        # 7æ—¥é–“ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
        if trends_7d.safety_score_trend > 10:
            summary["key_improvements"].append("7æ—¥é–“ã§å”æ¥­å®‰å…¨æ€§ãŒç¶™ç¶šæ”¹å–„")
            summary["trend_confidence"] = "high"
        elif trends_7d.safety_score_trend < -10:
            summary["key_concerns"].append("7æ—¥é–“ã§å”æ¥­å®‰å…¨æ€§ãŒç¶™ç¶šæ‚ªåŒ–")
            summary["trend_confidence"] = "high"

        return summary

    def _generate_next_actions(self, metrics: CollaborationMetrics, trends: EfficiencyTrend) -> List[str]:
        """æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ææ¡ˆ"""

        actions = []

        # å®‰å…¨æ€§ã‚¹ã‚³ã‚¢åˆ¥ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
        if metrics.overall_safety_score < 60:
            actions.append("ğŸš¨ ç·Šæ€¥ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: 3å±¤æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ ã®å…¨é¢ç›£æŸ»ã¨æ”¹å–„")
            actions.append("ğŸ“Š è©³ç´°ãƒ­ã‚°åˆ†æã§ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®š")
            actions.append("ğŸ”§ ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•æ©Ÿèƒ½ã®æœ€é©åŒ–")

        # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ¥ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
        if BottleneckType.LAYER1_SYNTAX in metrics.detected_bottlenecks:
            actions.append("ğŸ“ Layer1æ§‹æ–‡æ¤œè¨¼ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ”¹å–„å®Ÿæ–½")

        if BottleneckType.FAILSAFE_OVERUSE in metrics.detected_bottlenecks:
            actions.append("ğŸ›¡ï¸ äºˆé˜²çš„å“è³ªãƒã‚§ãƒƒã‚¯æ©Ÿèƒ½ã®å¼·åŒ–")

        # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ¥ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
        if trends.safety_score_trend < 0:
            actions.append("ğŸ“ˆ å”æ¥­å®‰å…¨æ€§æ‚ªåŒ–ã¸ã®å¯¾ç­–æ¤œè¨")

        # å®šæœŸçš„ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³
        actions.append("ğŸ“Š é€±æ¬¡åŠ¹ç‡ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®å®Ÿæ–½")
        actions.append("ğŸ”„ å”æ¥­ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ç¶™ç¶šæœ€é©åŒ–")

        return actions


def main() -> None:
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    monitor = CollaborationEfficiencyMonitor()

    # ç¾åœ¨ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
    metrics = monitor.collect_current_metrics()
    print(f"å”æ¥­å®‰å…¨æ€§ã‚¹ã‚³ã‚¢: {metrics.overall_safety_score:.1f}/100")
    print(f"å®‰å…¨æ€§ãƒ¬ãƒ™ãƒ«: {metrics.safety_level.value}")

    # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
    trend = monitor.analyze_efficiency_trends("24h")
    print(f"24æ™‚é–“ãƒˆãƒ¬ãƒ³ãƒ‰: {trend.safety_score_trend:+.1f}")

    # åŠ¹ç‡ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report = monitor.generate_efficiency_report()
    print("å”æ¥­åŠ¹ç‡ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†")


if __name__ == "__main__":
    main()
