#!/usr/bin/env python3
"""
Dual-Agent Workflow Coordinator
Claude Code â†” Gemini CLI å”æ¥­çµ±åˆã‚·ã‚¹ãƒ†ãƒ 
"""

import json
import os
import subprocess
import time
from typing import Dict, List, Any, Optional
from pathlib import Path
import datetime

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‹ã‚‰postboxãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import sys
sys.path.append(str(Path(__file__).parent.parent))

from utils.task_manager import TaskManager
from utils.gemini_helper import GeminiHelper
from utils.task_analyzer import TaskAnalyzer
from templates.flash_templates import Flash25Templates
from templates.enhanced_flash_templates import EnhancedFlash25Templates
from quality.syntax_validator import SyntaxValidator, TypeAnnotationTemplate
from core.workflow_decision_engine import WorkflowDecisionEngine, AutomationLevel
from quality.quality_manager import QualityManager
from monitoring.quality_monitor import QualityMonitor
from reporting.quality_reporter import QualityReporter

# TokenMeasurementSystemçµ±åˆ
try:
    from utils.token_measurement import TokenMeasurementSystem
except ImportError:
    print("âš ï¸ TokenMeasurementSystemã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ")
    TokenMeasurementSystem = None

# Issue #844: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å®Ÿè£…ãƒ•ãƒ­ãƒ¼çµ±åˆ
from workflow.hybrid_implementation_flow import HybridImplementationFlow, HybridImplementationSpec
from phases.phase_a_architecture import PhaseAArchitecture
from phases.phase_b_implementation import PhaseBImplementation
from phases.phase_c_integration import PhaseCIntegration
from monitoring.success_rate_monitor import SuccessRateMonitor

# Issue #870: é«˜åº¦é–‹ç™ºã‚·ã‚¹ãƒ†ãƒ çµ±åˆ
from advanced.dependency_analyzer import DependencyAnalyzer
from advanced.multi_file_coordinator import MultiFileCoordinator
from advanced.pattern_implementation_engine import PatternImplementationEngine
from advanced.refactoring_engine import RefactoringEngine
from advanced.performance_optimizer import PerformanceOptimizer

class DualAgentCoordinator:
    """Claude â†” Geminiå”æ¥­ã®çµ±åˆã‚³ãƒ¼ãƒ‡ã‚£ãƒãƒ¼ã‚¿ãƒ¼"""

    def __init__(self):
        self.task_manager = TaskManager()
        self.gemini_helper = GeminiHelper()
        self.task_analyzer = TaskAnalyzer()
        self.flash_templates = Flash25Templates()
        self.enhanced_templates = EnhancedFlash25Templates()
        self.syntax_validator = SyntaxValidator()
        self.type_template = TypeAnnotationTemplate()
        self.decision_engine = WorkflowDecisionEngine()
        self.quality_manager = QualityManager()
        self.quality_monitor = QualityMonitor()

        # TokenMeasurementSystemåˆæœŸåŒ–
        self.token_measurement = TokenMeasurementSystem() if TokenMeasurementSystem else None
        self.quality_reporter = QualityReporter()
        self.session_id = f"session_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}"

        # Issue #844: 3æ®µéšãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å®Ÿè£…ãƒ•ãƒ­ãƒ¼ã‚·ã‚¹ãƒ†ãƒ 
        self.hybrid_flow = HybridImplementationFlow()
        self.phase_a = PhaseAArchitecture()
        self.phase_b = PhaseBImplementation(self.hybrid_flow.success_criteria)
        self.phase_c = PhaseCIntegration(self.hybrid_flow.success_criteria)
        self.success_monitor = SuccessRateMonitor()

        # æˆåŠŸç‡ç›£è¦–é–‹å§‹
        self.success_monitor.start_monitoring(check_interval_seconds=300)

        # Issue #870: é«˜åº¦é–‹ç™ºã‚·ã‚¹ãƒ†ãƒ çµ±åˆ
        self.dependency_analyzer = DependencyAnalyzer()
        self.multi_file_coordinator = MultiFileCoordinator()
        self.pattern_engine = PatternImplementationEngine()
        self.refactoring_engine = RefactoringEngine()
        self.performance_optimizer = PerformanceOptimizer()

        print(f"ğŸ¤– Dual-Agent Workflow é–‹å§‹ (Flash 2.5 æœ€é©åŒ–)")
        print(f"ğŸ“‹ ã‚»ãƒƒã‚·ãƒ§ãƒ³ID: {self.session_id}")
        print(f"ğŸ”„ Claude Code â†” Gemini CLI å”æ¥­ã‚·ã‚¹ãƒ†ãƒ ")
        print(f"ğŸ§  ã‚¿ã‚¹ã‚¯å¾®åˆ†åŒ–ã‚¨ãƒ³ã‚¸ãƒ³: æœ‰åŠ¹")
        print(f"ğŸ“ Flash 2.5 ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ: æœ‰åŠ¹")
        print(f"ğŸ¯ è‡ªå‹•åˆ¤å®šã‚¨ãƒ³ã‚¸ãƒ³: æœ‰åŠ¹")
        print(f"ğŸ” çµ±åˆå“è³ªç®¡ç†: æœ‰åŠ¹")
        print(f"ğŸ“Š å“è³ªç›£è¦–ã‚·ã‚¹ãƒ†ãƒ : æœ‰åŠ¹")
        print(f"ğŸ›¡ï¸ æ§‹æ–‡ã‚¨ãƒ©ãƒ¼é˜²æ­¢æ©Ÿæ§‹: æœ‰åŠ¹")
        print(f"âœ… å“è³ªä¿è¨¼ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ: æœ‰åŠ¹")
        print(f"ğŸ—ï¸ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å®Ÿè£…ãƒ•ãƒ­ãƒ¼: æœ‰åŠ¹ (Issue #844)")
        print(f"ğŸ“Š æˆåŠŸç‡ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ : æœ‰åŠ¹ (ç›®æ¨™: å®Ÿè£…â‰¥70%, Tokenâ‰¥90%, å“è³ªâ‰¥80%, çµ±åˆâ‰¥95%)")
        print(f"âš¡ é«˜åº¦é–‹ç™ºã‚·ã‚¹ãƒ†ãƒ : 5ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«çµ±åˆå®Œäº† (Issue #870)")

    def create_mypy_fix_task(self,
                           target_files: List[str],
                           error_type: str = "no-untyped-def",
                           priority: str = "high",
                           use_micro_tasks: bool = True,
                           force_mode: str = None,
                           auto_execute: bool = True) -> List[str]:
        """mypyä¿®æ­£ã‚¿ã‚¹ã‚¯ä½œæˆï¼ˆè‡ªå‹•åˆ¤å®šãƒ»å®Ÿè¡Œæ©Ÿèƒ½ä»˜ãï¼‰"""
        return self._create_task_with_type(
            "code_modification", target_files, error_type, priority,
            use_micro_tasks, force_mode, auto_execute
        )

    def create_new_implementation_task(self,
                                     target_files: List[str],
                                     implementation_spec: Dict[str, Any],
                                     priority: str = "high",
                                     force_mode: str = None,
                                     auto_execute: bool = True) -> List[str]:
        """æ–°è¦å®Ÿè£…ã‚¿ã‚¹ã‚¯ä½œæˆ"""
        return self._create_implementation_task(
            "new_implementation", target_files, implementation_spec,
            priority, force_mode, auto_execute
        )

    def create_hybrid_implementation_task(self,
                                        target_files: List[str],
                                        implementation_spec: Dict[str, Any],
                                        priority: str = "high",
                                        force_mode: str = None,
                                        auto_execute: bool = True) -> List[str]:
        """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å®Ÿè£…ã‚¿ã‚¹ã‚¯ä½œæˆ"""
        return self._create_implementation_task(
            "hybrid_implementation", target_files, implementation_spec,
            priority, force_mode, auto_execute
        )

    def create_feature_development_task(self,
                                      target_files: List[str],
                                      feature_spec: Dict[str, Any],
                                      priority: str = "high",
                                      force_mode: str = None,
                                      auto_execute: bool = True) -> List[str]:
        """æ–°æ©Ÿèƒ½é–‹ç™ºã‚¿ã‚¹ã‚¯ä½œæˆ"""
        return self._create_implementation_task(
            "new_feature_development", target_files, feature_spec,
            priority, force_mode, auto_execute
        )

    # === Issue #844: 3æ®µéšãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å®Ÿè£…ãƒ•ãƒ­ãƒ¼ ===

    def execute_hybrid_implementation_flow(self,
                                         target_files: List[str],
                                         implementation_spec: Dict[str, Any],
                                         auto_execute: bool = True) -> str:
        """Issue #844: 3æ®µéšãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å®Ÿè£…ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œ

        Args:
            target_files: å®Ÿè£…å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆ
            implementation_spec: å®Ÿè£…ä»•æ§˜
            auto_execute: è‡ªå‹•å®Ÿè¡Œãƒ•ãƒ©ã‚°

        Returns:
            str: å®Ÿè£…ID
        """

        print(f"ğŸ—ï¸ 3æ®µéšãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å®Ÿè£…ãƒ•ãƒ­ãƒ¼é–‹å§‹")
        print(f"ğŸ“ å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {len(target_files)}ä»¶")

        # å®Ÿè£…ä»•æ§˜ä½œæˆ
        hybrid_spec = HybridImplementationSpec(
            implementation_id=f"hybrid_{self.session_id}_{int(time.time())}",
            implementation_type=implementation_spec.get("type", "hybrid_implementation"),
            target_files=target_files,
            requirements=implementation_spec.get("requirements", {}),
            success_criteria=implementation_spec.get("success_criteria", {
                "quality_score": 0.80,
                "functionality": "complete",
                "integration_success": 0.95
            }),
            quality_standards=implementation_spec.get("quality_standards", {
                "code_quality": {"style": "pep8", "typing": "strict"},
                "test_coverage": 0.8,
                "documentation": {"required": True}
            }),
            context=implementation_spec.get("context", {
                "session_id": self.session_id,
                "coordinator": "dual_agent"
            })
        )

        # ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œ
        implementation_id = self.hybrid_flow.start_hybrid_implementation(
            hybrid_spec, auto_execute=auto_execute
        )

        print(f"âœ… 3æ®µéšãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å®Ÿè£…ãƒ•ãƒ­ãƒ¼é–‹å§‹å®Œäº†: {implementation_id}")
        return implementation_id

    def get_hybrid_implementation_status(self, implementation_id: str) -> Dict[str, Any]:
        """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å®Ÿè£…ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹å–å¾—

        Args:
            implementation_id: å®Ÿè£…ID

        Returns:
            Dict[str, Any]: å®Ÿè£…ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
        """

        return self.hybrid_flow.get_implementation_status(implementation_id)

    def execute_phase_a_only(self, target_files: List[str],
                           implementation_spec: Dict[str, Any]) -> Dict[str, Any]:
        """Phase A (ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆ) ã®ã¿å®Ÿè¡Œ

        Args:
            target_files: å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«
            implementation_spec: å®Ÿè£…ä»•æ§˜

        Returns:
            Dict[str, Any]: Phase Aå®Ÿè¡Œçµæœ
        """

        print(f"ğŸ§  Phase A (ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆ) å˜ç‹¬å®Ÿè¡Œé–‹å§‹")

        # Phase Aå®Ÿè¡Œ
        analysis = self.phase_a.execute_architecture_analysis(implementation_spec)
        interfaces = self.phase_a.design_component_interfaces(analysis, target_files)
        strategy = self.phase_a.create_implementation_strategy(analysis, interfaces, implementation_spec)
        quality_eval = self.phase_a.evaluate_architecture_quality(analysis, interfaces, strategy)
        handoff = self.phase_a.generate_phase_b_handoff(analysis, interfaces, strategy)

        result = {
            "phase": "A",
            "status": "completed" if quality_eval["phase_a_pass"] else "needs_review",
            "architecture_analysis": analysis.__dict__,
            "interface_design": {k: v.__dict__ for k, v in interfaces.items()},
            "implementation_strategy": strategy,
            "quality_evaluation": quality_eval,
            "phase_b_handoff": handoff
        }

        # æˆåŠŸç‡è¨˜éŒ²
        self.success_monitor.record_implementation_success(
            f"phase_a_{int(time.time())}", "phase_a", quality_eval["phase_a_pass"]
        )
        self.success_monitor.record_quality_score(
            f"phase_a_{int(time.time())}", "phase_a", quality_eval["overall_quality_score"]
        )

        print(f"âœ… Phase A å®Œäº†: {'æˆåŠŸ' if quality_eval['phase_a_pass'] else 'è¦æ”¹å–„'}")
        return result

    def get_success_rate_dashboard(self) -> Dict[str, Any]:
        """Issue #844æˆåŠŸç‡ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿å–å¾—

        Returns:
            Dict[str, Any]: ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿
        """

        dashboard_data = self.success_monitor.get_metrics_dashboard_data()
        current_rates = self.success_monitor.get_current_success_rates()

        # è¿½åŠ ã®çµ±åˆæƒ…å ±
        dashboard_data["session_info"] = {
            "session_id": self.session_id,
            "coordinator_type": "dual_agent",
            "hybrid_flow_enabled": True,
            "monitoring_active": True
        }

        dashboard_data["issue_844_compliance"] = {
            "overall_success": current_rates["overall_success"],
            "criteria_summary": current_rates["summary"],
            "targets_met": {
                "implementation_success": current_rates["individual_metrics"]["implementation_success"]["meets_target"],
                "token_savings": current_rates["individual_metrics"]["token_savings"]["meets_target"],
                "quality_score": current_rates["individual_metrics"]["quality_score"]["meets_target"],
                "integration_success": current_rates["individual_metrics"]["integration_success"]["meets_target"]
            }
        }

        return dashboard_data

    def generate_hybrid_flow_report(self) -> Dict[str, Any]:
        """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ•ãƒ­ãƒ¼çµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ

        Returns:
            Dict[str, Any]: çµ±åˆãƒ¬ãƒãƒ¼ãƒˆ
        """

        print(f"ğŸ“Š ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ•ãƒ­ãƒ¼çµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­...")

        # å„ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿åé›†
        success_metrics = self.success_monitor.get_success_rate_metrics()
        current_rates = self.success_monitor.get_current_success_rates()

        report = {
            "report_timestamp": datetime.datetime.now().isoformat(),
            "session_id": self.session_id,
            "system_status": {
                "hybrid_flow_active": True,
                "success_monitoring_active": self.success_monitor.monitoring_active,
                "phase_systems_loaded": {
                    "phase_a": bool(self.phase_a),
                    "phase_b": bool(self.phase_b),
                    "phase_c": bool(self.phase_c)
                }
            },
            "issue_844_compliance": {
                "overall_criteria_met": current_rates["overall_success"],
                "individual_targets": {
                    "implementation_success_rate": {
                        "current": current_rates["summary"]["implementation_success"],
                        "target": 0.70,
                        "met": current_rates["summary"]["implementation_success"] >= 0.70
                    },
                    "token_savings_rate": {
                        "current": current_rates["summary"]["token_savings"],
                        "target": 0.90,
                        "met": current_rates["summary"]["token_savings"] >= 0.90
                    },
                    "quality_score": {
                        "current": current_rates["summary"]["quality_score"],
                        "target": 0.80,
                        "met": current_rates["summary"]["quality_score"] >= 0.80
                    },
                    "integration_success_rate": {
                        "current": current_rates["summary"]["integration_success"],
                        "target": 0.95,
                        "met": current_rates["summary"]["integration_success"] >= 0.95
                    }
                },
                "compliance_percentage": sum(
                    1 for metric in current_rates["individual_metrics"].values()
                    if metric["meets_target"]
                ) / len(current_rates["individual_metrics"]) * 100
            },
            "performance_metrics": success_metrics,
            "recommendations": self._generate_integration_recommendations(current_rates),
            "next_actions": self._generate_next_actions(current_rates)
        }

        print(f"ğŸ“Š çµ±åˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†")
        print(f"ğŸ¯ Issue #844åŸºæº–é”æˆ: {'âœ…' if report['issue_844_compliance']['overall_criteria_met'] else 'âŒ'}")
        print(f"ğŸ“ˆ é”æˆç‡: {report['issue_844_compliance']['compliance_percentage']:.1f}%")

        return report

    def _generate_integration_recommendations(self, current_rates: Dict[str, Any]) -> List[str]:
        """çµ±åˆæ¨å¥¨äº‹é …ç”Ÿæˆ"""

        recommendations = []
        individual_metrics = current_rates["individual_metrics"]

        for metric_name, metric_data in individual_metrics.items():
            if not metric_data["meets_target"]:
                current = metric_data["current_rate"]
                target = metric_data["target_rate"]
                deficit = target - current

                if metric_name == "implementation_success":
                    recommendations.append(f"å®Ÿè£…æˆåŠŸç‡æ”¹å–„ãŒå¿…è¦: {current:.1%} â†’ {target:.1%} (+{deficit:.1%})")
                    recommendations.append("- Phase Aè¨­è¨ˆå“è³ªã®å‘ä¸Š")
                    recommendations.append("- GeminiæŒ‡ç¤ºã®å…·ä½“æ€§å¼·åŒ–")
                elif metric_name == "token_savings":
                    recommendations.append(f"Tokenç¯€ç´„ç‡æ”¹å–„ãŒå¿…è¦: {current:.1%} â†’ {target:.1%} (+{deficit:.1%})")
                    recommendations.append("- ã‚¿ã‚¹ã‚¯åˆ†å‰²ã®æœ€é©åŒ–")
                    recommendations.append("- ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆåŠ¹ç‡åŒ–")
                elif metric_name == "quality_score":
                    recommendations.append(f"å“è³ªã‚¹ã‚³ã‚¢æ”¹å–„ãŒå¿…è¦: {current:.2f} â†’ {target:.2f} (+{deficit:.2f})")
                    recommendations.append("- ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹å¼·åŒ–")
                    recommendations.append("- è‡ªå‹•å“è³ªãƒã‚§ãƒƒã‚¯æ‹¡å……")
                elif metric_name == "integration_success":
                    recommendations.append(f"çµ±åˆæˆåŠŸç‡æ”¹å–„ãŒå¿…è¦: {current:.1%} â†’ {target:.1%} (+{deficit:.1%})")
                    recommendations.append("- çµ±åˆãƒ†ã‚¹ãƒˆã®å……å®Ÿ")
                    recommendations.append("- ä¾å­˜é–¢ä¿‚ç®¡ç†æ”¹å–„")

        if not recommendations:
            recommendations.append("å…¨ã¦ã®æˆåŠŸåŸºæº–ã‚’é”æˆã—ã¦ã„ã¾ã™ã€‚ç¾åœ¨ã®å“è³ªã‚’ç¶­æŒã—ã¦ãã ã•ã„ã€‚")

        return recommendations

    def _generate_next_actions(self, current_rates: Dict[str, Any]) -> List[str]:
        """æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç”Ÿæˆ"""

        actions = []

        if current_rates["overall_success"]:
            actions.extend([
                "Issue #844åŸºæº–é”æˆã‚’ç¶­æŒ",
                "ç¶™ç¶šçš„ãªå“è³ªç›£è¦–",
                "ã‚·ã‚¹ãƒ†ãƒ æœ€é©åŒ–ã®ç¶™ç¶šå®Ÿæ–½"
            ])
        else:
            actions.extend([
                "æœªé”æˆåŸºæº–ã®é‡ç‚¹æ”¹å–„",
                "æ”¹å–„è¨ˆç”»ã®ç­–å®šãƒ»å®Ÿè¡Œ",
                "ç›£è¦–é »åº¦ã®å¢—åŠ "
            ])

        return actions

    def cleanup_hybrid_systems(self) -> None:
        """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚·ã‚¹ãƒ†ãƒ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""

        print("ğŸ§¹ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚·ã‚¹ãƒ†ãƒ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—é–‹å§‹")

        # ç›£è¦–åœæ­¢
        if hasattr(self, 'success_monitor'):
            self.success_monitor.stop_monitoring()
            print("ğŸ“Š æˆåŠŸç‡ç›£è¦–åœæ­¢")

        print("âœ… ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚·ã‚¹ãƒ†ãƒ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†")

    def _create_task_with_type(self,
                             task_type: str,
                             target_files: List[str],
                             error_type: str,
                             priority: str,
                             use_micro_tasks: bool,
                             force_mode: str,
                             auto_execute: bool) -> List[str]:
        """ã‚¿ã‚¹ã‚¯ä½œæˆï¼ˆã‚¿ã‚¤ãƒ—åˆ¥çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼‰"""

        task_description = f"{error_type} ã‚¨ãƒ©ãƒ¼ä¿®æ­£ - {len(target_files)}ãƒ•ã‚¡ã‚¤ãƒ«" if task_type == "code_modification" else f"{task_type} - {len(target_files)}ãƒ•ã‚¡ã‚¤ãƒ«"

        print(f"ğŸ” ã‚¿ã‚¹ã‚¯ä½œæˆé–‹å§‹: {task_description}")
        print(f"ğŸ“ å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {len(target_files)}ä»¶")
        print(f"ğŸ§  å¾®åˆ†åŒ–ãƒ¢ãƒ¼ãƒ‰: {'æœ‰åŠ¹' if use_micro_tasks else 'ç„¡åŠ¹'}")

        # ===== è‡ªå‹•åˆ¤å®šãƒ•ã‚§ãƒ¼ã‚º =====

        # ã‚¿ã‚¹ã‚¯åˆ†æå®Ÿè¡Œ
        task_analysis = self.decision_engine.analyze_task(
            task_description, target_files, error_type,
            context={"priority": priority, "session_id": self.session_id, "task_type": task_type}
        )

        # Geminiä½¿ç”¨åˆ¤å®š
        user_prefs = {"force_mode": force_mode} if force_mode else {}
        decision = self.decision_engine.make_decision(task_analysis, user_prefs)

        print(f"\nğŸ¯ è‡ªå‹•åˆ¤å®šçµæœ:")
        print(f"   Geminiä½¿ç”¨: {'ã¯ã„' if decision.use_gemini else 'ã„ã„ãˆ'}")
        print(f"   è‡ªå‹•åŒ–ãƒ¬ãƒ™ãƒ«: {decision.automation_level.value}")
        print(f"   æ¨å®šã‚³ã‚¹ãƒˆ: ${decision.task_analysis.estimated_cost:.4f}")
        print(f"   åŠ¹æœã‚¹ã‚³ã‚¢: {decision.task_analysis.gemini_benefit_score:.2f}")
        print(f"   ç†ç”±: {decision.reasoning}")

        # ===== å®Ÿè¡Œæ–¹å¼æ±ºå®š =====
        if decision.use_gemini:
            print(f"\nğŸš€ Geminiå”æ¥­ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œ")
            created_task_ids = self._create_with_gemini_mode(
                target_files, error_type, priority, use_micro_tasks, decision, task_type
            )
        else:
            print(f"\nğŸ§  Claudeå˜ç‹¬ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œ")
            created_task_ids = self._create_with_claude_mode(
                target_files, error_type, priority, decision, task_type
            )

        # ===== è‡ªå‹•å®Ÿè¡Œåˆ¤å®š =====
        if auto_execute and decision.automation_level in [AutomationLevel.FULL_AUTO, AutomationLevel.SEMI_AUTO]:
            print(f"\nâš¡ è‡ªå‹•å®Ÿè¡Œé–‹å§‹ï¼ˆãƒ¬ãƒ™ãƒ«: {decision.automation_level.value}ï¼‰")

            if decision.automation_level == AutomationLevel.SEMI_AUTO:
                # é‡è¦ãªå¤‰æ›´ã®å ´åˆã®ã¿ç¢ºèª
                if decision.task_analysis.risk_level == "high" or decision.task_analysis.estimated_cost > 0.005:
                    print("âš ï¸ é‡è¦ãªå¤‰æ›´ã®ãŸã‚æ‰‹å‹•ç¢ºèªæ¨å¥¨")
                else:
                    self._auto_execute_tasks(created_task_ids, decision)
            else:
                self._auto_execute_tasks(created_task_ids, decision)

        elif decision.automation_level == AutomationLevel.APPROVAL_REQUIRED:
            print(f"\nğŸ¤š æ‰¿èªå¿…é ˆ: å®Ÿè¡Œå‰ã«ãƒ¦ãƒ¼ã‚¶ãƒ¼ç¢ºèªãŒå¿…è¦")
            print(f"   å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰: coordinator.execute_workflow_cycle()")

        print(f"\nâœ… ã‚¿ã‚¹ã‚¯ä½œæˆå®Œäº†: {len(created_task_ids)}ä»¶")
        return created_task_ids

    def _create_implementation_task(self,
                                  task_type: str,
                                  target_files: List[str],
                                  implementation_spec: Dict[str, Any],
                                  priority: str,
                                  force_mode: str,
                                  auto_execute: bool) -> List[str]:
        """æ–°è¦å®Ÿè£…ã‚¿ã‚¹ã‚¯ä½œæˆï¼ˆçµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼‰"""

        print(f"ğŸ¨ æ–°è¦å®Ÿè£…ã‚¿ã‚¹ã‚¯ä½œæˆé–‹å§‹: {task_type}")
        print(f"ğŸ“ å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {len(target_files)}ä»¶")

        try:
            # å…¥åŠ›æ¤œè¨¼
            if not target_files:
                print("âš ï¸ å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                target_files = [f"tmp/{task_type}_implementation.py"]

            if not implementation_spec:
                print("âš ï¸ å®Ÿè£…ä»•æ§˜ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆä»•æ§˜ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
                implementation_spec = {"template_type": "generic", "complexity": "medium"}

            # ã‚¿ã‚¹ã‚¯åˆ†æå®Ÿè¡Œ
            task_description = f"{task_type} - {len(target_files)}ãƒ•ã‚¡ã‚¤ãƒ«å®Ÿè£…"
            task_analysis = self.decision_engine.analyze_task(
                task_description, target_files, "new_implementation",
                context={
                    "priority": priority,
                    "session_id": self.session_id,
                    "task_type": task_type,
                    "implementation_spec": implementation_spec
                }
            )
        except Exception as e:
            print(f"âŒ ã‚¿ã‚¹ã‚¯åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            print("ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™...")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç°¡æ˜“çš„ãªã‚¿ã‚¹ã‚¯åˆ†æçµæœã‚’ä½œæˆ
            from postbox.core.workflow_decision_engine import TaskAnalysis, TaskComplexity
            task_analysis = TaskAnalysis(
                complexity=TaskComplexity.MODERATE,
                estimated_time=60,
                estimated_tokens=self._estimate_task_tokens(task_description, target_files),
                estimated_cost=0.005,
                risk_level="medium",
                gemini_benefit_score=0.7,
                automation_recommendation=AutomationLevel.SEMI_AUTO,
                confidence=0.5
            )

        try:
            # Geminiä½¿ç”¨åˆ¤å®š
            user_prefs = {"force_mode": force_mode} if force_mode else {}
            decision = self.decision_engine.make_decision(task_analysis, user_prefs)

            print(f"\nğŸ¯ è‡ªå‹•åˆ¤å®šçµæœ:")
            print(f"   Geminiä½¿ç”¨: {'ã¯ã„' if decision.use_gemini else 'ã„ã„ãˆ'}")
            print(f"   è‡ªå‹•åŒ–ãƒ¬ãƒ™ãƒ«: {decision.automation_level.value}")
            print(f"   æ¨å®šã‚³ã‚¹ãƒˆ: ${decision.task_analysis.estimated_cost:.4f}")
            print(f"   ç†ç”±: {decision.reasoning}")

        except Exception as e:
            print(f"âŒ Geminiåˆ¤å®šã‚¨ãƒ©ãƒ¼: {e}")
            print("ğŸ”„ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆåˆ¤å®šã‚’ä½¿ç”¨ã—ã¾ã™...")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆåˆ¤å®šçµæœã‚’ä½œæˆ
            from postbox.core.workflow_decision_engine import DecisionResult
            decision = DecisionResult(
                use_gemini=False,
                automation_level=AutomationLevel.SEMI_AUTO,
                task_analysis=task_analysis,
                reasoning="ã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚Šãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨",
                alternative_approaches=["æ‰‹å‹•å®Ÿè¡Œ"],
                cost_benefit_analysis={"estimated_cost": 0.005}
            )

        # å®Ÿè£…ã‚¿ã‚¹ã‚¯ä½œæˆ
        created_task_ids = []

        for file_path in target_files:
            try:
                print(f"\nğŸ“„ å®Ÿè£…ã‚¿ã‚¹ã‚¯ä½œæˆ: {file_path}")

                # å®Ÿè£…ã‚¿ã‚¹ã‚¯ä½œæˆ
                task_id = self._create_implementation_task_for_file(
                    file_path, task_type, implementation_spec, priority, decision
                )
                created_task_ids.append(task_id)

            except Exception as e:
                print(f"âŒ ã‚¿ã‚¹ã‚¯ä½œæˆã‚¨ãƒ©ãƒ¼ ({file_path}): {e}")
                print("ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ã§ã‚¿ã‚¹ã‚¯ã‚’ä½œæˆã—ã¾ã™...")
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç°¡æ˜“ã‚¿ã‚¹ã‚¯ä½œæˆ
                try:
                    fallback_task_id = self._create_fallback_implementation_task(
                        file_path, task_type, implementation_spec, priority
                    )
                    created_task_ids.append(fallback_task_id)
                    print(f"âœ… ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¿ã‚¹ã‚¯ä½œæˆæˆåŠŸ: {fallback_task_id}")
                except Exception as fallback_error:
                    print(f"âŒ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¿ã‚¹ã‚¯ä½œæˆã‚‚å¤±æ•—: {fallback_error}")
                    continue

        # è‡ªå‹•å®Ÿè¡Œåˆ¤å®š
        if auto_execute and decision.automation_level in [AutomationLevel.FULL_AUTO, AutomationLevel.SEMI_AUTO]:
            print(f"\nâš¡ è‡ªå‹•å®Ÿè¡Œé–‹å§‹ï¼ˆãƒ¬ãƒ™ãƒ«: {decision.automation_level.value}ï¼‰")

            if decision.automation_level == AutomationLevel.SEMI_AUTO:
                # é‡è¦ãªå¤‰æ›´ã®å ´åˆã®ã¿ç¢ºèª
                if decision.task_analysis.risk_level == "high" or decision.task_analysis.estimated_cost > 0.005:
                    print("âš ï¸ é‡è¦ãªå¤‰æ›´ã®ãŸã‚æ‰‹å‹•ç¢ºèªæ¨å¥¨")
                else:
                    self._auto_execute_tasks(created_task_ids, decision)
            else:
                self._auto_execute_tasks(created_task_ids, decision)
        elif decision.automation_level == AutomationLevel.APPROVAL_REQUIRED:
            print(f"\nğŸ¤š æ‰¿èªå¿…é ˆ: å®Ÿè¡Œå‰ã«ãƒ¦ãƒ¼ã‚¶ãƒ¼ç¢ºèªãŒå¿…è¦")
            print(f"   å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰: coordinator.execute_workflow_cycle()")

        print(f"\nâœ… å®Ÿè£…ã‚¿ã‚¹ã‚¯ä½œæˆå®Œäº†: {len(created_task_ids)}ä»¶")
        return created_task_ids

    def _create_implementation_task_for_file(self,
                                           file_path: str,
                                           task_type: str,
                                           implementation_spec: Dict[str, Any],
                                           priority: str,
                                           decision) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«å˜ä½ã®å®Ÿè£…ã‚¿ã‚¹ã‚¯ä½œæˆ"""

        # Claudeã«ã‚ˆã‚‹è©³ç´°åˆ†æ
        claude_analysis = self._claude_analyze_implementation_task(
            file_path, task_type, implementation_spec
        )

        # å®Ÿè£…æŒ‡ç¤ºç”Ÿæˆ
        implementation_instruction = self._generate_implementation_instruction(
            file_path, task_type, implementation_spec
        )

        task_id = self.task_manager.create_task(
            task_type=task_type,
            description=f"{task_type} - {file_path}",
            target_files=[file_path],
            priority=priority,
            requirements={
                "implementation_spec": implementation_spec,
                "task_type": task_type,
                "implementation_instruction": implementation_instruction,
                "quality_requirements": {
                    "syntax_check": True,
                    "type_check": True,
                    "style_check": True,
                    "test_creation": implementation_spec.get("create_tests", False)
                }
            },
            claude_analysis=claude_analysis,
            expected_outcome=f"{task_type}å®Œäº† - {file_path}",
            constraints=[
                "å“è³ªåŸºæº–éµå®ˆ",
                "ãƒ†ã‚¹ãƒˆé€šéå¿…é ˆ",
                "mypy strict modeé©åˆ"
            ],
            context={
                "task_type": task_type,
                "implementation_type": implementation_spec.get("template_type", "generic"),
                "decision_engine_result": {
                    "complexity": decision.task_analysis.complexity.value,
                    "estimated_time": decision.task_analysis.estimated_time,
                    "estimated_tokens": decision.task_analysis.estimated_tokens,
                    "estimated_cost": decision.task_analysis.estimated_cost,
                    "risk_level": decision.task_analysis.risk_level,
                    "gemini_benefit_score": decision.task_analysis.gemini_benefit_score,
                    "automation_recommendation": decision.task_analysis.automation_recommendation.value,
                    "confidence": decision.task_analysis.confidence
                },
                "automation_level": decision.automation_level.value,
                "session_id": self.session_id
            }
        )

        print(f"ğŸ“„ å®Ÿè£…ã‚¿ã‚¹ã‚¯ä½œæˆ: {file_path}")
        return task_id

    def _claude_analyze_implementation_task(self,
                                          file_path: str,
                                          task_type: str,
                                          implementation_spec: Dict[str, Any]) -> str:
        """å®Ÿè£…ã‚¿ã‚¹ã‚¯ç”¨Claudeåˆ†æ"""

        template_type = implementation_spec.get("template_type", "generic")
        complexity_estimate = implementation_spec.get("complexity", "medium")

        analysis = f"""
ğŸ“Š Claude å®Ÿè£…ã‚¿ã‚¹ã‚¯åˆ†æ - {task_type}

ğŸ¯ å®Ÿè£…å¯¾è±¡:
- ãƒ•ã‚¡ã‚¤ãƒ«: {file_path}
- ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—: {task_type}
- ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ: {template_type}
- è¤‡é›‘åº¦: {complexity_estimate}

ğŸ§  å®Ÿè£…æ–¹é‡:
"""

        if task_type == "new_implementation":
            analysis += """
- ç´”ç²‹æ–°è¦å®Ÿè£…ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
- ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ™ãƒ¼ã‚¹ã®ã‚³ãƒ¼ãƒ‰ç”Ÿæˆ
- å“è³ªåŸºæº–é©åˆã‚’å¿…é ˆã¨ã—ãŸå®Ÿè£…
"""
        elif task_type == "hybrid_implementation":
            analysis += """
- æ—¢å­˜ã‚³ãƒ¼ãƒ‰ã¨ã®çµ±åˆã‚’è€ƒæ…®
- æ®µéšçš„ãªå®Ÿè£…ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
- æ—¢å­˜æ©Ÿèƒ½ã¸ã®å½±éŸ¿æœ€å°åŒ–
"""
        elif task_type == "new_feature_development":
            analysis += """
- åŒ…æ‹¬çš„ãªæ©Ÿèƒ½é–‹ç™ºã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
- é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã®æ•´åˆæ€§ç¢ºä¿
- ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’å«ã‚€å®Œå…¨å®Ÿè£…
"""

        # å®Ÿè£…ä»•æ§˜ã®è©³ç´°
        if "class_name" in implementation_spec:
            analysis += f"\nğŸ·ï¸ ã‚¯ãƒ©ã‚¹å®Ÿè£…: {implementation_spec['class_name']}"
        if "methods" in implementation_spec:
            analysis += f"\nğŸ”§ ãƒ¡ã‚½ãƒƒãƒ‰æ•°: {len(implementation_spec['methods'])}ä»¶"
        if "functions" in implementation_spec:
            analysis += f"\nâš™ï¸ é–¢æ•°æ•°: {len(implementation_spec['functions'])}ä»¶"

        analysis += f"""

ğŸ›¡ï¸ å“è³ªä¿è¨¼:
- mypy strict modeé©åˆå¿…é ˆ
- ã‚³ãƒ¼ãƒ‰ã‚¹ã‚¿ã‚¤ãƒ«ã‚¬ã‚¤ãƒ‰éµå®ˆ
- å¿…è¦ã«å¿œã˜ã¦ãƒ†ã‚¹ãƒˆä½œæˆ
- ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚³ãƒ¡ãƒ³ãƒˆå¿…é ˆ

ğŸ“ æ¨å¥¨ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ:
1. ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ™ãƒ¼ã‚¹ã§ã®åŸºæœ¬æ§‹é€ ä½œæˆ
2. æ®µéšçš„ãªå®Ÿè£…ã¨ç¢ºèª
3. å“è³ªãƒã‚§ãƒƒã‚¯ã¨ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
4. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨ã‚³ãƒ¡ãƒ³ãƒˆã®å……å®Ÿ
"""

        return analysis

    def _generate_implementation_instruction(self,
                                           file_path: str,
                                           task_type: str,
                                           implementation_spec: Dict[str, Any]) -> str:
        """å®Ÿè£…æŒ‡ç¤ºç”Ÿæˆï¼ˆå¼·åŒ–ç‰ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‡¦ç†ï¼‰"""

        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¤œè¨¼ãƒ»æ­£è¦åŒ–
        validated_spec = self._validate_and_sanitize_implementation_spec(implementation_spec)
        template_type = validated_spec["template_type"]

        instruction = f"""
ğŸ¯ {task_type}å®Ÿè£…æŒ‡ç¤º - {file_path}

ğŸ“„ åŸºæœ¬æƒ…å ±:
- ãƒ•ã‚¡ã‚¤ãƒ«: {file_path}
- ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—: {task_type}
- ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ: {template_type}
- è¤‡é›‘åº¦: {validated_spec.get("complexity", "medium")}

ğŸ”§ å®Ÿè£…æ‰‹é †:
"""

        if template_type == "class":
            class_info = self._process_class_parameters(validated_spec)
            instruction += f"""
1. ã‚¯ãƒ©ã‚¹ {class_info['name']} ã®å®šç¾©ä½œæˆ
   - ç¶™æ‰¿: {class_info['inheritance']}
   - å‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {class_info['type_params']}
2. __init__ ãƒ¡ã‚½ãƒƒãƒ‰ã®å®Ÿè£…
   - å¼•æ•°: {class_info['init_params']}
   - å‹æ³¨é‡ˆå®Œå…¨å¯¾å¿œ
"""
            for i, method_info in enumerate(class_info['methods'], 3):
                instruction += f"{i}. {method_info['name']} ãƒ¡ã‚½ãƒƒãƒ‰ã®å®Ÿè£…\n"
                instruction += f"   - å¼•æ•°: {method_info['params']}\n"
                instruction += f"   - æˆ»ã‚Šå€¤: {method_info['return_type']}\n"
                if method_info.get('description'):
                    instruction += f"   - ç›®çš„: {method_info['description']}\n"

        elif template_type == "module":
            module_info = self._process_module_parameters(validated_spec)
            instruction += "1. ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ¬ãƒ™ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆè¨­å®š\n"
            instruction += f"   - å¿…é ˆã‚¤ãƒ³ãƒãƒ¼ãƒˆ: {module_info['required_imports']}\n"
            for i, func_info in enumerate(module_info['functions'], 2):
                instruction += f"{i}. {func_info['name']} é–¢æ•°ã®å®Ÿè£…\n"
                instruction += f"   - å¼•æ•°: {func_info['params']}\n"
                instruction += f"   - æˆ»ã‚Šå€¤: {func_info['return_type']}\n"
                if func_info.get('description'):
                    instruction += f"   - ç›®çš„: {func_info['description']}\n"

        elif template_type == "function":
            func_info = self._process_function_parameters(validated_spec)
            instruction += f"""
1. {func_info['name']} é–¢æ•°ã®å®šç¾©ä½œæˆ
   - å¼•æ•°: {func_info['params']}
   - æˆ»ã‚Šå€¤: {func_info['return_type']}
   - ç›®çš„: {func_info.get('description', 'ä¸»è¦æ©Ÿèƒ½ã®å®Ÿè£…')}
2. ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œéƒ¨ã®è¨­å®š
   - ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¿…é ˆ
   - ãƒ­ã‚°å‡ºåŠ›å¯¾å¿œ
"""

        # å“è³ªè¦ä»¶ã®è©³ç´°åŒ–
        quality_requirements = self._generate_quality_requirements(validated_spec, task_type)
        instruction += f"""

ğŸ“ å¿…é ˆè¦ä»¶:
{quality_requirements['basic_requirements']}

ğŸ› ï¸ å“è³ªãƒã‚§ãƒƒã‚¯:
{quality_requirements['quality_checks']}

ğŸ¯ æˆåŠŸåŸºæº–:
{quality_requirements['success_criteria']}

ğŸ” æ¤œè¨¼æ‰‹é †:
{quality_requirements['verification_steps']}
"""

        return instruction

    def _validate_and_sanitize_implementation_spec(self, implementation_spec: Dict[str, Any]) -> Dict[str, Any]:
        """å®Ÿè£…ä»•æ§˜ã®æ¤œè¨¼ãƒ»æ­£è¦åŒ–ï¼ˆå¼·åŒ–ç‰ˆï¼‰"""

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤è¨­å®š
        validated_spec = {
            "template_type": "generic",
            "complexity": "medium",
            "class_name": "NewClass",
            "function_name": "main_function",
            "methods": [],
            "functions": [],
            "create_tests": False,
            "documentation_level": "standard"
        }

        # å…¥åŠ›å€¤ã®æ¤œè¨¼ãƒ»ãƒãƒ¼ã‚¸
        if isinstance(implementation_spec, dict):
            for key, value in implementation_spec.items():
                if key == "template_type" and value in ["class", "module", "function", "generic"]:
                    validated_spec[key] = value
                elif key == "complexity" and value in ["simple", "medium", "complex"]:
                    validated_spec[key] = value
                elif key in ["class_name", "function_name"] and isinstance(value, str) and value.strip():
                    validated_spec[key] = value.strip()
                elif key in ["methods", "functions"] and isinstance(value, list):
                    validated_spec[key] = self._sanitize_method_function_list(value)
                elif key in ["create_tests", "documentation_level"]:
                    validated_spec[key] = value

        return validated_spec

    def _sanitize_method_function_list(self, items: List[Any]) -> List[Dict[str, Any]]:
        """ãƒ¡ã‚½ãƒƒãƒ‰ãƒ»é–¢æ•°ãƒªã‚¹ãƒˆã®æ­£è¦åŒ–"""

        sanitized_items = []

        for i, item in enumerate(items):
            if isinstance(item, dict):
                # è¾æ›¸å‹ã®å ´åˆã€å¿…è¦ãªã‚­ãƒ¼ã‚’ç¢ºèªãƒ»è£œå®Œ
                sanitized_item = {
                    "name": item.get("name", f"method_{i+1}" if "method" in str(type(item)).lower() else f"function_{i+1}"),
                    "params": item.get("params", "self" if "method" in str(type(item)).lower() else ""),
                    "return_type": item.get("return_type", "None"),
                    "description": item.get("description", "")
                }
            elif isinstance(item, str) and item.strip():
                # æ–‡å­—åˆ—å‹ã®å ´åˆã€åŸºæœ¬æƒ…å ±ã‚’è¨­å®š
                sanitized_item = {
                    "name": item.strip(),
                    "params": "self" if "method" in str(type(item)).lower() else "",
                    "return_type": "None",
                    "description": ""
                }
            else:
                # ç„¡åŠ¹ãªå‹ã®å ´åˆã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
                sanitized_item = {
                    "name": f"item_{i+1}",
                    "params": "",
                    "return_type": "None",
                    "description": ""
                }

            sanitized_items.append(sanitized_item)

        return sanitized_items

    def _process_class_parameters(self, validated_spec: Dict[str, Any]) -> Dict[str, Any]:
        """ã‚¯ãƒ©ã‚¹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‡¦ç†"""

        class_name = validated_spec["class_name"]
        methods = validated_spec["methods"]

        # ç¶™æ‰¿æƒ…å ±ã®å‡¦ç†
        inheritance = validated_spec.get("inheritance", "")
        if inheritance and not inheritance.startswith("("):
            inheritance = f"({inheritance})"

        # å‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å‡¦ç†
        type_params = validated_spec.get("type_params", "")

        # __init__ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å‡¦ç†
        init_params = validated_spec.get("init_params", "self")
        if not init_params.startswith("self"):
            init_params = f"self, {init_params}"

        return {
            "name": class_name,
            "inheritance": inheritance or "é©ç”¨ãªã—",
            "type_params": type_params or "é©ç”¨ãªã—",
            "init_params": init_params,
            "methods": methods
        }

    def _process_module_parameters(self, validated_spec: Dict[str, Any]) -> Dict[str, Any]:
        """ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‡¦ç†"""

        functions = validated_spec["functions"]

        # å¿…é ˆã‚¤ãƒ³ãƒãƒ¼ãƒˆã®å‡¦ç†
        required_imports = [
            "from typing import Any, Dict, List, Optional",
            "from kumihan_formatter.core.utilities.logger import get_logger"
        ]

        # è¿½åŠ ã‚¤ãƒ³ãƒãƒ¼ãƒˆã®å‡¦ç†
        additional_imports = validated_spec.get("additional_imports", [])
        if isinstance(additional_imports, list):
            required_imports.extend(additional_imports)

        return {
            "functions": functions,
            "required_imports": required_imports
        }

    def _process_function_parameters(self, validated_spec: Dict[str, Any]) -> Dict[str, Any]:
        """é–¢æ•°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‡¦ç†"""

        func_name = validated_spec["function_name"]

        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å‡¦ç†
        params = validated_spec.get("params", "")
        return_type = validated_spec.get("return_type", "None")
        description = validated_spec.get("description", "ä¸»è¦æ©Ÿèƒ½ã®å®Ÿè£…")

        return {
            "name": func_name,
            "params": params or "é©ç”¨ãªã—",
            "return_type": return_type,
            "description": description
        }

    def _generate_quality_requirements(self, validated_spec: Dict[str, Any], task_type: str) -> Dict[str, str]:
        """å“è³ªè¦ä»¶ç”Ÿæˆ"""

        complexity = validated_spec["complexity"]
        create_tests = validated_spec["create_tests"]
        doc_level = validated_spec["documentation_level"]

        # åŸºæœ¬è¦ä»¶
        basic_requirements = """- ã™ã¹ã¦ã®é–¢æ•°ãƒ»ãƒ¡ã‚½ãƒƒãƒ‰ã«å‹æ³¨é‡ˆå¿…é ˆï¼ˆmypy strict modeå¯¾å¿œï¼‰
- docstring ã§ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå¿…é ˆï¼ˆGoogle Styleæ¨å¥¨ï¼‰
- from typing import ã®é©åˆ‡ãªã‚¤ãƒ³ãƒãƒ¼ãƒˆ
- ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨ãƒ­ã‚°å‡ºåŠ›ã®å®Ÿè£…
- ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¨™æº–ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¦ç´„éµå®ˆ"""

        # è¤‡é›‘åº¦åˆ¥è¦ä»¶è¿½åŠ 
        if complexity == "complex":
            basic_requirements += "\n- è©³ç´°ãªã‚³ãƒ¡ãƒ³ãƒˆã«ã‚ˆã‚‹å®Ÿè£…æ„å›³ã®èª¬æ˜\n- è¤‡é›‘ãªå‡¦ç†ã®åˆ†å‰²ãƒ»ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–"

        # å“è³ªãƒã‚§ãƒƒã‚¯
        quality_checks = """1. å®Ÿè£…å®Œäº†å¾Œã« Python æ§‹æ–‡ãƒã‚§ãƒƒã‚¯ï¼ˆpy_compileï¼‰
2. mypy strict mode ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ
3. Black + isort ã«ã‚ˆã‚‹ã‚³ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãƒã‚§ãƒƒã‚¯
4. flake8 ã«ã‚ˆã‚‹ãƒªãƒ³ãƒˆãƒã‚§ãƒƒã‚¯
5. ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒã‚§ãƒƒã‚¯ï¼ˆåŸºæœ¬ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰"""

        if create_tests:
            quality_checks += "\n6. pytest ã«ã‚ˆã‚‹å˜ä½“ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"

        # æˆåŠŸåŸºæº–
        success_criteria = """âœ… ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£å¸¸ã«ä½œæˆã•ã‚Œã‚‹
âœ… å…¨ã¦ã®é–¢æ•°ãƒ»ãƒ¡ã‚½ãƒƒãƒ‰ã«é©åˆ‡ãªå‹æ³¨é‡ˆ
âœ… Python æ§‹æ–‡ã‚¨ãƒ©ãƒ¼ 0ä»¶
âœ… mypy strict mode ã‚¨ãƒ©ãƒ¼ 0ä»¶
âœ… é©åˆ‡ãª docstring ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
âœ… ã‚³ãƒ¼ãƒ‰ã‚¹ã‚¿ã‚¤ãƒ«ã‚¬ã‚¤ãƒ‰æº–æ‹ ï¼ˆBlack + isortï¼‰
âœ… ãƒªãƒ³ãƒˆãƒã‚§ãƒƒã‚¯åˆæ ¼ï¼ˆflake8ï¼‰"""

        if create_tests:
            success_criteria += "\nâœ… ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ä½œæˆãƒ»å®Ÿè¡ŒæˆåŠŸ"

        # æ¤œè¨¼æ‰‹é †
        verification_steps = """1. æ§‹æ–‡æ¤œè¨¼ï¼ˆLayer 1ï¼‰- ASTè§£æãƒ»åŸºæœ¬å‹ãƒã‚§ãƒƒã‚¯
2. å“è³ªæ¤œè¨¼ï¼ˆLayer 2ï¼‰- ãƒªãƒ³ãƒˆãƒ»ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãƒ»ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£
3. Claudeæœ€çµ‚æ‰¿èªï¼ˆLayer 3ï¼‰- ç·åˆå“è³ªè©•ä¾¡ãƒ»å®Ÿè£…æ–¹é‡ç¢ºèª
4. 3å±¤æ¤œè¨¼å®Œå…¨é€šéã®ç¢ºèª"""

        return {
            "basic_requirements": basic_requirements,
            "quality_checks": quality_checks,
            "success_criteria": success_criteria,
            "verification_steps": verification_steps
        }

    def _create_with_gemini_mode(self, target_files: List[str], error_type: str,
                                priority: str, use_micro_tasks: bool, decision, task_type: str = "code_modification") -> List[str]:
        """Geminiå”æ¥­ãƒ¢ãƒ¼ãƒ‰ã§ã®ã‚¿ã‚¹ã‚¯ä½œæˆ"""

        created_task_ids = []

        for file_path in target_files:
            print(f"\nğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æ: {file_path}")

            if use_micro_tasks and task_type == "code_modification":
                # ã‚¿ã‚¹ã‚¯å¾®åˆ†åŒ–: ãƒ•ã‚¡ã‚¤ãƒ« â†’ é–¢æ•°ãƒ¬ãƒ™ãƒ«åˆ†å‰²ï¼ˆã‚³ãƒ¼ãƒ‰ä¿®æ­£ã‚¿ã‚¹ã‚¯ã®ã¿ï¼‰
                micro_tasks = self.task_analyzer.analyze_file_for_micro_tasks(file_path, error_type)

                if micro_tasks:
                    # å®Ÿè¡Œè¨ˆç”»ä½œæˆ
                    execution_plan = self.task_analyzer.create_step_by_step_plan(micro_tasks)

                    print(f"ğŸ¯ å¾®ç´°ã‚¿ã‚¹ã‚¯ç”Ÿæˆ: {len(micro_tasks)}ä»¶")
                    print(f"â±ï¸ ç·æ‰€è¦æ™‚é–“: {execution_plan['total_estimated_time']}åˆ†")

                    # ãƒãƒƒãƒå‡¦ç†ã§ã‚¿ã‚¹ã‚¯ä½œæˆ
                    for batch in execution_plan['batch_recommendations']:
                        batch_task_id = self._create_batch_task(
                            batch, file_path, error_type, priority, execution_plan
                        )
                        created_task_ids.append(batch_task_id)

                else:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ•ã‚¡ã‚¤ãƒ«å…¨ä½“ã‚¿ã‚¹ã‚¯
                    fallback_task_id = self._create_file_level_task(
                        file_path, error_type, priority
                    )
                    created_task_ids.append(fallback_task_id)
            else:
                # å¾“æ¥æ–¹å¼: ãƒ•ã‚¡ã‚¤ãƒ«å˜ä½å‡¦ç†
                file_task_id = self._create_file_level_task(file_path, error_type, priority)
                created_task_ids.append(file_task_id)

        return created_task_ids

    def _create_with_claude_mode(self, target_files: List[str], error_type: str,
                                priority: str, decision, task_type: str = "code_modification") -> List[str]:
        """Claudeå˜ç‹¬ãƒ¢ãƒ¼ãƒ‰ã§ã®ã‚¿ã‚¹ã‚¯ä½œæˆ"""

        print("ğŸ“ Claudeå˜ç‹¬ãƒ¢ãƒ¼ãƒ‰ã§ã‚¿ã‚¹ã‚¯ä½œæˆï¼ˆGeminiä½¿ç”¨ãªã—ï¼‰")

        # ç°¡æ˜“ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¬ãƒ™ãƒ«ã‚¿ã‚¹ã‚¯ã®ã¿ä½œæˆ
        created_task_ids = []
        for file_path in target_files:
            task_id = self._create_claude_only_task(file_path, error_type, priority, decision)
            created_task_ids.append(task_id)

        return created_task_ids

    def _create_claude_only_task(self, file_path: str, error_type: str,
                                priority: str, decision) -> str:
        """Claudeå˜ç‹¬ã‚¿ã‚¹ã‚¯ä½œæˆ"""

        claude_analysis = self._claude_analyze_files([file_path], error_type)

        task_id = self.task_manager.create_task(
            task_type="claude_only_modification",
            description=f"{error_type} Claudeå˜ç‹¬ä¿®æ­£ - {file_path}",
            target_files=[file_path],
            priority=priority,
            requirements={
                "error_type": error_type,
                "fix_pattern": self._get_fix_pattern(error_type),
                "claude_only": True,
                "decision_reasoning": decision.reasoning
            },
            claude_analysis=claude_analysis,
            expected_outcome=f"{error_type} ã‚¨ãƒ©ãƒ¼ã‚’Claudeå˜ç‹¬ã§ä¿®æ­£",
            constraints=[
                "Geminiä½¿ç”¨ãªã—",
                "Claude Codeæ¨™æº–æ‰‹æ³•ã®ã¿ä½¿ç”¨",
                "æ®µéšçš„ãƒ»ç¢ºå®Ÿãªä¿®æ­£"
            ],
            context={
                "decision_engine_result": decision.task_analysis.__dict__,
                "automation_level": decision.automation_level.value,
                "session_id": self.session_id
            }
        )

        print(f"ğŸ“„ Claudeå˜ç‹¬ã‚¿ã‚¹ã‚¯ä½œæˆ: {file_path}")
        return task_id

    def _auto_execute_tasks(self, task_ids: List[str], decision) -> None:
        """è‡ªå‹•å®Ÿè¡Œ"""

        print(f"âš¡ è‡ªå‹•å®Ÿè¡Œé–‹å§‹: {len(task_ids)}ã‚¿ã‚¹ã‚¯")

        start_time = time.time()
        results = []

        for i, task_id in enumerate(task_ids, 1):
            print(f"\nğŸ”„ è‡ªå‹•å®Ÿè¡Œ {i}/{len(task_ids)}: {task_id}")

            try:
                result = self.execute_workflow_cycle()
                results.append(result)

                # å®Ÿè¡Œçµæœãƒã‚§ãƒƒã‚¯
                if result.get("status") != "completed":
                    print(f"âš ï¸ ã‚¿ã‚¹ã‚¯ {task_id} å®Ÿè¡Œå¤±æ•—ã€è‡ªå‹•å®Ÿè¡Œã‚’åœæ­¢")
                    break

                # Claudeå“è³ªãƒ¬ãƒ“ãƒ¥ãƒ¼
                claude_review = result.get("claude_review", {})
                approval = claude_review.get("approval", "unknown")

                if approval == "rejected":
                    print(f"âŒ ã‚¿ã‚¹ã‚¯ {task_id} å“è³ªä¸åˆæ ¼ã€è‡ªå‹•å®Ÿè¡Œã‚’åœæ­¢")
                    break
                elif approval == "requires_review":
                    print(f"ğŸ¤š ã‚¿ã‚¹ã‚¯ {task_id} æ‰‹å‹•ãƒ¬ãƒ“ãƒ¥ãƒ¼å¿…è¦ã€è‡ªå‹•å®Ÿè¡Œã‚’ä¸€æ™‚åœæ­¢")
                    break

                print(f"âœ… ã‚¿ã‚¹ã‚¯ {task_id} è‡ªå‹•å®Ÿè¡ŒæˆåŠŸ ({approval})")

            except Exception as e:
                print(f"âŒ ã‚¿ã‚¹ã‚¯ {task_id} å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
                break

        execution_time = time.time() - start_time

        # è‡ªå‹•å®Ÿè¡Œã‚µãƒãƒªãƒ¼
        successful = len([r for r in results if r.get("status") == "completed"])
        total_cost = sum(r.get("claude_review", {}).get("quality_metrics", {}).get("errors_fixed", 0)
                        for r in results) * decision.task_analysis.estimated_cost / max(1, len(task_ids))

        print(f"\nğŸ“Š è‡ªå‹•å®Ÿè¡Œã‚µãƒãƒªãƒ¼:")
        print(f"   æˆåŠŸ: {successful}/{len(task_ids)}ã‚¿ã‚¹ã‚¯")
        print(f"   å®Ÿè¡Œæ™‚é–“: {execution_time:.1f}ç§’")
        print(f"   æ¨å®šã‚³ã‚¹ãƒˆ: ${total_cost:.4f}")
        print(f"   åŠ¹ç‡æ€§: {decision.task_analysis.gemini_benefit_score:.2f}")

    def get_decision_stats(self) -> Dict[str, Any]:
        """åˆ¤å®šçµ±è¨ˆæƒ…å ±å–å¾—"""
        return self.decision_engine.get_decision_stats()

    def set_automation_preferences(self, preferences: Dict[str, Any]) -> None:
        """è‡ªå‹•åŒ–è¨­å®šå¤‰æ›´"""

        # WorkflowDecisionEngineè¨­å®šæ›´æ–°
        self.decision_engine.thresholds.update(preferences.get("thresholds", {}))

        print(f"âš™ï¸ è‡ªå‹•åŒ–è¨­å®šæ›´æ–°: {preferences}")

    def _create_batch_task(self, batch: Dict[str, Any], file_path: str,
                          error_type: str, priority: str, execution_plan: Dict) -> str:
        """ãƒãƒƒãƒã‚¿ã‚¹ã‚¯ä½œæˆï¼ˆFlash 2.5æœ€é©åŒ–ï¼‰"""

        batch_id = batch['batch_id']
        tasks = batch['tasks']

        # Flash 2.5å‘ã‘å…·ä½“çš„æŒ‡ç¤ºç”Ÿæˆï¼ˆå“è³ªä¿è¨¼ä»˜ãï¼‰
        flash_instruction = self._generate_batch_flash_instruction(tasks, error_type, file_path)

        # Claude ã«ã‚ˆã‚‹è©³ç´°åˆ†æ
        claude_analysis = self._claude_analyze_micro_tasks(tasks, error_type, file_path)

        task_id = self.task_manager.create_task(
            task_type="micro_code_modification",
            description=f"{error_type} ãƒãƒƒãƒä¿®æ­£ - {file_path} Batch#{batch_id}",
            target_files=[file_path],
            priority=priority,
            requirements={
                "error_type": error_type,
                "batch_info": batch,
                "micro_tasks": tasks,
                "flash_instruction": flash_instruction,
                "max_context_tokens": 2000,
                "step_by_step": True
            },
            claude_analysis=claude_analysis,
            expected_outcome=f"ãƒãƒƒãƒå†…å…¨ã‚¿ã‚¹ã‚¯å®Œäº† ({len(tasks)}ä»¶)",
            constraints=[
                "Flash 2.5ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ¶é™éµå®ˆ",
                "1ãƒãƒƒãƒ20åˆ†ä»¥å†…",
                "å…·ä½“çš„æŒ‡ç¤ºã«å¾“ã£ãŸä¿®æ­£ã®ã¿"
            ],
            context={
                "batch_id": batch_id,
                "total_batches": len(execution_plan['batch_recommendations']),
                "execution_plan": execution_plan['flash25_optimization'],
                "session_id": self.session_id
            }
        )

        print(f"ğŸ“¦ ãƒãƒƒãƒã‚¿ã‚¹ã‚¯ä½œæˆ: Batch#{batch_id} ({len(tasks)}ä»¶, {batch['estimated_time']}åˆ†)")
        return task_id

    def _create_file_level_task(self, file_path: str, error_type: str, priority: str) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¬ãƒ™ãƒ«ã‚¿ã‚¹ã‚¯ä½œæˆï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""

        # å¾“æ¥æ–¹å¼ã®åˆ†æ
        claude_analysis = self._claude_analyze_files([file_path], error_type)

        task_id = self.task_manager.create_task(
            task_type="file_code_modification",
            description=f"{error_type} ãƒ•ã‚¡ã‚¤ãƒ«ä¿®æ­£ - {file_path}",
            target_files=[file_path],
            priority=priority,
            requirements={
                "error_type": error_type,
                "fix_pattern": self._get_fix_pattern(error_type),
                "test_required": True,
                "quality_check": True
            },
            claude_analysis=claude_analysis,
            expected_outcome=f"{error_type} ã‚¨ãƒ©ãƒ¼ã‚’0ä»¶ã«å‰Šæ¸›",
            constraints=[
                "æ—¢å­˜æ©Ÿèƒ½ã¸ã®å½±éŸ¿æœ€å°åŒ–",
                "ãƒ†ã‚¹ãƒˆé€šéå¿…é ˆ",
                "pre-commit hooksé€šéå¿…é ˆ"
            ],
            context={
                "fallback_mode": True,
                "session_id": self.session_id
            }
        )

        print(f"ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¹ã‚¯ä½œæˆ: {file_path}")
        return task_id

    def _generate_batch_flash_instruction(self, tasks: List[Dict], error_type: str, file_path: str = "") -> str:
        """å“è³ªä¿è¨¼ä»˜ããƒãƒƒãƒç”¨Flash 2.5æŒ‡ç¤ºç”Ÿæˆ"""

        # å¼·åŒ–ç‰ˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ç”¨
        enhanced_instruction = self.enhanced_templates.generate_quality_assured_instruction(
            error_type, tasks, file_path
        )

        # ã‚¨ãƒ©ãƒ¼é˜²æ­¢ã‚¬ã‚¤ãƒ‰ã‚’è¿½åŠ 
        error_prevention_guide = self.enhanced_templates.get_error_prevention_guide(error_type)

        final_instruction = f"""
{enhanced_instruction}

{error_prevention_guide}

ğŸ”’ å“è³ªä¿è¨¼ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ:
1. ä¿®æ­£å‰: å¯¾è±¡é–¢æ•°ã®ç¢ºèª
2. ä¿®æ­£ä¸­: æ§‹æ–‡ã‚¨ãƒ©ãƒ¼ã®å³åº§ç¢ºèª
3. ä¿®æ­£å¾Œ: Pythonæ§‹æ–‡æ¤œè¨¼å®Ÿè¡Œ
4. å®Œäº†å¾Œ: å…¨ä½“å“è³ªãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ

âš ï¸ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã®å¯¾å¿œ:
- æ§‹æ–‡ã‚¨ãƒ©ãƒ¼ â†’ ç¦æ­¢ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯ãƒ»ä¿®æ­£
- å‹æ³¨é‡ˆã‚¨ãƒ©ãƒ¼ â†’ æ­£è§£ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç¢ºèªãƒ»é©ç”¨
- import ã‚¨ãƒ©ãƒ¼ â†’ 'from typing import Any' ã‚’è¿½åŠ 
- ä¸æ˜æ™‚ â†’ 'Any' å‹ã‚’ä½¿ç”¨

ğŸ¯ æœ€çµ‚ç¢ºèªé …ç›®:
â–¡ å…¨é–¢æ•°ã«å‹æ³¨é‡ˆè¿½åŠ å®Œäº†
â–¡ Pythonæ§‹æ–‡ã‚¨ãƒ©ãƒ¼ 0ä»¶
â–¡ å¿…è¦ãªimportæ–‡è¿½åŠ å®Œäº†
â–¡ ä¿®æ­£å†…å®¹ãŒæœŸå¾…é€šã‚Š
"""

        return final_instruction

    def _claude_analyze_micro_tasks(self, tasks: List[Dict], error_type: str, file_path: str) -> str:
        """å¾®ç´°ã‚¿ã‚¹ã‚¯ç”¨Claudeåˆ†æ"""

        analysis = f"""
ğŸ“Š Claudeå¾®ç´°ã‚¿ã‚¹ã‚¯åˆ†æ - {error_type}

ğŸ¯ å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {file_path}
ğŸ”§ ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: {error_type}
ğŸ“¦ ãƒãƒƒãƒã‚µã‚¤ã‚º: {len(tasks)}ä»¶

ğŸ§  åˆ†æçµæœ:
"""

        for task in tasks:
            func_name = task.get('target_function', 'unknown')
            complexity = task.get('complexity', 'medium')
            error_count = task.get('error_count', 0)

            analysis += f"""
  â€¢ {func_name}é–¢æ•°:
    - ã‚¨ãƒ©ãƒ¼æ•°: {error_count}ä»¶
    - è¤‡é›‘åº¦: {complexity}
    - æ¨å®šæ™‚é–“: {task.get('estimated_time', 10)}åˆ†
    - FlashæŒ‡ç¤º: é©ç”¨æ¸ˆã¿
"""

        analysis += f"""
âš ï¸ Flash 2.5 è€ƒæ…®äº‹é …:
- ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ¶é™: 2000ãƒˆãƒ¼ã‚¯ãƒ³ä»¥å†…
- å…·ä½“çš„ä¾‹ç¤ºã«ã‚ˆã‚‹æŒ‡ç¤º
- æ®µéšçš„å®Ÿè¡Œã«ã‚ˆã‚‹ç¢ºå®Ÿæ€§
- ã‚¨ãƒ©ãƒ¼æ™‚ã®é©åˆ‡ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

ğŸ“‹ æ¨å¥¨ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ:
1. é–¢æ•°å˜ä½ã§ã®é€æ¬¡å‡¦ç†
2. ä¿®æ­£ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å³æ ¼é©ç”¨
3. å„ã‚¹ãƒ†ãƒƒãƒ—ã§ã®æ¤œè¨¼
4. äºˆæœŸã—ãªã„çŠ¶æ³ã§ã®type: ignoreä½¿ç”¨
"""

        return analysis

    def execute_workflow_cycle(self) -> Dict[str, Any]:
        """1ã‚µã‚¤ã‚¯ãƒ«ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œ"""

        print("\nğŸ”„ ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ ã‚µã‚¤ã‚¯ãƒ«é–‹å§‹")

        # Step 1: Claude - æ¬¡ã®ã‚¿ã‚¹ã‚¯ã‚’å–å¾—
        task_data = self.gemini_helper.get_next_task()

        if not task_data:
            print("ğŸ“­ å®Ÿè¡Œå¾…ã¡ã‚¿ã‚¹ã‚¯ãªã—")
            return {"status": "no_tasks", "message": "å®Ÿè¡Œå¾…ã¡ã‚¿ã‚¹ã‚¯ãŒã‚ã‚Šã¾ã›ã‚“"}

        print(f"ğŸ“‹ å®Ÿè¡Œã‚¿ã‚¹ã‚¯: {task_data['task_id']}")
        print(f"ğŸ“ èª¬æ˜: {task_data['description']}")

        # Step 2: Gemini - ã‚¿ã‚¹ã‚¯å®Ÿè¡Œ
        print("\nğŸš€ Gemini CLI ã§ã‚¿ã‚¹ã‚¯å®Ÿè¡Œä¸­...")

        start_time = time.time()

        # Gemini CLIçµŒç”±ã§ã‚¿ã‚¹ã‚¯å®Ÿè¡Œ
        gemini_result = self._execute_with_gemini(task_data)

        execution_time = time.time() - start_time

        # Step 3: Claude - çµæœãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ»æ¤œè¨¼
        print("\nğŸ” Claude ã«ã‚ˆã‚‹çµæœãƒ¬ãƒ“ãƒ¥ãƒ¼...")

        claude_review = self._claude_review_result(gemini_result)

        # Step 4: çµ±åˆãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
        cycle_result = {
            "session_id": self.session_id,
            "cycle_timestamp": datetime.datetime.now().isoformat(),
            "task_executed": task_data,
            "gemini_result": gemini_result,
            "claude_review": claude_review,
            "execution_time": execution_time,
            "status": "completed",
            "next_recommendations": self._generate_next_recommendations(gemini_result, claude_review)
        }

        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ­ã‚°ä¿å­˜
        self._save_cycle_log(cycle_result)

        print(f"\nâœ… ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚µã‚¤ã‚¯ãƒ«å®Œäº† ({execution_time:.1f}ç§’)")

        return cycle_result

    def run_continuous_workflow(self, max_cycles: int = 10) -> List[Dict[str, Any]]:
        """é€£ç¶šãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œ"""

        print(f"\nğŸ”„ é€£ç¶šãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼é–‹å§‹ (æœ€å¤§{max_cycles}ã‚µã‚¤ã‚¯ãƒ«)")

        results = []

        for cycle in range(max_cycles):
            print(f"\n" + "="*60)
            print(f"ğŸ”„ Cycle {cycle + 1}/{max_cycles}")
            print("="*60)

            cycle_result = self.execute_workflow_cycle()
            results.append(cycle_result)

            # ã‚¿ã‚¹ã‚¯ãŒãªã„å ´åˆã¯çµ‚äº†
            if cycle_result["status"] == "no_tasks":
                print(f"\nğŸ å…¨ã‚¿ã‚¹ã‚¯å®Œäº† (å®Ÿè¡Œã‚µã‚¤ã‚¯ãƒ«: {cycle + 1})")
                break

            # çŸ­ã„ä¼‘æ¯
            if cycle < max_cycles - 1:
                print("â³ æ¬¡ã®ã‚µã‚¤ã‚¯ãƒ«ã¾ã§3ç§’å¾…æ©Ÿ...")
                time.sleep(3)

        # æœ€çµ‚çµ±è¨ˆ
        self._print_session_summary(results)

        return results

    def __del__(self):
        """ãƒ‡ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ - ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚·ã‚¹ãƒ†ãƒ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        if hasattr(self, 'success_monitor'):
            try:
                self.cleanup_hybrid_systems()
            except:
                pass

    def _claude_analyze_files(self, target_files: List[str], error_type: str) -> str:
        """Claude ã«ã‚ˆã‚‹è©³ç´°äº‹å‰åˆ†æ"""

        print(f"ğŸ§  Claudeåˆ†æé–‹å§‹: {error_type} ã‚¨ãƒ©ãƒ¼")

        analysis_results = []
        total_errors = 0
        complexity_factors = []

        for file_path in target_files:
            if not os.path.exists(file_path):
                continue

            # ãƒ•ã‚¡ã‚¤ãƒ«è©³ç´°åˆ†æ
            file_analysis = self._analyze_file_complexity(file_path, error_type)
            analysis_results.append(file_analysis)
            total_errors += file_analysis.get("error_count", 0)
            complexity_factors.extend(file_analysis.get("complexity_factors", []))

        # ç·åˆåˆ†æ
        overall_complexity = self._assess_overall_complexity(analysis_results, error_type)
        risk_assessment = self._assess_modification_risk(analysis_results)
        recommended_strategy = self._recommend_fix_strategy(error_type, overall_complexity, total_errors)

        analysis = f"""
ğŸ“Š Claude è©³ç´°äº‹å‰åˆ†æ - {error_type} ã‚¨ãƒ©ãƒ¼ä¿®æ­£

ğŸ¯ å¯¾è±¡ç¯„å›²:
- ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(target_files)}ä»¶
- ç·ã‚¨ãƒ©ãƒ¼æ•°: {total_errors}ä»¶
- ä¿®æ­£è¤‡é›‘åº¦: {overall_complexity['level']} ({overall_complexity['score']}/10)
- æ¨å®šæ‰€è¦æ™‚é–“: {overall_complexity['estimated_time']}åˆ†

ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥åˆ†æ:
{self._format_file_analysis(analysis_results)}

âš ï¸ ãƒªã‚¹ã‚¯è©•ä¾¡:
{self._format_risk_assessment(risk_assessment)}

ğŸ¯ æ¨å¥¨æˆ¦ç•¥:
{recommended_strategy}

ğŸ”§ Flash 2.5 æœ€é©åŒ–:
- å¾®ç´°ã‚¿ã‚¹ã‚¯åˆ†å‰²: {'æ¨å¥¨' if total_errors > 10 else 'ä¸è¦'}
- ãƒãƒƒãƒã‚µã‚¤ã‚º: {min(3, max(1, total_errors // 5))}ã‚¿ã‚¹ã‚¯/ãƒãƒƒãƒ
- ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ¶é™: 2000ãƒˆãƒ¼ã‚¯ãƒ³/ã‚¿ã‚¹ã‚¯

ğŸ“‹ æˆåŠŸè¦å› :
1. æ®µéšçš„ãƒ»ç¢ºå®Ÿãªä¿®æ­£ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
2. å„ã‚¹ãƒ†ãƒƒãƒ—ã§ã®å“è³ªç¢ºèª
3. æ—¢å­˜æ©Ÿèƒ½ã¸ã®å½±éŸ¿æœ€å°åŒ–
4. é©åˆ‡ãªãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
"""

        return analysis

    def _analyze_file_complexity(self, file_path: str, error_type: str) -> Dict[str, Any]:
        """ãƒ•ã‚¡ã‚¤ãƒ«è¤‡é›‘åº¦åˆ†æ"""

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            # åŸºæœ¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            lines = content.split('\n')
            line_count = len(lines)
            function_count = content.count('def ')
            class_count = content.count('class ')

            # ã‚¨ãƒ©ãƒ¼æ•°ã‚«ã‚¦ãƒ³ãƒˆ
            error_count = self._count_file_errors(file_path, error_type)

            # è¤‡é›‘åº¦è¦å› ç‰¹å®š
            complexity_factors = []
            if line_count > 500:
                complexity_factors.append("å¤§è¦æ¨¡ãƒ•ã‚¡ã‚¤ãƒ«")
            if function_count > 20:
                complexity_factors.append("å¤šæ•°ã®é–¢æ•°")
            if class_count > 5:
                complexity_factors.append("è¤‡æ•°ã‚¯ãƒ©ã‚¹")
            if 'import' in content and content.count('import') > 10:
                complexity_factors.append("å¤šæ•°ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ")
            if 'typing' in content:
                complexity_factors.append("æ—¢å­˜å‹æ³¨é‡ˆ")

            # è¤‡é›‘åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—
            complexity_score = min(10, (
                line_count // 100 +
                function_count // 5 +
                class_count * 2 +
                error_count // 3
            ))

            return {
                "file_path": file_path,
                "line_count": line_count,
                "function_count": function_count,
                "class_count": class_count,
                "error_count": error_count,
                "complexity_score": complexity_score,
                "complexity_factors": complexity_factors,
                "estimated_time": max(5, error_count * 2 + complexity_score)
            }

        except Exception as e:
            return {
                "file_path": file_path,
                "error": str(e),
                "complexity_score": 5,
                "estimated_time": 10
            }

    def _count_file_errors(self, file_path: str, error_type: str) -> int:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ©ãƒ¼æ•°ã‚«ã‚¦ãƒ³ãƒˆ"""
        try:
            result = subprocess.run(
                ["python3", "-m", "mypy", "--strict", file_path],
                capture_output=True,
                text=True
            )

            error_count = 0
            for line in result.stdout.split('\n'):
                if error_type in line and 'error:' in line:
                    error_count += 1

            return error_count
        except:
            return 0

    def _assess_overall_complexity(self, analysis_results: List[Dict], error_type: str) -> Dict[str, Any]:
        """å…¨ä½“è¤‡é›‘åº¦è©•ä¾¡"""

        if not analysis_results:
            return {"level": "ä½", "score": 1, "estimated_time": 10}

        avg_score = sum(a.get("complexity_score", 5) for a in analysis_results) / len(analysis_results)
        total_time = sum(a.get("estimated_time", 10) for a in analysis_results)

        if avg_score <= 3:
            level = "ä½"
        elif avg_score <= 6:
            level = "ä¸­"
        else:
            level = "é«˜"

        return {
            "level": level,
            "score": round(avg_score, 1),
            "estimated_time": total_time
        }

    def _assess_modification_risk(self, analysis_results: List[Dict]) -> Dict[str, Any]:
        """ä¿®æ­£ãƒªã‚¹ã‚¯è©•ä¾¡"""

        risk_factors = []
        mitigation_strategies = []

        for analysis in analysis_results:
            factors = analysis.get("complexity_factors", [])

            if "å¤§è¦æ¨¡ãƒ•ã‚¡ã‚¤ãƒ«" in factors:
                risk_factors.append("å¤§è¦æ¨¡ãƒ•ã‚¡ã‚¤ãƒ«ä¿®æ­£ã«ã‚ˆã‚‹å½±éŸ¿ç¯„å›²æ‹¡å¤§")
                mitigation_strategies.append("é–¢æ•°å˜ä½ã§ã®æ®µéšçš„ä¿®æ­£")

            if "æ—¢å­˜å‹æ³¨é‡ˆ" in factors:
                risk_factors.append("æ—¢å­˜å‹æ³¨é‡ˆã¨ã®æ•´åˆæ€§å•é¡Œ")
                mitigation_strategies.append("æ—¢å­˜æ³¨é‡ˆã®è©³ç´°ç¢ºèª")

            if "è¤‡æ•°ã‚¯ãƒ©ã‚¹" in factors:
                risk_factors.append("ã‚¯ãƒ©ã‚¹é–“ä¾å­˜é–¢ä¿‚ã¸ã®å½±éŸ¿")
                mitigation_strategies.append("ã‚¯ãƒ©ã‚¹åˆ¥ä¿®æ­£ãƒ»ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")

        risk_level = "é«˜" if len(risk_factors) > 3 else "ä¸­" if len(risk_factors) > 1 else "ä½"

        return {
            "level": risk_level,
            "factors": risk_factors,
            "mitigation": mitigation_strategies
        }

    def _recommend_fix_strategy(self, error_type: str, complexity: Dict, total_errors: int) -> str:
        """ä¿®æ­£æˆ¦ç•¥æ¨å¥¨"""

        strategy = f"""
ğŸ¯ {error_type} ä¿®æ­£æˆ¦ç•¥:

"""

        if total_errors <= 5:
            strategy += "ã€ã‚·ãƒ³ãƒ—ãƒ«æˆ¦ç•¥ã€‘\n- ãƒ•ã‚¡ã‚¤ãƒ«å˜ä½ã§ã®ä¸€æ‹¬ä¿®æ­£\n- æ¨™æº–çš„ãªä¿®æ­£ãƒ‘ã‚¿ãƒ¼ãƒ³é©ç”¨"
        elif total_errors <= 20:
            strategy += "ã€ãƒãƒ©ãƒ³ã‚¹æˆ¦ç•¥ã€‘\n- é–¢æ•°å˜ä½ã§ã®æ®µéšçš„ä¿®æ­£\n- ãƒãƒƒãƒå‡¦ç†ã«ã‚ˆã‚‹åŠ¹ç‡åŒ–"
        else:
            strategy += "ã€å¾®ç´°åˆ†å‰²æˆ¦ç•¥ã€‘\n- é–¢æ•°ãƒ¬ãƒ™ãƒ«ã§ã®ç´°åˆ†åŒ–\n- Flash 2.5æœ€é©åŒ–ãƒãƒƒãƒå‡¦ç†"

        if complexity["level"] == "é«˜":
            strategy += "\n\nâš ï¸ é«˜è¤‡é›‘åº¦å¯¾å¿œ:\n- è©³ç´°ãªäº‹å‰ãƒ†ã‚¹ãƒˆ\n- ä¿®æ­£å¾Œã®å…¨ä½“å½±éŸ¿ç¢ºèª\n- æ®µéšçš„ãƒªãƒªãƒ¼ã‚¹æ¨å¥¨"

        return strategy

    def _format_file_analysis(self, analysis_results: List[Dict]) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æçµæœãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""

        formatted = ""
        for analysis in analysis_results[:5]:  # æœ€å¤§5ä»¶è¡¨ç¤º
            path = analysis.get("file_path", "unknown")
            errors = analysis.get("error_count", 0)
            score = analysis.get("complexity_score", 0)
            time = analysis.get("estimated_time", 0)

            formatted += f"  â€¢ {path}\n"
            formatted += f"    ã‚¨ãƒ©ãƒ¼: {errors}ä»¶, è¤‡é›‘åº¦: {score}/10, æ™‚é–“: {time}åˆ†\n"

        if len(analysis_results) > 5:
            formatted += f"  ... ä»–{len(analysis_results) - 5}ãƒ•ã‚¡ã‚¤ãƒ«\n"

        return formatted

    def _format_risk_assessment(self, risk_assessment: Dict) -> str:
        """ãƒªã‚¹ã‚¯è©•ä¾¡ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""

        level = risk_assessment["level"]
        factors = risk_assessment["factors"]
        mitigation = risk_assessment["mitigation"]

        formatted = f"ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«: {level}\n"

        if factors:
            formatted += "ä¸»è¦ãƒªã‚¹ã‚¯è¦å› :\n"
            for factor in factors[:3]:
                formatted += f"  - {factor}\n"

        if mitigation:
            formatted += "æ¨å¥¨å¯¾ç­–:\n"
            for strategy in mitigation[:3]:
                formatted += f"  + {strategy}\n"

        return formatted

    def _get_fix_pattern(self, error_type: str) -> str:
        """ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—åˆ¥ã®ä¿®æ­£ãƒ‘ã‚¿ãƒ¼ãƒ³"""
        patterns = {
            "no-untyped-def": "add_return_type_annotations",
            "no-untyped-call": "add_type_ignore_or_stubs",
            "type-arg": "add_generic_type_parameters",
            "call-arg": "fix_function_arguments",
            "attr-defined": "fix_attribute_access"
        }
        return patterns.get(error_type, "generic_fix")

    def _execute_with_gemini(self, task_data: Dict[str, Any]) -> Dict[str, Any]:
        """å“è³ªä¿è¨¼ä»˜ãGemini CLI ã§ã®ã‚¿ã‚¹ã‚¯å®Ÿè¡Œ"""

        try:
            # Gemini HelperçµŒç”±ã§å®Ÿè¡Œ
            print("ğŸ”„ Geminiå®Ÿè¡Œä¸­...")
            raw_result = self.gemini_helper.execute_task(task_data)

            # å“è³ªä¿è¨¼ãƒ•ã‚§ãƒ¼ã‚º
            print("ğŸ›¡ï¸ æ§‹æ–‡æ¤œè¨¼ãƒ»å“è³ªä¿è¨¼é–‹å§‹...")
            quality_assured_result = self._apply_quality_assurance(raw_result, task_data)

            # å®Ÿéš›ã®Tokenä½¿ç”¨é‡ã§ã‚³ã‚¹ãƒˆè¿½è·¡ï¼ˆå‹•çš„æ¸¬å®šï¼‰
            actual_token_usage = self._measure_actual_token_usage(
                quality_assured_result, task_id, "gemini-2.5-flash"
            )

            self.task_manager.track_cost(task_data["task_id"], actual_token_usage)

            print(f"ğŸ’° ã‚³ã‚¹ãƒˆè¿½è·¡: å…¥åŠ›Token={actual_token_usage.get('input_tokens', 0)}, "
                  f"å‡ºåŠ›Token={actual_token_usage.get('output_tokens', 0)}")

            if "measurement_method" in actual_token_usage:
                print(f"ğŸ“Š æ¸¬å®šæ–¹å¼: {actual_token_usage['measurement_method']}")

            return quality_assured_result

        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "task_id": task_data["task_id"]
            }

    def _apply_quality_assurance(self, gemini_result: Dict[str, Any],
                                task_data: Dict[str, Any]) -> Dict[str, Any]:
        """Geminiçµæœã¸ã®å“è³ªä¿è¨¼é©ç”¨"""

        print("ğŸ” Geminiå‡ºåŠ›å“è³ªæ¤œè¨¼ä¸­...")

        modifications = gemini_result.get("modifications", {})
        files_modified = modifications.get("files_modified", [])

        quality_issues = []
        quality_fixes = []
        total_syntax_fixes = 0

        for file_mod in files_modified:
            file_path = file_mod.get("file", "")

            if not file_path or not os.path.exists(file_path):
                continue

            try:
                # ä¿®æ­£ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’èª­ã¿å–ã‚Š
                with open(file_path, 'r', encoding='utf-8') as f:
                    modified_code = f.read()

                # æ§‹æ–‡æ¤œè¨¼å®Ÿè¡Œ
                validation_result = self.syntax_validator.comprehensive_validation(
                    modified_code, file_path
                )

                print(f"ğŸ“Š {file_path} æ¤œè¨¼çµæœ:")
                print(f"   æ§‹æ–‡OK: {validation_result['syntax_valid']}")
                print(f"   å‹æ³¨é‡ˆOK: {validation_result['type_annotations_valid']}")
                print(f"   å“è³ªã‚¹ã‚³ã‚¢: {validation_result['validation_score']:.2f}")

                # å“è³ªå•é¡Œã®è¨˜éŒ²
                if not validation_result['syntax_valid']:
                    quality_issues.extend(validation_result['syntax_errors'])

                if not validation_result['type_annotations_valid']:
                    quality_issues.extend(validation_result['type_errors'])

                # è‡ªå‹•ä¿®æ­£ã®é©ç”¨
                if validation_result['fixed_code'] and (
                    not validation_result['syntax_valid'] or
                    not validation_result['type_annotations_valid']
                ):
                    print(f"ğŸ”§ {file_path} è‡ªå‹•ä¿®æ­£é©ç”¨ä¸­...")

                    # ä¿®æ­£ã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°
                    with open(file_path, 'w', encoding='utf-8') as f:
                        f.write(validation_result['fixed_code'])

                    quality_fixes.append(f"{file_path}: æ§‹æ–‡ã‚¨ãƒ©ãƒ¼è‡ªå‹•ä¿®æ­£")
                    total_syntax_fixes += 1

                    # ä¿®æ­£å¾Œã®å†æ¤œè¨¼
                    revalidation = self.syntax_validator.comprehensive_validation(
                        validation_result['fixed_code'], file_path
                    )

                    print(f"âœ… {file_path} ä¿®æ­£å¾Œ: ã‚¹ã‚³ã‚¢ {revalidation['validation_score']:.2f}")

                # ãƒ•ã‚¡ã‚¤ãƒ«ä¿®æ­£æƒ…å ±ã®æ›´æ–°
                file_mod["quality_validation"] = {
                    "syntax_valid": validation_result['syntax_valid'],
                    "type_annotations_valid": validation_result['type_annotations_valid'],
                    "validation_score": validation_result['validation_score'],
                    "auto_fixed": bool(validation_result['fixed_code'])
                }

            except Exception as e:
                quality_issues.append(f"{file_path}: å“è³ªæ¤œè¨¼ã‚¨ãƒ©ãƒ¼ - {str(e)}")

        # å“è³ªä¿è¨¼çµæœã®çµ±åˆ
        enhanced_result = gemini_result.copy()
        enhanced_result["quality_assurance"] = {
            "syntax_validation_applied": True,
            "quality_issues_found": quality_issues,
            "quality_fixes_applied": quality_fixes,
            "total_syntax_fixes": total_syntax_fixes,
            "overall_quality_score": self._calculate_overall_quality_score(files_modified)
        }

        # ä¿®æ­£æƒ…å ±ã®æ›´æ–°
        if total_syntax_fixes > 0:
            enhanced_result["modifications"]["supervisor_fixes"] = total_syntax_fixes
            enhanced_result["modifications"]["quality_enhanced"] = True

            print(f"ğŸ”§ ç›£ç£è€…å“è³ªä¿®æ­£: {total_syntax_fixes}ä»¶ã®æ§‹æ–‡ã‚¨ãƒ©ãƒ¼ä¿®æ­£å®Œäº†")

        if quality_issues:
            print(f"âš ï¸ å“è³ªå•é¡Œæ¤œå‡º: {len(quality_issues)}ä»¶")
            for issue in quality_issues[:5]:  # æœ€åˆã®5ä»¶ã®ã¿è¡¨ç¤º
                print(f"  - {issue}")
        else:
            print("âœ… å“è³ªæ¤œè¨¼: å•é¡Œãªã—")

        return enhanced_result

    def _calculate_overall_quality_score(self, files_modified: List[Dict]) -> float:
        """å…¨ä½“å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—"""

        if not files_modified:
            return 0.0

        total_score = 0.0
        valid_files = 0

        for file_mod in files_modified:
            quality_val = file_mod.get("quality_validation")
            if quality_val:
                total_score += quality_val.get("validation_score", 0.0)
                valid_files += 1

        return total_score / max(1, valid_files)

    def _claude_review_result(self, gemini_result: Dict[str, Any]) -> Dict[str, Any]:
        """Claude ã«ã‚ˆã‚‹è©³ç´°çµæœãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ»å“è³ªè©•ä¾¡"""

        print("ğŸ” Claudeå“è³ªãƒ¬ãƒ“ãƒ¥ãƒ¼é–‹å§‹")

        status = gemini_result.get("status", "unknown")
        modifications = gemini_result.get("modifications", {})
        gemini_report = gemini_result.get("gemini_report", {})

        # å¤šè§’çš„å“è³ªè©•ä¾¡
        quality_assessment = self._assess_code_quality(modifications)
        completeness_check = self._check_completeness(modifications, gemini_result)
        risk_evaluation = self._evaluate_modification_risk(modifications)
        test_validation = self._validate_test_results(modifications)

        # ç·åˆåˆ¤å®š
        overall_score = self._calculate_overall_score(
            quality_assessment, completeness_check, risk_evaluation, test_validation
        )

        approval_decision = self._make_approval_decision(overall_score, status)

        # ãƒªãƒˆãƒ©ã‚¤ãƒ»ãƒ•ã‚©ãƒ­ãƒ¼ã‚¢ãƒƒãƒ—ææ¡ˆ
        followup_actions = self._generate_followup_actions(
            approval_decision, quality_assessment, modifications
        )

        review = {
            "overall_quality": approval_decision["quality_level"],
            "approval": approval_decision["decision"],
            "confidence_score": overall_score,
            "detailed_assessment": {
                "code_quality": quality_assessment,
                "completeness": completeness_check,
                "risk_evaluation": risk_evaluation,
                "test_validation": test_validation
            },
            "quality_metrics": {
                "errors_fixed": modifications.get("total_errors_fixed", 0),
                "files_modified": len(modifications.get("files_modified", [])),
                "modification_scope": self._assess_modification_scope(modifications),
                "regression_risk": risk_evaluation.get("level", "medium")
            },
            "recommendations": followup_actions["recommendations"],
            "required_actions": followup_actions["required_actions"],
            "retry_strategy": followup_actions.get("retry_strategy"),
            "claude_feedback": self._generate_claude_feedback(gemini_report, overall_score)
        }

        print(f"ğŸ“Š ãƒ¬ãƒ“ãƒ¥ãƒ¼å®Œäº†: {approval_decision['decision']} (ä¿¡é ¼åº¦: {overall_score:.2f})")

        return review

    def _assess_code_quality(self, modifications: Dict) -> Dict[str, Any]:
        """ã‚³ãƒ¼ãƒ‰å“è³ªè©•ä¾¡"""

        files_modified = modifications.get("files_modified", [])
        quality_checks = modifications.get("quality_checks", {})

        quality_score = 0.0
        quality_issues = []
        quality_positives = []

        # ä¿®æ­£å†…å®¹ã®è³ªè©•ä¾¡
        for file_mod in files_modified:
            errors_fixed = file_mod.get("errors_fixed", 0)
            changes = file_mod.get("changes", "")

            if errors_fixed > 0:
                quality_positives.append(f"{file_mod['file']}: {errors_fixed}ã‚¨ãƒ©ãƒ¼ä¿®æ­£")
                quality_score += 0.2

            if "type annotation" in changes or "type: ignore" in changes:
                quality_positives.append("é©åˆ‡ãªå‹å‡¦ç†")
                quality_score += 0.1

        # å“è³ªãƒã‚§ãƒƒã‚¯çµæœè©•ä¾¡
        for tool, result in quality_checks.items():
            if result == "passed" or result == "available":
                quality_score += 0.15
            else:
                quality_issues.append(f"{tool}ãƒã‚§ãƒƒã‚¯å¤±æ•—")

        # ãƒ†ã‚¹ãƒˆé€šéçŠ¶æ³
        if modifications.get("tests_passed", False):
            quality_positives.append("ãƒ†ã‚¹ãƒˆå…¨é€šé")
            quality_score += 0.3
        else:
            quality_issues.append("ãƒ†ã‚¹ãƒˆæœªé€šéã¾ãŸã¯æœªå®Ÿè¡Œ")

        return {
            "score": min(1.0, quality_score),
            "level": "high" if quality_score >= 0.7 else "medium" if quality_score >= 0.4 else "low",
            "positives": quality_positives,
            "issues": quality_issues
        }

    def _check_completeness(self, modifications: Dict, gemini_result: Dict) -> Dict[str, Any]:
        """å®Œäº†åº¦ãƒã‚§ãƒƒã‚¯"""

        task_data = gemini_result.get("task_executed", {})
        requirements = task_data.get("requirements", {})
        expected_fixes = requirements.get("error_count", 0) or 1
        actual_fixes = modifications.get("total_errors_fixed", 0)

        completion_rate = actual_fixes / max(1, expected_fixes)

        completeness_issues = []
        if completion_rate < 0.8:
            completeness_issues.append(f"ä¿®æ­£å®Œäº†ç‡ä½: {completion_rate:.1%}")

        if not modifications.get("quality_checks"):
            completeness_issues.append("å“è³ªãƒã‚§ãƒƒã‚¯æœªå®Ÿè¡Œ")

        return {
            "completion_rate": completion_rate,
            "level": "complete" if completion_rate >= 0.9 else "partial" if completion_rate >= 0.6 else "incomplete",
            "expected_fixes": expected_fixes,
            "actual_fixes": actual_fixes,
            "issues": completeness_issues
        }

    def _evaluate_modification_risk(self, modifications: Dict) -> Dict[str, Any]:
        """ä¿®æ­£ãƒªã‚¹ã‚¯è©•ä¾¡"""

        files_modified = modifications.get("files_modified", [])

        risk_factors = []
        risk_score = 0.0

        for file_mod in files_modified:
            lines_changed = file_mod.get("lines_changed", 0)
            file_path = file_mod.get("file", "")

            if lines_changed > 50:
                risk_factors.append(f"å¤§è¦æ¨¡ä¿®æ­£: {file_path}")
                risk_score += 0.3

            if "core" in file_path or "main" in file_path:
                risk_factors.append(f"é‡è¦ãƒ•ã‚¡ã‚¤ãƒ«ä¿®æ­£: {file_path}")
                risk_score += 0.2

        # ãƒ†ã‚¹ãƒˆå¤±æ•—ãƒªã‚¹ã‚¯
        if not modifications.get("tests_passed", False):
            risk_factors.append("ãƒ†ã‚¹ãƒˆæœªé€šé")
            risk_score += 0.4

        risk_level = "high" if risk_score >= 0.7 else "medium" if risk_score >= 0.3 else "low"

        return {
            "level": risk_level,
            "score": risk_score,
            "factors": risk_factors
        }

    def _validate_test_results(self, modifications: Dict) -> Dict[str, Any]:
        """ãƒ†ã‚¹ãƒˆçµæœæ¤œè¨¼"""

        tests_passed = modifications.get("tests_passed", False)
        quality_checks = modifications.get("quality_checks", {})

        validation_score = 0.0
        validation_issues = []

        if tests_passed:
            validation_score += 0.5
        else:
            validation_issues.append("ãƒ†ã‚¹ãƒˆå®Ÿè¡Œå¤±æ•—ã¾ãŸã¯æœªå®Ÿè¡Œ")

        # å„å“è³ªãƒã‚§ãƒƒã‚¯ãƒ„ãƒ¼ãƒ«ã®çµæœ
        for tool, result in quality_checks.items():
            if result in ["passed", "available"]:
                validation_score += 0.1
            else:
                validation_issues.append(f"{tool}æ¤œè¨¼å¤±æ•—")

        return {
            "score": min(1.0, validation_score),
            "level": "passed" if validation_score >= 0.6 else "failed",
            "issues": validation_issues
        }

    def _calculate_overall_score(self, quality: Dict, completeness: Dict,
                                risk: Dict, test: Dict) -> float:
        """ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—"""

        # é‡ã¿ä»˜ã‘å¹³å‡
        weights = {
            "quality": 0.3,
            "completeness": 0.3,
            "risk": -0.2,  # ãƒªã‚¹ã‚¯ã¯è² ã®é‡ã¿
            "test": 0.3
        }

        score = (
            quality["score"] * weights["quality"] +
            completeness["completion_rate"] * weights["completeness"] +
            (1.0 - risk["score"]) * abs(weights["risk"]) +
            test["score"] * weights["test"]
        )

        return max(0.0, min(1.0, score))

    def _make_approval_decision(self, score: float, status: str) -> Dict[str, str]:
        """æ‰¿èªåˆ¤å®š"""

        if status != "completed":
            return {"decision": "rejected", "quality_level": "failed"}

        if score >= 0.8:
            return {"decision": "approved", "quality_level": "excellent"}
        elif score >= 0.6:
            return {"decision": "approved_with_conditions", "quality_level": "good"}
        elif score >= 0.4:
            return {"decision": "requires_review", "quality_level": "needs_improvement"}
        else:
            return {"decision": "rejected", "quality_level": "poor"}

    def _generate_followup_actions(self, approval: Dict, quality: Dict,
                                  modifications: Dict) -> Dict[str, Any]:
        """ãƒ•ã‚©ãƒ­ãƒ¼ã‚¢ãƒƒãƒ—ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç”Ÿæˆ"""

        recommendations = []
        required_actions = []
        retry_strategy = None

        decision = approval["decision"]

        if decision == "approved":
            recommendations.extend([
                "é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã§ã®åŒæ§˜ä¿®æ­£ã®å®Ÿæ–½",
                "çµ±åˆãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ",
                "ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®å®Ÿæ–½"
            ])

        elif decision == "approved_with_conditions":
            required_actions.extend([
                "è¿½åŠ ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ",
                "å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ç¢ºèª"
            ])
            recommendations.extend([
                "æ®µéšçš„ãƒªãƒªãƒ¼ã‚¹ã®æ¤œè¨",
                "ç›£è¦–ä½“åˆ¶ã®å¼·åŒ–"
            ])

        elif decision == "requires_review":
            required_actions.extend([
                "æ‰‹å‹•ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®å®Ÿæ–½",
                "å½±éŸ¿ç¯„å›²ã®è©³ç´°ç¢ºèª",
                "ä¿®æ­£å†…å®¹ã®æ¤œè¨¼"
            ])

            retry_strategy = {
                "approach": "manual_review",
                "priority": "high",
                "estimated_time": "30-60åˆ†"
            }

        elif decision == "rejected":
            required_actions.extend([
                "ã‚¨ãƒ©ãƒ¼åŸå› ã®è©³ç´°åˆ†æ",
                "ä¿®æ­£ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®è¦‹ç›´ã—",
                "ã‚¿ã‚¹ã‚¯ã®å†å®Ÿè¡Œ"
            ])

            retry_strategy = {
                "approach": "retry_with_modifications",
                "priority": "immediate",
                "estimated_time": "15-30åˆ†",
                "modifications": [
                    "ã‚ˆã‚Šå…·ä½“çš„ãªæŒ‡ç¤º",
                    "æ®µéšçš„ãªä¿®æ­£ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ",
                    "ã‚¨ãƒ©ãƒ¼å‡¦ç†ã®å¼·åŒ–"
                ]
            }

        # å“è³ªå•é¡Œã«åŸºã¥ãè¿½åŠ æ¨å¥¨
        if quality["level"] == "low":
            recommendations.append("ä¿®æ­£ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è¦‹ç›´ã—")

        return {
            "recommendations": recommendations,
            "required_actions": required_actions,
            "retry_strategy": retry_strategy
        }

    def _assess_modification_scope(self, modifications: Dict) -> str:
        """ä¿®æ­£ç¯„å›²è©•ä¾¡"""

        files_count = len(modifications.get("files_modified", []))
        total_errors = modifications.get("total_errors_fixed", 0)

        if files_count <= 1 and total_errors <= 5:
            return "minimal"
        elif files_count <= 3 and total_errors <= 15:
            return "moderate"
        else:
            return "extensive"

    def _generate_claude_feedback(self, gemini_report: Dict, score: float) -> str:
        """Claude ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ç”Ÿæˆ"""

        feedback = f"""
ğŸ“ Claude å“è³ªãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯

ğŸ¯ ç·åˆè©•ä¾¡: {score:.2f}/1.0

"""

        if score >= 0.8:
            feedback += """âœ… å„ªç§€ãªä¿®æ­£çµæœ
- Flash 2.5ãŒé©åˆ‡ã«æŒ‡ç¤ºã‚’ç†è§£ã—å®Ÿè¡Œ
- æœŸå¾…ã•ã‚Œã‚‹å“è³ªåŸºæº–ã‚’æº€è¶³
- å®‰å…¨ã§ç¢ºå®Ÿãªä¿®æ­£ãŒå®Œäº†"""

        elif score >= 0.6:
            feedback += """âš ï¸ è‰¯å¥½ã ãŒæ”¹å–„ä½™åœ°ã‚ã‚Š
- åŸºæœ¬çš„ãªä¿®æ­£ã¯é©åˆ‡ã«å®Ÿè¡Œ
- ä¸€éƒ¨ã®å“è³ªæŒ‡æ¨™ã§æ”¹å–„ã®ä½™åœ°
- ãƒ•ã‚©ãƒ­ãƒ¼ã‚¢ãƒƒãƒ—ã§ã®å“è³ªå‘ä¸Šã‚’æ¨å¥¨"""

        else:
            feedback += """âŒ ä¿®æ­£å“è³ªã«å•é¡Œ
- Flash 2.5ã®ç†è§£ã¾ãŸã¯å®Ÿè¡Œã«èª²é¡Œ
- æŒ‡ç¤ºã®æ˜ç¢ºåŒ–ã¾ãŸã¯æ‰‹æ³•ã®è¦‹ç›´ã—ãŒå¿…è¦
- ãƒªãƒˆãƒ©ã‚¤ã¾ãŸã¯æ‰‹å‹•ç¢ºèªã‚’æ¨å¥¨"""

        # Geminiãƒ¬ãƒãƒ¼ãƒˆã®è©•ä¾¡
        approach = gemini_report.get("approach", "")
        if approach:
            feedback += f"\n\nğŸ¤– Geminiå®Ÿè¡Œã‚¢ãƒ—ãƒ­ãƒ¼ãƒ: {approach}"

        return feedback

    def _generate_next_recommendations(self, gemini_result: Dict[str, Any], claude_review: Dict[str, Any]) -> List[str]:
        """æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ¨å¥¨ç”Ÿæˆ"""

        recommendations = []

        if claude_review.get("approval") == "approved":
            recommendations.extend([
                "åŒæ§˜ã®ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—ã§æ¬¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ç¾¤ã‚’å‡¦ç†",
                "çµ±åˆãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ",
                "å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æ›´æ–°"
            ])
        else:
            recommendations.extend([
                "ã‚¨ãƒ©ãƒ¼åŸå› ã®è©³ç´°åˆ†æ",
                "ä¿®æ­£ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®è¦‹ç›´ã—",
                "æ‰‹å‹•ç¢ºèªã«ã‚ˆã‚‹å•é¡Œç‰¹å®š"
            ])

        return recommendations

    def _save_cycle_log(self, cycle_result: Dict[str, Any]) -> None:
        """ã‚µã‚¤ã‚¯ãƒ«å®Ÿè¡Œãƒ­ã‚°ä¿å­˜"""

        log_file = self.task_manager.monitoring_dir / f"session_{self.session_id}.json"

        # æ—¢å­˜ãƒ­ã‚°èª­ã¿è¾¼ã¿
        if log_file.exists():
            with open(log_file, 'r', encoding='utf-8') as f:
                session_log = json.load(f)
        else:
            session_log = {
                "session_id": self.session_id,
                "start_time": datetime.datetime.now().isoformat(),
                "cycles": []
            }

        # æ–°ã—ã„ã‚µã‚¤ã‚¯ãƒ«è¿½åŠ 
        session_log["cycles"].append(cycle_result)
        session_log["last_update"] = datetime.datetime.now().isoformat()

        # ä¿å­˜
        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump(session_log, f, indent=2, ensure_ascii=False)

    def _print_session_summary(self, results: List[Dict[str, Any]]) -> None:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ±è¨ˆè¡¨ç¤º"""

        print("\n" + "="*60)
        print("ğŸ“Š ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ±è¨ˆ")
        print("="*60)

        total_cycles = len(results)
        successful_cycles = len([r for r in results if r.get("status") == "completed"])
        total_time = sum(r.get("execution_time", 0) for r in results)

        print(f"ğŸ”„ å®Ÿè¡Œã‚µã‚¤ã‚¯ãƒ«: {total_cycles}å›")
        print(f"âœ… æˆåŠŸã‚µã‚¤ã‚¯ãƒ«: {successful_cycles}å›")
        print(f"â±ï¸ ç·å®Ÿè¡Œæ™‚é–“: {total_time:.1f}ç§’")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {successful_cycles/total_cycles*100:.1f}%" if total_cycles > 0 else "æˆåŠŸç‡: N/A")

        # é€²æ—ãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤º
        progress_report = self.task_manager.generate_progress_report()
        print(f"\nğŸ“‹ ã‚¿ã‚¹ã‚¯é€²æ—:")
        print(f"  å®Œäº†: {progress_report['task_summary']['completed']}ä»¶")
        print(f"  æ®‹ã‚Š: {progress_report['task_summary']['pending']}ä»¶")
        print(f"  ã‚¨ãƒ©ãƒ¼ä¿®æ­£: {progress_report['quality_metrics']['total_errors_fixed']}ä»¶")

    def _estimate_task_tokens(self, task_description: str, target_files: List[str]) -> int:
        """ã‚¿ã‚¹ã‚¯ã®Tokenæ•°ã‚’å‹•çš„æ¨å®š"""

        if self.token_measurement:
            # TokenMeasurementSystemã‚’ä½¿ç”¨ã—ãŸæ¨å®š
            try:
                # ç°¡æ˜“æ¨å®šï¼ˆå®Ÿéš›ã®APIå‘¼ã³å‡ºã—ã¯å¾Œã§æ¸¬å®šï¼‰
                base_tokens = 500
                description_tokens = len(task_description.split()) * 2
                file_tokens = len(target_files) * 200

                estimated_total = base_tokens + description_tokens + file_tokens
                return max(estimated_total, 600)  # æœ€å°600ãƒˆãƒ¼ã‚¯ãƒ³

            except Exception as e:
                print(f"âš ï¸ Tokenæ¨å®šã‚¨ãƒ©ãƒ¼: {e}")

        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®æ¨å®š
        return max(len(task_description) * 3 + len(target_files) * 300, 800)

    def _measure_actual_token_usage(self, result: Dict[str, Any], task_id: str, model: str) -> Dict[str, Any]:
        """å®Ÿéš›ã®Tokenä½¿ç”¨é‡ã‚’æ¸¬å®š"""

        if self.token_measurement:
            try:
                # APIå¿œç­”ã‹ã‚‰Tokenæƒ…å ±ã‚’å–å¾—ï¼ˆã‚‚ã—ã‚ã‚Œã°ï¼‰
                api_response = result.get("api_response", {})

                if api_response:
                    # å®Ÿæ¸¬å®š
                    usage = self.token_measurement.measure_actual_tokens(api_response, model)

                    # ã‚³ã‚¹ãƒˆè¿½è·¡æ›´æ–°
                    self.token_measurement.update_cost_tracking(task_id, usage)

                    return {
                        "input_tokens": usage.input_tokens,
                        "output_tokens": usage.output_tokens,
                        "total_tokens": usage.total_tokens,
                        "cost": usage.cost,
                        "model": usage.model,
                        "measurement_method": "api_response"
                    }
                else:
                    # çµæœã‹ã‚‰æ¨å®šæ¸¬å®š
                    modifications = result.get("modifications", {})
                    files_count = len(modifications.get("files_modified", [])) + len(modifications.get("files_created", []))

                    estimated_input = max(600, files_count * 150)
                    estimated_output = max(400, files_count * 100)
                    cost = self.token_measurement.calculate_real_cost(estimated_input, estimated_output, model)

                    return {
                        "input_tokens": estimated_input,
                        "output_tokens": estimated_output,
                        "total_tokens": estimated_input + estimated_output,
                        "cost": cost,
                        "model": model,
                        "measurement_method": "result_estimation"
                    }

            except Exception as e:
                print(f"âš ï¸ Tokenæ¸¬å®šã‚¨ãƒ©ãƒ¼: {e}")

        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬æ¨å®š
        return {
            "input_tokens": 800,
            "output_tokens": 500,
            "total_tokens": 1300,
            "cost": 0.00225,
            "model": model,
            "measurement_method": "fallback"
        }
        print(f"  ç·ã‚³ã‚¹ãƒˆ: ${progress_report['cost_metrics']['total_cost']:.4f}")

        print("\nğŸ‰ Dual-Agent Workflow ã‚»ãƒƒã‚·ãƒ§ãƒ³å®Œäº†!")

    def run_quality_check(self, target_files: List[str] = None, ai_agent: str = "claude") -> Dict[str, Any]:
        """å“è³ªãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ"""
        # target_filesãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå‡¦ç†
        if target_files is None:
            target_files = []
            print("âš ï¸ å“è³ªãƒã‚§ãƒƒã‚¯: å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ç©ºã®ãƒªã‚¹ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")

        print(f"ğŸ” å“è³ªãƒã‚§ãƒƒã‚¯é–‹å§‹: {len(target_files)}ãƒ•ã‚¡ã‚¤ãƒ«å¯¾è±¡")

        # åŒ…æ‹¬çš„å“è³ªãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ
        metrics = self.quality_manager.run_comprehensive_check(target_files, ai_agent)

        # å“è³ªåŸºæº–ãƒã‚§ãƒƒã‚¯
        quality_passed = self._check_quality_standards(metrics)

        result = {
            "quality_metrics": metrics,
            "quality_passed": quality_passed,
            "recommendations": metrics.improvement_suggestions,
            "quality_level": metrics.quality_level.value
        }

        # å“è³ªåŸºæº–ã‚’æº€ãŸã•ãªã„å ´åˆã®è­¦å‘Š
        if not quality_passed:
            print(f"âš ï¸ å“è³ªåŸºæº–æœªé”æˆ: ã‚¹ã‚³ã‚¢ {metrics.overall_score:.3f}")
            print("ğŸ“ æ”¹å–„ææ¡ˆ:")
            for suggestion in metrics.improvement_suggestions:
                print(f"  - {suggestion}")
        else:
            print(f"âœ… å“è³ªåŸºæº–é”æˆ: ã‚¹ã‚³ã‚¢ {metrics.overall_score:.3f}")

        return result

    def _check_quality_standards(self, metrics: 'QualityMetrics') -> bool:
        """å“è³ªåŸºæº–ãƒã‚§ãƒƒã‚¯ï¼ˆå‹å®‰å…¨æ€§å¼·åŒ–ç‰ˆï¼‰"""

        try:
            # å…¥åŠ›æ¤œè¨¼
            if not hasattr(metrics, 'overall_score') or not hasattr(metrics, 'error_count'):
                print("âš ï¸ ç„¡åŠ¹ãªå“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ")
                return False

            # æœ€ä½å“è³ªåŸºæº– (standards.jsonã‹ã‚‰å–å¾—)
            minimum_score = 0.7
            maximum_errors = 10
            minimum_type_score = 0.8
            minimum_security_score = 0.9

            standards_met = True
            failed_criteria = []

            # ç·åˆã‚¹ã‚³ã‚¢ãƒã‚§ãƒƒã‚¯
            if metrics.overall_score < minimum_score:
                standards_met = False
                failed_criteria.append(f"ç·åˆã‚¹ã‚³ã‚¢ä¸è¶³: {metrics.overall_score:.3f} < {minimum_score}")

            # ã‚¨ãƒ©ãƒ¼æ•°ãƒã‚§ãƒƒã‚¯
            if metrics.error_count > maximum_errors:
                standards_met = False
                failed_criteria.append(f"ã‚¨ãƒ©ãƒ¼æ•°è¶…é: {metrics.error_count} > {maximum_errors}")

            # é‡è¦ãªå“è³ªæŒ‡æ¨™ã®å€‹åˆ¥ãƒã‚§ãƒƒã‚¯
            if hasattr(metrics, 'type_score') and metrics.type_score < minimum_type_score:
                standards_met = False
                failed_criteria.append(f"å‹ãƒã‚§ãƒƒã‚¯ä¸è¶³: {metrics.type_score:.3f} < {minimum_type_score}")

            if hasattr(metrics, 'security_score') and metrics.security_score < minimum_security_score:
                standards_met = False
                failed_criteria.append(f"ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¹ã‚³ã‚¢ä¸è¶³: {metrics.security_score:.3f} < {minimum_security_score}")

            # å¤±æ•—åŸºæº–ã®ãƒ­ã‚°å‡ºåŠ›
            if failed_criteria:
                print(f"âš ï¸ å“è³ªåŸºæº–æœªé”æˆ: {len(failed_criteria)}ä»¶")
                for criteria in failed_criteria[:3]:  # æœ€åˆã®3ä»¶ã®ã¿è¡¨ç¤º
                    print(f"  - {criteria}")
            else:
                print("âœ… å…¨ã¦ã®å“è³ªåŸºæº–ã‚’æº€ãŸã—ã¦ã„ã¾ã™")

            return standards_met

        except Exception as e:
            print(f"âŒ å“è³ªåŸºæº–ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def generate_quality_report(self, format_type: str = "html") -> str:
        """å“è³ªãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        print(f"ğŸ“Š å“è³ªãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­ (å½¢å¼: {format_type})")

        report_path = self.quality_reporter.generate_comprehensive_report(format_type)

        print(f"âœ… å“è³ªãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {report_path}")
        return report_path

    def start_quality_monitoring(self) -> None:
        """å“è³ªç›£è¦–é–‹å§‹ï¼ˆå‹å®‰å…¨æ€§å¼·åŒ–ç‰ˆï¼‰"""

        try:
            print("ğŸ“Š å“è³ªç›£è¦–ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹")

            if not hasattr(self, 'quality_monitor') or self.quality_monitor is None:
                print("âš ï¸ å“è³ªç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
                return

            self.quality_monitor.start_monitoring()

            # ã‚¢ãƒ©ãƒ¼ãƒˆé€šçŸ¥è¨­å®šï¼ˆå‹å®‰å…¨æ€§å¼·åŒ–ï¼‰
            def alert_handler(alert: Any) -> None:
                """Quality alert handler with enhanced error handling"""
                try:
                    if hasattr(alert, 'message'):
                        print(f"ğŸš¨ å“è³ªã‚¢ãƒ©ãƒ¼ãƒˆ: {alert.message}")
                    elif isinstance(alert, str):
                        print(f"ğŸš¨ å“è³ªã‚¢ãƒ©ãƒ¼ãƒˆ: {alert}")
                    else:
                        print(f"ğŸš¨ å“è³ªã‚¢ãƒ©ãƒ¼ãƒˆ: {str(alert)}")
                except Exception as handler_error:
                    print(f"âŒ ã‚¢ãƒ©ãƒ¼ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚¨ãƒ©ãƒ¼: {handler_error}")

            self.quality_monitor.subscribe_to_alerts(alert_handler)
            print("âœ… å“è³ªç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã¨ã‚¢ãƒ©ãƒ¼ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®è¨­å®šãŒå®Œäº†ã—ã¾ã—ãŸ")

        except Exception as e:
            print(f"âŒ å“è³ªç›£è¦–é–‹å§‹ã‚¨ãƒ©ãƒ¼: {e}")
            print("ğŸ”„ æ‰‹å‹•å“è³ªãƒã‚§ãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ã§ç¶™ç¶šã—ã¾ã™")

    def get_quality_status(self) -> Dict[str, Any]:
        """ç¾åœ¨ã®å“è³ªã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹å–å¾—ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–ç‰ˆï¼‰"""

        try:
            status_data = {
                "timestamp": datetime.datetime.now().isoformat(),
                "monitor_status": None,
                "quality_report_data": None,
                "error_status": "none"
            }

            # å“è³ªç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
            try:
                if hasattr(self, 'quality_monitor') and self.quality_monitor is not None:
                    status_data["monitor_status"] = self.quality_monitor.get_current_status()
                else:
                    status_data["monitor_status"] = {"error": "å“è³ªç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“"}
                    status_data["error_status"] = "monitor_unavailable"
            except Exception as monitor_error:
                print(f"âš ï¸ å“è³ªç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹å–å¾—ã‚¨ãƒ©ãƒ¼: {monitor_error}")
                status_data["monitor_status"] = {"error": str(monitor_error)}
                status_data["error_status"] = "monitor_error"

            # å“è³ªãƒ¬ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿
            try:
                if hasattr(self, 'quality_manager') and self.quality_manager is not None:
                    status_data["quality_report_data"] = self.quality_manager.get_quality_report()
                else:
                    status_data["quality_report_data"] = {"error": "å“è³ªãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“"}
                    status_data["error_status"] = "manager_unavailable" if status_data["error_status"] == "none" else "multiple_errors"
            except Exception as manager_error:
                print(f"âš ï¸ å“è³ªãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ãƒ¬ãƒãƒ¼ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {manager_error}")
                status_data["quality_report_data"] = {"error": str(manager_error)}
                status_data["error_status"] = "manager_error" if status_data["error_status"] == "none" else "multiple_errors"

            return status_data

        except Exception as e:
            print(f"âŒ å“è³ªã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return {
                "timestamp": datetime.datetime.now().isoformat(),
                "error": str(e),
                "error_status": "critical_failure"
            }

        return {
            "monitoring_status": monitor_status,
            "quality_report": quality_report_data,
            "session_id": self.session_id
        }

    def run_quality_gate_check(self, target_files: List[str], gate_type: str = "pre_commit") -> bool:
        """å“è³ªã‚²ãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ"""
        print(f"ğŸšª å“è³ªã‚²ãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯: {gate_type}")

        # å“è³ªãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ
        quality_result = self.run_quality_check(target_files, "claude")
        metrics = quality_result["quality_metrics"]

        # ã‚²ãƒ¼ãƒˆç¨®åˆ¥ã«ã‚ˆã‚‹åŸºæº–è¨­å®š
        gate_standards = {
            "pre_commit": {"minimum_score": 0.8, "required_checks": ["syntax", "type_check", "lint", "format"]},
            "pre_push": {"minimum_score": 0.85, "required_checks": ["syntax", "type_check", "lint", "format", "security", "test"]},
            "production": {"minimum_score": 0.9, "required_checks": ["syntax", "type_check", "lint", "format", "security", "performance", "test"]}
        }

        standard = gate_standards.get(gate_type, gate_standards["pre_commit"])

        # åŸºæº–ãƒã‚§ãƒƒã‚¯
        gate_passed = metrics.overall_score >= standard["minimum_score"]

        if gate_passed:
            print(f"âœ… å“è³ªã‚²ãƒ¼ãƒˆé€šé: {gate_type} (ã‚¹ã‚³ã‚¢: {metrics.overall_score:.3f})")
        else:
            print(f"âŒ å“è³ªã‚²ãƒ¼ãƒˆå¤±æ•—: {gate_type} (ã‚¹ã‚³ã‚¢: {metrics.overall_score:.3f})")
            print(f"   å¿…è¦ã‚¹ã‚³ã‚¢: {standard['minimum_score']:.3f}")

        return gate_passed

    def run_integrated_workflow_with_quality(self, target_files: List[str],
                                           error_type: str = "no-untyped-def") -> Dict[str, Any]:
        """å“è³ªç®¡ç†çµ±åˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œ"""
        print("ğŸ”„ å“è³ªç®¡ç†çµ±åˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼é–‹å§‹")

        # 1. äº‹å‰å“è³ªãƒã‚§ãƒƒã‚¯
        print("\nğŸ“‹ Step 1: äº‹å‰å“è³ªãƒã‚§ãƒƒã‚¯")
        pre_quality = self.run_quality_check(target_files, "claude")

        # 2. å“è³ªã‚²ãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯
        print("\nğŸ“‹ Step 2: å“è³ªã‚²ãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯")
        if not self.run_quality_gate_check(target_files, "pre_commit"):
            return {
                "status": "quality_gate_failed",
                "pre_quality": pre_quality,
                "message": "äº‹å‰å“è³ªã‚²ãƒ¼ãƒˆã‚’é€šéã—ã¾ã›ã‚“ã§ã—ãŸ"
            }

        # 3. ã‚¿ã‚¹ã‚¯ä½œæˆãƒ»å®Ÿè¡Œ
        print("\nğŸ“‹ Step 3: ä¿®æ­£ã‚¿ã‚¹ã‚¯å®Ÿè¡Œ")
        task_ids = self.create_mypy_fix_task(
            target_files=target_files,
            error_type=error_type,
            auto_execute=True
        )

        # 4. ä¿®æ­£å¾Œå“è³ªãƒã‚§ãƒƒã‚¯
        print("\nğŸ“‹ Step 4: ä¿®æ­£å¾Œå“è³ªãƒã‚§ãƒƒã‚¯")
        post_quality = self.run_quality_check(target_files, "claude")

        # 5. å“è³ªæ”¹å–„ç¢ºèª
        print("\nğŸ“‹ Step 5: å“è³ªæ”¹å–„ç¢ºèª")
        improvement = post_quality["quality_metrics"].overall_score - pre_quality["quality_metrics"].overall_score

        if improvement > 0:
            print(f"âœ… å“è³ªæ”¹å–„: +{improvement:.3f}")
        else:
            print(f"âš ï¸ å“è³ªæ”¹å–„ãªã—: {improvement:.3f}")

        # 6. æœ€çµ‚å“è³ªã‚²ãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯
        print("\nğŸ“‹ Step 6: æœ€çµ‚å“è³ªã‚²ãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯")
        final_gate_passed = self.run_quality_gate_check(target_files, "pre_push")

        return {
            "status": "completed" if final_gate_passed else "quality_gate_failed",
            "pre_quality": pre_quality,
            "post_quality": post_quality,
            "improvement": improvement,
            "task_ids": task_ids,
            "final_gate_passed": final_gate_passed
        }

    def _create_fallback_implementation_task(self,
                                           file_path: str,
                                           task_type: str,
                                           implementation_spec: Dict[str, Any],
                                           priority: str) -> str:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ç°¡æ˜“å®Ÿè£…ã‚¿ã‚¹ã‚¯ä½œæˆï¼ˆå‹å®‰å…¨æ€§å¼·åŒ–ç‰ˆï¼‰"""

        try:
            # å…¥åŠ›å€¤æ¤œè¨¼
            if not file_path or not isinstance(file_path, str):
                raise ValueError("æœ‰åŠ¹ãªãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ãŒå¿…è¦ã§ã™")

            if not task_type or not isinstance(task_type, str):
                raise ValueError("æœ‰åŠ¹ãªã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ãŒå¿…è¦ã§ã™")

            if not isinstance(implementation_spec, dict):
                print("âš ï¸ å®Ÿè£…ä»•æ§˜ãŒè¾æ›¸å‹ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆä»•æ§˜ã‚’ä½¿ç”¨ã—ã¾ã™")
                implementation_spec = {"template_type": "generic", "complexity": "simple"}

            if priority not in ["low", "medium", "high"]:
                print(f"âš ï¸ ç„¡åŠ¹ãªå„ªå…ˆåº¦'{priority}'ã€‚'medium'ã‚’ä½¿ç”¨ã—ã¾ã™")
                priority = "medium"

            print(f"ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¿ã‚¹ã‚¯ä½œæˆ: {file_path}")
            print(f"   ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—: {task_type}")
            print(f"   å„ªå…ˆåº¦: {priority}")
            print(f"   å®Ÿè£…ä»•æ§˜: {implementation_spec.get('template_type', 'unknown')}")

            # ã‚¿ã‚¹ã‚¯ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®å­˜åœ¨ç¢ºèª
            if not hasattr(self, 'task_manager') or self.task_manager is None:
                raise RuntimeError("ã‚¿ã‚¹ã‚¯ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")

            # æœ€å°é™ã®è¦ä»¶ã§ã‚¿ã‚¹ã‚¯ä½œæˆ
            fallback_task_id = self.task_manager.create_task(
                task_type=f"fallback_{task_type}",
                description=f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ {task_type} - {file_path}",
                target_files=[file_path],
                priority=priority,
                requirements={
                    "implementation_spec": implementation_spec,
                    "task_type": task_type,
                    "fallback_mode": True,
                    "quality_requirements": {
                        "syntax_check": True,
                        "basic_implementation": True,
                        "error_tolerance": "high",  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ™‚ã¯å¯›å®¹
                        "minimum_functionality": True
                    }
                },
                claude_analysis=self._generate_fallback_analysis(file_path, task_type, implementation_spec),
                expected_outcome=f"åŸºæœ¬çš„ãª{task_type}å®Œäº† - {file_path}",
                constraints=[
                    "æœ€å°é™ã®å®Ÿè£…è¦ä»¶",
                    "ã‚¨ãƒ©ãƒ¼æ™‚ã®å®‰å…¨ãªå‡¦ç†",
                    "åŸºæœ¬å“è³ªåŸºæº–éµå®ˆ",
                    "ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å“è³ªåŸºæº–é©ç”¨"
                ],
                context={
                    "fallback_task": True,
                    "original_error": "é€šå¸¸ã‚¿ã‚¹ã‚¯ä½œæˆå¤±æ•—",
                    "session_id": getattr(self, 'session_id', 'unknown'),
                    "fallback_timestamp": datetime.datetime.now().isoformat(),
                    "reduced_quality_expectations": True
                }
            )

            print(f"âœ… ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¿ã‚¹ã‚¯ä½œæˆæˆåŠŸ: {fallback_task_id}")
            return fallback_task_id

        except ValueError as ve:
            print(f"âŒ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¿ã‚¹ã‚¯ä½œæˆå¤±æ•—ï¼ˆå…¥åŠ›å€¤ã‚¨ãƒ©ãƒ¼ï¼‰: {ve}")
            return self._generate_emergency_fallback_task_id("input_validation_error")

        except RuntimeError as re:
            print(f"âŒ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¿ã‚¹ã‚¯ä½œæˆå¤±æ•—ï¼ˆã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ï¼‰: {re}")
            return self._generate_emergency_fallback_task_id("system_error")

        except Exception as e:
            print(f"âŒ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¿ã‚¹ã‚¯ä½œæˆå¤±æ•—ï¼ˆäºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ï¼‰: {e}")
            return self._generate_emergency_fallback_task_id("unexpected_error")

    def _generate_fallback_analysis(self, file_path: str, task_type: str, implementation_spec: Dict[str, Any]) -> str:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨Claudeåˆ†æç”Ÿæˆ"""

        return f"""
ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¿ã‚¹ã‚¯åˆ†æ - {task_type}

ğŸ“„ å¯¾è±¡æƒ…å ±:
- ãƒ•ã‚¡ã‚¤ãƒ«: {file_path}
- ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—: {task_type}
- å®Ÿè£…ä»•æ§˜: {implementation_spec.get('template_type', 'generic')}

ğŸ¯ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ–¹é‡:
- æœ€å°é™ã®æ©Ÿèƒ½å®Ÿè£…ã«ç„¦ç‚¹
- ã‚¨ãƒ©ãƒ¼è€æ€§ã‚’é‡è¦–ã—ãŸè¨­è¨ˆ
- åŸºæœ¬çš„ãªå“è³ªåŸºæº–ã‚’æº€ãŸã™å®Ÿè£…
- æ®µéšçš„ãªæ”¹å–„ãŒå¯èƒ½ãªæ§‹é€ 

âš ï¸ æ³¨æ„äº‹é …:
- é€šå¸¸ã‚¿ã‚¹ã‚¯ä½œæˆãŒå¤±æ•—ã—ãŸãŸã‚ç°¡æ˜“ç‰ˆã‚’å®Ÿè¡Œ
- å“è³ªåŸºæº–ã¯é€šå¸¸ã‚ˆã‚Šç·©å’Œã•ã‚Œã¦ã„ã¾ã™
- å®Ÿè£…å®Œäº†å¾Œã«æ‰‹å‹•ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’æ¨å¥¨
"""

    def _generate_emergency_fallback_task_id(self, error_type: str) -> str:
        """ç·Šæ€¥ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¿ã‚¹ã‚¯IDç”Ÿæˆ"""

        timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
        task_id = f"emergency_fallback_{error_type}_{timestamp}"

        print(f"ğŸ†˜ ç·Šæ€¥ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚¿ã‚¹ã‚¯IDç”Ÿæˆ: {task_id}")
        print("   âš ï¸ ã“ã®ã‚¿ã‚¹ã‚¯ã¯æ‰‹å‹•å‡¦ç†ãŒå¿…è¦ã§ã™")

        return task_id

    def run_three_layer_verification(self, target_files: List[str], context: Dict[str, Any] = None) -> Dict[str, Any]:
        """3å±¤æ¤œè¨¼ä½“åˆ¶å®Ÿè¡Œï¼ˆæ–°è¦å®Ÿè£…ã‚¿ã‚¹ã‚¯å‘ã‘ï¼‰"""

        print("ğŸ”’ 3å±¤æ¤œè¨¼ä½“åˆ¶é–‹å§‹...")
        print(f"ğŸ“ å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {len(target_files)}ä»¶")

        context = context or {}
        verification_start_time = datetime.datetime.now()

        # Layer 1: æ§‹æ–‡æ¤œè¨¼
        print("\n" + "="*50)
        layer1_result = self.quality_manager.validate_syntax(target_files)

        # Layer 2: å“è³ªæ¤œè¨¼
        print("\n" + "="*50)
        layer2_result = self.quality_manager.check_code_quality(target_files, layer1_result)

        # Layer 3: Claudeæœ€çµ‚æ‰¿èª
        print("\n" + "="*50)
        layer3_result = self.quality_manager.claude_final_approval(
            target_files, layer1_result, layer2_result, context
        )

        verification_end_time = datetime.datetime.now()
        verification_duration = str(verification_end_time - verification_start_time)

        # çµ±åˆçµæœä½œæˆ
        overall_result = {
            "verification_type": "three_layer_verification",
            "execution_summary": {
                "start_time": verification_start_time.isoformat(),
                "end_time": verification_end_time.isoformat(),
                "duration": verification_duration,
                "target_files_count": len(target_files)
            },
            "layer_results": {
                "layer1_syntax": layer1_result,
                "layer2_quality": layer2_result,
                "layer3_approval": layer3_result
            },
            "overall_status": {
                "layer1_passed": layer1_result.get("passed", False),
                "layer2_passed": layer2_result.get("passed", False),
                "layer3_approved": layer3_result.get("approved", False),
                "all_layers_passed": (
                    layer1_result.get("passed", False) and
                    layer2_result.get("passed", False) and
                    layer3_result.get("approved", False)
                )
            },
            "quality_metrics": {
                "syntax_score": layer1_result.get("syntax_check", {}).get("score", 0),
                "type_score": layer1_result.get("type_check", {}).get("score", 0),
                "overall_quality_score": layer2_result.get("overall_quality_score", 0),
                "final_score": layer3_result.get("final_metrics", {}).get("overall_score", 0)
            },
            "context": context,
            "recommendations": self._generate_verification_recommendations(
                layer1_result, layer2_result, layer3_result
            ),
            "timestamp": verification_end_time.isoformat()
        }

        # çµæœã‚µãƒãƒªãƒ¼è¡¨ç¤º
        print("\n" + "="*50)
        print("ğŸ¯ 3å±¤æ¤œè¨¼ä½“åˆ¶çµæœã‚µãƒãƒªãƒ¼")
        print("="*50)
        print(f"Layer 1 (æ§‹æ–‡): {'âœ… PASS' if layer1_result.get('passed') else 'âŒ FAIL'}")
        print(f"Layer 2 (å“è³ª): {'âœ… PASS' if layer2_result.get('passed') else 'âŒ FAIL'}")
        print(f"Layer 3 (æ‰¿èª): {'âœ… APPROVED' if layer3_result.get('approved') else 'âŒ REJECTED'}")
        print(f"ç·åˆçµæœ: {'ğŸ‰ å®Œå…¨é€šé' if overall_result['overall_status']['all_layers_passed'] else 'âš ï¸ è¦æ”¹å–„'}")
        print(f"å®Ÿè¡Œæ™‚é–“: {verification_duration}")

        return overall_result

    def _generate_verification_recommendations(self, layer1_result: Dict, layer2_result: Dict, layer3_result: Dict) -> List[str]:
        """3å±¤æ¤œè¨¼çµæœã«åŸºã¥ãæ¨å¥¨äº‹é …ç”Ÿæˆ"""

        recommendations = []

        # Layer 1ã®æ¨å¥¨äº‹é …
        if not layer1_result.get("passed", False):
            syntax_errors = layer1_result.get("syntax_check", {}).get("errors", 0)
            type_errors = layer1_result.get("type_check", {}).get("errors", 0)

            if syntax_errors > 0:
                recommendations.append(f"ğŸ”§ æ§‹æ–‡ã‚¨ãƒ©ãƒ¼{syntax_errors}ä»¶ã®ä¿®æ­£ãŒå¿…è¦")
            if type_errors > 0:
                recommendations.append(f"ğŸ·ï¸ å‹æ³¨é‡ˆã‚¨ãƒ©ãƒ¼{type_errors}ä»¶ã®ä¿®æ­£ãŒå¿…è¦")

        # Layer 2ã®æ¨å¥¨äº‹é …
        if not layer2_result.get("passed", False) and not layer2_result.get("skipped", False):
            quality_score = layer2_result.get("overall_quality_score", 0)
            recommendations.append(f"ğŸ“Š å“è³ªã‚¹ã‚³ã‚¢æ”¹å–„ãŒå¿…è¦ (ç¾åœ¨: {quality_score:.2f}, å¿…è¦: 0.75ä»¥ä¸Š)")

            # å€‹åˆ¥å“è³ªãƒã‚§ãƒƒã‚¯ã®æ¨å¥¨äº‹é …
            quality_checks = layer2_result.get("quality_checks", {})
            for check_name, check_result in quality_checks.items():
                if not check_result.get("passed", True):
                    recommendations.append(f"ğŸ” {check_name}å“è³ªã®æ”¹å–„ãŒå¿…è¦")

        # Layer 3ã®æ¨å¥¨äº‹é …
        if not layer3_result.get("approved", False) and not layer3_result.get("skipped", False):
            final_score = layer3_result.get("final_metrics", {}).get("overall_score", 0)
            recommendations.append(f"ğŸ‘¨â€ğŸ’» Claudeæœ€çµ‚æ‰¿èªåŸºæº–æœªé” (ç¾åœ¨: {final_score:.2f}, å¿…è¦: 0.80ä»¥ä¸Š)")

            # æœ€çµ‚ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æ¨å¥¨äº‹é …
            final_recommendations = layer3_result.get("recommendations", [])
            recommendations.extend(final_recommendations)

        # æˆåŠŸæ™‚ã®æ¨å¥¨äº‹é …
        if (layer1_result.get("passed") and layer2_result.get("passed") and layer3_result.get("approved")):
            recommendations.extend([
                "ğŸ‰ 3å±¤æ¤œè¨¼å®Œå…¨é€šéï¼å“è³ªåŸºæº–ã‚’æº€ãŸã—ã¦ã„ã¾ã™",
                "ğŸ“ˆ ç¶™ç¶šçš„ãªå“è³ªç›£è¦–ã‚’æ¨å¥¨ã—ã¾ã™",
                "ğŸ”„ å®šæœŸçš„ãªãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ã‚’æ¤œè¨ã—ã¦ãã ã•ã„"
            ])

        return recommendations

    # Issue #870: é«˜åº¦é–‹ç™ºã‚·ã‚¹ãƒ†ãƒ çµ±åˆãƒ¡ã‚½ãƒƒãƒ‰

    def execute_complex_task_pattern(
        self,
        target_files: List[str],
        task_description: str,
        coordination_strategy: str = "hybrid"
    ) -> Dict[str, Any]:
        """è¤‡é›‘ã‚¿ã‚¹ã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³å®Ÿè¡Œ - Issue #870å¯¾å¿œ"""

        print(f"ğŸ¯ è¤‡é›‘ã‚¿ã‚¹ã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³å®Ÿè¡Œé–‹å§‹: {len(target_files)}ãƒ•ã‚¡ã‚¤ãƒ«")
        print(f"ğŸ“‹ ã‚¿ã‚¹ã‚¯æ¦‚è¦: {task_description}")
        print(f"ğŸ›ï¸ å”èª¿æˆ¦ç•¥: {coordination_strategy}")

        # 1. ä¾å­˜é–¢ä¿‚è§£æ
        dependency_graph = self.dependency_analyzer.analyze_project_dependencies(target_files)
        print(f"ğŸ” ä¾å­˜é–¢ä¿‚è§£æå®Œäº†: {len(dependency_graph.nodes)}ãƒãƒ¼ãƒ‰, {len(dependency_graph.edges)}ã‚¨ãƒƒã‚¸")

        # 2. ãƒãƒ«ãƒãƒ•ã‚¡ã‚¤ãƒ«å®Ÿè£…è¨ˆç”»ä½œæˆ
        from advanced.multi_file_coordinator import CoordinationStrategy
        strategy_map = {
            "sequential": CoordinationStrategy.SEQUENTIAL,
            "parallel": CoordinationStrategy.PARALLEL,
            "hybrid": CoordinationStrategy.HYBRID,
            "dependency_driven": CoordinationStrategy.DEPENDENCY_DRIVEN
        }

        implementation_plan = self.multi_file_coordinator.create_implementation_plan(
            target_files=target_files,
            strategy=strategy_map.get(coordination_strategy, CoordinationStrategy.HYBRID)
        )
        print(f"ğŸ“‹ å®Ÿè£…è¨ˆç”»ä½œæˆå®Œäº†: {len(implementation_plan.implementation_phases)}ãƒ•ã‚§ãƒ¼ã‚º")

        # 3. æ€§èƒ½è§£æãƒ»æœ€é©åŒ–æ©Ÿä¼šæ¤œå‡º
        performance_issues = self.performance_optimizer.analyze_project_performance(target_files)
        total_issues = sum(len(issues) for issues in performance_issues.values())
        print(f"âš¡ æ€§èƒ½è§£æå®Œäº†: {total_issues}ä»¶ã®æœ€é©åŒ–æ©Ÿä¼š")

        # 4. ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°æ©Ÿä¼šæ¤œå‡º
        refactoring_analysis = self.refactoring_engine.analyze_project(target_files)
        total_refactoring_opportunities = sum(
            len(opportunities) for _, opportunities in refactoring_analysis.values()
        )
        print(f"ğŸ”§ ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°è§£æå®Œäº†: {total_refactoring_opportunities}ä»¶ã®æ”¹å–„æ©Ÿä¼š")

        # 5. å®Ÿè¡Œçµæœçµ±åˆ
        execution_result = {
            "status": "completed",
            "target_files": target_files,
            "coordination_strategy": coordination_strategy,
            "dependency_analysis": {
                "nodes": len(dependency_graph.nodes),
                "edges": len(dependency_graph.edges),
                "cycles": len(dependency_graph.cycles),
                "implementation_levels": max(dependency_graph.levels.values()) if dependency_graph.levels else 0
            },
            "implementation_plan": {
                "phases": len(implementation_plan.implementation_phases),
                "parallel_groups": len(implementation_plan.parallel_groups),
                "estimated_duration": implementation_plan.estimated_duration
            },
            "performance_analysis": {
                "total_issues": total_issues,
                "critical_issues": sum(
                    len([issue for issue in issues if issue.severity == "critical"])
                    for issues in performance_issues.values()
                )
            },
            "refactoring_analysis": {
                "total_opportunities": total_refactoring_opportunities,
                "high_priority": sum(
                    len([op for _, ops in analysis.values() for op in ops if op.priority.value == "high"])
                    for analysis in [refactoring_analysis]
                )
            }
        }

        print(f"âœ… è¤‡é›‘ã‚¿ã‚¹ã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³å®Ÿè¡Œå®Œäº†")
        return execution_result

    def apply_design_pattern(
        self,
        pattern_name: str,
        target_classes: List[str],
        output_path: Optional[str] = None
    ) -> Dict[str, Any]:
        """è¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³è‡ªå‹•å®Ÿè£… - Issue #870å¯¾å¿œ"""

        print(f"ğŸ¨ è¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³å®Ÿè£…é–‹å§‹: {pattern_name}")
        print(f"ğŸ¯ å¯¾è±¡ã‚¯ãƒ©ã‚¹: {target_classes}")

        try:
            from advanced.pattern_implementation_engine import DesignPattern, PatternCategory, PatternRequirement

            # ãƒ‘ã‚¿ãƒ¼ãƒ³åã‹ã‚‰Enumã«å¤‰æ›
            pattern_map = {
                "factory": DesignPattern.FACTORY,
                "observer": DesignPattern.OBSERVER,
                "strategy": DesignPattern.STRATEGY,
                "singleton": DesignPattern.SINGLETON,
                "decorator": DesignPattern.DECORATOR
            }

            pattern = pattern_map.get(pattern_name.lower())
            if not pattern:
                return {"status": "error", "message": f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ‘ã‚¿ãƒ¼ãƒ³: {pattern_name}"}

            # ãƒ‘ã‚¿ãƒ¼ãƒ³è¦ä»¶å®šç¾©
            requirements = PatternRequirement(
                pattern=pattern,
                category=PatternCategory.CREATIONAL if pattern in [DesignPattern.FACTORY, DesignPattern.SINGLETON] else PatternCategory.BEHAVIORAL,
                target_classes=target_classes,
                interfaces=[target_classes[0]] if target_classes else [],
                methods=["execute", "operation"],
                properties=[],
                solid_principles=["SRP", "OCP", "DIP"],
                complexity_level=3,
                estimated_effort=4
            )

            # ãƒ‘ã‚¿ãƒ¼ãƒ³å®Ÿè£…
            implementation = self.pattern_engine.implement_pattern(pattern, requirements, output_path)

            # å®Ÿè£…ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            self.pattern_engine.save_implementation(implementation)

            result = {
                "status": "success",
                "pattern": pattern_name,
                "file_path": implementation.file_path,
                "quality_score": implementation.quality_score,
                "solid_compliance": implementation.solid_compliance,
                "classes_generated": len(implementation.classes),
                "methods_generated": len(implementation.methods)
            }

            print(f"âœ… ãƒ‘ã‚¿ãƒ¼ãƒ³å®Ÿè£…å®Œäº†: {implementation.file_path}")
            print(f"â­ å“è³ªã‚¹ã‚³ã‚¢: {implementation.quality_score:.2f}")

            return result

        except Exception as e:
            error_result = {"status": "error", "message": str(e)}
            print(f"âŒ ãƒ‘ã‚¿ãƒ¼ãƒ³å®Ÿè£…ã‚¨ãƒ©ãƒ¼: {e}")
            return error_result

    def analyze_and_optimize_performance(
        self,
        target_files: List[str],
        auto_apply: bool = False
    ) -> Dict[str, Any]:
        """æ€§èƒ½è§£æãƒ»æœ€é©åŒ–å®Ÿè¡Œ - Issue #870å¯¾å¿œ"""

        print(f"âš¡ æ€§èƒ½è§£æãƒ»æœ€é©åŒ–é–‹å§‹: {len(target_files)}ãƒ•ã‚¡ã‚¤ãƒ«")

        # æ€§èƒ½è§£æå®Ÿè¡Œ
        analysis_results = self.performance_optimizer.analyze_project_performance(target_files)

        optimization_results = []

        for file_path, issues in analysis_results.items():
            if issues:
                print(f"ğŸ“„ æœ€é©åŒ–å¯¾è±¡: {os.path.relpath(file_path)}")

                # é«˜å„ªå…ˆåº¦å•é¡Œã‚’è‡ªå‹•æœ€é©åŒ–
                high_priority_issues = [
                    issue for issue in issues
                    if issue.severity in ["critical", "high"]
                ]

                if high_priority_issues or auto_apply:
                    optimization_result = self.performance_optimizer.apply_optimizations(
                        file_path, issues, auto_apply
                    )
                    optimization_results.append(optimization_result)

                    print(f"  âœ… æœ€é©åŒ–å®Œäº†: {optimization_result.improvement_percentage:.1%}æ”¹å–„")

        # æ€§èƒ½ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report = self.performance_optimizer.generate_performance_report(analysis_results)
        report_path = f"tmp/performance_report_{int(time.time())}.md"
        os.makedirs("tmp", exist_ok=True)
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)

        total_issues = sum(len(issues) for issues in analysis_results.values())
        total_optimizations = len(optimization_results)

        result = {
            "status": "completed",
            "analyzed_files": len(target_files),
            "total_issues": total_issues,
            "applied_optimizations": total_optimizations,
            "report_path": report_path,
            "optimization_summary": [
                {
                    "file": getattr(result, 'original_file', 'unknown'),
                    "improvement": getattr(result, 'improvement_percentage', 0.0),
                    "time_reduction": getattr(result, 'execution_time_reduction', 0.0),
                    "memory_reduction": getattr(result, 'memory_reduction', 0.0)
                }
                for result in optimization_results
            ]
        }

        print(f"âœ… æ€§èƒ½è§£æãƒ»æœ€é©åŒ–å®Œäº†")
        print(f"ğŸ“Š è§£æãƒ•ã‚¡ã‚¤ãƒ«: {len(target_files)}")
        print(f"ğŸ” æ¤œå‡ºå•é¡Œ: {total_issues}ä»¶")
        print(f"âš¡ é©ç”¨æœ€é©åŒ–: {total_optimizations}ä»¶")
        print(f"ğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆ: {report_path}")

        return result

    def execute_comprehensive_code_improvement(
        self,
        target_files: List[str],
        include_patterns: bool = True,
        include_refactoring: bool = True,
        include_performance: bool = True
    ) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„ã‚³ãƒ¼ãƒ‰æ”¹å–„å®Ÿè¡Œ - Issue #870çµ±åˆæ©Ÿèƒ½"""

        print(f"ğŸš€ åŒ…æ‹¬çš„ã‚³ãƒ¼ãƒ‰æ”¹å–„é–‹å§‹: {len(target_files)}ãƒ•ã‚¡ã‚¤ãƒ«")
        print(f"ğŸ¨ ãƒ‘ã‚¿ãƒ¼ãƒ³é©ç”¨: {'æœ‰åŠ¹' if include_patterns else 'ç„¡åŠ¹'}")
        print(f"ğŸ”§ ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°: {'æœ‰åŠ¹' if include_refactoring else 'ç„¡åŠ¹'}")
        print(f"âš¡ æ€§èƒ½æœ€é©åŒ–: {'æœ‰åŠ¹' if include_performance else 'ç„¡åŠ¹'}")

        improvement_results = {
            "status": "completed",
            "target_files": target_files,
            "improvements_applied": [],
            "reports_generated": [],
            "quality_metrics": {}
        }

        # 1. ä¾å­˜é–¢ä¿‚è§£æï¼ˆå¿…é ˆï¼‰
        dependency_analysis = self.dependency_analyzer.analyze_project_dependencies(target_files)
        improvement_results["dependency_analysis"] = {
            "files_analyzed": len(dependency_analysis.nodes),
            "dependencies_found": len(dependency_analysis.edges),
            "circular_dependencies": len(dependency_analysis.cycles)
        }
        print(f"ğŸ” ä¾å­˜é–¢ä¿‚è§£æå®Œäº†: {len(dependency_analysis.cycles)}ä»¶ã®å¾ªç’°ä¾å­˜")

        # 2. ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°è§£æãƒ»é©ç”¨
        if include_refactoring:
            refactoring_analysis = self.refactoring_engine.analyze_project(target_files)

            for file_path, (metrics, opportunities) in refactoring_analysis.items():
                high_confidence_ops = [
                    op for op in opportunities
                    if op.confidence_score > 0.8
                ]

                if high_confidence_ops:
                    refactoring_result = self.refactoring_engine.apply_refactorings(
                        file_path, high_confidence_ops, auto_apply=True
                    )
                    improvement_results["improvements_applied"].append({
                        "type": "refactoring",
                        "file": file_path,
                        "applied_count": len(refactoring_result.applied_refactorings),
                        "quality_improvement": refactoring_result.quality_improvement
                    })

            # ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            refactoring_report = self.refactoring_engine.generate_refactoring_report(refactoring_analysis)
            refactoring_report_path = f"tmp/refactoring_report_{int(time.time())}.md"
            with open(refactoring_report_path, 'w', encoding='utf-8') as f:
                f.write(refactoring_report)
            improvement_results["reports_generated"].append(refactoring_report_path)

            print(f"ğŸ”§ ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°å®Œäº†: ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ {refactoring_report_path}")

        # 3. æ€§èƒ½æœ€é©åŒ–
        if include_performance:
            performance_result = self.analyze_and_optimize_performance(target_files, auto_apply=True)
            improvement_results["improvements_applied"].append({
                "type": "performance",
                "total_issues": performance_result["total_issues"],
                "optimizations": performance_result["applied_optimizations"]
            })
            improvement_results["reports_generated"].append(performance_result["report_path"])

            print(f"âš¡ æ€§èƒ½æœ€é©åŒ–å®Œäº†: {performance_result['applied_optimizations']}ä»¶é©ç”¨")

        # 4. è¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³é©ç”¨ææ¡ˆ
        if include_patterns:
            pattern_opportunities = self.pattern_engine.analyze_code_for_patterns(target_files[0]) if target_files else []

            improvement_results["pattern_opportunities"] = len(pattern_opportunities)
            if pattern_opportunities:
                print(f"ğŸ¨ è¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³æ©Ÿä¼š: {len(pattern_opportunities)}ä»¶æ¤œå‡º")

        # 5. å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        improvement_results["quality_metrics"] = {
            "total_improvements": len(improvement_results["improvements_applied"]),
            "reports_generated": len(improvement_results["reports_generated"]),
            "dependency_health": "good" if len(dependency_analysis.cycles) == 0 else "needs_attention"
        }

        print(f"âœ… åŒ…æ‹¬çš„ã‚³ãƒ¼ãƒ‰æ”¹å–„å®Œäº†")
        print(f"ğŸ“Š ç·æ”¹å–„é …ç›®: {improvement_results['quality_metrics']['total_improvements']}")
        print(f"ğŸ“„ ç”Ÿæˆãƒ¬ãƒãƒ¼ãƒˆ: {improvement_results['quality_metrics']['reports_generated']}ä»¶")

        return improvement_results

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    import argparse

    parser = argparse.ArgumentParser(description="Dual-Agent Workflow Coordinator")
    parser.add_argument("--mode", choices=["single", "continuous"], default="single",
                       help="å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ (single: 1ã‚µã‚¤ã‚¯ãƒ«, continuous: é€£ç¶šå®Ÿè¡Œ)")
    parser.add_argument("--max-cycles", type=int, default=10,
                       help="é€£ç¶šå®Ÿè¡Œæ™‚ã®æœ€å¤§ã‚µã‚¤ã‚¯ãƒ«æ•°")
    parser.add_argument("--create-test-task", action="store_true",
                       help="ãƒ†ã‚¹ãƒˆç”¨ã‚¿ã‚¹ã‚¯ã‚’ä½œæˆ")

    args = parser.parse_args()

    coordinator = DualAgentCoordinator()

    if args.create_test_task:
        # ãƒ†ã‚¹ãƒˆç”¨ã‚¿ã‚¹ã‚¯ä½œæˆ
        test_files = ["kumihan_formatter/core/utilities/logger.py"]  # ä¾‹
        task_id = coordinator.create_mypy_fix_task(
            target_files=test_files,
            error_type="no-untyped-def",
            priority="high"
        )
        print(f"âœ… ãƒ†ã‚¹ãƒˆã‚¿ã‚¹ã‚¯ä½œæˆ: {task_id}")
        return

    if args.mode == "single":
        # å˜ä¸€ã‚µã‚¤ã‚¯ãƒ«å®Ÿè¡Œ
        result = coordinator.execute_workflow_cycle()
        print(f"\nğŸ“Š å®Ÿè¡Œçµæœ: {result['status']}")

    elif args.mode == "continuous":
        # é€£ç¶šå®Ÿè¡Œ
        results = coordinator.run_continuous_workflow(max_cycles=args.max_cycles)
        print(f"\nğŸ“Š é€£ç¶šå®Ÿè¡Œå®Œäº†: {len(results)}ã‚µã‚¤ã‚¯ãƒ«")

if __name__ == "__main__":
    main()
