#!/usr/bin/env python3
"""
Dual-Agent Workflow Coordinator
Claude Code â†” Gemini CLI å”æ¥­çµ±åˆã‚·ã‚¹ãƒ†ãƒ 
"""

import json
import os
import subprocess
import time
from typing import Dict, List, Any, Optional
from pathlib import Path
import datetime

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‹ã‚‰postboxãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import sys
sys.path.append(str(Path(__file__).parent.parent))

from utils.task_manager import TaskManager
from utils.gemini_helper import GeminiHelper
from utils.task_analyzer import TaskAnalyzer
from templates.flash_templates import Flash25Templates
from core.workflow_decision_engine import WorkflowDecisionEngine, AutomationLevel
from quality.quality_manager import QualityManager
from monitoring.quality_monitor import QualityMonitor
from reporting.quality_reporter import QualityReporter

class DualAgentCoordinator:
    """Claude â†” Geminiå”æ¥­ã®çµ±åˆã‚³ãƒ¼ãƒ‡ã‚£ãƒãƒ¼ã‚¿ãƒ¼"""

    def __init__(self):
        self.task_manager = TaskManager()
        self.gemini_helper = GeminiHelper()
        self.task_analyzer = TaskAnalyzer()
        self.flash_templates = Flash25Templates()
        self.decision_engine = WorkflowDecisionEngine()
        self.quality_manager = QualityManager()
        self.quality_monitor = QualityMonitor()
        self.quality_reporter = QualityReporter()
        self.session_id = f"session_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}"

        print(f"ğŸ¤– Dual-Agent Workflow é–‹å§‹ (Flash 2.5 æœ€é©åŒ–)")
        print(f"ğŸ“‹ ã‚»ãƒƒã‚·ãƒ§ãƒ³ID: {self.session_id}")
        print(f"ğŸ”„ Claude Code â†” Gemini CLI å”æ¥­ã‚·ã‚¹ãƒ†ãƒ ")
        print(f"ğŸ§  ã‚¿ã‚¹ã‚¯å¾®åˆ†åŒ–ã‚¨ãƒ³ã‚¸ãƒ³: æœ‰åŠ¹")
        print(f"ğŸ“ Flash 2.5 ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ: æœ‰åŠ¹")
        print(f"ğŸ¯ è‡ªå‹•åˆ¤å®šã‚¨ãƒ³ã‚¸ãƒ³: æœ‰åŠ¹")
        print(f"ğŸ” çµ±åˆå“è³ªç®¡ç†: æœ‰åŠ¹")
        print(f"ğŸ“Š å“è³ªç›£è¦–ã‚·ã‚¹ãƒ†ãƒ : æœ‰åŠ¹")

    def create_mypy_fix_task(self,
                           target_files: List[str],
                           error_type: str = "no-untyped-def",
                           priority: str = "high",
                           use_micro_tasks: bool = True,
                           force_mode: str = None,
                           auto_execute: bool = True) -> List[str]:
        """mypyä¿®æ­£ã‚¿ã‚¹ã‚¯ä½œæˆï¼ˆè‡ªå‹•åˆ¤å®šãƒ»å®Ÿè¡Œæ©Ÿèƒ½ä»˜ãï¼‰"""

        print(f"ğŸ” ã‚¿ã‚¹ã‚¯ä½œæˆé–‹å§‹: {error_type} ã‚¨ãƒ©ãƒ¼ä¿®æ­£")
        print(f"ğŸ“ å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {len(target_files)}ä»¶")
        print(f"ğŸ§  å¾®åˆ†åŒ–ãƒ¢ãƒ¼ãƒ‰: {'æœ‰åŠ¹' if use_micro_tasks else 'ç„¡åŠ¹'}")

        # ===== è‡ªå‹•åˆ¤å®šãƒ•ã‚§ãƒ¼ã‚º =====
        task_description = f"{error_type} ã‚¨ãƒ©ãƒ¼ä¿®æ­£ - {len(target_files)}ãƒ•ã‚¡ã‚¤ãƒ«"

        # ã‚¿ã‚¹ã‚¯åˆ†æå®Ÿè¡Œ
        task_analysis = self.decision_engine.analyze_task(
            task_description, target_files, error_type,
            context={"priority": priority, "session_id": self.session_id}
        )

        # Geminiä½¿ç”¨åˆ¤å®š
        user_prefs = {"force_mode": force_mode} if force_mode else {}
        decision = self.decision_engine.make_decision(task_analysis, user_prefs)

        print(f"\nğŸ¯ è‡ªå‹•åˆ¤å®šçµæœ:")
        print(f"   Geminiä½¿ç”¨: {'ã¯ã„' if decision.use_gemini else 'ã„ã„ãˆ'}")
        print(f"   è‡ªå‹•åŒ–ãƒ¬ãƒ™ãƒ«: {decision.automation_level.value}")
        print(f"   æ¨å®šã‚³ã‚¹ãƒˆ: ${decision.task_analysis.estimated_cost:.4f}")
        print(f"   åŠ¹æœã‚¹ã‚³ã‚¢: {decision.task_analysis.gemini_benefit_score:.2f}")
        print(f"   ç†ç”±: {decision.reasoning}")

        # ===== å®Ÿè¡Œæ–¹å¼æ±ºå®š =====
        if decision.use_gemini:
            print(f"\nğŸš€ Geminiå”æ¥­ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œ")
            created_task_ids = self._create_with_gemini_mode(
                target_files, error_type, priority, use_micro_tasks, decision
            )
        else:
            print(f"\nğŸ§  Claudeå˜ç‹¬ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œ")
            created_task_ids = self._create_with_claude_mode(
                target_files, error_type, priority, decision
            )

        # ===== è‡ªå‹•å®Ÿè¡Œåˆ¤å®š =====
        if auto_execute and decision.automation_level in [AutomationLevel.FULL_AUTO, AutomationLevel.SEMI_AUTO]:
            print(f"\nâš¡ è‡ªå‹•å®Ÿè¡Œé–‹å§‹ï¼ˆãƒ¬ãƒ™ãƒ«: {decision.automation_level.value}ï¼‰")

            if decision.automation_level == AutomationLevel.SEMI_AUTO:
                # é‡è¦ãªå¤‰æ›´ã®å ´åˆã®ã¿ç¢ºèª
                if decision.task_analysis.risk_level == "high" or decision.task_analysis.estimated_cost > 0.005:
                    print("âš ï¸ é‡è¦ãªå¤‰æ›´ã®ãŸã‚æ‰‹å‹•ç¢ºèªæ¨å¥¨")
                else:
                    self._auto_execute_tasks(created_task_ids, decision)
            else:
                self._auto_execute_tasks(created_task_ids, decision)

        elif decision.automation_level == AutomationLevel.APPROVAL_REQUIRED:
            print(f"\nğŸ¤š æ‰¿èªå¿…é ˆ: å®Ÿè¡Œå‰ã«ãƒ¦ãƒ¼ã‚¶ãƒ¼ç¢ºèªãŒå¿…è¦")
            print(f"   å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰: coordinator.execute_workflow_cycle()")

        print(f"\nâœ… ã‚¿ã‚¹ã‚¯ä½œæˆå®Œäº†: {len(created_task_ids)}ä»¶")
        return created_task_ids

    def _create_with_gemini_mode(self, target_files: List[str], error_type: str,
                                priority: str, use_micro_tasks: bool, decision) -> List[str]:
        """Geminiå”æ¥­ãƒ¢ãƒ¼ãƒ‰ã§ã®ã‚¿ã‚¹ã‚¯ä½œæˆ"""

        created_task_ids = []

        for file_path in target_files:
            print(f"\nğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æ: {file_path}")

            if use_micro_tasks:
                # ã‚¿ã‚¹ã‚¯å¾®åˆ†åŒ–: ãƒ•ã‚¡ã‚¤ãƒ« â†’ é–¢æ•°ãƒ¬ãƒ™ãƒ«åˆ†å‰²
                micro_tasks = self.task_analyzer.analyze_file_for_micro_tasks(file_path, error_type)

                if micro_tasks:
                    # å®Ÿè¡Œè¨ˆç”»ä½œæˆ
                    execution_plan = self.task_analyzer.create_step_by_step_plan(micro_tasks)

                    print(f"ğŸ¯ å¾®ç´°ã‚¿ã‚¹ã‚¯ç”Ÿæˆ: {len(micro_tasks)}ä»¶")
                    print(f"â±ï¸ ç·æ‰€è¦æ™‚é–“: {execution_plan['total_estimated_time']}åˆ†")

                    # ãƒãƒƒãƒå‡¦ç†ã§ã‚¿ã‚¹ã‚¯ä½œæˆ
                    for batch in execution_plan['batch_recommendations']:
                        batch_task_id = self._create_batch_task(
                            batch, file_path, error_type, priority, execution_plan
                        )
                        created_task_ids.append(batch_task_id)

                else:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ•ã‚¡ã‚¤ãƒ«å…¨ä½“ã‚¿ã‚¹ã‚¯
                    fallback_task_id = self._create_file_level_task(
                        file_path, error_type, priority
                    )
                    created_task_ids.append(fallback_task_id)
            else:
                # å¾“æ¥æ–¹å¼: ãƒ•ã‚¡ã‚¤ãƒ«å˜ä½å‡¦ç†
                file_task_id = self._create_file_level_task(file_path, error_type, priority)
                created_task_ids.append(file_task_id)

        return created_task_ids

    def _create_with_claude_mode(self, target_files: List[str], error_type: str,
                                priority: str, decision) -> List[str]:
        """Claudeå˜ç‹¬ãƒ¢ãƒ¼ãƒ‰ã§ã®ã‚¿ã‚¹ã‚¯ä½œæˆ"""

        print("ğŸ“ Claudeå˜ç‹¬ãƒ¢ãƒ¼ãƒ‰ã§ã‚¿ã‚¹ã‚¯ä½œæˆï¼ˆGeminiä½¿ç”¨ãªã—ï¼‰")

        # ç°¡æ˜“ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¬ãƒ™ãƒ«ã‚¿ã‚¹ã‚¯ã®ã¿ä½œæˆ
        created_task_ids = []
        for file_path in target_files:
            task_id = self._create_claude_only_task(file_path, error_type, priority, decision)
            created_task_ids.append(task_id)

        return created_task_ids

    def _create_claude_only_task(self, file_path: str, error_type: str,
                                priority: str, decision) -> str:
        """Claudeå˜ç‹¬ã‚¿ã‚¹ã‚¯ä½œæˆ"""

        claude_analysis = self._claude_analyze_files([file_path], error_type)

        task_id = self.task_manager.create_task(
            task_type="claude_only_modification",
            description=f"{error_type} Claudeå˜ç‹¬ä¿®æ­£ - {file_path}",
            target_files=[file_path],
            priority=priority,
            requirements={
                "error_type": error_type,
                "fix_pattern": self._get_fix_pattern(error_type),
                "claude_only": True,
                "decision_reasoning": decision.reasoning
            },
            claude_analysis=claude_analysis,
            expected_outcome=f"{error_type} ã‚¨ãƒ©ãƒ¼ã‚’Claudeå˜ç‹¬ã§ä¿®æ­£",
            constraints=[
                "Geminiä½¿ç”¨ãªã—",
                "Claude Codeæ¨™æº–æ‰‹æ³•ã®ã¿ä½¿ç”¨",
                "æ®µéšçš„ãƒ»ç¢ºå®Ÿãªä¿®æ­£"
            ],
            context={
                "decision_engine_result": decision.task_analysis.__dict__,
                "automation_level": decision.automation_level.value,
                "session_id": self.session_id
            }
        )

        print(f"ğŸ“„ Claudeå˜ç‹¬ã‚¿ã‚¹ã‚¯ä½œæˆ: {file_path}")
        return task_id

    def _auto_execute_tasks(self, task_ids: List[str], decision) -> None:
        """è‡ªå‹•å®Ÿè¡Œ"""

        print(f"âš¡ è‡ªå‹•å®Ÿè¡Œé–‹å§‹: {len(task_ids)}ã‚¿ã‚¹ã‚¯")

        start_time = time.time()
        results = []

        for i, task_id in enumerate(task_ids, 1):
            print(f"\nğŸ”„ è‡ªå‹•å®Ÿè¡Œ {i}/{len(task_ids)}: {task_id}")

            try:
                result = self.execute_workflow_cycle()
                results.append(result)

                # å®Ÿè¡Œçµæœãƒã‚§ãƒƒã‚¯
                if result.get("status") != "completed":
                    print(f"âš ï¸ ã‚¿ã‚¹ã‚¯ {task_id} å®Ÿè¡Œå¤±æ•—ã€è‡ªå‹•å®Ÿè¡Œã‚’åœæ­¢")
                    break

                # Claudeå“è³ªãƒ¬ãƒ“ãƒ¥ãƒ¼
                claude_review = result.get("claude_review", {})
                approval = claude_review.get("approval", "unknown")

                if approval == "rejected":
                    print(f"âŒ ã‚¿ã‚¹ã‚¯ {task_id} å“è³ªä¸åˆæ ¼ã€è‡ªå‹•å®Ÿè¡Œã‚’åœæ­¢")
                    break
                elif approval == "requires_review":
                    print(f"ğŸ¤š ã‚¿ã‚¹ã‚¯ {task_id} æ‰‹å‹•ãƒ¬ãƒ“ãƒ¥ãƒ¼å¿…è¦ã€è‡ªå‹•å®Ÿè¡Œã‚’ä¸€æ™‚åœæ­¢")
                    break

                print(f"âœ… ã‚¿ã‚¹ã‚¯ {task_id} è‡ªå‹•å®Ÿè¡ŒæˆåŠŸ ({approval})")

            except Exception as e:
                print(f"âŒ ã‚¿ã‚¹ã‚¯ {task_id} å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
                break

        execution_time = time.time() - start_time

        # è‡ªå‹•å®Ÿè¡Œã‚µãƒãƒªãƒ¼
        successful = len([r for r in results if r.get("status") == "completed"])
        total_cost = sum(r.get("claude_review", {}).get("quality_metrics", {}).get("errors_fixed", 0)
                        for r in results) * decision.task_analysis.estimated_cost / max(1, len(task_ids))

        print(f"\nğŸ“Š è‡ªå‹•å®Ÿè¡Œã‚µãƒãƒªãƒ¼:")
        print(f"   æˆåŠŸ: {successful}/{len(task_ids)}ã‚¿ã‚¹ã‚¯")
        print(f"   å®Ÿè¡Œæ™‚é–“: {execution_time:.1f}ç§’")
        print(f"   æ¨å®šã‚³ã‚¹ãƒˆ: ${total_cost:.4f}")
        print(f"   åŠ¹ç‡æ€§: {decision.task_analysis.gemini_benefit_score:.2f}")

    def get_decision_stats(self) -> Dict[str, Any]:
        """åˆ¤å®šçµ±è¨ˆæƒ…å ±å–å¾—"""
        return self.decision_engine.get_decision_stats()

    def set_automation_preferences(self, preferences: Dict[str, Any]) -> None:
        """è‡ªå‹•åŒ–è¨­å®šå¤‰æ›´"""

        # WorkflowDecisionEngineè¨­å®šæ›´æ–°
        self.decision_engine.thresholds.update(preferences.get("thresholds", {}))

        print(f"âš™ï¸ è‡ªå‹•åŒ–è¨­å®šæ›´æ–°: {preferences}")

    def _create_batch_task(self, batch: Dict[str, Any], file_path: str,
                          error_type: str, priority: str, execution_plan: Dict) -> str:
        """ãƒãƒƒãƒã‚¿ã‚¹ã‚¯ä½œæˆï¼ˆFlash 2.5æœ€é©åŒ–ï¼‰"""

        batch_id = batch['batch_id']
        tasks = batch['tasks']

        # Flash 2.5å‘ã‘å…·ä½“çš„æŒ‡ç¤ºç”Ÿæˆ
        flash_instruction = self._generate_batch_flash_instruction(tasks, error_type)

        # Claude ã«ã‚ˆã‚‹è©³ç´°åˆ†æ
        claude_analysis = self._claude_analyze_micro_tasks(tasks, error_type, file_path)

        task_id = self.task_manager.create_task(
            task_type="micro_code_modification",
            description=f"{error_type} ãƒãƒƒãƒä¿®æ­£ - {file_path} Batch#{batch_id}",
            target_files=[file_path],
            priority=priority,
            requirements={
                "error_type": error_type,
                "batch_info": batch,
                "micro_tasks": tasks,
                "flash_instruction": flash_instruction,
                "max_context_tokens": 2000,
                "step_by_step": True
            },
            claude_analysis=claude_analysis,
            expected_outcome=f"ãƒãƒƒãƒå†…å…¨ã‚¿ã‚¹ã‚¯å®Œäº† ({len(tasks)}ä»¶)",
            constraints=[
                "Flash 2.5ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ¶é™éµå®ˆ",
                "1ãƒãƒƒãƒ20åˆ†ä»¥å†…",
                "å…·ä½“çš„æŒ‡ç¤ºã«å¾“ã£ãŸä¿®æ­£ã®ã¿"
            ],
            context={
                "batch_id": batch_id,
                "total_batches": len(execution_plan['batch_recommendations']),
                "execution_plan": execution_plan['flash25_optimization'],
                "session_id": self.session_id
            }
        )

        print(f"ğŸ“¦ ãƒãƒƒãƒã‚¿ã‚¹ã‚¯ä½œæˆ: Batch#{batch_id} ({len(tasks)}ä»¶, {batch['estimated_time']}åˆ†)")
        return task_id

    def _create_file_level_task(self, file_path: str, error_type: str, priority: str) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¬ãƒ™ãƒ«ã‚¿ã‚¹ã‚¯ä½œæˆï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""

        # å¾“æ¥æ–¹å¼ã®åˆ†æ
        claude_analysis = self._claude_analyze_files([file_path], error_type)

        task_id = self.task_manager.create_task(
            task_type="file_code_modification",
            description=f"{error_type} ãƒ•ã‚¡ã‚¤ãƒ«ä¿®æ­£ - {file_path}",
            target_files=[file_path],
            priority=priority,
            requirements={
                "error_type": error_type,
                "fix_pattern": self._get_fix_pattern(error_type),
                "test_required": True,
                "quality_check": True
            },
            claude_analysis=claude_analysis,
            expected_outcome=f"{error_type} ã‚¨ãƒ©ãƒ¼ã‚’0ä»¶ã«å‰Šæ¸›",
            constraints=[
                "æ—¢å­˜æ©Ÿèƒ½ã¸ã®å½±éŸ¿æœ€å°åŒ–",
                "ãƒ†ã‚¹ãƒˆé€šéå¿…é ˆ",
                "pre-commit hooksé€šéå¿…é ˆ"
            ],
            context={
                "fallback_mode": True,
                "session_id": self.session_id
            }
        )

        print(f"ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¹ã‚¯ä½œæˆ: {file_path}")
        return task_id

    def _generate_batch_flash_instruction(self, tasks: List[Dict], error_type: str) -> str:
        """ãƒãƒƒãƒç”¨Flash 2.5æŒ‡ç¤ºç”Ÿæˆ"""

        template = self.flash_templates.get_template(error_type)
        base_instruction = template.get("flash_instruction_template", "")

        # ãƒãƒƒãƒå›ºæœ‰ã®æŒ‡ç¤º
        batch_instruction = f"""
ğŸš€ Flash 2.5 ãƒãƒƒãƒä¿®æ­£æŒ‡ç¤º

ğŸ“¦ ãƒãƒƒãƒæ¦‚è¦:
- ä¿®æ­£å¯¾è±¡: {len(tasks)}å€‹ã®é–¢æ•°
- ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: {template.get('name', error_type)}
- é›£æ˜“åº¦: {template.get('difficulty', 'medium')}

{base_instruction}

ğŸ“‹ ä¿®æ­£å¯¾è±¡ãƒªã‚¹ãƒˆ:
"""

        for i, task in enumerate(tasks, 1):
            func_name = task.get('target_function', 'unknown')
            error_count = task.get('error_count', 0)
            batch_instruction += f"{i}. {func_name}é–¢æ•° ({error_count}ã‚¨ãƒ©ãƒ¼)\n"

        batch_instruction += f"""
âš¡ Flash 2.5 æœ€é©åŒ–ãƒ«ãƒ¼ãƒ«:
1. ä¸Šè¨˜ãƒªã‚¹ãƒˆã®é †ç•ªã§1ã¤ãšã¤ä¿®æ­£
2. å„é–¢æ•°ã®ä¿®æ­£å®Œäº†å¾Œã€æ¬¡ã®é–¢æ•°ã¸
3. åˆ†ã‹ã‚‰ãªã„å ´åˆã¯ type: ignore ä½¿ç”¨
4. ä¿®æ­£æ™‚é–“: 1é–¢æ•°ã‚ãŸã‚Šæœ€å¤§5åˆ†

ğŸ¯ æˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³:
- ä¾‹ã«å¾“ã£ãŸæ­£ç¢ºãªä¿®æ­£
- æ—¢å­˜ã‚³ãƒ¼ãƒ‰ã®æœ€å°å¤‰æ›´
- æ®µéšçš„ãªé€²è¡Œ
"""

        return batch_instruction

    def _claude_analyze_micro_tasks(self, tasks: List[Dict], error_type: str, file_path: str) -> str:
        """å¾®ç´°ã‚¿ã‚¹ã‚¯ç”¨Claudeåˆ†æ"""

        analysis = f"""
ğŸ“Š Claudeå¾®ç´°ã‚¿ã‚¹ã‚¯åˆ†æ - {error_type}

ğŸ¯ å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {file_path}
ğŸ”§ ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: {error_type}
ğŸ“¦ ãƒãƒƒãƒã‚µã‚¤ã‚º: {len(tasks)}ä»¶

ğŸ§  åˆ†æçµæœ:
"""

        for task in tasks:
            func_name = task.get('target_function', 'unknown')
            complexity = task.get('complexity', 'medium')
            error_count = task.get('error_count', 0)

            analysis += f"""
  â€¢ {func_name}é–¢æ•°:
    - ã‚¨ãƒ©ãƒ¼æ•°: {error_count}ä»¶
    - è¤‡é›‘åº¦: {complexity}
    - æ¨å®šæ™‚é–“: {task.get('estimated_time', 10)}åˆ†
    - FlashæŒ‡ç¤º: é©ç”¨æ¸ˆã¿
"""

        analysis += f"""
âš ï¸ Flash 2.5 è€ƒæ…®äº‹é …:
- ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ¶é™: 2000ãƒˆãƒ¼ã‚¯ãƒ³ä»¥å†…
- å…·ä½“çš„ä¾‹ç¤ºã«ã‚ˆã‚‹æŒ‡ç¤º
- æ®µéšçš„å®Ÿè¡Œã«ã‚ˆã‚‹ç¢ºå®Ÿæ€§
- ã‚¨ãƒ©ãƒ¼æ™‚ã®é©åˆ‡ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

ğŸ“‹ æ¨å¥¨ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ:
1. é–¢æ•°å˜ä½ã§ã®é€æ¬¡å‡¦ç†
2. ä¿®æ­£ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å³æ ¼é©ç”¨
3. å„ã‚¹ãƒ†ãƒƒãƒ—ã§ã®æ¤œè¨¼
4. äºˆæœŸã—ãªã„çŠ¶æ³ã§ã®type: ignoreä½¿ç”¨
"""

        return analysis

    def execute_workflow_cycle(self) -> Dict[str, Any]:
        """1ã‚µã‚¤ã‚¯ãƒ«ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œ"""

        print("\nğŸ”„ ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ ã‚µã‚¤ã‚¯ãƒ«é–‹å§‹")

        # Step 1: Claude - æ¬¡ã®ã‚¿ã‚¹ã‚¯ã‚’å–å¾—
        task_data = self.gemini_helper.get_next_task()

        if not task_data:
            print("ğŸ“­ å®Ÿè¡Œå¾…ã¡ã‚¿ã‚¹ã‚¯ãªã—")
            return {"status": "no_tasks", "message": "å®Ÿè¡Œå¾…ã¡ã‚¿ã‚¹ã‚¯ãŒã‚ã‚Šã¾ã›ã‚“"}

        print(f"ğŸ“‹ å®Ÿè¡Œã‚¿ã‚¹ã‚¯: {task_data['task_id']}")
        print(f"ğŸ“ èª¬æ˜: {task_data['description']}")

        # Step 2: Gemini - ã‚¿ã‚¹ã‚¯å®Ÿè¡Œ
        print("\nğŸš€ Gemini CLI ã§ã‚¿ã‚¹ã‚¯å®Ÿè¡Œä¸­...")

        start_time = time.time()

        # Gemini CLIçµŒç”±ã§ã‚¿ã‚¹ã‚¯å®Ÿè¡Œ
        gemini_result = self._execute_with_gemini(task_data)

        execution_time = time.time() - start_time

        # Step 3: Claude - çµæœãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ»æ¤œè¨¼
        print("\nğŸ” Claude ã«ã‚ˆã‚‹çµæœãƒ¬ãƒ“ãƒ¥ãƒ¼...")

        claude_review = self._claude_review_result(gemini_result)

        # Step 4: çµ±åˆãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
        cycle_result = {
            "session_id": self.session_id,
            "cycle_timestamp": datetime.datetime.now().isoformat(),
            "task_executed": task_data,
            "gemini_result": gemini_result,
            "claude_review": claude_review,
            "execution_time": execution_time,
            "status": "completed",
            "next_recommendations": self._generate_next_recommendations(gemini_result, claude_review)
        }

        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ­ã‚°ä¿å­˜
        self._save_cycle_log(cycle_result)

        print(f"\nâœ… ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚µã‚¤ã‚¯ãƒ«å®Œäº† ({execution_time:.1f}ç§’)")

        return cycle_result

    def run_continuous_workflow(self, max_cycles: int = 10) -> List[Dict[str, Any]]:
        """é€£ç¶šãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œ"""

        print(f"\nğŸ”„ é€£ç¶šãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼é–‹å§‹ (æœ€å¤§{max_cycles}ã‚µã‚¤ã‚¯ãƒ«)")

        results = []

        for cycle in range(max_cycles):
            print(f"\n" + "="*60)
            print(f"ğŸ”„ Cycle {cycle + 1}/{max_cycles}")
            print("="*60)

            cycle_result = self.execute_workflow_cycle()
            results.append(cycle_result)

            # ã‚¿ã‚¹ã‚¯ãŒãªã„å ´åˆã¯çµ‚äº†
            if cycle_result["status"] == "no_tasks":
                print(f"\nğŸ å…¨ã‚¿ã‚¹ã‚¯å®Œäº† (å®Ÿè¡Œã‚µã‚¤ã‚¯ãƒ«: {cycle + 1})")
                break

            # çŸ­ã„ä¼‘æ¯
            if cycle < max_cycles - 1:
                print("â³ æ¬¡ã®ã‚µã‚¤ã‚¯ãƒ«ã¾ã§3ç§’å¾…æ©Ÿ...")
                time.sleep(3)

        # æœ€çµ‚çµ±è¨ˆ
        self._print_session_summary(results)

        return results

    def _claude_analyze_files(self, target_files: List[str], error_type: str) -> str:
        """Claude ã«ã‚ˆã‚‹è©³ç´°äº‹å‰åˆ†æ"""

        print(f"ğŸ§  Claudeåˆ†æé–‹å§‹: {error_type} ã‚¨ãƒ©ãƒ¼")

        analysis_results = []
        total_errors = 0
        complexity_factors = []

        for file_path in target_files:
            if not os.path.exists(file_path):
                continue

            # ãƒ•ã‚¡ã‚¤ãƒ«è©³ç´°åˆ†æ
            file_analysis = self._analyze_file_complexity(file_path, error_type)
            analysis_results.append(file_analysis)
            total_errors += file_analysis.get("error_count", 0)
            complexity_factors.extend(file_analysis.get("complexity_factors", []))

        # ç·åˆåˆ†æ
        overall_complexity = self._assess_overall_complexity(analysis_results, error_type)
        risk_assessment = self._assess_modification_risk(analysis_results)
        recommended_strategy = self._recommend_fix_strategy(error_type, overall_complexity, total_errors)

        analysis = f"""
ğŸ“Š Claude è©³ç´°äº‹å‰åˆ†æ - {error_type} ã‚¨ãƒ©ãƒ¼ä¿®æ­£

ğŸ¯ å¯¾è±¡ç¯„å›²:
- ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(target_files)}ä»¶
- ç·ã‚¨ãƒ©ãƒ¼æ•°: {total_errors}ä»¶
- ä¿®æ­£è¤‡é›‘åº¦: {overall_complexity['level']} ({overall_complexity['score']}/10)
- æ¨å®šæ‰€è¦æ™‚é–“: {overall_complexity['estimated_time']}åˆ†

ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥åˆ†æ:
{self._format_file_analysis(analysis_results)}

âš ï¸ ãƒªã‚¹ã‚¯è©•ä¾¡:
{self._format_risk_assessment(risk_assessment)}

ğŸ¯ æ¨å¥¨æˆ¦ç•¥:
{recommended_strategy}

ğŸ”§ Flash 2.5 æœ€é©åŒ–:
- å¾®ç´°ã‚¿ã‚¹ã‚¯åˆ†å‰²: {'æ¨å¥¨' if total_errors > 10 else 'ä¸è¦'}
- ãƒãƒƒãƒã‚µã‚¤ã‚º: {min(3, max(1, total_errors // 5))}ã‚¿ã‚¹ã‚¯/ãƒãƒƒãƒ
- ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ¶é™: 2000ãƒˆãƒ¼ã‚¯ãƒ³/ã‚¿ã‚¹ã‚¯

ğŸ“‹ æˆåŠŸè¦å› :
1. æ®µéšçš„ãƒ»ç¢ºå®Ÿãªä¿®æ­£ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
2. å„ã‚¹ãƒ†ãƒƒãƒ—ã§ã®å“è³ªç¢ºèª
3. æ—¢å­˜æ©Ÿèƒ½ã¸ã®å½±éŸ¿æœ€å°åŒ–
4. é©åˆ‡ãªãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
"""

        return analysis

    def _analyze_file_complexity(self, file_path: str, error_type: str) -> Dict[str, Any]:
        """ãƒ•ã‚¡ã‚¤ãƒ«è¤‡é›‘åº¦åˆ†æ"""

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            # åŸºæœ¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹
            lines = content.split('\n')
            line_count = len(lines)
            function_count = content.count('def ')
            class_count = content.count('class ')

            # ã‚¨ãƒ©ãƒ¼æ•°ã‚«ã‚¦ãƒ³ãƒˆ
            error_count = self._count_file_errors(file_path, error_type)

            # è¤‡é›‘åº¦è¦å› ç‰¹å®š
            complexity_factors = []
            if line_count > 500:
                complexity_factors.append("å¤§è¦æ¨¡ãƒ•ã‚¡ã‚¤ãƒ«")
            if function_count > 20:
                complexity_factors.append("å¤šæ•°ã®é–¢æ•°")
            if class_count > 5:
                complexity_factors.append("è¤‡æ•°ã‚¯ãƒ©ã‚¹")
            if 'import' in content and content.count('import') > 10:
                complexity_factors.append("å¤šæ•°ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ")
            if 'typing' in content:
                complexity_factors.append("æ—¢å­˜å‹æ³¨é‡ˆ")

            # è¤‡é›‘åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—
            complexity_score = min(10, (
                line_count // 100 +
                function_count // 5 +
                class_count * 2 +
                error_count // 3
            ))

            return {
                "file_path": file_path,
                "line_count": line_count,
                "function_count": function_count,
                "class_count": class_count,
                "error_count": error_count,
                "complexity_score": complexity_score,
                "complexity_factors": complexity_factors,
                "estimated_time": max(5, error_count * 2 + complexity_score)
            }

        except Exception as e:
            return {
                "file_path": file_path,
                "error": str(e),
                "complexity_score": 5,
                "estimated_time": 10
            }

    def _count_file_errors(self, file_path: str, error_type: str) -> int:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¨ãƒ©ãƒ¼æ•°ã‚«ã‚¦ãƒ³ãƒˆ"""
        try:
            result = subprocess.run(
                ["python3", "-m", "mypy", "--strict", file_path],
                capture_output=True,
                text=True
            )

            error_count = 0
            for line in result.stdout.split('\n'):
                if error_type in line and 'error:' in line:
                    error_count += 1

            return error_count
        except:
            return 0

    def _assess_overall_complexity(self, analysis_results: List[Dict], error_type: str) -> Dict[str, Any]:
        """å…¨ä½“è¤‡é›‘åº¦è©•ä¾¡"""

        if not analysis_results:
            return {"level": "ä½", "score": 1, "estimated_time": 10}

        avg_score = sum(a.get("complexity_score", 5) for a in analysis_results) / len(analysis_results)
        total_time = sum(a.get("estimated_time", 10) for a in analysis_results)

        if avg_score <= 3:
            level = "ä½"
        elif avg_score <= 6:
            level = "ä¸­"
        else:
            level = "é«˜"

        return {
            "level": level,
            "score": round(avg_score, 1),
            "estimated_time": total_time
        }

    def _assess_modification_risk(self, analysis_results: List[Dict]) -> Dict[str, Any]:
        """ä¿®æ­£ãƒªã‚¹ã‚¯è©•ä¾¡"""

        risk_factors = []
        mitigation_strategies = []

        for analysis in analysis_results:
            factors = analysis.get("complexity_factors", [])

            if "å¤§è¦æ¨¡ãƒ•ã‚¡ã‚¤ãƒ«" in factors:
                risk_factors.append("å¤§è¦æ¨¡ãƒ•ã‚¡ã‚¤ãƒ«ä¿®æ­£ã«ã‚ˆã‚‹å½±éŸ¿ç¯„å›²æ‹¡å¤§")
                mitigation_strategies.append("é–¢æ•°å˜ä½ã§ã®æ®µéšçš„ä¿®æ­£")

            if "æ—¢å­˜å‹æ³¨é‡ˆ" in factors:
                risk_factors.append("æ—¢å­˜å‹æ³¨é‡ˆã¨ã®æ•´åˆæ€§å•é¡Œ")
                mitigation_strategies.append("æ—¢å­˜æ³¨é‡ˆã®è©³ç´°ç¢ºèª")

            if "è¤‡æ•°ã‚¯ãƒ©ã‚¹" in factors:
                risk_factors.append("ã‚¯ãƒ©ã‚¹é–“ä¾å­˜é–¢ä¿‚ã¸ã®å½±éŸ¿")
                mitigation_strategies.append("ã‚¯ãƒ©ã‚¹åˆ¥ä¿®æ­£ãƒ»ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")

        risk_level = "é«˜" if len(risk_factors) > 3 else "ä¸­" if len(risk_factors) > 1 else "ä½"

        return {
            "level": risk_level,
            "factors": risk_factors,
            "mitigation": mitigation_strategies
        }

    def _recommend_fix_strategy(self, error_type: str, complexity: Dict, total_errors: int) -> str:
        """ä¿®æ­£æˆ¦ç•¥æ¨å¥¨"""

        strategy = f"""
ğŸ¯ {error_type} ä¿®æ­£æˆ¦ç•¥:

"""

        if total_errors <= 5:
            strategy += "ã€ã‚·ãƒ³ãƒ—ãƒ«æˆ¦ç•¥ã€‘\n- ãƒ•ã‚¡ã‚¤ãƒ«å˜ä½ã§ã®ä¸€æ‹¬ä¿®æ­£\n- æ¨™æº–çš„ãªä¿®æ­£ãƒ‘ã‚¿ãƒ¼ãƒ³é©ç”¨"
        elif total_errors <= 20:
            strategy += "ã€ãƒãƒ©ãƒ³ã‚¹æˆ¦ç•¥ã€‘\n- é–¢æ•°å˜ä½ã§ã®æ®µéšçš„ä¿®æ­£\n- ãƒãƒƒãƒå‡¦ç†ã«ã‚ˆã‚‹åŠ¹ç‡åŒ–"
        else:
            strategy += "ã€å¾®ç´°åˆ†å‰²æˆ¦ç•¥ã€‘\n- é–¢æ•°ãƒ¬ãƒ™ãƒ«ã§ã®ç´°åˆ†åŒ–\n- Flash 2.5æœ€é©åŒ–ãƒãƒƒãƒå‡¦ç†"

        if complexity["level"] == "é«˜":
            strategy += "\n\nâš ï¸ é«˜è¤‡é›‘åº¦å¯¾å¿œ:\n- è©³ç´°ãªäº‹å‰ãƒ†ã‚¹ãƒˆ\n- ä¿®æ­£å¾Œã®å…¨ä½“å½±éŸ¿ç¢ºèª\n- æ®µéšçš„ãƒªãƒªãƒ¼ã‚¹æ¨å¥¨"

        return strategy

    def _format_file_analysis(self, analysis_results: List[Dict]) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æçµæœãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""

        formatted = ""
        for analysis in analysis_results[:5]:  # æœ€å¤§5ä»¶è¡¨ç¤º
            path = analysis.get("file_path", "unknown")
            errors = analysis.get("error_count", 0)
            score = analysis.get("complexity_score", 0)
            time = analysis.get("estimated_time", 0)

            formatted += f"  â€¢ {path}\n"
            formatted += f"    ã‚¨ãƒ©ãƒ¼: {errors}ä»¶, è¤‡é›‘åº¦: {score}/10, æ™‚é–“: {time}åˆ†\n"

        if len(analysis_results) > 5:
            formatted += f"  ... ä»–{len(analysis_results) - 5}ãƒ•ã‚¡ã‚¤ãƒ«\n"

        return formatted

    def _format_risk_assessment(self, risk_assessment: Dict) -> str:
        """ãƒªã‚¹ã‚¯è©•ä¾¡ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""

        level = risk_assessment["level"]
        factors = risk_assessment["factors"]
        mitigation = risk_assessment["mitigation"]

        formatted = f"ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«: {level}\n"

        if factors:
            formatted += "ä¸»è¦ãƒªã‚¹ã‚¯è¦å› :\n"
            for factor in factors[:3]:
                formatted += f"  - {factor}\n"

        if mitigation:
            formatted += "æ¨å¥¨å¯¾ç­–:\n"
            for strategy in mitigation[:3]:
                formatted += f"  + {strategy}\n"

        return formatted

    def _get_fix_pattern(self, error_type: str) -> str:
        """ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—åˆ¥ã®ä¿®æ­£ãƒ‘ã‚¿ãƒ¼ãƒ³"""
        patterns = {
            "no-untyped-def": "add_return_type_annotations",
            "no-untyped-call": "add_type_ignore_or_stubs",
            "type-arg": "add_generic_type_parameters",
            "call-arg": "fix_function_arguments",
            "attr-defined": "fix_attribute_access"
        }
        return patterns.get(error_type, "generic_fix")

    def _execute_with_gemini(self, task_data: Dict[str, Any]) -> Dict[str, Any]:
        """Gemini CLI ã§ã®ã‚¿ã‚¹ã‚¯å®Ÿè¡Œ"""

        try:
            # Gemini HelperçµŒç”±ã§å®Ÿè¡Œ
            result = self.gemini_helper.execute_task(task_data)

            # ã‚³ã‚¹ãƒˆè¿½è·¡
            token_usage = {
                "input_tokens": 1500,  # æ¨å®šå€¤
                "output_tokens": 800,   # æ¨å®šå€¤
                "model": "gemini-2.5-flash"
            }

            self.task_manager.track_cost(task_data["task_id"], token_usage)

            return result

        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "task_id": task_data["task_id"]
            }

    def _claude_review_result(self, gemini_result: Dict[str, Any]) -> Dict[str, Any]:
        """Claude ã«ã‚ˆã‚‹è©³ç´°çµæœãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ»å“è³ªè©•ä¾¡"""

        print("ğŸ” Claudeå“è³ªãƒ¬ãƒ“ãƒ¥ãƒ¼é–‹å§‹")

        status = gemini_result.get("status", "unknown")
        modifications = gemini_result.get("modifications", {})
        gemini_report = gemini_result.get("gemini_report", {})

        # å¤šè§’çš„å“è³ªè©•ä¾¡
        quality_assessment = self._assess_code_quality(modifications)
        completeness_check = self._check_completeness(modifications, gemini_result)
        risk_evaluation = self._evaluate_modification_risk(modifications)
        test_validation = self._validate_test_results(modifications)

        # ç·åˆåˆ¤å®š
        overall_score = self._calculate_overall_score(
            quality_assessment, completeness_check, risk_evaluation, test_validation
        )

        approval_decision = self._make_approval_decision(overall_score, status)

        # ãƒªãƒˆãƒ©ã‚¤ãƒ»ãƒ•ã‚©ãƒ­ãƒ¼ã‚¢ãƒƒãƒ—ææ¡ˆ
        followup_actions = self._generate_followup_actions(
            approval_decision, quality_assessment, modifications
        )

        review = {
            "overall_quality": approval_decision["quality_level"],
            "approval": approval_decision["decision"],
            "confidence_score": overall_score,
            "detailed_assessment": {
                "code_quality": quality_assessment,
                "completeness": completeness_check,
                "risk_evaluation": risk_evaluation,
                "test_validation": test_validation
            },
            "quality_metrics": {
                "errors_fixed": modifications.get("total_errors_fixed", 0),
                "files_modified": len(modifications.get("files_modified", [])),
                "modification_scope": self._assess_modification_scope(modifications),
                "regression_risk": risk_evaluation.get("level", "medium")
            },
            "recommendations": followup_actions["recommendations"],
            "required_actions": followup_actions["required_actions"],
            "retry_strategy": followup_actions.get("retry_strategy"),
            "claude_feedback": self._generate_claude_feedback(gemini_report, overall_score)
        }

        print(f"ğŸ“Š ãƒ¬ãƒ“ãƒ¥ãƒ¼å®Œäº†: {approval_decision['decision']} (ä¿¡é ¼åº¦: {overall_score:.2f})")

        return review

    def _assess_code_quality(self, modifications: Dict) -> Dict[str, Any]:
        """ã‚³ãƒ¼ãƒ‰å“è³ªè©•ä¾¡"""

        files_modified = modifications.get("files_modified", [])
        quality_checks = modifications.get("quality_checks", {})

        quality_score = 0.0
        quality_issues = []
        quality_positives = []

        # ä¿®æ­£å†…å®¹ã®è³ªè©•ä¾¡
        for file_mod in files_modified:
            errors_fixed = file_mod.get("errors_fixed", 0)
            changes = file_mod.get("changes", "")

            if errors_fixed > 0:
                quality_positives.append(f"{file_mod['file']}: {errors_fixed}ã‚¨ãƒ©ãƒ¼ä¿®æ­£")
                quality_score += 0.2

            if "type annotation" in changes or "type: ignore" in changes:
                quality_positives.append("é©åˆ‡ãªå‹å‡¦ç†")
                quality_score += 0.1

        # å“è³ªãƒã‚§ãƒƒã‚¯çµæœè©•ä¾¡
        for tool, result in quality_checks.items():
            if result == "passed" or result == "available":
                quality_score += 0.15
            else:
                quality_issues.append(f"{tool}ãƒã‚§ãƒƒã‚¯å¤±æ•—")

        # ãƒ†ã‚¹ãƒˆé€šéçŠ¶æ³
        if modifications.get("tests_passed", False):
            quality_positives.append("ãƒ†ã‚¹ãƒˆå…¨é€šé")
            quality_score += 0.3
        else:
            quality_issues.append("ãƒ†ã‚¹ãƒˆæœªé€šéã¾ãŸã¯æœªå®Ÿè¡Œ")

        return {
            "score": min(1.0, quality_score),
            "level": "high" if quality_score >= 0.7 else "medium" if quality_score >= 0.4 else "low",
            "positives": quality_positives,
            "issues": quality_issues
        }

    def _check_completeness(self, modifications: Dict, gemini_result: Dict) -> Dict[str, Any]:
        """å®Œäº†åº¦ãƒã‚§ãƒƒã‚¯"""

        task_data = gemini_result.get("task_executed", {})
        requirements = task_data.get("requirements", {})
        expected_fixes = requirements.get("error_count", 0) or 1
        actual_fixes = modifications.get("total_errors_fixed", 0)

        completion_rate = actual_fixes / max(1, expected_fixes)

        completeness_issues = []
        if completion_rate < 0.8:
            completeness_issues.append(f"ä¿®æ­£å®Œäº†ç‡ä½: {completion_rate:.1%}")

        if not modifications.get("quality_checks"):
            completeness_issues.append("å“è³ªãƒã‚§ãƒƒã‚¯æœªå®Ÿè¡Œ")

        return {
            "completion_rate": completion_rate,
            "level": "complete" if completion_rate >= 0.9 else "partial" if completion_rate >= 0.6 else "incomplete",
            "expected_fixes": expected_fixes,
            "actual_fixes": actual_fixes,
            "issues": completeness_issues
        }

    def _evaluate_modification_risk(self, modifications: Dict) -> Dict[str, Any]:
        """ä¿®æ­£ãƒªã‚¹ã‚¯è©•ä¾¡"""

        files_modified = modifications.get("files_modified", [])

        risk_factors = []
        risk_score = 0.0

        for file_mod in files_modified:
            lines_changed = file_mod.get("lines_changed", 0)
            file_path = file_mod.get("file", "")

            if lines_changed > 50:
                risk_factors.append(f"å¤§è¦æ¨¡ä¿®æ­£: {file_path}")
                risk_score += 0.3

            if "core" in file_path or "main" in file_path:
                risk_factors.append(f"é‡è¦ãƒ•ã‚¡ã‚¤ãƒ«ä¿®æ­£: {file_path}")
                risk_score += 0.2

        # ãƒ†ã‚¹ãƒˆå¤±æ•—ãƒªã‚¹ã‚¯
        if not modifications.get("tests_passed", False):
            risk_factors.append("ãƒ†ã‚¹ãƒˆæœªé€šé")
            risk_score += 0.4

        risk_level = "high" if risk_score >= 0.7 else "medium" if risk_score >= 0.3 else "low"

        return {
            "level": risk_level,
            "score": risk_score,
            "factors": risk_factors
        }

    def _validate_test_results(self, modifications: Dict) -> Dict[str, Any]:
        """ãƒ†ã‚¹ãƒˆçµæœæ¤œè¨¼"""

        tests_passed = modifications.get("tests_passed", False)
        quality_checks = modifications.get("quality_checks", {})

        validation_score = 0.0
        validation_issues = []

        if tests_passed:
            validation_score += 0.5
        else:
            validation_issues.append("ãƒ†ã‚¹ãƒˆå®Ÿè¡Œå¤±æ•—ã¾ãŸã¯æœªå®Ÿè¡Œ")

        # å„å“è³ªãƒã‚§ãƒƒã‚¯ãƒ„ãƒ¼ãƒ«ã®çµæœ
        for tool, result in quality_checks.items():
            if result in ["passed", "available"]:
                validation_score += 0.1
            else:
                validation_issues.append(f"{tool}æ¤œè¨¼å¤±æ•—")

        return {
            "score": min(1.0, validation_score),
            "level": "passed" if validation_score >= 0.6 else "failed",
            "issues": validation_issues
        }

    def _calculate_overall_score(self, quality: Dict, completeness: Dict,
                                risk: Dict, test: Dict) -> float:
        """ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—"""

        # é‡ã¿ä»˜ã‘å¹³å‡
        weights = {
            "quality": 0.3,
            "completeness": 0.3,
            "risk": -0.2,  # ãƒªã‚¹ã‚¯ã¯è² ã®é‡ã¿
            "test": 0.3
        }

        score = (
            quality["score"] * weights["quality"] +
            completeness["completion_rate"] * weights["completeness"] +
            (1.0 - risk["score"]) * abs(weights["risk"]) +
            test["score"] * weights["test"]
        )

        return max(0.0, min(1.0, score))

    def _make_approval_decision(self, score: float, status: str) -> Dict[str, str]:
        """æ‰¿èªåˆ¤å®š"""

        if status != "completed":
            return {"decision": "rejected", "quality_level": "failed"}

        if score >= 0.8:
            return {"decision": "approved", "quality_level": "excellent"}
        elif score >= 0.6:
            return {"decision": "approved_with_conditions", "quality_level": "good"}
        elif score >= 0.4:
            return {"decision": "requires_review", "quality_level": "needs_improvement"}
        else:
            return {"decision": "rejected", "quality_level": "poor"}

    def _generate_followup_actions(self, approval: Dict, quality: Dict,
                                  modifications: Dict) -> Dict[str, Any]:
        """ãƒ•ã‚©ãƒ­ãƒ¼ã‚¢ãƒƒãƒ—ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç”Ÿæˆ"""

        recommendations = []
        required_actions = []
        retry_strategy = None

        decision = approval["decision"]

        if decision == "approved":
            recommendations.extend([
                "é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã§ã®åŒæ§˜ä¿®æ­£ã®å®Ÿæ–½",
                "çµ±åˆãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ",
                "ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®å®Ÿæ–½"
            ])

        elif decision == "approved_with_conditions":
            required_actions.extend([
                "è¿½åŠ ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ",
                "å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ç¢ºèª"
            ])
            recommendations.extend([
                "æ®µéšçš„ãƒªãƒªãƒ¼ã‚¹ã®æ¤œè¨",
                "ç›£è¦–ä½“åˆ¶ã®å¼·åŒ–"
            ])

        elif decision == "requires_review":
            required_actions.extend([
                "æ‰‹å‹•ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®å®Ÿæ–½",
                "å½±éŸ¿ç¯„å›²ã®è©³ç´°ç¢ºèª",
                "ä¿®æ­£å†…å®¹ã®æ¤œè¨¼"
            ])

            retry_strategy = {
                "approach": "manual_review",
                "priority": "high",
                "estimated_time": "30-60åˆ†"
            }

        elif decision == "rejected":
            required_actions.extend([
                "ã‚¨ãƒ©ãƒ¼åŸå› ã®è©³ç´°åˆ†æ",
                "ä¿®æ­£ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®è¦‹ç›´ã—",
                "ã‚¿ã‚¹ã‚¯ã®å†å®Ÿè¡Œ"
            ])

            retry_strategy = {
                "approach": "retry_with_modifications",
                "priority": "immediate",
                "estimated_time": "15-30åˆ†",
                "modifications": [
                    "ã‚ˆã‚Šå…·ä½“çš„ãªæŒ‡ç¤º",
                    "æ®µéšçš„ãªä¿®æ­£ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ",
                    "ã‚¨ãƒ©ãƒ¼å‡¦ç†ã®å¼·åŒ–"
                ]
            }

        # å“è³ªå•é¡Œã«åŸºã¥ãè¿½åŠ æ¨å¥¨
        if quality["level"] == "low":
            recommendations.append("ä¿®æ­£ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è¦‹ç›´ã—")

        return {
            "recommendations": recommendations,
            "required_actions": required_actions,
            "retry_strategy": retry_strategy
        }

    def _assess_modification_scope(self, modifications: Dict) -> str:
        """ä¿®æ­£ç¯„å›²è©•ä¾¡"""

        files_count = len(modifications.get("files_modified", []))
        total_errors = modifications.get("total_errors_fixed", 0)

        if files_count <= 1 and total_errors <= 5:
            return "minimal"
        elif files_count <= 3 and total_errors <= 15:
            return "moderate"
        else:
            return "extensive"

    def _generate_claude_feedback(self, gemini_report: Dict, score: float) -> str:
        """Claude ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ç”Ÿæˆ"""

        feedback = f"""
ğŸ“ Claude å“è³ªãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯

ğŸ¯ ç·åˆè©•ä¾¡: {score:.2f}/1.0

"""

        if score >= 0.8:
            feedback += """âœ… å„ªç§€ãªä¿®æ­£çµæœ
- Flash 2.5ãŒé©åˆ‡ã«æŒ‡ç¤ºã‚’ç†è§£ã—å®Ÿè¡Œ
- æœŸå¾…ã•ã‚Œã‚‹å“è³ªåŸºæº–ã‚’æº€è¶³
- å®‰å…¨ã§ç¢ºå®Ÿãªä¿®æ­£ãŒå®Œäº†"""

        elif score >= 0.6:
            feedback += """âš ï¸ è‰¯å¥½ã ãŒæ”¹å–„ä½™åœ°ã‚ã‚Š
- åŸºæœ¬çš„ãªä¿®æ­£ã¯é©åˆ‡ã«å®Ÿè¡Œ
- ä¸€éƒ¨ã®å“è³ªæŒ‡æ¨™ã§æ”¹å–„ã®ä½™åœ°
- ãƒ•ã‚©ãƒ­ãƒ¼ã‚¢ãƒƒãƒ—ã§ã®å“è³ªå‘ä¸Šã‚’æ¨å¥¨"""

        else:
            feedback += """âŒ ä¿®æ­£å“è³ªã«å•é¡Œ
- Flash 2.5ã®ç†è§£ã¾ãŸã¯å®Ÿè¡Œã«èª²é¡Œ
- æŒ‡ç¤ºã®æ˜ç¢ºåŒ–ã¾ãŸã¯æ‰‹æ³•ã®è¦‹ç›´ã—ãŒå¿…è¦
- ãƒªãƒˆãƒ©ã‚¤ã¾ãŸã¯æ‰‹å‹•ç¢ºèªã‚’æ¨å¥¨"""

        # Geminiãƒ¬ãƒãƒ¼ãƒˆã®è©•ä¾¡
        approach = gemini_report.get("approach", "")
        if approach:
            feedback += f"\n\nğŸ¤– Geminiå®Ÿè¡Œã‚¢ãƒ—ãƒ­ãƒ¼ãƒ: {approach}"

        return feedback

    def _generate_next_recommendations(self, gemini_result: Dict[str, Any], claude_review: Dict[str, Any]) -> List[str]:
        """æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ¨å¥¨ç”Ÿæˆ"""

        recommendations = []

        if claude_review.get("approval") == "approved":
            recommendations.extend([
                "åŒæ§˜ã®ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—ã§æ¬¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ç¾¤ã‚’å‡¦ç†",
                "çµ±åˆãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ",
                "å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æ›´æ–°"
            ])
        else:
            recommendations.extend([
                "ã‚¨ãƒ©ãƒ¼åŸå› ã®è©³ç´°åˆ†æ",
                "ä¿®æ­£ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®è¦‹ç›´ã—",
                "æ‰‹å‹•ç¢ºèªã«ã‚ˆã‚‹å•é¡Œç‰¹å®š"
            ])

        return recommendations

    def _save_cycle_log(self, cycle_result: Dict[str, Any]) -> None:
        """ã‚µã‚¤ã‚¯ãƒ«å®Ÿè¡Œãƒ­ã‚°ä¿å­˜"""

        log_file = self.task_manager.monitoring_dir / f"session_{self.session_id}.json"

        # æ—¢å­˜ãƒ­ã‚°èª­ã¿è¾¼ã¿
        if log_file.exists():
            with open(log_file, 'r', encoding='utf-8') as f:
                session_log = json.load(f)
        else:
            session_log = {
                "session_id": self.session_id,
                "start_time": datetime.datetime.now().isoformat(),
                "cycles": []
            }

        # æ–°ã—ã„ã‚µã‚¤ã‚¯ãƒ«è¿½åŠ 
        session_log["cycles"].append(cycle_result)
        session_log["last_update"] = datetime.datetime.now().isoformat()

        # ä¿å­˜
        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump(session_log, f, indent=2, ensure_ascii=False)

    def _print_session_summary(self, results: List[Dict[str, Any]]) -> None:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ±è¨ˆè¡¨ç¤º"""

        print("\n" + "="*60)
        print("ğŸ“Š ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ±è¨ˆ")
        print("="*60)

        total_cycles = len(results)
        successful_cycles = len([r for r in results if r.get("status") == "completed"])
        total_time = sum(r.get("execution_time", 0) for r in results)

        print(f"ğŸ”„ å®Ÿè¡Œã‚µã‚¤ã‚¯ãƒ«: {total_cycles}å›")
        print(f"âœ… æˆåŠŸã‚µã‚¤ã‚¯ãƒ«: {successful_cycles}å›")
        print(f"â±ï¸ ç·å®Ÿè¡Œæ™‚é–“: {total_time:.1f}ç§’")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {successful_cycles/total_cycles*100:.1f}%" if total_cycles > 0 else "æˆåŠŸç‡: N/A")

        # é€²æ—ãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤º
        progress_report = self.task_manager.generate_progress_report()
        print(f"\nğŸ“‹ ã‚¿ã‚¹ã‚¯é€²æ—:")
        print(f"  å®Œäº†: {progress_report['task_summary']['completed']}ä»¶")
        print(f"  æ®‹ã‚Š: {progress_report['task_summary']['pending']}ä»¶")
        print(f"  ã‚¨ãƒ©ãƒ¼ä¿®æ­£: {progress_report['quality_metrics']['total_errors_fixed']}ä»¶")
        print(f"  ç·ã‚³ã‚¹ãƒˆ: ${progress_report['cost_metrics']['total_cost']:.4f}")

        print("\nğŸ‰ Dual-Agent Workflow ã‚»ãƒƒã‚·ãƒ§ãƒ³å®Œäº†!")

    def run_quality_check(self, target_files: List[str], ai_agent: str = "claude") -> Dict[str, Any]:
        """å“è³ªãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ"""
        print(f"ğŸ” å“è³ªãƒã‚§ãƒƒã‚¯é–‹å§‹: {len(target_files)}ãƒ•ã‚¡ã‚¤ãƒ«å¯¾è±¡")

        # åŒ…æ‹¬çš„å“è³ªãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ
        metrics = self.quality_manager.run_comprehensive_check(target_files, ai_agent)

        # å“è³ªåŸºæº–ãƒã‚§ãƒƒã‚¯
        quality_passed = self._check_quality_standards(metrics)

        result = {
            "quality_metrics": metrics,
            "quality_passed": quality_passed,
            "recommendations": metrics.improvement_suggestions,
            "quality_level": metrics.quality_level.value
        }

        # å“è³ªåŸºæº–ã‚’æº€ãŸã•ãªã„å ´åˆã®è­¦å‘Š
        if not quality_passed:
            print(f"âš ï¸ å“è³ªåŸºæº–æœªé”æˆ: ã‚¹ã‚³ã‚¢ {metrics.overall_score:.3f}")
            print("ğŸ“ æ”¹å–„ææ¡ˆ:")
            for suggestion in metrics.improvement_suggestions:
                print(f"  - {suggestion}")
        else:
            print(f"âœ… å“è³ªåŸºæº–é”æˆ: ã‚¹ã‚³ã‚¢ {metrics.overall_score:.3f}")

        return result

    def _check_quality_standards(self, metrics) -> bool:
        """å“è³ªåŸºæº–ãƒã‚§ãƒƒã‚¯"""

        # æœ€ä½å“è³ªåŸºæº– (standards.jsonã‹ã‚‰å–å¾—)
        minimum_score = 0.7
        maximum_errors = 10

        standards_met = True

        if metrics.overall_score < minimum_score:
            standards_met = False

        if metrics.error_count > maximum_errors:
            standards_met = False

        # é‡è¦ãªå“è³ªæŒ‡æ¨™ã®å€‹åˆ¥ãƒã‚§ãƒƒã‚¯
        if metrics.type_score < 0.8:  # å‹ãƒã‚§ãƒƒã‚¯ã¯ç‰¹ã«é‡è¦
            standards_met = False

        if metrics.security_score < 0.9:  # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚‚é‡è¦
            standards_met = False

        return standards_met

    def generate_quality_report(self, format_type: str = "html") -> str:
        """å“è³ªãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        print(f"ğŸ“Š å“è³ªãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­ (å½¢å¼: {format_type})")

        report_path = self.quality_reporter.generate_comprehensive_report(format_type)

        print(f"âœ… å“è³ªãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {report_path}")
        return report_path

    def start_quality_monitoring(self) -> None:
        """å“è³ªç›£è¦–é–‹å§‹"""
        print("ğŸ“Š å“è³ªç›£è¦–ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹")
        self.quality_monitor.start_monitoring()

        # ã‚¢ãƒ©ãƒ¼ãƒˆé€šçŸ¥è¨­å®š
        def alert_handler(alert):
            print(f"ğŸš¨ å“è³ªã‚¢ãƒ©ãƒ¼ãƒˆ: {alert.message}")

        self.quality_monitor.subscribe_to_alerts(alert_handler)

    def get_quality_status(self) -> Dict[str, Any]:
        """ç¾åœ¨ã®å“è³ªã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹å–å¾—"""

        # å“è³ªç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
        monitor_status = self.quality_monitor.get_current_status()

        # å“è³ªãƒ¬ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿
        quality_report_data = self.quality_manager.get_quality_report()

        return {
            "monitoring_status": monitor_status,
            "quality_report": quality_report_data,
            "session_id": self.session_id
        }

    def run_quality_gate_check(self, target_files: List[str], gate_type: str = "pre_commit") -> bool:
        """å“è³ªã‚²ãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ"""
        print(f"ğŸšª å“è³ªã‚²ãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯: {gate_type}")

        # å“è³ªãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ
        quality_result = self.run_quality_check(target_files, "claude")
        metrics = quality_result["quality_metrics"]

        # ã‚²ãƒ¼ãƒˆç¨®åˆ¥ã«ã‚ˆã‚‹åŸºæº–è¨­å®š
        gate_standards = {
            "pre_commit": {"minimum_score": 0.8, "required_checks": ["syntax", "type_check", "lint", "format"]},
            "pre_push": {"minimum_score": 0.85, "required_checks": ["syntax", "type_check", "lint", "format", "security", "test"]},
            "production": {"minimum_score": 0.9, "required_checks": ["syntax", "type_check", "lint", "format", "security", "performance", "test"]}
        }

        standard = gate_standards.get(gate_type, gate_standards["pre_commit"])

        # åŸºæº–ãƒã‚§ãƒƒã‚¯
        gate_passed = metrics.overall_score >= standard["minimum_score"]

        if gate_passed:
            print(f"âœ… å“è³ªã‚²ãƒ¼ãƒˆé€šé: {gate_type} (ã‚¹ã‚³ã‚¢: {metrics.overall_score:.3f})")
        else:
            print(f"âŒ å“è³ªã‚²ãƒ¼ãƒˆå¤±æ•—: {gate_type} (ã‚¹ã‚³ã‚¢: {metrics.overall_score:.3f})")
            print(f"   å¿…è¦ã‚¹ã‚³ã‚¢: {standard['minimum_score']:.3f}")

        return gate_passed

    def run_integrated_workflow_with_quality(self, target_files: List[str],
                                           error_type: str = "no-untyped-def") -> Dict[str, Any]:
        """å“è³ªç®¡ç†çµ±åˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œ"""
        print("ğŸ”„ å“è³ªç®¡ç†çµ±åˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼é–‹å§‹")

        # 1. äº‹å‰å“è³ªãƒã‚§ãƒƒã‚¯
        print("\nğŸ“‹ Step 1: äº‹å‰å“è³ªãƒã‚§ãƒƒã‚¯")
        pre_quality = self.run_quality_check(target_files, "claude")

        # 2. å“è³ªã‚²ãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯
        print("\nğŸ“‹ Step 2: å“è³ªã‚²ãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯")
        if not self.run_quality_gate_check(target_files, "pre_commit"):
            return {
                "status": "quality_gate_failed",
                "pre_quality": pre_quality,
                "message": "äº‹å‰å“è³ªã‚²ãƒ¼ãƒˆã‚’é€šéã—ã¾ã›ã‚“ã§ã—ãŸ"
            }

        # 3. ã‚¿ã‚¹ã‚¯ä½œæˆãƒ»å®Ÿè¡Œ
        print("\nğŸ“‹ Step 3: ä¿®æ­£ã‚¿ã‚¹ã‚¯å®Ÿè¡Œ")
        task_ids = self.create_mypy_fix_task(
            target_files=target_files,
            error_type=error_type,
            auto_execute=True
        )

        # 4. ä¿®æ­£å¾Œå“è³ªãƒã‚§ãƒƒã‚¯
        print("\nğŸ“‹ Step 4: ä¿®æ­£å¾Œå“è³ªãƒã‚§ãƒƒã‚¯")
        post_quality = self.run_quality_check(target_files, "claude")

        # 5. å“è³ªæ”¹å–„ç¢ºèª
        print("\nğŸ“‹ Step 5: å“è³ªæ”¹å–„ç¢ºèª")
        improvement = post_quality["quality_metrics"].overall_score - pre_quality["quality_metrics"].overall_score

        if improvement > 0:
            print(f"âœ… å“è³ªæ”¹å–„: +{improvement:.3f}")
        else:
            print(f"âš ï¸ å“è³ªæ”¹å–„ãªã—: {improvement:.3f}")

        # 6. æœ€çµ‚å“è³ªã‚²ãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯
        print("\nğŸ“‹ Step 6: æœ€çµ‚å“è³ªã‚²ãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯")
        final_gate_passed = self.run_quality_gate_check(target_files, "pre_push")

        return {
            "status": "completed" if final_gate_passed else "quality_gate_failed",
            "pre_quality": pre_quality,
            "post_quality": post_quality,
            "improvement": improvement,
            "task_ids": task_ids,
            "final_gate_passed": final_gate_passed
        }

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    import argparse

    parser = argparse.ArgumentParser(description="Dual-Agent Workflow Coordinator")
    parser.add_argument("--mode", choices=["single", "continuous"], default="single",
                       help="å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ (single: 1ã‚µã‚¤ã‚¯ãƒ«, continuous: é€£ç¶šå®Ÿè¡Œ)")
    parser.add_argument("--max-cycles", type=int, default=10,
                       help="é€£ç¶šå®Ÿè¡Œæ™‚ã®æœ€å¤§ã‚µã‚¤ã‚¯ãƒ«æ•°")
    parser.add_argument("--create-test-task", action="store_true",
                       help="ãƒ†ã‚¹ãƒˆç”¨ã‚¿ã‚¹ã‚¯ã‚’ä½œæˆ")

    args = parser.parse_args()

    coordinator = DualAgentCoordinator()

    if args.create_test_task:
        # ãƒ†ã‚¹ãƒˆç”¨ã‚¿ã‚¹ã‚¯ä½œæˆ
        test_files = ["kumihan_formatter/core/utilities/logger.py"]  # ä¾‹
        task_id = coordinator.create_mypy_fix_task(
            target_files=test_files,
            error_type="no-untyped-def",
            priority="high"
        )
        print(f"âœ… ãƒ†ã‚¹ãƒˆã‚¿ã‚¹ã‚¯ä½œæˆ: {task_id}")
        return

    if args.mode == "single":
        # å˜ä¸€ã‚µã‚¤ã‚¯ãƒ«å®Ÿè¡Œ
        result = coordinator.execute_workflow_cycle()
        print(f"\nğŸ“Š å®Ÿè¡Œçµæœ: {result['status']}")

    elif args.mode == "continuous":
        # é€£ç¶šå®Ÿè¡Œ
        results = coordinator.run_continuous_workflow(max_cycles=args.max_cycles)
        print(f"\nğŸ“Š é€£ç¶šå®Ÿè¡Œå®Œäº†: {len(results)}ã‚µã‚¤ã‚¯ãƒ«")

if __name__ == "__main__":
    main()
