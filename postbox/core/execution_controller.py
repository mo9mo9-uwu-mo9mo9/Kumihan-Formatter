#!/usr/bin/env python3
"""
Execution Controller for Postbox System
ä¾å­˜é–¢ä¿‚ãƒ™ãƒ¼ã‚¹å®Ÿè¡Œåˆ¶å¾¡ãƒ»ä¸¦è¡Œå®Ÿè¡Œãƒ»ã‚¨ãƒ©ãƒ¼æ™‚fallbackåˆ¶å¾¡
"""

import asyncio
import json
import threading
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Any, Optional, Set, Tuple
from pathlib import Path
from dataclasses import dataclass, asdict
from enum import Enum
import datetime


class ExecutionStatus(Enum):
    """å®Ÿè¡Œã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹"""
    PENDING = "pending"              # å®Ÿè¡Œå¾…ã¡
    READY = "ready"                  # å®Ÿè¡Œå¯èƒ½
    RUNNING = "running"              # å®Ÿè¡Œä¸­
    COMPLETED = "completed"          # å®Œäº†
    FAILED = "failed"                # å¤±æ•—
    SKIPPED = "skipped"              # ã‚¹ã‚­ãƒƒãƒ—
    BLOCKED = "blocked"              # ãƒ–ãƒ­ãƒƒã‚¯ä¸­


@dataclass
class TaskExecution:
    """ã‚¿ã‚¹ã‚¯å®Ÿè¡Œæƒ…å ±"""
    task_id: str
    status: ExecutionStatus
    dependencies: List[str]          # ä¾å­˜ã‚¿ã‚¹ã‚¯ID
    dependents: List[str]            # ã“ã®ã‚¿ã‚¹ã‚¯ã«ä¾å­˜ã™ã‚‹ã‚¿ã‚¹ã‚¯ID
    priority: int                    # å„ªå…ˆåº¦ï¼ˆ0ãŒæœ€é«˜ï¼‰
    estimated_time: int              # æ¨å®šå®Ÿè¡Œæ™‚é–“ï¼ˆåˆ†ï¼‰
    retry_count: int = 0             # ãƒªãƒˆãƒ©ã‚¤å›æ•°
    start_time: Optional[str] = None
    end_time: Optional[str] = None
    error_message: Optional[str] = None
    result: Optional[Dict[str, Any]] = None


@dataclass
class ExecutionPlan:
    """å®Ÿè¡Œè¨ˆç”»"""
    total_tasks: int
    execution_batches: List[List[str]]  # ãƒãƒƒãƒå®Ÿè¡Œé †åº
    parallel_groups: List[List[str]]    # ä¸¦è¡Œå®Ÿè¡Œã‚°ãƒ«ãƒ¼ãƒ—
    critical_path: List[str]            # ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãƒ‘ã‚¹
    estimated_total_time: int           # ç·æ¨å®šæ™‚é–“


class ExecutionController:
    """é«˜åº¦ãªå®Ÿè¡Œåˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, postbox_dir: str = "postbox"):
        self.postbox_dir = Path(postbox_dir)
        self.todo_dir = self.postbox_dir / "todo"
        self.completed_dir = self.postbox_dir / "completed"
        self.monitoring_dir = self.postbox_dir / "monitoring"
        
        # å®Ÿè¡Œç®¡ç†
        self.task_executions: Dict[str, TaskExecution] = {}
        self.execution_lock = threading.Lock()
        self.max_concurrent_tasks = 3  # æœ€å¤§ä¸¦è¡Œå®Ÿè¡Œæ•°
        
        print("ğŸ›ï¸ ExecutionController åˆæœŸåŒ–å®Œäº†")
        print(f"ğŸ“Š æœ€å¤§ä¸¦è¡Œå®Ÿè¡Œæ•°: {self.max_concurrent_tasks}")

    def create_execution_plan(self, task_ids: List[str]) -> ExecutionPlan:
        """ä¾å­˜é–¢ä¿‚ã‚’è€ƒæ…®ã—ãŸå®Ÿè¡Œè¨ˆç”»ä½œæˆ"""
        
        print(f"ğŸ“‹ å®Ÿè¡Œè¨ˆç”»ä½œæˆé–‹å§‹: {len(task_ids)}ã‚¿ã‚¹ã‚¯")
        
        # ã‚¿ã‚¹ã‚¯æƒ…å ±åé›†
        task_info = self._gather_task_info(task_ids)
        
        # ä¾å­˜é–¢ä¿‚åˆ†æ
        dependencies = self._analyze_dependencies(task_info)
        
        # å®Ÿè¡Œå¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯
        self._validate_dependencies(dependencies)
        
        # ãƒãƒƒãƒå®Ÿè¡Œé †åºè¨ˆç®—
        execution_batches = self._calculate_execution_batches(dependencies)
        
        # ä¸¦è¡Œå®Ÿè¡Œã‚°ãƒ«ãƒ¼ãƒ—è­˜åˆ¥
        parallel_groups = self._identify_parallel_groups(execution_batches, dependencies)
        
        # ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãƒ‘ã‚¹è¨ˆç®—
        critical_path = self._calculate_critical_path(dependencies, task_info)
        
        # ç·å®Ÿè¡Œæ™‚é–“æ¨å®š
        total_time = self._estimate_total_execution_time(parallel_groups, task_info)
        
        plan = ExecutionPlan(
            total_tasks=len(task_ids),
            execution_batches=execution_batches,
            parallel_groups=parallel_groups,
            critical_path=critical_path,
            estimated_total_time=total_time
        )
        
        print(f"âœ… å®Ÿè¡Œè¨ˆç”»ä½œæˆå®Œäº†:")
        print(f"   ãƒãƒƒãƒæ•°: {len(execution_batches)}")
        print(f"   ä¸¦è¡Œã‚°ãƒ«ãƒ¼ãƒ—: {len(parallel_groups)}")
        print(f"   æ¨å®šå®Ÿè¡Œæ™‚é–“: {total_time}åˆ†")
        
        return plan

    def execute_plan(self, execution_plan: ExecutionPlan) -> Dict[str, Any]:
        """å®Ÿè¡Œè¨ˆç”»ã«åŸºã¥ãä¸¦è¡Œå®Ÿè¡Œ"""
        
        print(f"ğŸš€ å®Ÿè¡Œè¨ˆç”»é–‹å§‹: {execution_plan.total_tasks}ã‚¿ã‚¹ã‚¯")
        
        start_time = datetime.datetime.now()
        execution_log = {
            "start_time": start_time.isoformat(),
            "plan": asdict(execution_plan),
            "execution_results": [],
            "errors": [],
            "performance_metrics": {}
        }
        
        try:
            # ä¸¦è¡Œå®Ÿè¡Œã‚°ãƒ«ãƒ¼ãƒ—ã‚’é †æ¬¡å®Ÿè¡Œ
            for group_idx, group in enumerate(execution_plan.parallel_groups):
                print(f"\nğŸ“¦ å®Ÿè¡Œã‚°ãƒ«ãƒ¼ãƒ— {group_idx + 1}/{len(execution_plan.parallel_groups)}")
                print(f"   ä¸¦è¡Œã‚¿ã‚¹ã‚¯: {len(group)}å€‹")
                
                group_results = self._execute_parallel_group(group)
                execution_log["execution_results"].extend(group_results)
                
                # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
                failed_tasks = [r for r in group_results if r["status"] == "failed"]
                if failed_tasks:
                    print(f"âš ï¸ ã‚°ãƒ«ãƒ¼ãƒ—å†…ã§{len(failed_tasks)}ã‚¿ã‚¹ã‚¯å¤±æ•—")
                    
                    # å¤±æ•—æ™‚ã®ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯
                    should_continue = self._handle_group_failures(failed_tasks, execution_plan)
                    if not should_continue:
                        print("âŒ é‡è¦ã‚¿ã‚¹ã‚¯å¤±æ•—ã«ã‚ˆã‚Šå®Ÿè¡Œä¸­æ–­")
                        break
        
        except Exception as e:
            print(f"âŒ å®Ÿè¡Œåˆ¶å¾¡ã‚¨ãƒ©ãƒ¼: {e}")
            execution_log["errors"].append({
                "type": "execution_control_error",
                "message": str(e),
                "timestamp": datetime.datetime.now().isoformat()
            })
        
        finally:
            end_time = datetime.datetime.now()
            execution_time = (end_time - start_time).total_seconds() / 60  # åˆ†
            
            execution_log["end_time"] = end_time.isoformat()
            execution_log["actual_execution_time"] = execution_time
            execution_log["performance_metrics"] = self._calculate_performance_metrics(
                execution_log["execution_results"], execution_time
            )
            
            # å®Ÿè¡Œãƒ­ã‚°ä¿å­˜
            self._save_execution_log(execution_log)
            
            print(f"\nğŸ“Š å®Ÿè¡Œå®Œäº†:")
            print(f"   å®Ÿè¡Œæ™‚é–“: {execution_time:.1f}åˆ†")
            print(f"   æ¨å®šæ™‚é–“: {execution_plan.estimated_total_time}åˆ†")
            print(f"   åŠ¹ç‡æ€§: {execution_plan.estimated_total_time/execution_time*100:.1f}%")
        
        return execution_log

    def _gather_task_info(self, task_ids: List[str]) -> Dict[str, Dict[str, Any]]:
        """ã‚¿ã‚¹ã‚¯æƒ…å ±åé›†"""
        
        task_info = {}
        
        for task_id in task_ids:
            task_file = self.todo_dir / f"{task_id}.json"
            
            if not task_file.exists():
                print(f"âš ï¸ ã‚¿ã‚¹ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {task_id}")
                continue
            
            try:
                with open(task_file, 'r', encoding='utf-8') as f:
                    task_data = json.load(f)
                
                task_info[task_id] = {
                    "data": task_data,
                    "target_files": task_data.get("target_files", []),
                    "priority": task_data.get("priority", "medium"),
                    "task_type": task_data.get("type", "unknown"),
                    "estimated_time": self._estimate_task_time(task_data)
                }
                
            except Exception as e:
                print(f"âš ï¸ ã‚¿ã‚¹ã‚¯ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {task_id}: {e}")
        
        return task_info

    def _analyze_dependencies(self, task_info: Dict[str, Dict[str, Any]]) -> Dict[str, TaskExecution]:
        """ä¾å­˜é–¢ä¿‚åˆ†æ"""
        
        dependencies = {}
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹ä¾å­˜é–¢ä¿‚ã®åˆ†æ
        file_to_tasks = {}  # ãƒ•ã‚¡ã‚¤ãƒ« -> ã‚¿ã‚¹ã‚¯ID ã®ãƒãƒƒãƒ”ãƒ³ã‚°
        
        # å„ã‚¿ã‚¹ã‚¯ã®å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¨˜éŒ²
        for task_id, info in task_info.items():
            target_files = info["target_files"]
            for file_path in target_files:
                if file_path not in file_to_tasks:
                    file_to_tasks[file_path] = []
                file_to_tasks[file_path].append(task_id)
        
        # ä¾å­˜é–¢ä¿‚æ§‹ç¯‰
        for task_id, info in task_info.items():
            target_files = info["target_files"]
            task_dependencies = []
            
            # åŒä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ“ä½œã™ã‚‹ä»–ã®ã‚¿ã‚¹ã‚¯ã¨ã®ä¾å­˜é–¢ä¿‚
            for file_path in target_files:
                for other_task_id in file_to_tasks.get(file_path, []):
                    if other_task_id != task_id:
                        # ã‚ˆã‚Šæ—©ã„ã‚¿ã‚¹ã‚¯ã«ä¾å­˜ï¼ˆFIFOé †åºï¼‰
                        if other_task_id < task_id:
                            task_dependencies.append(other_task_id)
            
            # TaskExecution ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ
            priority_map = {"high": 0, "medium": 1, "low": 2}
            dependencies[task_id] = TaskExecution(
                task_id=task_id,
                status=ExecutionStatus.PENDING,
                dependencies=task_dependencies,
                dependents=[],  # å¾Œã§è¨­å®š
                priority=priority_map.get(info["priority"], 1),
                estimated_time=info["estimated_time"]
            )
        
        # é€†ä¾å­˜é–¢ä¿‚è¨­å®š
        for task_id, execution in dependencies.items():
            for dep_id in execution.dependencies:
                if dep_id in dependencies:
                    dependencies[dep_id].dependents.append(task_id)
        
        return dependencies

    def _validate_dependencies(self, dependencies: Dict[str, TaskExecution]) -> None:
        """ä¾å­˜é–¢ä¿‚ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯"""
        
        # å¾ªç’°ä¾å­˜ãƒã‚§ãƒƒã‚¯
        def has_cycle(task_id: str, visited: Set[str], rec_stack: Set[str]) -> bool:
            visited.add(task_id)
            rec_stack.add(task_id)
            
            for dep_id in dependencies[task_id].dependencies:
                if dep_id not in dependencies:
                    continue
                if dep_id not in visited:
                    if has_cycle(dep_id, visited, rec_stack):
                        return True
                elif dep_id in rec_stack:
                    return True
            
            rec_stack.remove(task_id)
            return False
        
        visited = set()
        for task_id in dependencies:
            if task_id not in visited:
                if has_cycle(task_id, visited, set()):
                    raise ValueError(f"å¾ªç’°ä¾å­˜ã‚’æ¤œå‡º: {task_id} å‘¨è¾º")
        
        print("âœ… ä¾å­˜é–¢ä¿‚æ¤œè¨¼: å¾ªç’°ä¾å­˜ãªã—")

    def _calculate_execution_batches(self, dependencies: Dict[str, TaskExecution]) -> List[List[str]]:
        """ä¾å­˜é–¢ä¿‚ã‚’è€ƒæ…®ã—ãŸãƒãƒƒãƒå®Ÿè¡Œé †åºè¨ˆç®—"""
        
        batches = []
        remaining = set(dependencies.keys())
        
        while remaining:
            # ç¾åœ¨ã®ãƒãƒƒãƒã§å®Ÿè¡Œå¯èƒ½ãªã‚¿ã‚¹ã‚¯ã‚’ç‰¹å®š
            current_batch = []
            
            for task_id in list(remaining):
                execution = dependencies[task_id]
                
                # å…¨ã¦ã®ä¾å­˜ã‚¿ã‚¹ã‚¯ãŒå®Œäº†ã—ã¦ã„ã‚‹
                deps_completed = all(
                    dep_id not in remaining for dep_id in execution.dependencies
                )
                
                if deps_completed:
                    current_batch.append(task_id)
            
            if not current_batch:
                # ãƒ‡ãƒƒãƒ‰ãƒ­ãƒƒã‚¯æ¤œå‡º
                raise ValueError(f"ä¾å­˜é–¢ä¿‚ãƒ‡ãƒƒãƒ‰ãƒ­ãƒƒã‚¯: æ®‹ã‚Šã‚¿ã‚¹ã‚¯ {remaining}")
            
            # å„ªå…ˆåº¦é †ã§ã‚½ãƒ¼ãƒˆ
            current_batch.sort(key=lambda tid: (
                dependencies[tid].priority,
                -dependencies[tid].estimated_time  # æ™‚é–“ãŒé•·ã„ã‚‚ã®ã‚’å…ˆã«
            ))
            
            batches.append(current_batch)
            remaining.difference_update(current_batch)
        
        return batches

    def _identify_parallel_groups(self, execution_batches: List[List[str]], 
                                 dependencies: Dict[str, TaskExecution]) -> List[List[str]]:
        """ä¸¦è¡Œå®Ÿè¡Œã‚°ãƒ«ãƒ¼ãƒ—è­˜åˆ¥"""
        
        parallel_groups = []
        
        for batch in execution_batches:
            # ãƒãƒƒãƒå†…ã®ã‚¿ã‚¹ã‚¯ã‚’ä¸¦è¡Œå®Ÿè¡Œã‚°ãƒ«ãƒ¼ãƒ—ã«åˆ†å‰²
            remaining_tasks = batch.copy()
            
            while remaining_tasks:
                parallel_group = []
                
                for task_id in remaining_tasks.copy():
                    # ãƒªã‚½ãƒ¼ã‚¹ç«¶åˆãƒã‚§ãƒƒã‚¯
                    if self._can_run_parallel(task_id, parallel_group, dependencies):
                        parallel_group.append(task_id)
                        remaining_tasks.remove(task_id)
                        
                        # æœ€å¤§ä¸¦è¡Œæ•°ã«é”ã—ãŸã‚‰æ¬¡ã®ã‚°ãƒ«ãƒ¼ãƒ—ã¸
                        if len(parallel_group) >= self.max_concurrent_tasks:
                            break
                
                if parallel_group:
                    parallel_groups.append(parallel_group)
                else:
                    # ä¸¦è¡Œå®Ÿè¡Œä¸å¯ã®å ´åˆã¯1ã¤ãšã¤
                    parallel_groups.append([remaining_tasks.pop(0)])
        
        return parallel_groups

    def _can_run_parallel(self, task_id: str, current_group: List[str], 
                         dependencies: Dict[str, TaskExecution]) -> bool:
        """ä¸¦è¡Œå®Ÿè¡Œå¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯"""
        
        if not current_group:
            return True
        
        task_execution = dependencies[task_id]
        task_files = set()
        
        # ç¾åœ¨ã®ã‚¿ã‚¹ã‚¯ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’å–å¾—ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        for other_id in current_group:
            other_execution = dependencies[other_id]
            # ãƒ•ã‚¡ã‚¤ãƒ«ç«¶åˆãƒã‚§ãƒƒã‚¯ï¼ˆå®Ÿè£…çœç•¥ï¼‰
            # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ task_data ã‹ã‚‰ target_files ã‚’å–å¾—
        
        return True  # ç°¡æ˜“ç‰ˆã§ã¯å¸¸ã«ä¸¦è¡Œå®Ÿè¡Œå¯èƒ½ã¨ã™ã‚‹

    def _calculate_critical_path(self, dependencies: Dict[str, TaskExecution],
                                task_info: Dict[str, Dict[str, Any]]) -> List[str]:
        """ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãƒ‘ã‚¹è¨ˆç®—"""
        
        # å„ãƒãƒ¼ãƒ‰ã®æœ€æ—©é–‹å§‹æ™‚é–“ã‚’è¨ˆç®—
        earliest_start = {}
        
        def calculate_earliest_start(task_id: str) -> int:
            if task_id in earliest_start:
                return earliest_start[task_id]
            
            if not dependencies[task_id].dependencies:
                earliest_start[task_id] = 0
            else:
                max_predecessor_finish = max(
                    calculate_earliest_start(dep_id) + dependencies[dep_id].estimated_time
                    for dep_id in dependencies[task_id].dependencies
                    if dep_id in dependencies
                )
                earliest_start[task_id] = max_predecessor_finish
            
            return earliest_start[task_id]
        
        # å…¨ã‚¿ã‚¹ã‚¯ã®æœ€æ—©é–‹å§‹æ™‚é–“ã‚’è¨ˆç®—
        for task_id in dependencies:
            calculate_earliest_start(task_id)
        
        # æœ€çµ‚æ™‚é–“ãŒæœ€å¤§ã®ã‚¿ã‚¹ã‚¯ã‹ã‚‰ãƒãƒƒã‚¯ãƒˆãƒ©ãƒƒã‚¯
        finish_times = {
            task_id: earliest_start[task_id] + execution.estimated_time
            for task_id, execution in dependencies.items()
        }
        
        end_task = max(finish_times.keys(), key=lambda t: finish_times[t])
        
        # ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãƒ‘ã‚¹æ§‹ç¯‰
        critical_path = []
        current = end_task
        
        while current:
            critical_path.append(current)
            
            # æœ€ã‚‚é…ã„ä¾å­˜ã‚¿ã‚¹ã‚¯ã‚’é¸æŠ
            if dependencies[current].dependencies:
                current = max(
                    dependencies[current].dependencies,
                    key=lambda dep: finish_times.get(dep, 0)
                )
            else:
                current = None
        
        return list(reversed(critical_path))

    def _estimate_total_execution_time(self, parallel_groups: List[List[str]], 
                                     task_info: Dict[str, Dict[str, Any]]) -> int:
        """ç·å®Ÿè¡Œæ™‚é–“æ¨å®š"""
        
        total_time = 0
        
        for group in parallel_groups:
            # ã‚°ãƒ«ãƒ¼ãƒ—å†…ã®æœ€å¤§å®Ÿè¡Œæ™‚é–“
            group_time = max(
                task_info[task_id]["estimated_time"]
                for task_id in group
                if task_id in task_info
            ) if group else 0
            
            total_time += group_time
        
        return total_time

    def _execute_parallel_group(self, task_group: List[str]) -> List[Dict[str, Any]]:
        """ä¸¦è¡Œå®Ÿè¡Œã‚°ãƒ«ãƒ¼ãƒ—ã®å®Ÿè¡Œ"""
        
        print(f"ğŸ”„ ä¸¦è¡Œå®Ÿè¡Œé–‹å§‹: {len(task_group)}ã‚¿ã‚¹ã‚¯")
        
        results = []
        
        with ThreadPoolExecutor(max_workers=min(len(task_group), self.max_concurrent_tasks)) as executor:
            # ã‚¿ã‚¹ã‚¯å®Ÿè¡Œã‚’ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ—ãƒ¼ãƒ«ã«æŠ•å…¥
            future_to_task = {
                executor.submit(self._execute_single_task, task_id): task_id
                for task_id in task_group
            }
            
            # å®Œäº†ã‚’å¾…æ©Ÿ
            for future in as_completed(future_to_task):
                task_id = future_to_task[future]
                
                try:
                    result = future.result()
                    results.append(result)
                    
                    if result["status"] == "completed":
                        print(f"âœ… {task_id}: å®Œäº†")
                    else:
                        print(f"âŒ {task_id}: {result['status']}")
                
                except Exception as e:
                    print(f"ğŸ’¥ {task_id}: ä¾‹å¤–ç™ºç”Ÿ - {e}")
                    results.append({
                        "task_id": task_id,
                        "status": "failed",
                        "error": str(e),
                        "timestamp": datetime.datetime.now().isoformat()
                    })
        
        print(f"ğŸ“Š ã‚°ãƒ«ãƒ¼ãƒ—å®Ÿè¡Œå®Œäº†: æˆåŠŸ{len([r for r in results if r['status'] == 'completed'])}/{len(results)}")
        
        return results

    def _execute_single_task(self, task_id: str) -> Dict[str, Any]:
        """å˜ä¸€ã‚¿ã‚¹ã‚¯ã®å®Ÿè¡Œ"""
        
        start_time = datetime.datetime.now()
        
        try:
            # GeminiHelperã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆå¾ªç’°ã‚¤ãƒ³ãƒãƒ¼ãƒˆå›é¿ã®ãŸã‚é…å»¶ï¼‰
            import sys
            sys.path.append(str(self.postbox_dir))
            from utils.gemini_helper import GeminiHelper
            
            helper = GeminiHelper()
            
            # ã‚¿ã‚¹ã‚¯ãƒ‡ãƒ¼ã‚¿å–å¾—
            task_file = self.todo_dir / f"{task_id}.json"
            if not task_file.exists():
                return {
                    "task_id": task_id,
                    "status": "failed",
                    "error": "ã‚¿ã‚¹ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“",
                    "timestamp": datetime.datetime.now().isoformat()
                }
            
            with open(task_file, 'r', encoding='utf-8') as f:
                task_data = json.load(f)
            
            # ã‚¿ã‚¹ã‚¯å®Ÿè¡Œ
            result = helper.execute_task(task_data)
            
            end_time = datetime.datetime.now()
            execution_time = (end_time - start_time).total_seconds()
            
            return {
                "task_id": task_id,
                "status": result.get("status", "unknown"),
                "execution_time": execution_time,
                "result": result,
                "timestamp": end_time.isoformat()
            }
        
        except Exception as e:
            return {
                "task_id": task_id,
                "status": "failed",
                "error": str(e),
                "execution_time": (datetime.datetime.now() - start_time).total_seconds(),
                "timestamp": datetime.datetime.now().isoformat()
            }

    def _handle_group_failures(self, failed_tasks: List[Dict[str, Any]], 
                              execution_plan: ExecutionPlan) -> bool:
        """ã‚°ãƒ«ãƒ¼ãƒ—å¤±æ•—æ™‚ã®å‡¦ç†"""
        
        failed_ids = [task["task_id"] for task in failed_tasks]
        
        # ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãƒ‘ã‚¹ã®ã‚¿ã‚¹ã‚¯ãŒå¤±æ•—ã—ãŸå ´åˆ
        critical_failures = set(failed_ids) & set(execution_plan.critical_path)
        if critical_failures:
            print(f"ğŸš¨ ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãƒ‘ã‚¹å¤±æ•—: {critical_failures}")
            return False  # å®Ÿè¡Œä¸­æ–­
        
        print(f"âš ï¸ éã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ã‚¿ã‚¹ã‚¯å¤±æ•—: {failed_ids}")
        return True  # å®Ÿè¡Œç¶™ç¶š

    def _estimate_task_time(self, task_data: Dict[str, Any]) -> int:
        """ã‚¿ã‚¹ã‚¯å®Ÿè¡Œæ™‚é–“æ¨å®š"""
        
        # åŸºæœ¬æ™‚é–“ï¼ˆã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—åˆ¥ï¼‰
        base_times = {
            "code_modification": 10,
            "new_implementation": 30,
            "hybrid_implementation": 45,
            "new_feature_development": 60,
            "micro_code_modification": 5
        }
        
        task_type = task_data.get("type", "code_modification")
        base_time = base_times.get(task_type, 15)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã«ã‚ˆã‚‹èª¿æ•´
        target_files = task_data.get("target_files", [])
        file_factor = len(target_files) * 0.3
        
        return int(base_time * (1 + file_factor))

    def _calculate_performance_metrics(self, execution_results: List[Dict[str, Any]], 
                                     actual_time: float) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—"""
        
        successful_tasks = [r for r in execution_results if r["status"] == "completed"]
        failed_tasks = [r for r in execution_results if r["status"] == "failed"]
        
        avg_execution_time = (
            sum(r.get("execution_time", 0) for r in execution_results) / 
            len(execution_results) if execution_results else 0
        )
        
        return {
            "total_tasks": len(execution_results),
            "successful_tasks": len(successful_tasks),
            "failed_tasks": len(failed_tasks),
            "success_rate": len(successful_tasks) / len(execution_results) if execution_results else 0,
            "average_task_time": avg_execution_time,
            "total_execution_time": actual_time,
            "throughput": len(execution_results) / actual_time if actual_time > 0 else 0
        }

    def _save_execution_log(self, execution_log: Dict[str, Any]) -> None:
        """å®Ÿè¡Œãƒ­ã‚°ä¿å­˜"""
        
        log_file = self.monitoring_dir / f"execution_log_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump(execution_log, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“ å®Ÿè¡Œãƒ­ã‚°ä¿å­˜: {log_file}")


def main():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    controller = ExecutionController()
    
    # ãƒ†ã‚¹ãƒˆç”¨ã‚¿ã‚¹ã‚¯ID
    test_task_ids = [
        "task_20250812_201916",
        "task_20250812_201920", 
        "task_20250812_201924"
    ]
    
    try:
        # å®Ÿè¡Œè¨ˆç”»ä½œæˆ
        plan = controller.create_execution_plan(test_task_ids)
        
        print("\nğŸ“‹ å®Ÿè¡Œè¨ˆç”»:")
        print(f"   ç·ã‚¿ã‚¹ã‚¯æ•°: {plan.total_tasks}")
        print(f"   æ¨å®šå®Ÿè¡Œæ™‚é–“: {plan.estimated_total_time}åˆ†")
        print(f"   ä¸¦è¡Œã‚°ãƒ«ãƒ¼ãƒ—æ•°: {len(plan.parallel_groups)}")
        
        # å®Ÿè¡Œå®Ÿè¡Œï¼ˆãƒ†ã‚¹ãƒˆæ™‚ã¯å®Ÿéš›ã®å®Ÿè¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰
        print("\nğŸš€ å®Ÿè¡Œãƒ†ã‚¹ãƒˆï¼ˆå®Ÿéš›ã®å®Ÿè¡Œã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰")
        
    except Exception as e:
        print(f"âŒ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")


if __name__ == "__main__":
    main()