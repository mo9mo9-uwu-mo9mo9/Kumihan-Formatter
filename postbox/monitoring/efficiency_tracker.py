#!/usr/bin/env python3
"""
Efficiency Tracking System
åŠ¹ç‡æ€§ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¿½è·¡ã‚·ã‚¹ãƒ†ãƒ  - ã‚¿ã‚¹ã‚¯ã‚­ãƒ¥ãƒ¼å‡¦ç†åŠ¹ç‡ã®å®šé‡çš„ç›£è¦–
"""

import json
import datetime
import os
import statistics
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
from dataclasses import dataclass, asdict
from enum import Enum


class EfficiencyLevel(Enum):
    """åŠ¹ç‡ãƒ¬ãƒ™ãƒ«"""
    EXCELLENT = "excellent"     # å„ªç§€ï¼ˆ90%ä»¥ä¸Šï¼‰
    GOOD = "good"              # è‰¯å¥½ï¼ˆ70-89%ï¼‰
    ACCEPTABLE = "acceptable"   # è¨±å®¹ï¼ˆ50-69%ï¼‰
    POOR = "poor"              # ä¸è‰¯ï¼ˆ30-49%ï¼‰
    CRITICAL = "critical"       # é‡å¤§ï¼ˆ30%æœªæº€ï¼‰


@dataclass
class TaskExecutionMetrics:
    """ã‚¿ã‚¹ã‚¯å®Ÿè¡Œãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    task_id: str
    execution_timestamp: str
    execution_time_seconds: float
    errors_before: int
    errors_after: int
    errors_fixed: int
    success_rate: float
    quality_score: float
    token_usage: int
    cost_estimate: float
    ai_agent: str  # claude, gemini
    task_type: str
    file_count: int


@dataclass
class QueueEfficiencySnapshot:
    """ã‚­ãƒ¥ãƒ¼åŠ¹ç‡æ€§ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ"""
    timestamp: str
    queue_size: int
    pending_tasks: int
    processing_rate: float  # ã‚¿ã‚¹ã‚¯/æ™‚é–“
    error_fix_rate: float   # ä¿®æ­£æˆåŠŸç‡
    duplicate_rate: float   # é‡è¤‡ã‚¿ã‚¹ã‚¯ç‡
    avg_processing_time: float  # å¹³å‡å‡¦ç†æ™‚é–“ï¼ˆç§’ï¼‰
    quality_approval_rate: float    # å“è³ªæ‰¿èªç‡
    token_efficiency: float # ãƒˆãƒ¼ã‚¯ãƒ³åŠ¹ç‡ï¼ˆä¿®æ­£ä»¶æ•°/Tokenæ•°ï¼‰
    cost_efficiency: float  # ã‚³ã‚¹ãƒˆåŠ¹ç‡ï¼ˆä¿®æ­£ä»¶æ•°/ã‚³ã‚¹ãƒˆï¼‰


@dataclass
class EfficiencyTrend:
    """åŠ¹ç‡æ€§ãƒˆãƒ¬ãƒ³ãƒ‰"""
    metric_name: str
    current_value: float
    previous_value: float
    change_percentage: float
    trend_direction: str    # improving, degrading, stable
    trend_strength: str     # strong, moderate, weak


@dataclass
class EfficiencyReport:
    """åŠ¹ç‡æ€§ãƒ¬ãƒãƒ¼ãƒˆ"""
    report_timestamp: str
    monitoring_period_hours: float
    current_snapshot: QueueEfficiencySnapshot
    efficiency_trends: List[EfficiencyTrend]
    performance_goals: Dict[str, Any]
    achievement_status: Dict[str, bool]
    recommendations: List[str]
    alert_level: EfficiencyLevel


class EfficiencyTracker:
    """åŠ¹ç‡æ€§ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¿½è·¡ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self) -> None:
        # ãƒ‡ãƒ¼ã‚¿ä¿å­˜ãƒ‘ã‚¹
        self.metrics_history_path = Path("postbox/monitoring/efficiency_metrics.json")
        self.snapshots_path = Path("postbox/monitoring/efficiency_snapshots.json")
        self.reports_path = Path("postbox/monitoring/efficiency_reports.json")

        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        for path in [self.metrics_history_path, self.snapshots_path, self.reports_path]:
            path.parent.mkdir(parents=True, exist_ok=True)

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›®æ¨™ï¼ˆIssue #858è¦ä»¶ï¼‰
        self.performance_goals = {
            "error_fix_rate": 0.60,      # 60%ä»¥ä¸Š
            "queue_size_target": 10,      # 10ä»¶ä»¥ä¸‹
            "processing_time_target": 2.0,   # 2ç§’ä»¥ä¸‹
            "quality_approval_rate": 0.70,   # 70%ä»¥ä¸Š
            "duplicate_rate_max": 0.20,       # 20%ä»¥ä¸‹
            "token_efficiency_min": 0.01,     # 0.01ä»¥ä¸Šï¼ˆä¿®æ­£/Tokenï¼‰
            "cost_efficiency_min": 10.0       # 10ä»¥ä¸Šï¼ˆä¿®æ­£/ãƒ‰ãƒ«ï¼‰
        }

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹å±¥æ­´ãƒ­ãƒ¼ãƒ‰
        self.metrics_history = self._load_metrics_history()
        self.snapshots_history = self._load_snapshots_history()

        # ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š
        self.alert_settings = {
            "queue_size_alert_threshold": 20,      # ã‚­ãƒ¥ãƒ¼ã‚µã‚¤ã‚ºè­¦å‘Šé–¾å€¤
            "error_fix_rate_alert_threshold": 0.3,  # ã‚¨ãƒ©ãƒ¼ä¿®æ­£ç‡è­¦å‘Šé–¾å€¤
            "processing_time_alert_threshold": 5.0, # å‡¦ç†æ™‚é–“è­¦å‘Šé–¾å€¤ï¼ˆç§’ï¼‰
            "alert_enabled": True                   # ã‚¢ãƒ©ãƒ¼ãƒˆæœ‰åŠ¹ãƒ•ãƒ©ã‚°
        }

        print("ğŸ“Š EfficiencyTracker åˆæœŸåŒ–å®Œäº†")

    def record_task_execution(self, task_metrics: TaskExecutionMetrics) -> None:
        """ã‚¿ã‚¹ã‚¯å®Ÿè¡Œãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨˜éŒ²"""

        print(f"ğŸ“ ã‚¿ã‚¹ã‚¯ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²: {task_metrics.task_id}")

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹å±¥æ­´ã«è¿½åŠ 
        self.metrics_history.append(asdict(task_metrics))

        # å±¥æ­´ä¿å­˜
        self._save_metrics_history()

        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
        self._update_realtime_metrics(task_metrics)

    def capture_queue_snapshot(self, queue_tasks: List[Dict[str, Any]]) -> QueueEfficiencySnapshot:
        """ã‚­ãƒ¥ãƒ¼çŠ¶æ…‹ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆå–å¾—"""

        print(f"ğŸ“¸ ã‚­ãƒ¥ãƒ¼ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆå–å¾—: {len(queue_tasks)}ã‚¿ã‚¹ã‚¯")

        # åŸºæœ¬çµ±è¨ˆè¨ˆç®—
        queue_size = len(queue_tasks)
        pending_tasks = sum(1 for task in queue_tasks if task.get("status", "pending") == "pending")

        # æœ€è¿‘ã®å®Ÿè¡Œãƒ‡ãƒ¼ã‚¿ã‹ã‚‰åŠ¹ç‡æ€§æŒ‡æ¨™ã‚’è¨ˆç®—
        recent_metrics = self._get_recent_metrics(hours=24)

        # å‡¦ç†ãƒ¬ãƒ¼ãƒˆè¨ˆç®—ï¼ˆã‚¿ã‚¹ã‚¯/æ™‚é–“ï¼‰
        processing_rate = self._calculate_processing_rate(recent_metrics)

        # ã‚¨ãƒ©ãƒ¼ä¿®æ­£ç‡è¨ˆç®—
        error_fix_rate = self._calculate_error_fix_rate(recent_metrics)

        # é‡è¤‡ç‡è¨ˆç®—ï¼ˆã‚­ãƒ¥ãƒ¼ã‹ã‚‰ï¼‰
        duplicate_rate = self._calculate_duplicate_rate(queue_tasks)

        # å¹³å‡å‡¦ç†æ™‚é–“è¨ˆç®—
        avg_processing_time = self._calculate_avg_processing_time(recent_metrics)

        # å“è³ªæ‰¿èªç‡è¨ˆç®—
        quality_approval_rate = self._calculate_quality_approval_rate(recent_metrics)

        # Tokenãƒ»ã‚³ã‚¹ãƒˆåŠ¹ç‡è¨ˆç®—
        token_efficiency = self._calculate_token_efficiency(recent_metrics)
        cost_efficiency = self._calculate_cost_efficiency(recent_metrics)

        snapshot = QueueEfficiencySnapshot(
            timestamp=datetime.datetime.now().isoformat(),
            queue_size=queue_size,
            pending_tasks=pending_tasks,
            processing_rate=processing_rate,
            error_fix_rate=error_fix_rate,
            duplicate_rate=duplicate_rate,
            avg_processing_time=avg_processing_time,
            quality_approval_rate=quality_approval_rate,
            token_efficiency=token_efficiency,
            cost_efficiency=cost_efficiency
        )

        # ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆå±¥æ­´ã«è¿½åŠ 
        self.snapshots_history.append(asdict(snapshot))
        self._save_snapshots_history()

        # ã‚¢ãƒ©ãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯
        alerts = self.check_and_send_alerts(snapshot)
        if alerts:
            print(f"ğŸš¨ {len(alerts)}ä»¶ã®ã‚¢ãƒ©ãƒ¼ãƒˆãŒç™ºç”Ÿã—ã¾ã—ãŸ")

        return snapshot

    def analyze_efficiency_trends(self, analysis_period_hours: float = 168) -> List[EfficiencyTrend]:
        """åŠ¹ç‡æ€§ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ1é€±é–“ï¼‰"""

        print(f"ğŸ“ˆ åŠ¹ç‡æ€§ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ: {analysis_period_hours}æ™‚é–“")

        # åˆ†æå¯¾è±¡æœŸé–“ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆå–å¾—
        current_time = datetime.datetime.now()
        cutoff_time = current_time - datetime.timedelta(hours=analysis_period_hours)

        relevant_snapshots = [
            snap for snap in self.snapshots_history
            if datetime.datetime.fromisoformat(snap["timestamp"]) >= cutoff_time
        ]

        if len(relevant_snapshots) < 2:
            print("âš ï¸ åˆ†æã«å¿…è¦ãªå±¥æ­´ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³")
            return []

        # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æå¯¾è±¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        metrics_to_analyze = [
            "error_fix_rate",
            "processing_rate",
            "duplicate_rate",
            "avg_processing_time",
            "quality_approval_rate",
            "token_efficiency",
            "cost_efficiency"
        ]

        trends = []

        for metric_name in metrics_to_analyze:
            trend = self._analyze_metric_trend(relevant_snapshots, metric_name)
            if trend:
                trends.append(trend)

        return trends

    def generate_efficiency_report(self, queue_tasks: List[Dict[str, Any]],
                                 monitoring_period_hours: float = 24) -> EfficiencyReport:
        """åŠ¹ç‡æ€§ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""

        print(f"ğŸ“‹ åŠ¹ç‡æ€§ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ: {monitoring_period_hours}æ™‚é–“åˆ†æ")

        # ç¾åœ¨ã®ã‚­ãƒ¥ãƒ¼ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ
        current_snapshot = self.capture_queue_snapshot(queue_tasks)

        # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
        efficiency_trends = self.analyze_efficiency_trends(monitoring_period_hours)

        # ç›®æ¨™é”æˆçŠ¶æ³
        achievement_status = self._evaluate_goal_achievement(current_snapshot)

        # æ¨å¥¨äº‹é …ç”Ÿæˆ
        recommendations = self._generate_recommendations(current_snapshot, efficiency_trends, achievement_status)

        # ã‚¢ãƒ©ãƒ¼ãƒˆãƒ¬ãƒ™ãƒ«æ±ºå®š
        alert_level = self._determine_alert_level(current_snapshot, achievement_status)

        report = EfficiencyReport(
            report_timestamp=datetime.datetime.now().isoformat(),
            monitoring_period_hours=monitoring_period_hours,
            current_snapshot=current_snapshot,
            efficiency_trends=efficiency_trends,
            performance_goals=self.performance_goals,
            achievement_status=achievement_status,
            recommendations=recommendations,
            alert_level=alert_level
        )

        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        self._save_efficiency_report(report)

        return report

    def _get_recent_metrics(self, hours: int = 24) -> List[Dict[str, Any]]:
        """æŒ‡å®šæ™‚é–“å†…ã®æœ€è¿‘ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—"""

        cutoff_time = datetime.datetime.now() - datetime.timedelta(hours=hours)

        recent_metrics = []
        for metric in self.metrics_history:
            try:
                metric_time = datetime.datetime.fromisoformat(metric["execution_timestamp"])
                if metric_time >= cutoff_time:
                    recent_metrics.append(metric)
            except (ValueError, KeyError):
                continue

        return recent_metrics

    def _calculate_processing_rate(self, metrics: List[Dict[str, Any]]) -> float:
        """å‡¦ç†ãƒ¬ãƒ¼ãƒˆè¨ˆç®—ï¼ˆã‚¿ã‚¹ã‚¯/æ™‚é–“ï¼‰"""

        if not metrics:
            return 0.0

        # æ™‚é–“ç¯„å›²è¨ˆç®—
        timestamps = [datetime.datetime.fromisoformat(m["execution_timestamp"]) for m in metrics]
        if len(timestamps) < 2:
            return len(metrics)  # 1æ™‚é–“ä»¥å†…ã¨ã—ã¦æ‰±ã†

        time_span_hours = (max(timestamps) - min(timestamps)).total_seconds() / 3600
        if time_span_hours == 0:
            return len(metrics)

        return len(metrics) / time_span_hours

    def _calculate_error_fix_rate(self, metrics: List[Dict[str, Any]]) -> float:
        """ã‚¨ãƒ©ãƒ¼ä¿®æ­£ç‡è¨ˆç®—"""

        if not metrics:
            return 0.0

        total_tasks = len(metrics)
        successful_fixes = sum(1 for m in metrics if m.get("errors_fixed", 0) > 0)

        return successful_fixes / total_tasks if total_tasks > 0 else 0.0

    def _calculate_duplicate_rate(self, queue_tasks: List[Dict[str, Any]]) -> float:
        """é‡è¤‡ç‡è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""

        if len(queue_tasks) < 2:
            return 0.0

        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ»ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—ã®çµ„ã¿åˆã‚ã›ã§é‡è¤‡åˆ¤å®š
        task_signatures = set()
        duplicates = 0

        for task in queue_tasks:
            target_files = tuple(sorted(task.get("target_files", [])))
            error_type = task.get("requirements", {}).get("error_type", "")
            signature = (target_files, error_type)

            if signature in task_signatures:
                duplicates += 1
            else:
                task_signatures.add(signature)

        return duplicates / len(queue_tasks) if queue_tasks else 0.0

    def _calculate_avg_processing_time(self, metrics: List[Dict[str, Any]]) -> float:
        """å¹³å‡å‡¦ç†æ™‚é–“è¨ˆç®—"""

        if not metrics:
            return 0.0

        processing_times = [m.get("execution_time_seconds", 0.0) for m in metrics]
        return statistics.mean(processing_times) if processing_times else 0.0

    def _calculate_quality_approval_rate(self, metrics: List[Dict[str, Any]]) -> float:
        """å“è³ªæ‰¿èªç‡è¨ˆç®—"""

        if not metrics:
            return 0.0

        quality_scores = [m.get("quality_score", 0.0) for m in metrics]
        approved = sum(1 for score in quality_scores if score >= 0.7)  # 70%ä»¥ä¸Šã‚’æ‰¿èªã¨åˆ¤å®š

        return approved / len(quality_scores) if quality_scores else 0.0

    def _calculate_token_efficiency(self, metrics: List[Dict[str, Any]]) -> float:
        """TokenåŠ¹ç‡è¨ˆç®—ï¼ˆä¿®æ­£ä»¶æ•°/Tokenæ•°ï¼‰"""

        if not metrics:
            return 0.0

        total_fixes = sum(m.get("errors_fixed", 0) for m in metrics)
        total_tokens = sum(m.get("token_usage", 0) for m in metrics)

        return total_fixes / total_tokens if total_tokens > 0 else 0.0

    def _calculate_cost_efficiency(self, metrics: List[Dict[str, Any]]) -> float:
        """ã‚³ã‚¹ãƒˆåŠ¹ç‡è¨ˆç®—ï¼ˆä¿®æ­£ä»¶æ•°/ã‚³ã‚¹ãƒˆï¼‰"""

        if not metrics:
            return 0.0

        total_fixes = sum(m.get("errors_fixed", 0) for m in metrics)
        total_cost = sum(m.get("cost_estimate", 0.0) for m in metrics)

        return total_fixes / total_cost if total_cost > 0 else 0.0

    def _analyze_metric_trend(self, snapshots: List[Dict[str, Any]], metric_name: str) -> Optional[EfficiencyTrend]:
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ"""

        values = [snap.get(metric_name, 0) for snap in snapshots]

        if len(values) < 2:
            return None

        # ç›´è¿‘ã¨åˆæœŸã®å€¤ã‚’æ¯”è¼ƒ
        current_value = values[-1]
        previous_value = values[0]

        if previous_value == 0:
            change_percentage = 100.0 if current_value > 0 else 0.0
        else:
            change_percentage = ((current_value - previous_value) / previous_value) * 100

        # ãƒˆãƒ¬ãƒ³ãƒ‰æ–¹å‘æ±ºå®š
        if abs(change_percentage) < 5:
            trend_direction = "stable"
        elif change_percentage > 0:
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«ã‚ˆã£ã¦æ”¹å–„/æ‚ªåŒ–ã®åˆ¤å®šãŒé€†è»¢
            if metric_name in ["duplicate_rate", "avg_processing_time"]:
                trend_direction = "degrading"  # ã“ã‚Œã‚‰ã¯ä¸‹ãŒã‚‹æ–¹ãŒè‰¯ã„
            else:
                trend_direction = "improving"  # ã“ã‚Œã‚‰ã¯ä¸ŠãŒã‚‹æ–¹ãŒè‰¯ã„
        else:
            if metric_name in ["duplicate_rate", "avg_processing_time"]:
                trend_direction = "improving"
            else:
                trend_direction = "degrading"

        # ãƒˆãƒ¬ãƒ³ãƒ‰å¼·åº¦
        abs_change = abs(change_percentage)
        if abs_change > 20:
            trend_strength = "strong"
        elif abs_change > 10:
            trend_strength = "moderate"
        else:
            trend_strength = "weak"

        return EfficiencyTrend(
            metric_name=metric_name,
            current_value=current_value,
            previous_value=previous_value,
            change_percentage=change_percentage,
            trend_direction=trend_direction,
            trend_strength=trend_strength
        )

    def _evaluate_goal_achievement(self, snapshot: QueueEfficiencySnapshot) -> Dict[str, bool]:
        """ç›®æ¨™é”æˆçŠ¶æ³è©•ä¾¡"""

        achievement_status = {}

        # å„ç›®æ¨™ã®é”æˆçŠ¶æ³ç¢ºèª
        achievement_status["error_fix_rate"] = snapshot.error_fix_rate >= self.performance_goals["error_fix_rate"]
        achievement_status["queue_size"] = snapshot.queue_size <= self.performance_goals["queue_size_target"]
        achievement_status["processing_time"] = snapshot.avg_processing_time <= self.performance_goals["processing_time_target"]
        achievement_status["quality_approval_rate"] = snapshot.quality_approval_rate >= self.performance_goals["quality_approval_rate"]
        achievement_status["duplicate_rate"] = snapshot.duplicate_rate <= self.performance_goals["duplicate_rate_max"]
        achievement_status["token_efficiency"] = snapshot.token_efficiency >= self.performance_goals["token_efficiency_min"]
        achievement_status["cost_efficiency"] = snapshot.cost_efficiency >= self.performance_goals["cost_efficiency_min"]

        return achievement_status

    def _generate_recommendations(self, snapshot: QueueEfficiencySnapshot,
                                trends: List[EfficiencyTrend],
                                achievement_status: Dict[str, bool]) -> List[str]:
        """æ¨å¥¨äº‹é …ç”Ÿæˆ"""

        recommendations = []

        # ç›®æ¨™æœªé”æˆé …ç›®ã¸ã®å¯¾å¿œ
        if not achievement_status.get("error_fix_rate", True):
            recommendations.append(f"ã‚¨ãƒ©ãƒ¼ä¿®æ­£ç‡ãŒç›®æ¨™æœªé”æˆï¼ˆç¾åœ¨: {snapshot.error_fix_rate:.1%}ã€ç›®æ¨™: {self.performance_goals['error_fix_rate']:.1%}ï¼‰")
            recommendations.append("- ã‚¿ã‚¹ã‚¯äº‹å‰æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ ã®å°å…¥ã‚’æ¤œè¨")
            recommendations.append("- ä¿®æ­£å›°é›£ãªã‚¿ã‚¹ã‚¯ã®æ‰‹å‹•å¯¾å¿œã¸ã®ç§»è¡Œ")

        if not achievement_status.get("queue_size", True):
            recommendations.append(f"ã‚­ãƒ¥ãƒ¼ã‚µã‚¤ã‚ºãŒç›®æ¨™ã‚’è¶…éï¼ˆç¾åœ¨: {snapshot.queue_size}ä»¶ã€ç›®æ¨™: {self.performance_goals['queue_size_target']}ä»¶ï¼‰")
            recommendations.append("- é‡è¤‡ã‚¿ã‚¹ã‚¯æ’é™¤ã®å®Ÿæ–½")
            recommendations.append("- ã‚¿ã‚¹ã‚¯å„ªå…ˆåº¦ã®å†è©•ä¾¡")

        if not achievement_status.get("duplicate_rate", True):
            recommendations.append(f"é‡è¤‡ç‡ãŒé«˜ã„ï¼ˆç¾åœ¨: {snapshot.duplicate_rate:.1%}ã€ä¸Šé™: {self.performance_goals['duplicate_rate_max']:.1%}ï¼‰")
            recommendations.append("- TaskDeduplicatorã®æ´»ç”¨")

        if not achievement_status.get("quality_approval_rate", True):
            recommendations.append(f"å“è³ªæ‰¿èªç‡ãŒä½ã„ï¼ˆç¾åœ¨: {snapshot.quality_approval_rate:.1%}ã€ç›®æ¨™: {self.performance_goals['quality_approval_rate']:.1%}ï¼‰")
            recommendations.append("- å“è³ªãƒã‚§ãƒƒã‚¯åŸºæº–ã®è¦‹ç›´ã—")
            recommendations.append("- AIä¿®æ­£ç²¾åº¦ã®å‘ä¸Šç­–æ¤œè¨")

        # ãƒˆãƒ¬ãƒ³ãƒ‰ã«åŸºã¥ãæ¨å¥¨äº‹é …
        for trend in trends:
            if trend.trend_direction == "degrading" and trend.trend_strength in ["strong", "moderate"]:
                recommendations.append(f"{trend.metric_name}ãŒæ‚ªåŒ–å‚¾å‘ï¼ˆ{trend.change_percentage:.1f}%å¤‰åŒ–ï¼‰")

        # å…¨èˆ¬çš„ãªæ¨å¥¨äº‹é …
        if len([status for status in achievement_status.values() if not status]) >= 3:
            recommendations.append("è¤‡æ•°æŒ‡æ¨™ã§ç›®æ¨™æœªé”æˆ - ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®è¦‹ç›´ã—ãŒå¿…è¦")

        return recommendations

    def _determine_alert_level(self, snapshot: QueueEfficiencySnapshot,
                             achievement_status: Dict[str, bool]) -> EfficiencyLevel:
        """ã‚¢ãƒ©ãƒ¼ãƒˆãƒ¬ãƒ™ãƒ«æ±ºå®š"""

        # é”æˆã—ã¦ã„ã‚‹ç›®æ¨™ã®æ•°
        achieved_goals = sum(achievement_status.values())
        total_goals = len(achievement_status)
        achievement_rate = achieved_goals / total_goals

        # ã‚¢ãƒ©ãƒ¼ãƒˆãƒ¬ãƒ™ãƒ«æ±ºå®š
        if achievement_rate >= 0.90:
            return EfficiencyLevel.EXCELLENT
        elif achievement_rate >= 0.70:
            return EfficiencyLevel.GOOD
        elif achievement_rate >= 0.50:
            return EfficiencyLevel.ACCEPTABLE
        elif achievement_rate >= 0.30:
            return EfficiencyLevel.POOR
        else:
            return EfficiencyLevel.CRITICAL

    def _update_realtime_metrics(self, metrics: TaskExecutionMetrics) -> None:
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°"""

        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ çµ±è¨ˆãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°
        realtime_path = Path("postbox/monitoring/realtime_efficiency.json")

        realtime_data = {
            "last_update": datetime.datetime.now().isoformat(),
            "recent_task": {
                "task_id": metrics.task_id,
                "success": metrics.errors_fixed > 0,
                "processing_time": metrics.execution_time_seconds,
                "quality_score": metrics.quality_score,
                "ai_agent": metrics.ai_agent
            }
        }

        with open(realtime_path, 'w', encoding='utf-8') as f:
            json.dump(realtime_data, f, indent=2, ensure_ascii=False)

    def _load_metrics_history(self) -> List[Dict[str, Any]]:
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹å±¥æ­´ãƒ­ãƒ¼ãƒ‰"""

        if self.metrics_history_path.exists():
            try:
                with open(self.metrics_history_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"âš ï¸ ãƒ¡ãƒˆãƒªã‚¯ã‚¹å±¥æ­´èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
                return []
        return []

    def _load_snapshots_history(self) -> List[Dict[str, Any]]:
        """ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆå±¥æ­´ãƒ­ãƒ¼ãƒ‰"""

        if self.snapshots_path.exists():
            try:
                with open(self.snapshots_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"âš ï¸ ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆå±¥æ­´èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
                return []
        return []

    def _save_metrics_history(self) -> None:
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹å±¥æ­´ä¿å­˜"""

        # éå»30æ—¥åˆ†ã®ã¿ä¿æŒ
        cutoff_time = datetime.datetime.now() - datetime.timedelta(days=30)

        filtered_history = []
        for metric in self.metrics_history:
            try:
                metric_time = datetime.datetime.fromisoformat(metric["execution_timestamp"])
                if metric_time >= cutoff_time:
                    filtered_history.append(metric)
            except (ValueError, KeyError):
                continue

        self.metrics_history = filtered_history

        with open(self.metrics_history_path, 'w', encoding='utf-8') as f:
            json.dump(self.metrics_history, f, indent=2, ensure_ascii=False)

    def _save_snapshots_history(self) -> None:
        """ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆå±¥æ­´ä¿å­˜"""

        # éå»30æ—¥åˆ†ã®ã¿ä¿æŒ
        cutoff_time = datetime.datetime.now() - datetime.timedelta(days=30)

        filtered_history = []
        for snapshot in self.snapshots_history:
            try:
                snapshot_time = datetime.datetime.fromisoformat(snapshot["timestamp"])
                if snapshot_time >= cutoff_time:
                    filtered_history.append(snapshot)
            except (ValueError, KeyError):
                continue

        self.snapshots_history = filtered_history

        with open(self.snapshots_path, 'w', encoding='utf-8') as f:
            json.dump(self.snapshots_history, f, indent=2, ensure_ascii=False)

    def _save_efficiency_report(self, report: EfficiencyReport) -> None:
        """åŠ¹ç‡æ€§ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜"""

        # æ—¢å­˜ãƒ¬ãƒãƒ¼ãƒˆèª­ã¿è¾¼ã¿
        reports = []
        if self.reports_path.exists():
            try:
                with open(self.reports_path, 'r', encoding='utf-8') as f:
                    reports = json.load(f)
            except Exception:
                reports = []

        # æ–°ã—ã„ãƒ¬ãƒãƒ¼ãƒˆè¿½åŠ ï¼ˆEnumã‚’æ–‡å­—åˆ—ã«å¤‰æ›ï¼‰
        report_dict = asdict(report)
        self._convert_enums_to_strings(report_dict)
        reports.append(report_dict)

        # éå»30ä»¶ã®ã¿ä¿æŒ
        reports = reports[-30:]

        with open(self.reports_path, 'w', encoding='utf-8') as f:
            json.dump(reports, f, indent=2, ensure_ascii=False)

    def _convert_enums_to_strings(self, data: Any) -> None:
        """Enumã‚’JSONç”¨ã«æ–‡å­—åˆ—ã«å¤‰æ›"""
        if isinstance(data, dict):
            for key, value in data.items():
                if hasattr(value, 'value'):  # Enumã®å ´åˆ
                    data[key] = value.value
                elif isinstance(value, (dict, list)):
                    self._convert_enums_to_strings(value)
        elif isinstance(data, list):
            for item in data:
                self._convert_enums_to_strings(item)

    def create_efficiency_dashboard_data(self) -> Dict[str, Any]:
        """åŠ¹ç‡æ€§ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”¨ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""

        # æœ€æ–°ã®çµ±è¨ˆ
        recent_metrics = self._get_recent_metrics(24)
        recent_snapshots = self.snapshots_history[-10:] if self.snapshots_history else []

        dashboard_data = {
            "timestamp": datetime.datetime.now().isoformat(),
            "summary": {
                "total_tasks_processed": len(self.metrics_history),
                "recent_tasks_24h": len(recent_metrics),
                "current_queue_size": recent_snapshots[-1]["queue_size"] if recent_snapshots else 0,
                "avg_error_fix_rate": self._calculate_error_fix_rate(recent_metrics),
                "avg_quality_score": statistics.mean([m.get("quality_score", 0) for m in recent_metrics]) if recent_metrics else 0
            },
            "charts": {
                "error_fix_rate_trend": [snap.get("error_fix_rate", 0) for snap in recent_snapshots],
                "queue_size_trend": [snap.get("queue_size", 0) for snap in recent_snapshots],
                "processing_time_trend": [snap.get("avg_processing_time", 0) for snap in recent_snapshots],
                "quality_approval_trend": [snap.get("quality_approval_rate", 0) for snap in recent_snapshots]
            },
            "performance_goals": self.performance_goals
        }

        return dashboard_data

    def save_dashboard_data(self, output_path: str = "tmp/efficiency_dashboard.json") -> str:
        """ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ä¿å­˜"""

        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)

        dashboard_data = self.create_efficiency_dashboard_data()

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(dashboard_data, f, indent=2, ensure_ascii=False)

        print(f"ğŸ“Š ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {output_path}")
        return output_path

    def check_and_send_alerts(self, snapshot: QueueEfficiencySnapshot) -> List[str]:
        """ã‚¢ãƒ©ãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯ãƒ»é€šçŸ¥é€ä¿¡"""

        if not self.alert_settings["alert_enabled"]:
            return []

        alerts = []

        # ã‚­ãƒ¥ãƒ¼ã‚µã‚¤ã‚ºã‚¢ãƒ©ãƒ¼ãƒˆ
        if snapshot.queue_size > self.alert_settings["queue_size_alert_threshold"]:
            alert_msg = f"ğŸš¨ ã‚­ãƒ¥ãƒ¼ã‚µã‚¤ã‚ºè­¦å‘Š: {snapshot.queue_size}ä»¶ (é–¾å€¤: {self.alert_settings['queue_size_alert_threshold']}ä»¶)"
            alerts.append(alert_msg)
            self._send_alert_notification(alert_msg, "queue_size")

        # ã‚¨ãƒ©ãƒ¼ä¿®æ­£ç‡ã‚¢ãƒ©ãƒ¼ãƒˆ
        if snapshot.error_fix_rate < self.alert_settings["error_fix_rate_alert_threshold"]:
            alert_msg = f"ğŸš¨ ã‚¨ãƒ©ãƒ¼ä¿®æ­£ç‡è­¦å‘Š: {snapshot.error_fix_rate:.1%} (é–¾å€¤: {self.alert_settings['error_fix_rate_alert_threshold']:.1%})"
            alerts.append(alert_msg)
            self._send_alert_notification(alert_msg, "error_fix_rate")

        # å‡¦ç†æ™‚é–“ã‚¢ãƒ©ãƒ¼ãƒˆ
        if snapshot.avg_processing_time > self.alert_settings["processing_time_alert_threshold"]:
            alert_msg = f"ğŸš¨ å‡¦ç†æ™‚é–“è­¦å‘Š: {snapshot.avg_processing_time:.1f}ç§’ (é–¾å€¤: {self.alert_settings['processing_time_alert_threshold']}ç§’)"
            alerts.append(alert_msg)
            self._send_alert_notification(alert_msg, "processing_time")

        # é‡è¤‡ç‡ã‚¢ãƒ©ãƒ¼ãƒˆ
        if snapshot.duplicate_rate > 0.3:  # 30%ä»¥ä¸Šã§è­¦å‘Š
            alert_msg = f"ğŸš¨ é‡è¤‡ç‡è­¦å‘Š: {snapshot.duplicate_rate:.1%} - é‡è¤‡æ’é™¤ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè¡Œã‚’æ¨å¥¨"
            alerts.append(alert_msg)
            self._send_alert_notification(alert_msg, "duplicate_rate")

        return alerts

    def _send_alert_notification(self, message: str, alert_type: str) -> None:
        """ã‚¢ãƒ©ãƒ¼ãƒˆé€šçŸ¥é€ä¿¡"""

        # ã‚¢ãƒ©ãƒ¼ãƒˆå±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ã«è¨˜éŒ²
        alert_file = Path("postbox/monitoring/alerts.json")
        alert_file.parent.mkdir(parents=True, exist_ok=True)

        alert_record = {
            "timestamp": datetime.datetime.now().isoformat(),
            "alert_type": alert_type,
            "message": message,
            "severity": "warning"
        }

        alerts_history = []
        if alert_file.exists():
            try:
                with open(alert_file, 'r', encoding='utf-8') as f:
                    alerts_history = json.load(f)
            except Exception:
                alerts_history = []

        alerts_history.append(alert_record)

        # éå»100ä»¶ã®ã¿ä¿æŒ
        alerts_history = alerts_history[-100:]

        with open(alert_file, 'w', encoding='utf-8') as f:
            json.dump(alerts_history, f, indent=2, ensure_ascii=False)

        # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›
        print(f"âš ï¸ {message}")

        # å°†æ¥çš„ã«ã¯Slackã€ãƒ¡ãƒ¼ãƒ«ç­‰ã®é€šçŸ¥ã‚‚å¯èƒ½


def main() -> None:
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""

    print("ğŸ§ª EfficiencyTracker ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")

    tracker = EfficiencyTracker()

    # ãƒ†ã‚¹ãƒˆç”¨ã‚¿ã‚¹ã‚¯å®Ÿè¡Œãƒ¡ãƒˆãƒªã‚¯ã‚¹ä½œæˆ
    test_metrics = [
        TaskExecutionMetrics(
            task_id="test_001",
            execution_timestamp=datetime.datetime.now().isoformat(),
            execution_time_seconds=3.5,
            errors_before=5,
            errors_after=0,
            errors_fixed=5,
            success_rate=1.0,
            quality_score=0.85,
            token_usage=1200,
            cost_estimate=0.0036,
            ai_agent="gemini",
            task_type="file_code_modification",
            file_count=1
        ),
        TaskExecutionMetrics(
            task_id="test_002",
            execution_timestamp=(datetime.datetime.now() - datetime.timedelta(hours=1)).isoformat(),
            execution_time_seconds=5.2,
            errors_before=3,
            errors_after=2,
            errors_fixed=1,
            success_rate=0.33,
            quality_score=0.6,
            token_usage=1800,
            cost_estimate=0.0054,
            ai_agent="claude",
            task_type="file_code_modification",
            file_count=2
        )
    ]

    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
    for metrics in test_metrics:
        tracker.record_task_execution(metrics)

    # ãƒ†ã‚¹ãƒˆç”¨ã‚­ãƒ¥ãƒ¼ã‚¿ã‚¹ã‚¯
    test_queue = [
        {"task_id": "queue_001", "target_files": ["file_a.py"], "requirements": {"error_type": "no-untyped-def"}},
        {"task_id": "queue_002", "target_files": ["file_a.py"], "requirements": {"error_type": "no-untyped-def"}},  # é‡è¤‡
        {"task_id": "queue_003", "target_files": ["file_b.py"], "requirements": {"error_type": "call-arg"}},
        {"task_id": "queue_004", "target_files": ["file_c.py"], "requirements": {"error_type": "type-arg"}}
    ]

    # åŠ¹ç‡æ€§ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    print("\nğŸ“‹ åŠ¹ç‡æ€§ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ:")
    report = tracker.generate_efficiency_report(test_queue, monitoring_period_hours=1)

    print(f"ç¾åœ¨ã®ã‚­ãƒ¥ãƒ¼ã‚µã‚¤ã‚º: {report.current_snapshot.queue_size}")
    print(f"ã‚¨ãƒ©ãƒ¼ä¿®æ­£ç‡: {report.current_snapshot.error_fix_rate:.1%}")
    print(f"é‡è¤‡ç‡: {report.current_snapshot.duplicate_rate:.1%}")
    print(f"å“è³ªæ‰¿èªç‡: {report.current_snapshot.quality_approval_rate:.1%}")
    print(f"ã‚¢ãƒ©ãƒ¼ãƒˆãƒ¬ãƒ™ãƒ«: {report.alert_level.value}")

    # ç›®æ¨™é”æˆçŠ¶æ³
    print(f"\nğŸ¯ ç›®æ¨™é”æˆçŠ¶æ³:")
    for goal, achieved in report.achievement_status.items():
        status = "âœ…" if achieved else "âŒ"
        print(f"  {status} {goal}")

    # æ¨å¥¨äº‹é …
    print(f"\nğŸ’¡ æ¨å¥¨äº‹é …: {len(report.recommendations)}ä»¶")
    for i, rec in enumerate(report.recommendations[:3], 1):
        print(f"  {i}. {rec}")

    # ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    print("\nğŸ“Š ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ:")
    dashboard_path = tracker.save_dashboard_data()

    print("\nâœ… EfficiencyTracker ãƒ†ã‚¹ãƒˆå®Œäº†")


if __name__ == "__main__":
    main()
