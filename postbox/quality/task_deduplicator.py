#!/usr/bin/env python3
"""
Task Deduplication System
ã‚¿ã‚¹ã‚¯é‡è¤‡æ’é™¤ã‚·ã‚¹ãƒ†ãƒ  - é‡è¤‡ãƒ»é¡ä¼¼ã‚¿ã‚¹ã‚¯ã®çµ±åˆã«ã‚ˆã‚‹åŠ¹ç‡åŒ–
"""

import json
import hashlib
import datetime
from typing import Dict, List, Any, Optional, Set, Tuple
from pathlib import Path
from dataclasses import dataclass
from enum import Enum


class DuplicationLevel(Enum):
    """é‡è¤‡ãƒ¬ãƒ™ãƒ«"""
    IDENTICAL = "identical"         # å®Œå…¨ä¸€è‡´
    HIGHLY_SIMILAR = "highly_similar"   # é«˜åº¦é¡ä¼¼ï¼ˆ90%ä»¥ä¸Šï¼‰
    SIMILAR = "similar"            # é¡ä¼¼ï¼ˆ70-89%ï¼‰
    RELATED = "related"           # é–¢é€£ï¼ˆ50-69%ï¼‰
    DIFFERENT = "different"        # ç•°ãªã‚‹ï¼ˆ50%æœªæº€ï¼‰


class MergeStrategy(Enum):
    """ãƒãƒ¼ã‚¸æˆ¦ç•¥"""
    KEEP_NEWEST = "keep_newest"     # æœ€æ–°ã‚’ä¿æŒ
    KEEP_HIGHEST_PRIORITY = "keep_highest_priority"  # é«˜å„ªå…ˆåº¦ã‚’ä¿æŒ
    MERGE_TARGETS = "merge_targets" # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµ±åˆ
    MERGE_REQUIREMENTS = "merge_requirements"  # è¦ä»¶ã‚’çµ±åˆ
    MANUAL_REVIEW = "manual_review" # æ‰‹å‹•ç¢ºèªãŒå¿…è¦


@dataclass
class TaskHash:
    """ã‚¿ã‚¹ã‚¯ãƒãƒƒã‚·ãƒ¥æƒ…å ±"""
    task_id: str
    content_hash: str
    targets_hash: str
    requirements_hash: str
    combined_hash: str
    hash_components: Dict[str, str]


@dataclass
class DuplicationGroup:
    """é‡è¤‡ã‚°ãƒ«ãƒ¼ãƒ—"""
    group_id: str
    duplication_level: DuplicationLevel
    similarity_score: float
    tasks: List[Dict[str, Any]]
    merge_strategy: MergeStrategy
    recommended_action: str
    merged_task: Optional[Dict[str, Any]]
    savings_estimate: Dict[str, Any]


@dataclass
class DeduplicationReport:
    """é‡è¤‡æ’é™¤ãƒ¬ãƒãƒ¼ãƒˆ"""
    original_task_count: int
    duplicate_groups_found: int
    tasks_removed: int
    tasks_merged: int
    estimated_time_saved: float
    estimated_token_saved: int
    duplication_groups: List[DuplicationGroup]
    action_summary: Dict[str, int]


class TaskDeduplicator:
    """ã‚¿ã‚¹ã‚¯é‡è¤‡æ’é™¤ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self) -> None:
        self.task_hashes = {}
        self.similarity_cache = {}

        # é‡è¤‡åˆ¤å®šè¨­å®š
        self.duplication_thresholds = {
            "identical": 1.0,
            "highly_similar": 0.90,
            "similar": 0.70,
            "related": 0.50
        }

        # ãƒãƒ¼ã‚¸ãƒ«ãƒ¼ãƒ«è¨­å®š
        self.merge_rules = {
            "identical": MergeStrategy.KEEP_NEWEST,
            "highly_similar": MergeStrategy.MERGE_TARGETS,
            "similar": MergeStrategy.MERGE_REQUIREMENTS,
            "related": MergeStrategy.MANUAL_REVIEW
        }

        print("ğŸ”„ TaskDeduplicator åˆæœŸåŒ–å®Œäº†")

    def deduplicate_tasks(self, task_list: List[Dict[str, Any]]) -> DeduplicationReport:
        """
        ã‚¿ã‚¹ã‚¯ã®é‡è¤‡æ’é™¤å®Ÿè¡Œ

        Args:
            task_list: å¯¾è±¡ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆ

        Returns:
            DeduplicationReport: é‡è¤‡æ’é™¤ãƒ¬ãƒãƒ¼ãƒˆ
        """

        print(f"ğŸ”„ ã‚¿ã‚¹ã‚¯é‡è¤‡æ’é™¤é–‹å§‹: {len(task_list)}ã‚¿ã‚¹ã‚¯")

        # Step 1: ã‚¿ã‚¹ã‚¯ãƒãƒƒã‚·ãƒ¥ç”Ÿæˆ
        task_hashes = self._generate_task_hashes(task_list)

        # Step 2: é‡è¤‡ã‚°ãƒ«ãƒ¼ãƒ—æ¤œå‡º
        duplicate_groups = self._detect_duplicate_groups(task_list, task_hashes)

        # Step 3: ãƒãƒ¼ã‚¸æˆ¦ç•¥æ±ºå®š
        for group in duplicate_groups:
            self._determine_merge_strategy(group)

        # Step 4: ã‚¿ã‚¹ã‚¯çµ±åˆå®Ÿè¡Œ
        deduplicated_tasks, action_summary = self._execute_deduplication(task_list, duplicate_groups)

        # Step 5: ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report = DeduplicationReport(
            original_task_count=len(task_list),
            duplicate_groups_found=len(duplicate_groups),
            tasks_removed=action_summary.get("removed", 0),
            tasks_merged=action_summary.get("merged", 0),
            estimated_time_saved=self._estimate_time_savings(duplicate_groups),
            estimated_token_saved=self._estimate_token_savings(duplicate_groups),
            duplication_groups=duplicate_groups,
            action_summary=action_summary
        )

        print(f"âœ… é‡è¤‡æ’é™¤å®Œäº†: {report.tasks_removed}ä»¶å‰Šé™¤, {report.tasks_merged}ä»¶çµ±åˆ")
        return report

    def _generate_task_hashes(self, task_list: List[Dict[str, Any]]) -> Dict[str, TaskHash]:
        """ã‚¿ã‚¹ã‚¯ãƒãƒƒã‚·ãƒ¥ç”Ÿæˆ"""

        task_hashes = {}

        for task_data in task_list:
            task_id = task_data.get("task_id", "unknown")

            # å„è¦ç´ ã®ãƒãƒƒã‚·ãƒ¥è¨ˆç®—
            content_elements = {
                "type": task_data.get("type", ""),
                "description": task_data.get("description", ""),
                "priority": task_data.get("priority", "")
            }
            content_hash = self._calculate_hash(json.dumps(content_elements, sort_keys=True))

            targets_hash = self._calculate_hash(json.dumps(sorted(task_data.get("target_files", []))))

            requirements = task_data.get("requirements", {})
            requirements_hash = self._calculate_hash(json.dumps(requirements, sort_keys=True))

            # çµ±åˆãƒãƒƒã‚·ãƒ¥
            combined_data = f"{content_hash}:{targets_hash}:{requirements_hash}"
            combined_hash = self._calculate_hash(combined_data)

            task_hash = TaskHash(
                task_id=task_id,
                content_hash=content_hash,
                targets_hash=targets_hash,
                requirements_hash=requirements_hash,
                combined_hash=combined_hash,
                hash_components={
                    "content": content_hash,
                    "targets": targets_hash,
                    "requirements": requirements_hash
                }
            )

            task_hashes[task_id] = task_hash

        print(f"ğŸ”¢ ã‚¿ã‚¹ã‚¯ãƒãƒƒã‚·ãƒ¥ç”Ÿæˆå®Œäº†: {len(task_hashes)}ä»¶")
        return task_hashes

    def _detect_duplicate_groups(self, task_list: List[Dict[str, Any]],
                               task_hashes: Dict[str, TaskHash]) -> List[DuplicationGroup]:
        """é‡è¤‡ã‚°ãƒ«ãƒ¼ãƒ—æ¤œå‡º"""

        duplicate_groups = []
        processed_tasks = set()

        for i, task_a in enumerate(task_list):
            task_id_a = task_a.get("task_id", "unknown")

            if task_id_a in processed_tasks:
                continue

            # é¡ä¼¼ã‚¿ã‚¹ã‚¯ã‚’æ¤œç´¢
            similar_tasks = [task_a]
            hash_a = task_hashes.get(task_id_a)

            for j, task_b in enumerate(task_list[i+1:], i+1):
                task_id_b = task_b.get("task_id", "unknown")

                if task_id_b in processed_tasks:
                    continue

                hash_b = task_hashes.get(task_id_b)
                if not hash_a or not hash_b:
                    continue

                # é¡ä¼¼åº¦è¨ˆç®—
                similarity_score = self._calculate_similarity(task_a, task_b, hash_a, hash_b)

                if similarity_score >= self.duplication_thresholds["related"]:
                    similar_tasks.append(task_b)
                    processed_tasks.add(task_id_b)

            # é‡è¤‡ã‚°ãƒ«ãƒ¼ãƒ—ãŒè¦‹ã¤ã‹ã£ãŸå ´åˆ
            if len(similar_tasks) > 1:
                # é¡ä¼¼åº¦ãƒ¬ãƒ™ãƒ«æ±ºå®š
                avg_similarity = self._calculate_group_similarity(similar_tasks, task_hashes)
                duplication_level = self._determine_duplication_level(avg_similarity)

                group = DuplicationGroup(
                    group_id=f"dup_group_{len(duplicate_groups):03d}",
                    duplication_level=duplication_level,
                    similarity_score=avg_similarity,
                    tasks=similar_tasks,
                    merge_strategy=self.merge_rules.get(duplication_level.value, MergeStrategy.MANUAL_REVIEW),
                    recommended_action="",
                    merged_task=None,
                    savings_estimate={}
                )

                duplicate_groups.append(group)
                processed_tasks.add(task_id_a)

        print(f"ğŸ” é‡è¤‡ã‚°ãƒ«ãƒ¼ãƒ—æ¤œå‡º: {len(duplicate_groups)}ã‚°ãƒ«ãƒ¼ãƒ—")
        return duplicate_groups

    def _calculate_similarity(self, task_a: Dict[str, Any], task_b: Dict[str, Any],
                            hash_a: TaskHash, hash_b: TaskHash) -> float:
        """ã‚¿ã‚¹ã‚¯é¡ä¼¼åº¦è¨ˆç®—"""

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        cache_key = f"{hash_a.task_id}:{hash_b.task_id}"
        if cache_key in self.similarity_cache:
            return self.similarity_cache[cache_key]

        # å„è¦ç´ ã®é¡ä¼¼åº¦ã‚’é‡ã¿ä»˜ãè¨ˆç®—
        similarities = {
            "content": 0.4,     # ã‚¿ã‚¤ãƒ—ãƒ»èª¬æ˜ãƒ»å„ªå…ˆåº¦
            "targets": 0.4,     # å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«
            "requirements": 0.2  # è¦ä»¶
        }

        total_similarity = 0.0

        for component, weight in similarities.items():
            component_similarity = self._calculate_component_similarity(
                task_a, task_b, component
            )
            total_similarity += component_similarity * weight

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
        self.similarity_cache[cache_key] = total_similarity
        self.similarity_cache[f"{hash_b.task_id}:{hash_a.task_id}"] = total_similarity

        return total_similarity

    def _calculate_component_similarity(self, task_a: Dict[str, Any],
                                     task_b: Dict[str, Any], component: str) -> float:
        """ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆé¡ä¼¼åº¦è¨ˆç®—"""

        if component == "content":
            # ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—
            type_match = 1.0 if task_a.get("type") == task_b.get("type") else 0.0

            # èª¬æ˜æ–‡ã®é¡ä¼¼åº¦ï¼ˆç°¡æ˜“ç‰ˆï¼‰
            desc_a = task_a.get("description", "").lower()
            desc_b = task_b.get("description", "").lower()
            desc_similarity = self._text_similarity(desc_a, desc_b)

            # å„ªå…ˆåº¦
            priority_match = 1.0 if task_a.get("priority") == task_b.get("priority") else 0.5

            return (type_match * 0.5 + desc_similarity * 0.3 + priority_match * 0.2)

        elif component == "targets":
            targets_a = set(task_a.get("target_files", []))
            targets_b = set(task_b.get("target_files", []))

            if not targets_a and not targets_b:
                return 1.0

            intersection = targets_a & targets_b
            union = targets_a | targets_b

            return len(intersection) / len(union) if union else 0.0

        elif component == "requirements":
            req_a = task_a.get("requirements", {})
            req_b = task_b.get("requirements", {})

            if not req_a and not req_b:
                return 1.0

            # ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—ã®æ¯”è¼ƒ
            error_type_match = 1.0 if req_a.get("error_type") == req_b.get("error_type") else 0.0

            # ä¿®æ­£ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¯”è¼ƒ
            fix_pattern_match = 1.0 if req_a.get("fix_pattern") == req_b.get("fix_pattern") else 0.0

            return (error_type_match * 0.7 + fix_pattern_match * 0.3)

        return 0.0

    def _text_similarity(self, text_a: str, text_b: str) -> float:
        """ãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""

        if not text_a and not text_b:
            return 1.0

        if not text_a or not text_b:
            return 0.0

        # å˜èªãƒ™ãƒ¼ã‚¹ã®é¡ä¼¼åº¦
        words_a = set(text_a.split())
        words_b = set(text_b.split())

        intersection = words_a & words_b
        union = words_a | words_b

        return len(intersection) / len(union) if union else 0.0

    def _calculate_group_similarity(self, tasks: List[Dict[str, Any]],
                                  task_hashes: Dict[str, TaskHash]) -> float:
        """ã‚°ãƒ«ãƒ¼ãƒ—å†…å¹³å‡é¡ä¼¼åº¦è¨ˆç®—"""

        if len(tasks) < 2:
            return 1.0

        total_similarity = 0.0
        comparison_count = 0

        for i in range(len(tasks)):
            for j in range(i+1, len(tasks)):
                task_id_a = tasks[i].get("task_id", "unknown")
                task_id_b = tasks[j].get("task_id", "unknown")

                hash_a = task_hashes.get(task_id_a)
                hash_b = task_hashes.get(task_id_b)

                if hash_a and hash_b:
                    similarity = self._calculate_similarity(tasks[i], tasks[j], hash_a, hash_b)
                    total_similarity += similarity
                    comparison_count += 1

        return total_similarity / comparison_count if comparison_count > 0 else 0.0

    def _determine_duplication_level(self, similarity_score: float) -> DuplicationLevel:
        """é‡è¤‡ãƒ¬ãƒ™ãƒ«æ±ºå®š"""

        if similarity_score >= self.duplication_thresholds["identical"]:
            return DuplicationLevel.IDENTICAL
        elif similarity_score >= self.duplication_thresholds["highly_similar"]:
            return DuplicationLevel.HIGHLY_SIMILAR
        elif similarity_score >= self.duplication_thresholds["similar"]:
            return DuplicationLevel.SIMILAR
        elif similarity_score >= self.duplication_thresholds["related"]:
            return DuplicationLevel.RELATED
        else:
            return DuplicationLevel.DIFFERENT

    def _determine_merge_strategy(self, group: DuplicationGroup) -> None:
        """ãƒãƒ¼ã‚¸æˆ¦ç•¥æ±ºå®š"""

        # åŸºæœ¬æˆ¦ç•¥
        base_strategy = self.merge_rules.get(
            group.duplication_level.value,
            MergeStrategy.MANUAL_REVIEW
        )

        # ã‚¿ã‚¹ã‚¯ç‰¹æ€§ã«ã‚ˆã‚‹èª¿æ•´
        task_types = set(task.get("type", "") for task in group.tasks)
        priorities = set(task.get("priority", "") for task in group.tasks)

        # å…¨ã¦åŒã˜ã‚¿ã‚¤ãƒ—ã®å ´åˆã¯ç©æ¥µçš„ã«ãƒãƒ¼ã‚¸
        if len(task_types) == 1 and group.similarity_score > 0.85:
            if base_strategy == MergeStrategy.MANUAL_REVIEW:
                base_strategy = MergeStrategy.MERGE_TARGETS

        # å„ªå…ˆåº¦ãŒæ··åœ¨ã—ã¦ã„ã‚‹å ´åˆã¯é«˜å„ªå…ˆåº¦ä¿æŒ
        if len(priorities) > 1:
            base_strategy = MergeStrategy.KEEP_HIGHEST_PRIORITY

        group.merge_strategy = base_strategy

        # æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³è¨­å®š
        action_descriptions = {
            MergeStrategy.KEEP_NEWEST: "æœ€æ–°ã®ã‚¿ã‚¹ã‚¯ã‚’ä¿æŒã—ã€å¤ã„ã‚¿ã‚¹ã‚¯ã‚’å‰Šé™¤",
            MergeStrategy.KEEP_HIGHEST_PRIORITY: "æœ€é«˜å„ªå…ˆåº¦ã®ã‚¿ã‚¹ã‚¯ã‚’ä¿æŒ",
            MergeStrategy.MERGE_TARGETS: "å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµ±åˆã—ãŸæ–°ã—ã„ã‚¿ã‚¹ã‚¯ã‚’ä½œæˆ",
            MergeStrategy.MERGE_REQUIREMENTS: "è¦ä»¶ã‚’çµ±åˆã—ãŸæ–°ã—ã„ã‚¿ã‚¹ã‚¯ã‚’ä½œæˆ",
            MergeStrategy.MANUAL_REVIEW: "æ‰‹å‹•ç¢ºèªãŒå¿…è¦"
        }

        group.recommended_action = action_descriptions.get(
            base_strategy,
            "æ‰‹å‹•ç¢ºèªãŒå¿…è¦"
        )

    def _execute_deduplication(self, task_list: List[Dict[str, Any]],
                             duplicate_groups: List[DuplicationGroup]) -> Tuple[List[Dict[str, Any]], Dict[str, int]]:
        """é‡è¤‡æ’é™¤å®Ÿè¡Œ"""

        action_summary = {
            "removed": 0,
            "merged": 0,
            "kept": 0,
            "manual_review": 0
        }

        # å‡¦ç†å¯¾è±¡ã‚¿ã‚¹ã‚¯IDã®åé›†
        processed_task_ids = set()
        for group in duplicate_groups:
            for task in group.tasks:
                processed_task_ids.add(task.get("task_id", "unknown"))

        # é‡è¤‡æ’é™¤ã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆ
        deduplicated_tasks = []

        # é‡è¤‡ã—ã¦ã„ãªã„ã‚¿ã‚¹ã‚¯ã¯ãã®ã¾ã¾ä¿æŒ
        for task in task_list:
            if task.get("task_id") not in processed_task_ids:
                deduplicated_tasks.append(task)
                action_summary["kept"] += 1

        # å„é‡è¤‡ã‚°ãƒ«ãƒ¼ãƒ—ã®å‡¦ç†
        for group in duplicate_groups:
            if group.merge_strategy == MergeStrategy.KEEP_NEWEST:
                # æœ€æ–°ã‚¿ã‚¹ã‚¯ã‚’ä¿æŒ
                newest_task = max(group.tasks, key=lambda t: t.get("timestamp", ""))
                deduplicated_tasks.append(newest_task)
                action_summary["removed"] += len(group.tasks) - 1
                action_summary["kept"] += 1

            elif group.merge_strategy == MergeStrategy.KEEP_HIGHEST_PRIORITY:
                # æœ€é«˜å„ªå…ˆåº¦ã‚¿ã‚¹ã‚¯ã‚’ä¿æŒ
                priority_order = {"high": 0, "medium": 1, "low": 2}
                highest_priority_task = min(
                    group.tasks,
                    key=lambda t: priority_order.get(t.get("priority", "medium"), 1)
                )
                deduplicated_tasks.append(highest_priority_task)
                action_summary["removed"] += len(group.tasks) - 1
                action_summary["kept"] += 1

            elif group.merge_strategy == MergeStrategy.MERGE_TARGETS:
                # å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒ¼ã‚¸ã—ãŸæ–°ã—ã„ã‚¿ã‚¹ã‚¯ã‚’ä½œæˆ
                merged_task = self._create_merged_task(group, "targets")
                if merged_task:
                    deduplicated_tasks.append(merged_task)
                    group.merged_task = merged_task
                    action_summary["merged"] += 1
                    action_summary["removed"] += len(group.tasks)

            elif group.merge_strategy == MergeStrategy.MERGE_REQUIREMENTS:
                # è¦ä»¶ã‚’ãƒãƒ¼ã‚¸ã—ãŸæ–°ã—ã„ã‚¿ã‚¹ã‚¯ã‚’ä½œæˆ
                merged_task = self._create_merged_task(group, "requirements")
                if merged_task:
                    deduplicated_tasks.append(merged_task)
                    group.merged_task = merged_task
                    action_summary["merged"] += 1
                    action_summary["removed"] += len(group.tasks)

            else:
                # æ‰‹å‹•ç¢ºèªãŒå¿…è¦ - å…¨ã¦ä¿æŒ
                deduplicated_tasks.extend(group.tasks)
                action_summary["manual_review"] += len(group.tasks)

        return deduplicated_tasks, action_summary

    def _create_merged_task(self, group: DuplicationGroup, merge_type: str) -> Optional[Dict[str, Any]]:
        """çµ±åˆã‚¿ã‚¹ã‚¯ã®ä½œæˆ"""

        base_task = group.tasks[0].copy()  # æœ€åˆã®ã‚¿ã‚¹ã‚¯ã‚’ãƒ™ãƒ¼ã‚¹ã«ã™ã‚‹

        # æ–°ã—ã„ã‚¿ã‚¹ã‚¯IDç”Ÿæˆ
        base_task["task_id"] = f"merged_{group.group_id}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}"

        if merge_type == "targets":
            # å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã®çµ±åˆ
            all_targets = set()
            for task in group.tasks:
                all_targets.update(task.get("target_files", []))

            base_task["target_files"] = sorted(list(all_targets))
            base_task["description"] = f"çµ±åˆã‚¿ã‚¹ã‚¯: {len(all_targets)}ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€æ‹¬å‡¦ç†"

        elif merge_type == "requirements":
            # è¦ä»¶ã®çµ±åˆï¼ˆç°¡æ˜“ç‰ˆï¼‰
            all_requirements = {}
            for task in group.tasks:
                task_req = task.get("requirements", {})
                for key, value in task_req.items():
                    if key not in all_requirements:
                        all_requirements[key] = value
                    elif isinstance(value, list) and isinstance(all_requirements[key], list):
                        all_requirements[key].extend(value)

            base_task["requirements"] = all_requirements
            base_task["description"] = f"çµ±åˆã‚¿ã‚¹ã‚¯: è¤‡æ•°è¦ä»¶ã®ä¸€æ‹¬å‡¦ç†"

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ›´æ–°
        base_task["merged_from"] = [task.get("task_id") for task in group.tasks]
        base_task["merge_timestamp"] = datetime.datetime.now().isoformat()
        base_task["created_by"] = "task_deduplicator"

        return base_task

    def _estimate_time_savings(self, duplicate_groups: List[DuplicationGroup]) -> float:
        """æ™‚é–“ç¯€ç´„é‡æ¨å®šï¼ˆåˆ†ï¼‰"""

        total_savings = 0.0

        for group in duplicate_groups:
            if group.merge_strategy in [MergeStrategy.KEEP_NEWEST, MergeStrategy.KEEP_HIGHEST_PRIORITY]:
                # é‡è¤‡ã‚¿ã‚¹ã‚¯ã®å®Ÿè¡Œæ™‚é–“ã‚’ç¯€ç´„
                duplicate_count = len(group.tasks) - 1
                estimated_time_per_task = 5.0  # 5åˆ†ã¨ä»®å®š
                total_savings += duplicate_count * estimated_time_per_task

            elif group.merge_strategy in [MergeStrategy.MERGE_TARGETS, MergeStrategy.MERGE_REQUIREMENTS]:
                # ãƒãƒ¼ã‚¸ã«ã‚ˆã‚‹åŠ¹ç‡åŒ–
                original_time = len(group.tasks) * 5.0
                merged_time = 8.0  # ãƒãƒ¼ã‚¸ã‚¿ã‚¹ã‚¯ã¯è‹¥å¹²æ™‚é–“ãŒã‹ã‹ã‚‹
                total_savings += max(0, original_time - merged_time)

        return total_savings

    def _estimate_token_savings(self, duplicate_groups: List[DuplicationGroup]) -> int:
        """ãƒˆãƒ¼ã‚¯ãƒ³ç¯€ç´„é‡æ¨å®š"""

        total_token_savings = 0

        for group in duplicate_groups:
            if group.merge_strategy in [MergeStrategy.KEEP_NEWEST, MergeStrategy.KEEP_HIGHEST_PRIORITY]:
                # é‡è¤‡ã‚¿ã‚¹ã‚¯ã®å®Ÿè¡Œãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç¯€ç´„
                duplicate_count = len(group.tasks) - 1
                estimated_tokens_per_task = 1500  # 1500ãƒˆãƒ¼ã‚¯ãƒ³ã¨ä»®å®š
                total_token_savings += duplicate_count * estimated_tokens_per_task

            elif group.merge_strategy in [MergeStrategy.MERGE_TARGETS, MergeStrategy.MERGE_REQUIREMENTS]:
                # ãƒãƒ¼ã‚¸ã«ã‚ˆã‚‹åŠ¹ç‡åŒ–ï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå…±æœ‰ï¼‰
                original_tokens = len(group.tasks) * 1500
                merged_tokens = 2200  # ãƒãƒ¼ã‚¸ã‚¿ã‚¹ã‚¯ã¯å¤šå°‘å¢—ãˆã‚‹
                total_token_savings += max(0, original_tokens - merged_tokens)

        return total_token_savings

    def _calculate_hash(self, data: str) -> str:
        """ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚·ãƒ¥è¨ˆç®—"""
        return hashlib.md5(data.encode('utf-8')).hexdigest()

    def analyze_duplication_patterns(self, task_list: List[Dict[str, Any]]) -> Dict[str, Any]:
        """é‡è¤‡ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""

        analysis = {
            "total_tasks": len(task_list),
            "duplication_patterns": {},
            "common_targets": {},
            "error_type_distribution": {},
            "recommendations": []
        }

        # ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—åˆ¥ã®åˆ†æ
        type_groups = {}
        for task in task_list:
            task_type = task.get("type", "unknown")
            if task_type not in type_groups:
                type_groups[task_type] = []
            type_groups[task_type].append(task)

        for task_type, tasks in type_groups.items():
            if len(tasks) > 1:
                analysis["duplication_patterns"][task_type] = {
                    "count": len(tasks),
                    "potential_duplicates": self._find_potential_duplicates_in_group(tasks)
                }

        # å…±é€šå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ†æ
        file_frequency = {}
        for task in task_list:
            for file_path in task.get("target_files", []):
                file_frequency[file_path] = file_frequency.get(file_path, 0) + 1

        analysis["common_targets"] = {
            file_path: count for file_path, count in file_frequency.items() if count > 1
        }

        # ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—åˆ†å¸ƒ
        error_types = {}
        for task in task_list:
            error_type = task.get("requirements", {}).get("error_type", "unknown")
            error_types[error_type] = error_types.get(error_type, 0) + 1

        analysis["error_type_distribution"] = error_types

        # æ¨å¥¨äº‹é …
        if analysis["common_targets"]:
            analysis["recommendations"].append("å…±é€šãƒ•ã‚¡ã‚¤ãƒ«ã‚’å¯¾è±¡ã¨ã™ã‚‹ã‚¿ã‚¹ã‚¯ã®çµ±åˆã‚’æ¤œè¨")

        if len(analysis["duplication_patterns"]) > 0:
            analysis["recommendations"].append("åŒä¸€ã‚¿ã‚¤ãƒ—ã®ã‚¿ã‚¹ã‚¯ã®é‡è¤‡ç¢ºèªã¨çµ±åˆ")

        return analysis

    def _find_potential_duplicates_in_group(self, tasks: List[Dict[str, Any]]) -> int:
        """ã‚°ãƒ«ãƒ¼ãƒ—å†…ã®æ½œåœ¨çš„é‡è¤‡æ•°ã‚’æ¨å®š"""

        potential_duplicates = 0

        for i in range(len(tasks)):
            for j in range(i+1, len(tasks)):
                # ç°¡æ˜“çš„ãªé‡è¤‡åˆ¤å®š
                targets_a = set(tasks[i].get("target_files", []))
                targets_b = set(tasks[j].get("target_files", []))

                overlap = len(targets_a & targets_b)
                if overlap > 0:
                    potential_duplicates += 1

        return potential_duplicates

    def save_deduplication_report(self, report: DeduplicationReport,
                                output_path: str = "tmp/task_deduplication_report.json") -> str:
        """é‡è¤‡æ’é™¤ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜"""

        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)

        # ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿æ§‹ç¯‰
        report_data = {
            "deduplication_timestamp": datetime.datetime.now().isoformat(),
            "summary": {
                "original_task_count": report.original_task_count,
                "duplicate_groups_found": report.duplicate_groups_found,
                "tasks_removed": report.tasks_removed,
                "tasks_merged": report.tasks_merged,
                "estimated_time_saved": report.estimated_time_saved,
                "estimated_token_saved": report.estimated_token_saved,
                "action_summary": report.action_summary
            },
            "duplicate_groups": []
        }

        # é‡è¤‡ã‚°ãƒ«ãƒ¼ãƒ—ã®è©³ç´°
        for group in report.duplication_groups:
            group_data = {
                "group_id": group.group_id,
                "duplication_level": group.duplication_level.value,
                "similarity_score": group.similarity_score,
                "task_count": len(group.tasks),
                "task_ids": [task.get("task_id") for task in group.tasks],
                "merge_strategy": group.merge_strategy.value,
                "recommended_action": group.recommended_action,
                "merged_task_id": group.merged_task.get("task_id") if group.merged_task else None,
                "savings_estimate": group.savings_estimate
            }
            report_data["duplicate_groups"].append(group_data)

        # JSONä¿å­˜
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)

        print(f"ğŸ“Š é‡è¤‡æ’é™¤ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜å®Œäº†: {output_path}")
        return output_path


def main():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""

    print("ğŸ§ª TaskDeduplicator ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")

    deduplicator = TaskDeduplicator()

    # ãƒ†ã‚¹ãƒˆã‚¿ã‚¹ã‚¯ãƒ‡ãƒ¼ã‚¿ï¼ˆé‡è¤‡ã‚’å«ã‚€ï¼‰
    test_tasks = [
        {
            "task_id": "task_001",
            "type": "file_code_modification",
            "target_files": ["file_a.py"],
            "requirements": {"error_type": "no-untyped-def"},
            "priority": "high",
            "description": "no-untyped-defä¿®æ­£",
            "timestamp": "2025-08-13T10:00:00"
        },
        {
            "task_id": "task_002",
            "type": "file_code_modification",
            "target_files": ["file_a.py"],
            "requirements": {"error_type": "no-untyped-def"},
            "priority": "medium",
            "description": "å‹æ³¨é‡ˆä¿®æ­£",
            "timestamp": "2025-08-13T10:05:00"
        },
        {
            "task_id": "task_003",
            "type": "file_code_modification",
            "target_files": ["file_b.py", "file_c.py"],
            "requirements": {"error_type": "no-untyped-call"},
            "priority": "low",
            "description": "callä¿®æ­£",
            "timestamp": "2025-08-13T10:10:00"
        },
        {
            "task_id": "task_004",
            "type": "file_code_modification",
            "target_files": ["file_a.py", "file_b.py"],
            "requirements": {"error_type": "no-untyped-def"},
            "priority": "high",
            "description": "untyped-def ã‚¨ãƒ©ãƒ¼ä¿®æ­£",
            "timestamp": "2025-08-13T10:15:00"
        }
    ]

    # é‡è¤‡ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
    print("\nğŸ“Š é‡è¤‡ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ:")
    analysis = deduplicator.analyze_duplication_patterns(test_tasks)
    print(f"ç·ã‚¿ã‚¹ã‚¯æ•°: {analysis['total_tasks']}")
    print(f"é‡è¤‡ãƒ‘ã‚¿ãƒ¼ãƒ³: {len(analysis['duplication_patterns'])}")
    print(f"å…±é€šã‚¿ãƒ¼ã‚²ãƒƒãƒˆ: {len(analysis['common_targets'])}")

    # é‡è¤‡æ’é™¤å®Ÿè¡Œ
    print("\nğŸ”„ é‡è¤‡æ’é™¤å®Ÿè¡Œ:")
    report = deduplicator.deduplicate_tasks(test_tasks)

    print(f"\nğŸ“Š çµæœã‚µãƒãƒªãƒ¼:")
    print(f"å…ƒã®ã‚¿ã‚¹ã‚¯æ•°: {report.original_task_count}")
    print(f"é‡è¤‡ã‚°ãƒ«ãƒ¼ãƒ—: {report.duplicate_groups_found}")
    print(f"å‰Šé™¤ã‚¿ã‚¹ã‚¯: {report.tasks_removed}")
    print(f"çµ±åˆã‚¿ã‚¹ã‚¯: {report.tasks_merged}")
    print(f"æ™‚é–“ç¯€ç´„: {report.estimated_time_saved:.1f}åˆ†")
    print(f"ãƒˆãƒ¼ã‚¯ãƒ³ç¯€ç´„: {report.estimated_token_saved}")

    # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
    deduplicator.save_deduplication_report(report)

    print("\nâœ… TaskDeduplicator ãƒ†ã‚¹ãƒˆå®Œäº†")


if __name__ == "__main__":
    main()
