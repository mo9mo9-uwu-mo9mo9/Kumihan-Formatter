#!/usr/bin/env python3
"""
Quality Standards Manager for Gemini Capability Enhancement
è‡ªå‹•å“è³ªæ¤œè¨¼ãƒ»æ”¹å–„ææ¡ˆã‚·ã‚¹ãƒ†ãƒ 
"""

import json
import os
import subprocess
import re
import ast
from typing import Dict, List, Any, Optional, Tuple, Set
from pathlib import Path
from dataclasses import dataclass, asdict
from enum import Enum
import time
from datetime import datetime


class QualityLevel(Enum):
    """å“è³ªãƒ¬ãƒ™ãƒ«"""
    EXCELLENT = "excellent"    # 90-100%
    GOOD = "good"             # 80-89%
    ACCEPTABLE = "acceptable"  # 70-79%
    POOR = "poor"             # 60-69%
    CRITICAL = "critical"     # <60%


class MetricType(Enum):
    """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¿ã‚¤ãƒ—"""
    TYPE_COVERAGE = "type_coverage"
    SYNTAX_ACCURACY = "syntax_accuracy"
    FORMAT_COMPLIANCE = "format_compliance"
    IMPORT_ORGANIZATION = "import_organization"
    DOCSTRING_COVERAGE = "docstring_coverage"
    ERROR_RATE = "error_rate"


@dataclass
class QualityMetric:
    """å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    metric_id: str
    name: str
    description: str
    current_value: float
    target_value: float
    measurement_method: str
    improvement_suggestions: List[str]
    severity: str
    last_measured: str


@dataclass
class QualityReport:
    """å“è³ªãƒ¬ãƒãƒ¼ãƒˆ"""
    report_id: str
    file_path: str
    timestamp: str
    overall_score: float
    quality_level: QualityLevel
    metrics: List[QualityMetric]
    violations: List[Dict[str, Any]]
    improvement_plan: List[Dict[str, Any]]
    estimated_improvement_time: int  # minutes


@dataclass
class ImprovementSuggestion:
    """æ”¹å–„ææ¡ˆ"""
    suggestion_id: str
    title: str
    description: str
    priority: int
    estimated_effort: int  # minutes
    expected_impact: float  # 0.0-1.0
    implementation_steps: List[str]
    code_examples: List[str]
    validation_method: str


class QualityChecker:
    """å“è³ªãƒã‚§ãƒƒã‚«ãƒ¼"""
    
    def __init__(self):
        self.checkers = {
            MetricType.TYPE_COVERAGE: self._check_type_coverage,
            MetricType.SYNTAX_ACCURACY: self._check_syntax_accuracy,
            MetricType.FORMAT_COMPLIANCE: self._check_format_compliance,
            MetricType.IMPORT_ORGANIZATION: self._check_import_organization,
            MetricType.DOCSTRING_COVERAGE: self._check_docstring_coverage,
        }
    
    def check_file_quality(self, file_path: str) -> Dict[str, Any]:
        """ãƒ•ã‚¡ã‚¤ãƒ«å“è³ªãƒã‚§ãƒƒã‚¯"""
        
        if not os.path.exists(file_path):
            return {"error": f"File not found: {file_path}"}
        
        results = {}
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # å„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ
            for metric_type, checker_func in self.checkers.items():
                try:
                    metric_result = checker_func(file_path, content)
                    results[metric_type.value] = metric_result
                except Exception as e:
                    results[metric_type.value] = {
                        "error": str(e),
                        "score": 0.0
                    }
        
        except Exception as e:
            return {"error": f"Failed to read file: {e}"}
        
        return results
    
    def _check_type_coverage(self, file_path: str, content: str) -> Dict[str, Any]:
        """å‹æ³¨é‡ˆã‚«ãƒãƒ¬ãƒƒã‚¸ãƒã‚§ãƒƒã‚¯"""
        
        try:
            tree = ast.parse(content)
            
            total_functions = 0
            typed_functions = 0
            total_args = 0
            typed_args = 0
            violations = []
            
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    total_functions += 1
                    func_name = node.name
                    line_num = node.lineno
                    
                    # è¿”ã‚Šå€¤å‹ãƒã‚§ãƒƒã‚¯
                    has_return_annotation = node.returns is not None
                    if has_return_annotation:
                        typed_functions += 1
                    else:
                        violations.append({
                            "type": "missing_return_type",
                            "function": func_name,
                            "line": line_num,
                            "description": f"Function '{func_name}' missing return type annotation"
                        })
                    
                    # å¼•æ•°å‹ãƒã‚§ãƒƒã‚¯
                    for arg in node.args.args:
                        total_args += 1
                        if arg.annotation:
                            typed_args += 1
                        else:
                            violations.append({
                                "type": "missing_arg_type",
                                "function": func_name,
                                "argument": arg.arg,
                                "line": line_num,
                                "description": f"Argument '{arg.arg}' in '{func_name}' missing type annotation"
                            })
            
            # ã‚«ãƒãƒ¬ãƒƒã‚¸è¨ˆç®—
            function_coverage = typed_functions / total_functions if total_functions > 0 else 1.0
            arg_coverage = typed_args / total_args if total_args > 0 else 1.0
            overall_coverage = (function_coverage + arg_coverage) / 2
            
            return {
                "score": overall_coverage,
                "function_coverage": function_coverage,
                "argument_coverage": arg_coverage,
                "total_functions": total_functions,
                "typed_functions": typed_functions,
                "total_args": total_args,
                "typed_args": typed_args,
                "violations": violations
            }
        
        except SyntaxError as e:
            return {
                "score": 0.0,
                "error": f"Syntax error: {e}",
                "violations": [{"type": "syntax_error", "line": e.lineno, "description": str(e)}]
            }
    
    def _check_syntax_accuracy(self, file_path: str, content: str) -> Dict[str, Any]:
        """æ§‹æ–‡æ­£ç¢ºæ€§ãƒã‚§ãƒƒã‚¯"""
        
        try:
            ast.parse(content)
            return {
                "score": 1.0,
                "violations": []
            }
        except SyntaxError as e:
            return {
                "score": 0.0,
                "violations": [{
                    "type": "syntax_error",
                    "line": e.lineno,
                    "column": e.offset,
                    "description": str(e),
                    "text": e.text.strip() if e.text else ""
                }]
            }
    
    def _check_format_compliance(self, file_path: str, content: str) -> Dict[str, Any]:
        """ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæº–æ‹ ãƒã‚§ãƒƒã‚¯"""
        
        violations = []
        score = 1.0
        
        try:
            # Black ãƒã‚§ãƒƒã‚¯
            result = subprocess.run(
                ['black', '--check', '--quiet', file_path],
                capture_output=True, text=True, timeout=30
            )
            
            if result.returncode != 0:
                violations.append({
                    "type": "black_format",
                    "description": "Code does not comply with Black formatting",
                    "tool": "black"
                })
                score -= 0.5
        except (subprocess.TimeoutExpired, FileNotFoundError):
            violations.append({
                "type": "black_unavailable",
                "description": "Black formatter not available or timeout",
                "tool": "black"
            })
            score -= 0.1
        
        try:
            # isort ãƒã‚§ãƒƒã‚¯
            result = subprocess.run(
                ['isort', '--check-only', '--quiet', file_path],
                capture_output=True, text=True, timeout=30
            )
            
            if result.returncode != 0:
                violations.append({
                    "type": "import_order",
                    "description": "Import statements are not properly ordered",
                    "tool": "isort"
                })
                score -= 0.3
        except (subprocess.TimeoutExpired, FileNotFoundError):
            violations.append({
                "type": "isort_unavailable",
                "description": "isort not available or timeout",
                "tool": "isort"
            })
            score -= 0.1
        
        return {
            "score": max(0.0, score),
            "violations": violations
        }
    
    def _check_import_organization(self, file_path: str, content: str) -> Dict[str, Any]:
        """ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ•´ç†ãƒã‚§ãƒƒã‚¯"""
        
        try:
            tree = ast.parse(content)
            
            imports = []
            violations = []
            
            for node in ast.walk(tree):
                if isinstance(node, (ast.Import, ast.ImportFrom)):
                    imports.append({
                        "type": type(node).__name__,
                        "line": node.lineno,
                        "module": getattr(node, 'module', None),
                        "names": [alias.name for alias in node.names]
                    })
            
            # é‡è¤‡ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯
            seen_imports = set()
            for imp in imports:
                import_key = (imp["module"], tuple(imp["names"]))
                if import_key in seen_imports:
                    violations.append({
                        "type": "duplicate_import",
                        "line": imp["line"],
                        "description": f"Duplicate import: {imp['module']} - {imp['names']}"
                    })
                seen_imports.add(import_key)
            
            # ã‚¹ã‚³ã‚¢è¨ˆç®—
            score = 1.0 - (len(violations) * 0.1)
            
            return {
                "score": max(0.0, score),
                "total_imports": len(imports),
                "violations": violations
            }
        
        except SyntaxError:
            return {
                "score": 0.0,
                "violations": [{"type": "syntax_error", "description": "Cannot parse file for import analysis"}]
            }
    
    def _check_docstring_coverage(self, file_path: str, content: str) -> Dict[str, Any]:
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ–‡å­—åˆ—ã‚«ãƒãƒ¬ãƒƒã‚¸ãƒã‚§ãƒƒã‚¯"""
        
        try:
            tree = ast.parse(content)
            
            total_items = 0
            documented_items = 0
            violations = []
            
            # ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ¬ãƒ™ãƒ«ã®docstring
            if ast.get_docstring(tree):
                documented_items += 1
            else:
                violations.append({
                    "type": "missing_module_docstring",
                    "line": 1,
                    "description": "Module missing docstring"
                })
            total_items += 1
            
            # ã‚¯ãƒ©ã‚¹ãƒ»é–¢æ•°ã®docstring
            for node in ast.walk(tree):
                if isinstance(node, ast.ClassDef):
                    total_items += 1
                    if ast.get_docstring(node):
                        documented_items += 1
                    else:
                        violations.append({
                            "type": "missing_class_docstring",
                            "line": node.lineno,
                            "name": node.name,
                            "description": f"Class '{node.name}' missing docstring"
                        })
                
                elif isinstance(node, ast.FunctionDef):
                    # ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆé–¢æ•°ã¯é™¤å¤–ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
                    if not node.name.startswith('_'):
                        total_items += 1
                        if ast.get_docstring(node):
                            documented_items += 1
                        else:
                            violations.append({
                                "type": "missing_function_docstring",
                                "line": node.lineno,
                                "name": node.name,
                                "description": f"Function '{node.name}' missing docstring"
                            })
            
            coverage = documented_items / total_items if total_items > 0 else 1.0
            
            return {
                "score": coverage,
                "total_items": total_items,
                "documented_items": documented_items,
                "coverage": coverage,
                "violations": violations
            }
        
        except SyntaxError:
            return {
                "score": 0.0,
                "violations": [{"type": "syntax_error", "description": "Cannot parse file for docstring analysis"}]
            }


class QualityStandardsManager:
    """å“è³ªåŸºæº–ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, config_path: Optional[str] = None):
        self.config_path = config_path or "postbox/quality/quality_config.json"
        self.quality_checker = QualityChecker()
        self.standards_config = self._load_standards_config()
        self.improvement_templates = self._initialize_improvement_templates()
    
    def evaluate_file_quality(self, file_path: str) -> QualityReport:
        """ãƒ•ã‚¡ã‚¤ãƒ«å“è³ªè©•ä¾¡"""
        
        print(f"ğŸ” å“è³ªè©•ä¾¡é–‹å§‹: {file_path}")
        
        # å“è³ªãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ
        check_results = self.quality_checker.check_file_quality(file_path)
        
        if "error" in check_results:
            return self._create_error_report(file_path, check_results["error"])
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç”Ÿæˆ
        metrics = self._generate_metrics(file_path, check_results)
        
        # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
        overall_score = self._calculate_overall_score(metrics)
        
        # å“è³ªãƒ¬ãƒ™ãƒ«æ±ºå®š
        quality_level = self._determine_quality_level(overall_score)
        
        # é•åäº‹é …ã®çµ±åˆ
        violations = self._aggregate_violations(check_results)
        
        # æ”¹å–„è¨ˆç”»ç”Ÿæˆ
        improvement_plan = self._generate_improvement_plan(metrics, violations)
        
        # æ”¹å–„æ™‚é–“æ¨å®š
        estimated_time = self._estimate_improvement_time(improvement_plan)
        
        report = QualityReport(
            report_id=f"qr_{int(time.time())}_{Path(file_path).stem}",
            file_path=file_path,
            timestamp=datetime.now().isoformat(),
            overall_score=overall_score,
            quality_level=quality_level,
            metrics=metrics,
            violations=violations,
            improvement_plan=improvement_plan,
            estimated_improvement_time=estimated_time
        )
        
        print(f"âœ… å“è³ªè©•ä¾¡å®Œäº†: ã‚¹ã‚³ã‚¢ {overall_score:.2f}, ãƒ¬ãƒ™ãƒ« {quality_level.value}")
        return report
    
    def generate_improvement_suggestions(self, report: QualityReport) -> List[ImprovementSuggestion]:
        """æ”¹å–„ææ¡ˆç”Ÿæˆ"""
        
        suggestions = []
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹åˆ¥ã®æ”¹å–„ææ¡ˆ
        for metric in report.metrics:
            if metric.current_value < metric.target_value:
                suggestion = self._create_metric_improvement_suggestion(metric, report)
                if suggestion:
                    suggestions.append(suggestion)
        
        # é•åäº‹é …åˆ¥ã®æ”¹å–„ææ¡ˆ
        violation_types = set(v.get('type', 'unknown') for v in report.violations)
        for violation_type in violation_types:
            suggestion = self._create_violation_improvement_suggestion(violation_type, report)
            if suggestion:
                suggestions.append(suggestion)
        
        # å„ªå…ˆåº¦é †ã§ã‚½ãƒ¼ãƒˆ
        suggestions.sort(key=lambda x: (-x.priority, -x.expected_impact))
        
        return suggestions
    
    def auto_apply_improvements(self, file_path: str, suggestions: List[ImprovementSuggestion]) -> Dict[str, Any]:
        """è‡ªå‹•æ”¹å–„é©ç”¨"""
        
        print(f"ğŸ”§ è‡ªå‹•æ”¹å–„é©ç”¨é–‹å§‹: {file_path}")
        
        results = {
            "file_path": file_path,
            "applied_improvements": [],
            "failed_improvements": [],
            "before_score": 0.0,
            "after_score": 0.0,
            "improvement_delta": 0.0
        }
        
        # æ”¹å–„å‰ã®ã‚¹ã‚³ã‚¢å–å¾—
        before_report = self.evaluate_file_quality(file_path)
        results["before_score"] = before_report.overall_score
        
        # è‡ªå‹•é©ç”¨å¯èƒ½ãªæ”¹å–„ã®å®Ÿè¡Œ
        for suggestion in suggestions:
            try:
                if self._can_auto_apply(suggestion):
                    success = self._apply_suggestion(file_path, suggestion)
                    if success:
                        results["applied_improvements"].append({
                            "suggestion_id": suggestion.suggestion_id,
                            "title": suggestion.title,
                            "impact": suggestion.expected_impact
                        })
                    else:
                        results["failed_improvements"].append({
                            "suggestion_id": suggestion.suggestion_id,
                            "title": suggestion.title,
                            "reason": "Application failed"
                        })
                else:
                    results["failed_improvements"].append({
                        "suggestion_id": suggestion.suggestion_id,
                        "title": suggestion.title,
                        "reason": "Manual intervention required"
                    })
            
            except Exception as e:
                results["failed_improvements"].append({
                    "suggestion_id": suggestion.suggestion_id,
                    "title": suggestion.title,
                    "reason": f"Error: {e}"
                })
        
        # æ”¹å–„å¾Œã®ã‚¹ã‚³ã‚¢å–å¾—
        if results["applied_improvements"]:
            after_report = self.evaluate_file_quality(file_path)
            results["after_score"] = after_report.overall_score
            results["improvement_delta"] = after_report.overall_score - before_report.overall_score
        
        print(f"âœ… è‡ªå‹•æ”¹å–„å®Œäº†: {len(results['applied_improvements'])}ä»¶é©ç”¨")
        return results
    
    def _load_standards_config(self) -> Dict[str, Any]:
        """å“è³ªåŸºæº–è¨­å®šèª­ã¿è¾¼ã¿"""
        
        default_config = {
            "target_scores": {
                "type_coverage": 0.95,
                "syntax_accuracy": 1.0,
                "format_compliance": 1.0,
                "import_organization": 0.9,
                "docstring_coverage": 0.8
            },
            "weights": {
                "type_coverage": 0.3,
                "syntax_accuracy": 0.3,
                "format_compliance": 0.2,
                "import_organization": 0.1,
                "docstring_coverage": 0.1
            },
            "quality_thresholds": {
                "excellent": 0.9,
                "good": 0.8,
                "acceptable": 0.7,
                "poor": 0.6
            }
        }
        
        if os.path.exists(self.config_path):
            try:
                with open(self.config_path, 'r', encoding='utf-8') as f:
                    loaded_config = json.load(f)
                # ãƒãƒ¼ã‚¸
                for key, value in loaded_config.items():
                    if isinstance(value, dict) and key in default_config:
                        default_config[key].update(value)
                    else:
                        default_config[key] = value
            except Exception as e:
                print(f"âš ï¸ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        
        return default_config
    
    def _initialize_improvement_templates(self) -> Dict[str, Any]:
        """æ”¹å–„ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆåˆæœŸåŒ–"""
        
        return {
            "type_coverage": {
                "template": "Add type annotations: {function_name}({args}: Any) -> {return_type}:",
                "auto_applicable": True,
                "validation": "mypy --strict"
            },
            "format_compliance": {
                "template": "Apply formatting: black {file_path} && isort {file_path}",
                "auto_applicable": True,
                "validation": "black --check && isort --check-only"
            },
            "docstring_coverage": {
                "template": 'Add docstring: """Description of {item_name}"""',
                "auto_applicable": False,  # å†…å®¹ãŒå¿…è¦ãªãŸã‚
                "validation": "manual_review"
            },
            "syntax_error": {
                "template": "Fix syntax error at line {line}: {description}",
                "auto_applicable": False,
                "validation": "python -m py_compile"
            }
        }
    
    def _generate_metrics(self, file_path: str, check_results: Dict[str, Any]) -> List[QualityMetric]:
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç”Ÿæˆ"""
        
        metrics = []
        current_time = datetime.now().isoformat()
        
        for metric_type, result in check_results.items():
            if isinstance(result, dict) and 'score' in result:
                target_value = self.standards_config["target_scores"].get(metric_type, 0.8)
                
                metric = QualityMetric(
                    metric_id=f"{metric_type}_{Path(file_path).stem}",
                    name=metric_type.replace('_', ' ').title(),
                    description=f"{metric_type} quality measurement",
                    current_value=result['score'],
                    target_value=target_value,
                    measurement_method=f"automated_{metric_type}_check",
                    improvement_suggestions=self._get_metric_suggestions(metric_type, result),
                    severity=self._determine_metric_severity(result['score'], target_value),
                    last_measured=current_time
                )
                metrics.append(metric)
        
        return metrics
    
    def _get_metric_suggestions(self, metric_type: str, result: Dict[str, Any]) -> List[str]:
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹åˆ¥æ”¹å–„ææ¡ˆ"""
        
        suggestions = []
        violations = result.get('violations', [])
        
        if metric_type == 'type_coverage':
            if violations:
                suggestions.append("Add missing type annotations to functions and arguments")
                suggestions.append("Import typing modules (Any, Dict, List, Optional)")
                suggestions.append("Use 'Any' type for unknown parameter types")
        
        elif metric_type == 'format_compliance':
            suggestions.append("Run 'black' formatter to fix code formatting")
            suggestions.append("Run 'isort' to organize import statements")
        
        elif metric_type == 'docstring_coverage':
            suggestions.append("Add docstrings to public functions and classes")
            suggestions.append("Include module-level docstring")
        
        elif metric_type == 'syntax_accuracy':
            if violations:
                suggestions.append("Fix syntax errors identified by Python parser")
                suggestions.append("Check indentation consistency")
        
        return suggestions
    
    def _determine_metric_severity(self, current: float, target: float) -> str:
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹é‡è¦åº¦åˆ¤å®š"""
        
        ratio = current / target if target > 0 else 1.0
        
        if ratio >= 0.9:
            return "low"
        elif ratio >= 0.7:
            return "medium"
        else:
            return "high"
    
    def _calculate_overall_score(self, metrics: List[QualityMetric]) -> float:
        """ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—"""
        
        if not metrics:
            return 0.0
        
        weighted_sum = 0.0
        total_weight = 0.0
        
        for metric in metrics:
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹åã‹ã‚‰é‡ã¿ã‚’å–å¾—
            metric_key = metric.name.lower().replace(' ', '_')
            weight = self.standards_config["weights"].get(metric_key, 0.1)
            
            weighted_sum += metric.current_value * weight
            total_weight += weight
        
        return weighted_sum / total_weight if total_weight > 0 else 0.0
    
    def _determine_quality_level(self, score: float) -> QualityLevel:
        """å“è³ªãƒ¬ãƒ™ãƒ«åˆ¤å®š"""
        
        thresholds = self.standards_config["quality_thresholds"]
        
        if score >= thresholds["excellent"]:
            return QualityLevel.EXCELLENT
        elif score >= thresholds["good"]:
            return QualityLevel.GOOD
        elif score >= thresholds["acceptable"]:
            return QualityLevel.ACCEPTABLE
        elif score >= thresholds["poor"]:
            return QualityLevel.POOR
        else:
            return QualityLevel.CRITICAL
    
    def _aggregate_violations(self, check_results: Dict[str, Any]) -> List[Dict[str, Any]]:
        """é•åäº‹é …çµ±åˆ"""
        
        all_violations = []
        
        for metric_type, result in check_results.items():
            if isinstance(result, dict) and 'violations' in result:
                for violation in result['violations']:
                    violation_entry = violation.copy()
                    violation_entry['metric_type'] = metric_type
                    all_violations.append(violation_entry)
        
        return all_violations
    
    def _generate_improvement_plan(self, metrics: List[QualityMetric], violations: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ”¹å–„è¨ˆç”»ç”Ÿæˆ"""
        
        plan = []
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹åŸºæº–ã®æ”¹å–„é …ç›®
        for metric in metrics:
            if metric.current_value < metric.target_value:
                improvement_item = {
                    "type": "metric_improvement",
                    "metric_name": metric.name,
                    "current_value": metric.current_value,
                    "target_value": metric.target_value,
                    "priority": self._calculate_improvement_priority(metric),
                    "suggestions": metric.improvement_suggestions,
                    "estimated_effort": self._estimate_metric_improvement_effort(metric)
                }
                plan.append(improvement_item)
        
        # é•ååŸºæº–ã®æ”¹å–„é …ç›®
        violation_groups = {}
        for violation in violations:
            vtype = violation.get('type', 'unknown')
            if vtype not in violation_groups:
                violation_groups[vtype] = []
            violation_groups[vtype].append(violation)
        
        for violation_type, violation_list in violation_groups.items():
            improvement_item = {
                "type": "violation_fix",
                "violation_type": violation_type,
                "count": len(violation_list),
                "priority": self._calculate_violation_priority(violation_type, len(violation_list)),
                "violations": violation_list,
                "estimated_effort": len(violation_list) * 2  # 1ä»¶ã‚ãŸã‚Š2åˆ†ã¨ä»®å®š
            }
            plan.append(improvement_item)
        
        # å„ªå…ˆåº¦é †ã§ã‚½ãƒ¼ãƒˆ
        plan.sort(key=lambda x: -x["priority"])
        
        return plan
    
    def _estimate_improvement_time(self, improvement_plan: List[Dict[str, Any]]) -> int:
        """æ”¹å–„æ™‚é–“æ¨å®š"""
        
        total_minutes = 0
        
        for item in improvement_plan:
            total_minutes += item.get("estimated_effort", 5)
        
        return total_minutes
    
    def _calculate_improvement_priority(self, metric: QualityMetric) -> int:
        """æ”¹å–„å„ªå…ˆåº¦è¨ˆç®—"""
        
        gap = metric.target_value - metric.current_value
        severity_weight = {"high": 3, "medium": 2, "low": 1}.get(metric.severity, 1)
        
        return int(gap * 10 * severity_weight)
    
    def _estimate_metric_improvement_effort(self, metric: QualityMetric) -> int:
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ”¹å–„å·¥æ•°æ¨å®š"""
        
        gap = metric.target_value - metric.current_value
        base_effort = {
            "Type Coverage": 3,      # åˆ†/ä¸è¶³é …ç›®
            "Syntax Accuracy": 5,    # åˆ†/ã‚¨ãƒ©ãƒ¼
            "Format Compliance": 1,   # åˆ†ï¼ˆè‡ªå‹•ï¼‰
            "Import Organization": 2, # åˆ†ï¼ˆè‡ªå‹•ï¼‰
            "Docstring Coverage": 8   # åˆ†/æ¬ å¦‚é …ç›®
        }.get(metric.name, 5)
        
        return int(gap * 10 * base_effort)  # gap * æƒ³å®šé•åæ•° * åŸºæœ¬å·¥æ•°
    
    def _calculate_violation_priority(self, violation_type: str, count: int) -> int:
        """é•åå„ªå…ˆåº¦è¨ˆç®—"""
        
        type_priorities = {
            "syntax_error": 10,
            "missing_return_type": 7,
            "missing_arg_type": 6,
            "missing_docstring": 4,
            "format_issue": 3,
            "import_order": 2
        }
        
        base_priority = type_priorities.get(violation_type, 5)
        return base_priority + min(count, 5)  # ä»¶æ•°ãƒœãƒ¼ãƒŠã‚¹ï¼ˆæœ€å¤§5ï¼‰
    
    def _create_error_report(self, file_path: str, error_message: str) -> QualityReport:
        """ã‚¨ãƒ©ãƒ¼ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ"""
        
        return QualityReport(
            report_id=f"error_{int(time.time())}_{Path(file_path).stem}",
            file_path=file_path,
            timestamp=datetime.now().isoformat(),
            overall_score=0.0,
            quality_level=QualityLevel.CRITICAL,
            metrics=[],
            violations=[{"type": "evaluation_error", "description": error_message}],
            improvement_plan=[{
                "type": "fix_evaluation_error",
                "priority": 10,
                "description": error_message,
                "estimated_effort": 10
            }],
            estimated_improvement_time=10
        )
    
    def _create_metric_improvement_suggestion(self, metric: QualityMetric, report: QualityReport) -> Optional[ImprovementSuggestion]:
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ”¹å–„ææ¡ˆä½œæˆ"""
        
        if metric.current_value >= metric.target_value:
            return None
        
        metric_key = metric.name.lower().replace(' ', '_')
        template = self.improvement_templates.get(metric_key, {})
        
        return ImprovementSuggestion(
            suggestion_id=f"metric_{metric.metric_id}_{int(time.time())}",
            title=f"Improve {metric.name}",
            description=f"Increase {metric.name} from {metric.current_value:.2f} to {metric.target_value:.2f}",
            priority=self._calculate_improvement_priority(metric),
            estimated_effort=self._estimate_metric_improvement_effort(metric),
            expected_impact=(metric.target_value - metric.current_value),
            implementation_steps=metric.improvement_suggestions,
            code_examples=self._get_code_examples(metric_key),
            validation_method=template.get("validation", "manual_check")
        )
    
    def _create_violation_improvement_suggestion(self, violation_type: str, report: QualityReport) -> Optional[ImprovementSuggestion]:
        """é•åæ”¹å–„ææ¡ˆä½œæˆ"""
        
        violations = [v for v in report.violations if v.get('type') == violation_type]
        if not violations:
            return None
        
        return ImprovementSuggestion(
            suggestion_id=f"violation_{violation_type}_{int(time.time())}",
            title=f"Fix {violation_type.replace('_', ' ').title()} Issues",
            description=f"Resolve {len(violations)} {violation_type} violations",
            priority=self._calculate_violation_priority(violation_type, len(violations)),
            estimated_effort=len(violations) * 2,
            expected_impact=0.1 * len(violations),
            implementation_steps=[f"Fix {violation_type} in {len(violations)} locations"],
            code_examples=self._get_violation_code_examples(violation_type),
            validation_method="automated_check"
        )
    
    def _get_code_examples(self, metric_type: str) -> List[str]:
        """ã‚³ãƒ¼ãƒ‰ä¾‹å–å¾—"""
        
        examples = {
            "type_coverage": [
                "def function(param: Any) -> None:",
                "def process(data: Dict[str, Any]) -> List[Any]:",
                "from typing import Any, Dict, List"
            ],
            "format_compliance": [
                "# Run: black file.py",
                "# Run: isort file.py"
            ],
            "docstring_coverage": [
                '"""Module description."""',
                'def function():\n    """Function description."""'
            ]
        }
        
        return examples.get(metric_type, [])
    
    def _get_violation_code_examples(self, violation_type: str) -> List[str]:
        """é•åã‚³ãƒ¼ãƒ‰ä¾‹å–å¾—"""
        
        examples = {
            "syntax_error": ["# Fix syntax errors as indicated by Python parser"],
            "missing_return_type": ["def function() -> None:"],
            "missing_arg_type": ["def function(param: Any):"]
        }
        
        return examples.get(violation_type, [])
    
    def _can_auto_apply(self, suggestion: ImprovementSuggestion) -> bool:
        """è‡ªå‹•é©ç”¨å¯èƒ½åˆ¤å®š"""
        
        auto_applicable_types = [
            "format_compliance",
            "import_organization"
        ]
        
        return any(t in suggestion.suggestion_id for t in auto_applicable_types)
    
    def _apply_suggestion(self, file_path: str, suggestion: ImprovementSuggestion) -> bool:
        """ææ¡ˆé©ç”¨"""
        
        try:
            if "format" in suggestion.suggestion_id.lower():
                # Black + isort å®Ÿè¡Œ
                subprocess.run(['black', file_path], check=True, capture_output=True)
                subprocess.run(['isort', file_path], check=True, capture_output=True)
                return True
            
            # ãã®ä»–ã®è‡ªå‹•é©ç”¨ã¯ä»Šå¾Œå®Ÿè£…
            return False
        
        except (subprocess.CalledProcessError, FileNotFoundError):
            return False
    
    def save_report(self, report: QualityReport, output_dir: str = "tmp/quality_reports") -> str:
        """ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜"""
        
        os.makedirs(output_dir, exist_ok=True)
        
        filename = f"{report.report_id}.json"
        filepath = os.path.join(output_dir, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(asdict(report), f, indent=2, ensure_ascii=False, default=str)
        
        print(f"ğŸ’¾ å“è³ªãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {filepath}")
        return filepath


def main():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    
    # ãƒ†ã‚¹ãƒˆç”¨ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
    test_file = "tmp/test_quality_sample.py"
    os.makedirs("tmp", exist_ok=True)
    
    sample_code = '''#!/usr/bin/env python3
import os
import json
from typing import Any

def process_data(data, options):
    """ãƒ‡ãƒ¼ã‚¿å‡¦ç†é–¢æ•°"""
    return data * 2

class DataProcessor:
    def __init__(self, config):
        self.config = config
    
    def process(self, items):
        results = []
        for item in items:
            processed = self.transform(item)
            results.append(processed)
        return results
'''
    
    with open(test_file, 'w', encoding='utf-8') as f:
        f.write(sample_code)
    
    print("ğŸ§ª QualityStandardsManager ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
    
    manager = QualityStandardsManager()
    
    # å“è³ªè©•ä¾¡ãƒ†ã‚¹ãƒˆ
    print("\n=== å“è³ªè©•ä¾¡ãƒ†ã‚¹ãƒˆ ===")
    report = manager.evaluate_file_quality(test_file)
    
    print(f"ãƒ•ã‚¡ã‚¤ãƒ«: {report.file_path}")
    print(f"ç·åˆã‚¹ã‚³ã‚¢: {report.overall_score:.2f}")
    print(f"å“è³ªãƒ¬ãƒ™ãƒ«: {report.quality_level.value}")
    print(f"ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ•°: {len(report.metrics)}")
    print(f"é•åäº‹é …: {len(report.violations)}ä»¶")
    print(f"æ”¹å–„è¨ˆç”»: {len(report.improvement_plan)}é …ç›®")
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è©³ç´°
    print("\n=== ãƒ¡ãƒˆãƒªã‚¯ã‚¹è©³ç´° ===")
    for metric in report.metrics:
        print(f"{metric.name}: {metric.current_value:.2f}/{metric.target_value:.2f}")
    
    # æ”¹å–„ææ¡ˆãƒ†ã‚¹ãƒˆ
    print("\n=== æ”¹å–„ææ¡ˆãƒ†ã‚¹ãƒˆ ===")
    suggestions = manager.generate_improvement_suggestions(report)
    
    for i, suggestion in enumerate(suggestions, 1):
        print(f"{i}. {suggestion.title}")
        print(f"   å„ªå…ˆåº¦: {suggestion.priority}, äºˆæƒ³åŠ¹æœ: {suggestion.expected_impact:.2f}")
        print(f"   æ¨å®šæ™‚é–“: {suggestion.estimated_effort}åˆ†")
    
    # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜ãƒ†ã‚¹ãƒˆ
    print("\n=== ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜ãƒ†ã‚¹ãƒˆ ===")
    saved_path = manager.save_report(report)
    print(f"ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜å®Œäº†: {saved_path}")
    
    # è‡ªå‹•æ”¹å–„ãƒ†ã‚¹ãƒˆï¼ˆãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®ã¿ï¼‰
    print("\n=== è‡ªå‹•æ”¹å–„ãƒ†ã‚¹ãƒˆ ===")
    format_suggestions = [s for s in suggestions if 'format' in s.title.lower()]
    if format_suggestions:
        auto_results = manager.auto_apply_improvements(test_file, format_suggestions[:1])
        print(f"é©ç”¨ã•ã‚ŒãŸæ”¹å–„: {len(auto_results['applied_improvements'])}ä»¶")
        print(f"å¤±æ•—ã—ãŸæ”¹å–„: {len(auto_results['failed_improvements'])}ä»¶")
        print(f"ã‚¹ã‚³ã‚¢æ”¹å–„: {auto_results['improvement_delta']:.3f}")
    
    print("\nğŸ‰ QualityStandardsManager ãƒ†ã‚¹ãƒˆå®Œäº†")


if __name__ == "__main__":
    main()