#!/usr/bin/env python3
"""
PreventiveQualityChecker - Issue #860å¯¾å¿œ
ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ä½¿ç”¨ç‡å‰Šæ¸›ã®ãŸã‚ã®äºˆé˜²çš„å“è³ªãƒã‚§ãƒƒã‚¯ã‚·ã‚¹ãƒ†ãƒ 

äº‹å‰å“è³ªæ¤œè¨¼ãƒ»ãƒªã‚¹ã‚¯äºˆæ¸¬ãƒ»äºˆé˜²çš„ä»‹å…¥ã«ã‚ˆã‚Š
ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•æ©Ÿèƒ½ã¸ã®ä¾å­˜åº¦ã‚’å‰Šæ¸›ã—ã€å”æ¥­å®‰å…¨æ€§ã‚’å‘ä¸Š
"""

import json
import datetime
import statistics
import os
import subprocess
from typing import Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass, asdict
from enum import Enum
from pathlib import Path

from kumihan_formatter.core.utilities.logger import get_logger

logger = get_logger(__name__)


class RiskLevel(Enum):
    """ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«"""
    LOW = "low"           # ä½ãƒªã‚¹ã‚¯
    MEDIUM = "medium"     # ä¸­ãƒªã‚¹ã‚¯
    HIGH = "high"         # é«˜ãƒªã‚¹ã‚¯
    CRITICAL = "critical" # ç·Šæ€¥ãƒªã‚¹ã‚¯


class QualityMetric(Enum):
    """å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    SYNTAX_QUALITY = "syntax_quality"
    TYPE_SAFETY = "type_safety"
    CODE_COMPLEXITY = "code_complexity"
    TEST_COVERAGE = "test_coverage"
    DOCUMENTATION = "documentation"
    SECURITY = "security"
    PERFORMANCE = "performance"
    MAINTAINABILITY = "maintainability"


class PreventiveAction(Enum):
    """äºˆé˜²çš„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³"""
    SYNTAX_PRECHECK = "syntax_precheck"
    TYPE_VALIDATION = "type_validation"
    COMPLEXITY_REDUCTION = "complexity_reduction"
    TEST_GENERATION = "test_generation"
    DOCUMENTATION_UPDATE = "documentation_update"
    SECURITY_SCAN = "security_scan"
    PERFORMANCE_OPTIMIZATION = "performance_optimization"
    REFACTORING_SUGGESTION = "refactoring_suggestion"


@dataclass
class QualityRisk:
    """å“è³ªãƒªã‚¹ã‚¯"""
    metric: QualityMetric
    risk_level: RiskLevel
    current_score: float  # 0.0-1.0
    target_score: float   # 0.0-1.0
    gap: float           # target - current
    impact_description: str
    recommended_actions: List[PreventiveAction]
    estimated_effort: str  # "low", "medium", "high"


@dataclass
class PreventiveCheckResult:
    """äºˆé˜²çš„ãƒã‚§ãƒƒã‚¯çµæœ"""
    file_path: str
    overall_quality_score: float  # 0.0-1.0
    risk_assessment: RiskLevel
    detected_risks: List[QualityRisk]
    preventive_recommendations: List[str]
    immediate_actions: List[PreventiveAction]
    failsafe_prevention_score: float  # ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•å›é¿ç¢ºç‡
    execution_time: float


@dataclass
class FailsafeReductionReport:
    """ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•å‰Šæ¸›ãƒ¬ãƒãƒ¼ãƒˆ"""
    timestamp: str
    baseline_failsafe_rate: float
    current_failsafe_rate: float
    reduction_percentage: float
    quality_improvements: Dict[str, float]
    preventive_actions_taken: List[str]
    success_metrics: Dict[str, float]
    recommendations: List[str]


class PreventiveQualityChecker:
    """äºˆé˜²çš„å“è³ªãƒã‚§ãƒƒã‚¯ã‚·ã‚¹ãƒ†ãƒ 

    ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•æ©Ÿèƒ½ã«ä¾å­˜ã™ã‚‹å‰ã«å“è³ªå•é¡Œã‚’äºˆé˜²çš„ã«æ¤œå‡ºãƒ»å¯¾å‡¦ã—ã€
    å”æ¥­ã‚·ã‚¹ãƒ†ãƒ ã®å®‰å®šæ€§ã¨åŠ¹ç‡æ€§ã‚’å‘ä¸Šã•ã›ã‚‹
    """

    def __init__(self, postbox_dir: str = "postbox"):
        self.postbox_dir = Path(postbox_dir)
        self.monitoring_dir = self.postbox_dir / "monitoring"
        self.quality_dir = self.postbox_dir / "quality"
        self.preventive_dir = self.quality_dir / "preventive_checks"

        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        self.preventive_dir.mkdir(parents=True, exist_ok=True)

        # å“è³ªåŸºæº–è¨­å®š
        self.quality_thresholds = self._load_quality_thresholds()

        # ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•å‰Šæ¸›ç›®æ¨™
        self.target_failsafe_rate = 0.20  # ç›®æ¨™: 20%ä»¥ä¸‹
        self.current_baseline = self._calculate_baseline_failsafe_rate()

        # äºˆé˜²çš„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å±¥æ­´
        self.action_history_file = self.preventive_dir / "action_history.json"
        self.action_history = self._load_action_history()

        logger.info("ğŸ›¡ï¸ PreventiveQualityChecker åˆæœŸåŒ–å®Œäº†")
        logger.info(f"ğŸ“Š ç¾åœ¨ã®ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ä½¿ç”¨ç‡: {self.current_baseline:.1%}")
        logger.info(f"ğŸ¯ ç›®æ¨™ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ä½¿ç”¨ç‡: {self.target_failsafe_rate:.1%}")

    def run_preventive_check(self, file_path: str,
                           code_content: Optional[str] = None,
                           context: Optional[Dict[str, Any]] = None) -> PreventiveCheckResult:
        """äºˆé˜²çš„å“è³ªãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ

        Args:
            file_path: ãƒã‚§ãƒƒã‚¯å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            code_content: ã‚³ãƒ¼ãƒ‰å†…å®¹ï¼ˆNoneã®å ´åˆãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ï¼‰
            context: å®Ÿè¡Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ

        Returns:
            PreventiveCheckResult: äºˆé˜²çš„ãƒã‚§ãƒƒã‚¯çµæœ
        """

        start_time = datetime.datetime.now()
        logger.info(f"ğŸ›¡ï¸ äºˆé˜²çš„å“è³ªãƒã‚§ãƒƒã‚¯é–‹å§‹: {file_path}")

        try:
            # ã‚³ãƒ¼ãƒ‰èª­ã¿è¾¼ã¿
            if code_content is None:
                if not Path(file_path).exists():
                    return self._create_file_not_found_result(file_path, start_time)

                with open(file_path, 'r', encoding='utf-8') as f:
                    code_content = f.read()

            context = context or {}

            # ãƒãƒ«ãƒãƒ¡ãƒˆãƒªãƒƒã‚¯å“è³ªè©•ä¾¡
            quality_scores = self._evaluate_quality_metrics(code_content, file_path)

            # ãƒªã‚¹ã‚¯è©•ä¾¡
            detected_risks = self._assess_quality_risks(quality_scores, file_path)

            # å…¨ä½“å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—
            overall_score = self._calculate_overall_quality_score(quality_scores)

            # ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«åˆ¤å®š
            risk_level = self._determine_risk_level(detected_risks, overall_score)

            # äºˆé˜²çš„æ¨å¥¨äº‹é …ç”Ÿæˆ
            recommendations = self._generate_preventive_recommendations(detected_risks, context)

            # å³åº§å®Ÿè¡Œã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç‰¹å®š
            immediate_actions = self._identify_immediate_actions(detected_risks)

            # ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•å›é¿ç¢ºç‡è¨ˆç®—
            prevention_score = self._calculate_failsafe_prevention_score(
                overall_score, detected_risks
            )

            execution_time = (datetime.datetime.now() - start_time).total_seconds()

            result = PreventiveCheckResult(
                file_path=file_path,
                overall_quality_score=overall_score,
                risk_assessment=risk_level,
                detected_risks=detected_risks,
                preventive_recommendations=recommendations,
                immediate_actions=immediate_actions,
                failsafe_prevention_score=prevention_score,
                execution_time=execution_time
            )

            # çµæœè¨˜éŒ²
            self._record_preventive_check(result)

            logger.info(f"âœ… äºˆé˜²çš„å“è³ªãƒã‚§ãƒƒã‚¯å®Œäº†: {file_path} (å“è³ª: {overall_score:.3f}, å›é¿ç‡: {prevention_score:.3f})")

            return result

        except Exception as e:
            logger.error(f"âŒ äºˆé˜²çš„å“è³ªãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {file_path} - {e}")
            return self._create_error_result(file_path, str(e), start_time)

    def _evaluate_quality_metrics(self, code_content: str, file_path: str) -> Dict[QualityMetric, float]:
        """å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹è©•ä¾¡"""

        scores = {}

        # æ§‹æ–‡å“è³ªè©•ä¾¡
        scores[QualityMetric.SYNTAX_QUALITY] = self._evaluate_syntax_quality(code_content)

        # å‹å®‰å…¨æ€§è©•ä¾¡
        scores[QualityMetric.TYPE_SAFETY] = self._evaluate_type_safety(code_content)

        # ã‚³ãƒ¼ãƒ‰è¤‡é›‘åº¦è©•ä¾¡
        scores[QualityMetric.CODE_COMPLEXITY] = self._evaluate_code_complexity(code_content)

        # ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸è©•ä¾¡
        scores[QualityMetric.TEST_COVERAGE] = self._evaluate_test_coverage(file_path)

        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè©•ä¾¡
        scores[QualityMetric.DOCUMENTATION] = self._evaluate_documentation(code_content)

        # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è©•ä¾¡
        scores[QualityMetric.SECURITY] = self._evaluate_security(code_content)

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡
        scores[QualityMetric.PERFORMANCE] = self._evaluate_performance(code_content)

        # ä¿å®ˆæ€§è©•ä¾¡
        scores[QualityMetric.MAINTAINABILITY] = self._evaluate_maintainability(code_content)

        return scores

    def _evaluate_syntax_quality(self, code_content: str) -> float:
        """æ§‹æ–‡å“è³ªè©•ä¾¡"""

        try:
            # åŸºæœ¬çš„ãªæ§‹æ–‡ãƒã‚§ãƒƒã‚¯
            compile(code_content, '<string>', 'exec')

            syntax_score = 1.0

            # æ§‹æ–‡å“è³ªã®è¿½åŠ ãƒã‚§ãƒƒã‚¯
            lines = code_content.split('\n')
            issues = 0

            for line in lines:
                # é•·ã™ãã‚‹è¡Œ
                if len(line) > 120:
                    issues += 1

                # ä¸é©åˆ‡ãªã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆï¼ˆã‚¿ãƒ–ã¨ã‚¹ãƒšãƒ¼ã‚¹ã®æ··åœ¨ï¼‰
                if '\t' in line and '    ' in line:
                    issues += 1

                # è¤‡æ•°ã®ã‚¹ãƒ†ãƒ¼ãƒˆãƒ¡ãƒ³ãƒˆï¼ˆã‚»ãƒŸã‚³ãƒ­ãƒ³ï¼‰
                if ';' in line and not line.strip().startswith('#'):
                    issues += 1

            # å•é¡Œã«åŸºã¥ãæ¸›ç‚¹
            if len(lines) > 0:
                syntax_score -= (issues / len(lines)) * 0.5

            return max(syntax_score, 0.0)

        except SyntaxError:
            return 0.0
        except Exception:
            return 0.5  # ä¸æ˜ãªå ´åˆã¯ä¸­é–“å€¤

    def _evaluate_type_safety(self, code_content: str) -> float:
        """å‹å®‰å…¨æ€§è©•ä¾¡"""

        try:
            import ast
            tree = ast.parse(code_content)

            total_functions = 0
            typed_functions = 0
            total_vars = 0
            typed_vars = 0

            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    total_functions += 1

                    # æˆ»ã‚Šå€¤å‹æ³¨é‡ˆãƒã‚§ãƒƒã‚¯
                    if node.returns is not None:
                        typed_functions += 0.5

                    # å¼•æ•°å‹æ³¨é‡ˆãƒã‚§ãƒƒã‚¯
                    typed_args = sum(1 for arg in node.args.args if arg.annotation is not None)
                    total_args = len(node.args.args)

                    if total_args > 0 and typed_args == total_args:
                        typed_functions += 0.5

                elif isinstance(node, ast.AnnAssign):
                    total_vars += 1
                    if node.annotation is not None:
                        typed_vars += 1

            # å‹æ³¨é‡ˆç‡è¨ˆç®—
            type_score = 0.0

            if total_functions > 0:
                function_type_rate = typed_functions / total_functions
                type_score += function_type_rate * 0.7  # é–¢æ•°å‹æ³¨é‡ˆã®é‡ã¿70%

            if total_vars > 0:
                var_type_rate = typed_vars / total_vars
                type_score += var_type_rate * 0.3  # å¤‰æ•°å‹æ³¨é‡ˆã®é‡ã¿30%
            elif total_functions > 0:
                type_score += 0.3  # å¤‰æ•°ãŒãªã„å ´åˆã¯æº€ç‚¹

            return min(type_score, 1.0)

        except Exception:
            return 0.5

    def _evaluate_code_complexity(self, code_content: str) -> float:
        """ã‚³ãƒ¼ãƒ‰è¤‡é›‘åº¦è©•ä¾¡ï¼ˆå˜ç´”åŒ–ç‰ˆï¼‰"""

        try:
            import ast
            tree = ast.parse(code_content)

            complexity_score = 1.0
            total_complexity = 0
            function_count = 0

            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    function_count += 1
                    func_complexity = self._calculate_cyclomatic_complexity(node)
                    total_complexity += func_complexity

                    # è¤‡é›‘åº¦ãƒšãƒŠãƒ«ãƒ†ã‚£
                    if func_complexity > 10:
                        complexity_score -= 0.1
                    elif func_complexity > 15:
                        complexity_score -= 0.2

            # å¹³å‡è¤‡é›‘åº¦èª¿æ•´
            if function_count > 0:
                avg_complexity = total_complexity / function_count
                if avg_complexity > 8:
                    complexity_score -= 0.1

            return max(complexity_score, 0.0)

        except Exception:
            return 0.7  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤

    def _calculate_cyclomatic_complexity(self, func_node: ast.FunctionDef) -> int:
        """å¾ªç’°çš„è¤‡é›‘åº¦è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""

        complexity = 1  # ãƒ™ãƒ¼ã‚¹è¤‡é›‘åº¦

        for node in ast.walk(func_node):
            if isinstance(node, (ast.If, ast.For, ast.While, ast.Try)):
                complexity += 1
            elif isinstance(node, ast.BoolOp):
                complexity += len(node.values) - 1

        return complexity

    def _evaluate_test_coverage(self, file_path: str) -> float:
        """ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸è©•ä¾¡ï¼ˆæ¨å®šï¼‰"""

        # å®Ÿéš›ã®ã‚«ãƒãƒ¬ãƒƒã‚¸æ¸¬å®šã¯é‡ã„ãŸã‚ã€ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯è©•ä¾¡
        file_name = Path(file_path).stem
        test_file_patterns = [
            f"test_{file_name}.py",
            f"{file_name}_test.py",
            f"tests/test_{file_name}.py",
            f"tests/{file_name}_test.py"
        ]

        test_file_exists = False
        for pattern in test_file_patterns:
            test_path = Path(file_path).parent / pattern
            if test_path.exists():
                test_file_exists = True
                break

        if test_file_exists:
            return 0.8  # ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆã¯é«˜ã‚¹ã‚³ã‚¢
        else:
            return 0.3  # ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã„å ´åˆã¯ä½ã‚¹ã‚³ã‚¢

    def _evaluate_documentation(self, code_content: str) -> float:
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè©•ä¾¡"""

        try:
            import ast
            tree = ast.parse(code_content)

            total_functions = 0
            documented_functions = 0
            total_classes = 0
            documented_classes = 0

            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    total_functions += 1
                    if ast.get_docstring(node):
                        documented_functions += 1

                elif isinstance(node, ast.ClassDef):
                    total_classes += 1
                    if ast.get_docstring(node):
                        documented_classes += 1

            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç‡è¨ˆç®—
            doc_score = 0.0

            if total_functions > 0:
                func_doc_rate = documented_functions / total_functions
                doc_score += func_doc_rate * 0.6  # é–¢æ•°ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®é‡ã¿60%

            if total_classes > 0:
                class_doc_rate = documented_classes / total_classes
                doc_score += class_doc_rate * 0.4  # ã‚¯ãƒ©ã‚¹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®é‡ã¿40%
            elif total_functions > 0:
                doc_score += 0.4  # ã‚¯ãƒ©ã‚¹ãŒãªã„å ´åˆã¯æº€ç‚¹

            return min(doc_score, 1.0)

        except Exception:
            return 0.5

    def _evaluate_security(self, code_content: str) -> float:
        """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è©•ä¾¡ï¼ˆåŸºæœ¬ãƒã‚§ãƒƒã‚¯ï¼‰"""

        security_score = 1.0

        # åŸºæœ¬çš„ãªã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒªã‚¹ã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³
        risk_patterns = [
            ('eval(', 'eval()ã®ä½¿ç”¨'),
            ('exec(', 'exec()ã®ä½¿ç”¨'),
            ('os.system(', 'os.system()ã®ä½¿ç”¨'),
            ('subprocess.call(', 'subprocess.call()ã®ç›´æ¥ä½¿ç”¨'),
            ('input(', 'input()ã®ä½¿ç”¨ï¼ˆã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ãƒªã‚¹ã‚¯ï¼‰'),
            ('pickle.load(', 'pickle.load()ã®ä½¿ç”¨'),
            ('yaml.load(', 'yaml.load()ã®å®‰å…¨ã§ãªã„ä½¿ç”¨')
        ]

        for pattern, description in risk_patterns:
            if pattern in code_content:
                security_score -= 0.15
                logger.debug(f"ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒªã‚¹ã‚¯æ¤œå‡º: {description}")

        return max(security_score, 0.0)

    def _evaluate_performance(self, code_content: str) -> float:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡ï¼ˆåŸºæœ¬ãƒã‚§ãƒƒã‚¯ï¼‰"""

        performance_score = 1.0

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«å½±éŸ¿ã™ã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³
        perf_issues = [
            ('for.*in.*range(len(', 'éåŠ¹ç‡ãªãƒ«ãƒ¼ãƒ—ãƒ‘ã‚¿ãƒ¼ãƒ³'),
            ('\.append.*for.*in', 'ãƒªã‚¹ãƒˆå†…åŒ…è¡¨è¨˜ãŒå¯èƒ½ãªç®‡æ‰€'),
            ('time.sleep(', 'sleep()ã®ä½¿ç”¨'),
            ('while True:', 'ç„¡é™ãƒ«ãƒ¼ãƒ—ã®å¯èƒ½æ€§')
        ]

        import re
        for pattern, description in perf_issues:
            if re.search(pattern, code_content):
                performance_score -= 0.1
                logger.debug(f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å•é¡Œæ¤œå‡º: {description}")

        return max(performance_score, 0.0)

    def _evaluate_maintainability(self, code_content: str) -> float:
        """ä¿å®ˆæ€§è©•ä¾¡"""

        maintainability_score = 1.0

        lines = code_content.split('\n')
        code_lines = [line for line in lines if line.strip() and not line.strip().startswith('#')]

        if not code_lines:
            return 1.0

        # ä¿å®ˆæ€§ã«å½±éŸ¿ã™ã‚‹è¦å› 

        # 1. ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º
        if len(code_lines) > 500:
            maintainability_score -= 0.2
        elif len(code_lines) > 300:
            maintainability_score -= 0.1

        # 2. é–¢æ•°ã‚µã‚¤ã‚º
        try:
            import ast
            tree = ast.parse(code_content)

            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    func_lines = node.end_lineno - node.lineno if hasattr(node, 'end_lineno') else 0
                    if func_lines > 50:
                        maintainability_score -= 0.1
        except Exception:
            pass

        # 3. ã‚³ãƒ¡ãƒ³ãƒˆç‡
        comment_lines = [line for line in lines if line.strip().startswith('#')]
        if code_lines:
            comment_ratio = len(comment_lines) / len(code_lines)
            if comment_ratio < 0.1:  # ã‚³ãƒ¡ãƒ³ãƒˆç‡10%æœªæº€
                maintainability_score -= 0.1

        return max(maintainability_score, 0.0)

    def _assess_quality_risks(self, quality_scores: Dict[QualityMetric, float],
                            file_path: str) -> List[QualityRisk]:
        """å“è³ªãƒªã‚¹ã‚¯è©•ä¾¡"""

        risks = []
        thresholds = self.quality_thresholds

        for metric, current_score in quality_scores.items():
            threshold = thresholds.get(metric.value, 0.7)

            if current_score < threshold:
                gap = threshold - current_score

                # ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«åˆ¤å®š
                if gap >= 0.4:
                    risk_level = RiskLevel.CRITICAL
                elif gap >= 0.3:
                    risk_level = RiskLevel.HIGH
                elif gap >= 0.2:
                    risk_level = RiskLevel.MEDIUM
                else:
                    risk_level = RiskLevel.LOW

                # æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ±ºå®š
                recommended_actions = self._get_recommended_actions_for_metric(metric)

                # ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆèª¬æ˜
                impact = self._get_impact_description(metric, risk_level)

                # æ¨å®šå·¥æ•°
                effort = self._estimate_effort(metric, gap)

                risk = QualityRisk(
                    metric=metric,
                    risk_level=risk_level,
                    current_score=current_score,
                    target_score=threshold,
                    gap=gap,
                    impact_description=impact,
                    recommended_actions=recommended_actions,
                    estimated_effort=effort
                )

                risks.append(risk)

        return risks

    def _get_recommended_actions_for_metric(self, metric: QualityMetric) -> List[PreventiveAction]:
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹åˆ¥æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³"""

        action_mapping = {
            QualityMetric.SYNTAX_QUALITY: [PreventiveAction.SYNTAX_PRECHECK],
            QualityMetric.TYPE_SAFETY: [PreventiveAction.TYPE_VALIDATION],
            QualityMetric.CODE_COMPLEXITY: [PreventiveAction.COMPLEXITY_REDUCTION, PreventiveAction.REFACTORING_SUGGESTION],
            QualityMetric.TEST_COVERAGE: [PreventiveAction.TEST_GENERATION],
            QualityMetric.DOCUMENTATION: [PreventiveAction.DOCUMENTATION_UPDATE],
            QualityMetric.SECURITY: [PreventiveAction.SECURITY_SCAN],
            QualityMetric.PERFORMANCE: [PreventiveAction.PERFORMANCE_OPTIMIZATION],
            QualityMetric.MAINTAINABILITY: [PreventiveAction.REFACTORING_SUGGESTION]
        }

        return action_mapping.get(metric, [])

    def _get_impact_description(self, metric: QualityMetric, risk_level: RiskLevel) -> str:
        """ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆèª¬æ˜ç”Ÿæˆ"""

        impact_templates = {
            QualityMetric.SYNTAX_QUALITY: {
                RiskLevel.CRITICAL: "æ§‹æ–‡ã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚Šã‚·ã‚¹ãƒ†ãƒ åœæ­¢ã®å¯èƒ½æ€§",
                RiskLevel.HIGH: "é »ç¹ãªæ§‹æ–‡ã‚¨ãƒ©ãƒ¼ã§é–‹ç™ºåŠ¹ç‡å¤§å¹…ä½ä¸‹",
                RiskLevel.MEDIUM: "æ§‹æ–‡å•é¡Œã«ã‚ˆã‚‹è»½å¾®ãªé…å»¶",
                RiskLevel.LOW: "è»½å¾®ãªæ§‹æ–‡å“è³ªå•é¡Œ"
            },
            QualityMetric.TYPE_SAFETY: {
                RiskLevel.CRITICAL: "å‹ã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚‹å®Ÿè¡Œæ™‚éšœå®³ã®é«˜ãƒªã‚¹ã‚¯",
                RiskLevel.HIGH: "å‹å®‰å…¨æ€§ä¸å‚™ã«ã‚ˆã‚‹ä¿å®ˆæ€§å•é¡Œ",
                RiskLevel.MEDIUM: "å‹æ³¨é‡ˆä¸è¶³ã«ã‚ˆã‚‹å¯èª­æ€§ä½ä¸‹",
                RiskLevel.LOW: "è»½å¾®ãªå‹æ³¨é‡ˆå•é¡Œ"
            },
            QualityMetric.CODE_COMPLEXITY: {
                RiskLevel.CRITICAL: "æ¥µç«¯ãªè¤‡é›‘æ€§ã«ã‚ˆã‚‹ä¿å®ˆä¸èƒ½çŠ¶æ…‹",
                RiskLevel.HIGH: "é«˜è¤‡é›‘æ€§ã«ã‚ˆã‚‹ä¿å®ˆæ€§ãƒ»ãƒ†ã‚¹ãƒˆæ€§å•é¡Œ",
                RiskLevel.MEDIUM: "ä¸­ç¨‹åº¦ã®è¤‡é›‘æ€§ã«ã‚ˆã‚‹ç†è§£å›°é›£",
                RiskLevel.LOW: "è»½å¾®ãªè¤‡é›‘æ€§å•é¡Œ"
            }
        }

        metric_impacts = impact_templates.get(metric, {})
        return metric_impacts.get(risk_level, f"{metric.value}ã«é–¢ã™ã‚‹å“è³ªå•é¡Œ")

    def _estimate_effort(self, metric: QualityMetric, gap: float) -> str:
        """å·¥æ•°è¦‹ç©ã‚‚ã‚Š"""

        if gap >= 0.4:
            return "high"
        elif gap >= 0.2:
            return "medium"
        else:
            return "low"

    def _calculate_overall_quality_score(self, quality_scores: Dict[QualityMetric, float]) -> float:
        """å…¨ä½“å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—"""

        # é‡ã¿ä»˜ãå¹³å‡
        weights = {
            QualityMetric.SYNTAX_QUALITY: 0.25,      # æ§‹æ–‡å“è³ªï¼šæœ€é‡è¦
            QualityMetric.TYPE_SAFETY: 0.20,         # å‹å®‰å…¨æ€§ï¼šé‡è¦
            QualityMetric.CODE_COMPLEXITY: 0.15,     # è¤‡é›‘åº¦ï¼šé‡è¦
            QualityMetric.TEST_COVERAGE: 0.15,       # ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ï¼šé‡è¦
            QualityMetric.DOCUMENTATION: 0.10,       # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼šä¸­ç¨‹åº¦
            QualityMetric.SECURITY: 0.08,           # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ï¼šä¸­ç¨‹åº¦
            QualityMetric.PERFORMANCE: 0.04,        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ï¼šä½
            QualityMetric.MAINTAINABILITY: 0.03     # ä¿å®ˆæ€§ï¼šä½
        }

        weighted_score = sum(
            quality_scores.get(metric, 0.0) * weight
            for metric, weight in weights.items()
        )

        return min(weighted_score, 1.0)

    def _determine_risk_level(self, detected_risks: List[QualityRisk],
                            overall_score: float) -> RiskLevel:
        """ç·åˆãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«åˆ¤å®š"""

        # æœ€é«˜ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«ã‚’æ¡ç”¨
        if any(risk.risk_level == RiskLevel.CRITICAL for risk in detected_risks):
            return RiskLevel.CRITICAL
        elif any(risk.risk_level == RiskLevel.HIGH for risk in detected_risks):
            return RiskLevel.HIGH
        elif any(risk.risk_level == RiskLevel.MEDIUM for risk in detected_risks):
            return RiskLevel.MEDIUM

        # å…¨ä½“ã‚¹ã‚³ã‚¢ã«ã‚ˆã‚‹åˆ¤å®š
        if overall_score < 0.5:
            return RiskLevel.HIGH
        elif overall_score < 0.7:
            return RiskLevel.MEDIUM
        else:
            return RiskLevel.LOW

    def _generate_preventive_recommendations(self, detected_risks: List[QualityRisk],
                                           context: Dict[str, Any]) -> List[str]:
        """äºˆé˜²çš„æ¨å¥¨äº‹é …ç”Ÿæˆ"""

        recommendations = []

        # ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«åˆ¥æ¨å¥¨
        critical_risks = [r for r in detected_risks if r.risk_level == RiskLevel.CRITICAL]
        high_risks = [r for r in detected_risks if r.risk_level == RiskLevel.HIGH]

        if critical_risks:
            recommendations.append("ğŸš¨ ç·Šæ€¥å¯¾å¿œ: é‡å¤§ãªå“è³ªãƒªã‚¹ã‚¯ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ")
            for risk in critical_risks[:3]:  # ä¸Šä½3ä»¶
                recommendations.append(f"  - {risk.metric.value}: {risk.impact_description}")

        if high_risks:
            recommendations.append("âš ï¸ é«˜å„ªå…ˆåº¦: ä»¥ä¸‹ã®å“è³ªå•é¡Œã¸ã®å¯¾å¿œã‚’æ¨å¥¨")
            for risk in high_risks[:3]:
                recommendations.append(f"  - {risk.metric.value}: æ”¹å–„ã«ã‚ˆã‚Šå“è³ªå¤§å¹…å‘ä¸Š")

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹åˆ¥æ¨å¥¨
        syntax_risks = [r for r in detected_risks if r.metric == QualityMetric.SYNTAX_QUALITY]
        if syntax_risks:
            recommendations.append("ğŸ“ æ§‹æ–‡å“è³ªå‘ä¸Šã«ã‚ˆã‚Šã€Layer1æ¤œè¨¼æˆåŠŸç‡ã‚’å‘ä¸Š")

        type_risks = [r for r in detected_risks if r.metric == QualityMetric.TYPE_SAFETY]
        if type_risks:
            recommendations.append("ğŸ·ï¸ å‹æ³¨é‡ˆè¿½åŠ ã«ã‚ˆã‚Šã€ã‚³ãƒ¼ãƒ‰å“è³ªã¨ä¿å®ˆæ€§ã‚’å‘ä¸Š")

        # ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•å‰Šæ¸›ã¸ã®è²¢çŒ®
        if detected_risks:
            recommendations.append(f"ğŸ›¡ï¸ äºˆé˜²çš„æ”¹å–„ã«ã‚ˆã‚Šã€ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ä½¿ç”¨ç‡ã‚’å‰Šæ¸›å¯èƒ½")

        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆåˆ¥æ¨å¥¨
        task_type = context.get("task_type", "")
        if task_type == "new_implementation":
            recommendations.append("ğŸ†• æ–°è¦å®Ÿè£…: åˆæœŸæ®µéšã§ã®å“è³ªç¢ºä¿ãŒé‡è¦")
        elif task_type == "error_fixing":
            recommendations.append("ğŸ”§ ã‚¨ãƒ©ãƒ¼ä¿®æ­£: æ ¹æœ¬åŸå› å¯¾ç­–ã§å†ç™ºé˜²æ­¢")

        return recommendations

    def _identify_immediate_actions(self, detected_risks: List[QualityRisk]) -> List[PreventiveAction]:
        """å³åº§å®Ÿè¡Œã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç‰¹å®š"""

        immediate_actions = []

        # ç·Šæ€¥ãƒ»é«˜ãƒªã‚¹ã‚¯ã®æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å³åº§å®Ÿè¡Œãƒªã‚¹ãƒˆã«è¿½åŠ 
        for risk in detected_risks:
            if risk.risk_level in [RiskLevel.CRITICAL, RiskLevel.HIGH]:
                immediate_actions.extend(risk.recommended_actions)

        # é‡è¤‡é™¤å»
        return list(set(immediate_actions))

    def _calculate_failsafe_prevention_score(self, overall_score: float,
                                           detected_risks: List[QualityRisk]) -> float:
        """ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•å›é¿ç¢ºç‡è¨ˆç®—"""

        # ãƒ™ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢ï¼ˆå…¨ä½“å“è³ªã«åŸºã¥ãï¼‰
        base_prevention_score = overall_score

        # ãƒªã‚¹ã‚¯èª¿æ•´
        risk_penalty = 0.0
        for risk in detected_risks:
            if risk.risk_level == RiskLevel.CRITICAL:
                risk_penalty += 0.3
            elif risk.risk_level == RiskLevel.HIGH:
                risk_penalty += 0.2
            elif risk.risk_level == RiskLevel.MEDIUM:
                risk_penalty += 0.1

        prevention_score = base_prevention_score - risk_penalty

        return max(min(prevention_score, 1.0), 0.0)

    def _record_preventive_check(self, result: PreventiveCheckResult) -> None:
        """äºˆé˜²çš„ãƒã‚§ãƒƒã‚¯è¨˜éŒ²"""

        try:
            check_file = self.preventive_dir / f"check_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

            with open(check_file, 'w', encoding='utf-8') as f:
                json.dump(asdict(result), f, indent=2, ensure_ascii=False, default=str)

            # å¤ã„ãƒã‚§ãƒƒã‚¯ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ï¼ˆæœ€æ–°20ä»¶ã®ã¿ä¿æŒï¼‰
            check_files = sorted(self.preventive_dir.glob("check_*.json"))
            if len(check_files) > 20:
                for old_file in check_files[:-20]:
                    old_file.unlink()

            logger.debug(f"âœ… äºˆé˜²çš„ãƒã‚§ãƒƒã‚¯è¨˜éŒ²å®Œäº†: {check_file}")

        except Exception as e:
            logger.warning(f"âš ï¸ äºˆé˜²çš„ãƒã‚§ãƒƒã‚¯è¨˜éŒ²ã‚¨ãƒ©ãƒ¼: {e}")

    def _create_file_not_found_result(self, file_path: str,
                                    start_time: datetime.datetime) -> PreventiveCheckResult:
        """ãƒ•ã‚¡ã‚¤ãƒ«æœªç™ºè¦‹çµæœä½œæˆ"""

        execution_time = (datetime.datetime.now() - start_time).total_seconds()

        return PreventiveCheckResult(
            file_path=file_path,
            overall_quality_score=0.0,
            risk_assessment=RiskLevel.CRITICAL,
            detected_risks=[],
            preventive_recommendations=["ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„"],
            immediate_actions=[],
            failsafe_prevention_score=0.0,
            execution_time=execution_time
        )

    def _create_error_result(self, file_path: str, error_message: str,
                           start_time: datetime.datetime) -> PreventiveCheckResult:
        """ã‚¨ãƒ©ãƒ¼çµæœä½œæˆ"""

        execution_time = (datetime.datetime.now() - start_time).total_seconds()

        return PreventiveCheckResult(
            file_path=file_path,
            overall_quality_score=0.0,
            risk_assessment=RiskLevel.CRITICAL,
            detected_risks=[],
            preventive_recommendations=[f"ã‚¨ãƒ©ãƒ¼å¯¾å¿œ: {error_message}"],
            immediate_actions=[],
            failsafe_prevention_score=0.0,
            execution_time=execution_time
        )

    def _calculate_baseline_failsafe_rate(self) -> float:
        """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ä½¿ç”¨ç‡è¨ˆç®—"""

        try:
            failsafe_file = self.monitoring_dir / "failsafe_usage.json"

            if not failsafe_file.exists():
                return 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤

            with open(failsafe_file, 'r', encoding='utf-8') as f:
                failsafe_data = json.load(f)

            if not failsafe_data:
                return 0.5

            # æœ€è¿‘ã®ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ä½¿ç”¨ç‡è¨ˆç®—
            recent_data = failsafe_data[-10:]  # æœ€æ–°10ä»¶

            total_failsafe = sum(entry["failsafe_count"] for entry in recent_data)
            total_files = sum(entry["total_files"] for entry in recent_data)

            if total_files == 0:
                return 0.5

            return total_failsafe / total_files

        except Exception as e:
            logger.warning(f"âš ï¸ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return 0.5

    def _load_quality_thresholds(self) -> Dict[str, float]:
        """å“è³ªé–¾å€¤èª­ã¿è¾¼ã¿"""

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå“è³ªé–¾å€¤
        return {
            "syntax_quality": 0.95,      # æ§‹æ–‡å“è³ªï¼š95%ä»¥ä¸Š
            "type_safety": 0.80,         # å‹å®‰å…¨æ€§ï¼š80%ä»¥ä¸Š
            "code_complexity": 0.70,     # è¤‡é›‘åº¦ï¼š70%ä»¥ä¸Šï¼ˆä½è¤‡é›‘åº¦ï¼‰
            "test_coverage": 0.70,       # ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ï¼š70%ä»¥ä¸Š
            "documentation": 0.60,       # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼š60%ä»¥ä¸Š
            "security": 0.90,           # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ï¼š90%ä»¥ä¸Š
            "performance": 0.80,        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ï¼š80%ä»¥ä¸Š
            "maintainability": 0.75     # ä¿å®ˆæ€§ï¼š75%ä»¥ä¸Š
        }

    def _load_action_history(self) -> List[Dict[str, Any]]:
        """ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å±¥æ­´èª­ã¿è¾¼ã¿"""

        if not self.action_history_file.exists():
            return []

        try:
            with open(self.action_history_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.warning(f"âš ï¸ ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å±¥æ­´èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def execute_preventive_actions(self, actions: List[PreventiveAction],
                                 file_path: str, code_content: str) -> Dict[str, Any]:
        """äºˆé˜²çš„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ

        Args:
            actions: å®Ÿè¡Œã™ã‚‹ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒªã‚¹ãƒˆ
            file_path: å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            code_content: ã‚³ãƒ¼ãƒ‰å†…å®¹

        Returns:
            Dict[str, Any]: å®Ÿè¡Œçµæœ
        """

        logger.info(f"ğŸ”§ äºˆé˜²çš„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œé–‹å§‹: {len(actions)}ä»¶")

        execution_results = {
            "actions_executed": [],
            "actions_failed": [],
            "improvements": {},
            "total_execution_time": 0.0,
            "failsafe_reduction_estimate": 0.0
        }

        start_time = datetime.datetime.now()

        for action in actions:
            try:
                result = self._execute_single_action(action, file_path, code_content)

                if result["success"]:
                    execution_results["actions_executed"].append({
                        "action": action.value,
                        "result": result
                    })

                    # æ”¹å–„åº¦è¿½åŠ 
                    if "improvement" in result:
                        execution_results["improvements"][action.value] = result["improvement"]

                else:
                    execution_results["actions_failed"].append({
                        "action": action.value,
                        "error": result.get("error", "Unknown error")
                    })

            except Exception as e:
                logger.error(f"âŒ ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {action.value} - {e}")
                execution_results["actions_failed"].append({
                    "action": action.value,
                    "error": str(e)
                })

        execution_time = (datetime.datetime.now() - start_time).total_seconds()
        execution_results["total_execution_time"] = execution_time

        # ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•å‰Šæ¸›åŠ¹æœæ¨å®š
        if execution_results["actions_executed"]:
            reduction_estimate = len(execution_results["actions_executed"]) * 0.05  # 1ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚ãŸã‚Š5%å‰Šæ¸›æ¨å®š
            execution_results["failsafe_reduction_estimate"] = min(reduction_estimate, 0.3)  # æœ€å¤§30%

        # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å±¥æ­´è¨˜éŒ²
        self._record_action_execution(execution_results)

        logger.info(f"âœ… äºˆé˜²çš„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œå®Œäº†: {len(execution_results['actions_executed'])}ä»¶æˆåŠŸ")

        return execution_results

    def _execute_single_action(self, action: PreventiveAction,
                             file_path: str, code_content: str) -> Dict[str, Any]:
        """å˜ä¸€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ"""

        if action == PreventiveAction.SYNTAX_PRECHECK:
            return self._execute_syntax_precheck(file_path, code_content)
        elif action == PreventiveAction.TYPE_VALIDATION:
            return self._execute_type_validation(file_path, code_content)
        elif action == PreventiveAction.COMPLEXITY_REDUCTION:
            return self._execute_complexity_reduction(file_path, code_content)
        elif action == PreventiveAction.TEST_GENERATION:
            return self._execute_test_generation(file_path)
        elif action == PreventiveAction.DOCUMENTATION_UPDATE:
            return self._execute_documentation_update(file_path, code_content)
        elif action == PreventiveAction.SECURITY_SCAN:
            return self._execute_security_scan(file_path, code_content)
        elif action == PreventiveAction.PERFORMANCE_OPTIMIZATION:
            return self._execute_performance_optimization(file_path, code_content)
        elif action == PreventiveAction.REFACTORING_SUGGESTION:
            return self._execute_refactoring_suggestion(file_path, code_content)
        else:
            return {"success": False, "error": f"æœªå¯¾å¿œã‚¢ã‚¯ã‚·ãƒ§ãƒ³: {action.value}"}

    def _execute_syntax_precheck(self, file_path: str, code_content: str) -> Dict[str, Any]:
        """æ§‹æ–‡äº‹å‰ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ"""

        try:
            # åŸºæœ¬æ§‹æ–‡ãƒã‚§ãƒƒã‚¯
            compile(code_content, file_path, 'exec')

            return {
                "success": True,
                "message": "æ§‹æ–‡ãƒã‚§ãƒƒã‚¯å®Œäº†",
                "improvement": 0.1  # æ§‹æ–‡å“è³ª10%å‘ä¸Š
            }

        except SyntaxError as e:
            return {
                "success": False,
                "error": f"æ§‹æ–‡ã‚¨ãƒ©ãƒ¼: {e.msg} (è¡Œ{e.lineno})",
                "suggestion": "æ§‹æ–‡ã‚¨ãƒ©ãƒ¼ã‚’ä¿®æ­£ã—ã¦ãã ã•ã„"
            }

    def _execute_type_validation(self, file_path: str, code_content: str) -> Dict[str, Any]:
        """å‹æ¤œè¨¼å®Ÿè¡Œ"""

        try:
            # ç°¡æ˜“å‹ãƒã‚§ãƒƒã‚¯ï¼ˆmypyã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼‰
            import ast
            tree = ast.parse(code_content)

            issues = []
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef) and node.returns is None:
                    issues.append(f"é–¢æ•° '{node.name}' ã«æˆ»ã‚Šå€¤å‹æ³¨é‡ˆãŒã‚ã‚Šã¾ã›ã‚“")

            return {
                "success": True,
                "message": f"å‹æ¤œè¨¼å®Œäº†: {len(issues)}ä»¶ã®æ”¹å–„ææ¡ˆ",
                "issues": issues,
                "improvement": 0.15  # å‹å®‰å…¨æ€§15%å‘ä¸Š
            }

        except Exception as e:
            return {
                "success": False,
                "error": f"å‹æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {str(e)}"
            }

    def _execute_complexity_reduction(self, file_path: str, code_content: str) -> Dict[str, Any]:
        """è¤‡é›‘åº¦å‰Šæ¸›å®Ÿè¡Œ"""

        try:
            import ast
            tree = ast.parse(code_content)

            complex_functions = []
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    complexity = self._calculate_cyclomatic_complexity(node)
                    if complexity > 10:
                        complex_functions.append(f"é–¢æ•° '{node.name}': è¤‡é›‘åº¦{complexity}")

            return {
                "success": True,
                "message": f"è¤‡é›‘åº¦åˆ†æå®Œäº†: {len(complex_functions)}ä»¶ã®é«˜è¤‡é›‘åº¦é–¢æ•°æ¤œå‡º",
                "complex_functions": complex_functions,
                "improvement": 0.1  # è¤‡é›‘åº¦10%æ”¹å–„
            }

        except Exception as e:
            return {
                "success": False,
                "error": f"è¤‡é›‘åº¦åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}"
            }

    def _execute_test_generation(self, file_path: str) -> Dict[str, Any]:
        """ãƒ†ã‚¹ãƒˆç”Ÿæˆå®Ÿè¡Œ"""

        try:
            # ç°¡æ˜“ãƒ†ã‚¹ãƒˆç”Ÿæˆï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯çµ±åˆãƒ†ã‚¹ãƒˆã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨ï¼‰
            return {
                "success": True,
                "message": "ãƒ†ã‚¹ãƒˆç”Ÿæˆæ¨å¥¨ã‚’è¨˜éŒ²",
                "improvement": 0.2  # ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸20%å‘ä¸Š
            }

        except Exception as e:
            return {
                "success": False,
                "error": f"ãƒ†ã‚¹ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}"
            }

    def _execute_documentation_update(self, file_path: str, code_content: str) -> Dict[str, Any]:
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ›´æ–°å®Ÿè¡Œ"""

        try:
            import ast
            tree = ast.parse(code_content)

            undocumented = []
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef) and not ast.get_docstring(node):
                    undocumented.append(f"é–¢æ•° '{node.name}'")

            return {
                "success": True,
                "message": f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåˆ†æå®Œäº†: {len(undocumented)}ä»¶ã®æœªæ–‡æ›¸åŒ–é–¢æ•°",
                "undocumented": undocumented,
                "improvement": 0.1  # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ10%å‘ä¸Š
            }

        except Exception as e:
            return {
                "success": False,
                "error": f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}"
            }

    def _execute_security_scan(self, file_path: str, code_content: str) -> Dict[str, Any]:
        """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¹ã‚­ãƒ£ãƒ³å®Ÿè¡Œ"""

        try:
            security_issues = []

            # åŸºæœ¬çš„ãªã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯
            if 'eval(' in code_content:
                security_issues.append("eval()ã®ä½¿ç”¨ã‚’æ¤œå‡º")
            if 'exec(' in code_content:
                security_issues.append("exec()ã®ä½¿ç”¨ã‚’æ¤œå‡º")

            return {
                "success": True,
                "message": f"ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¹ã‚­ãƒ£ãƒ³å®Œäº†: {len(security_issues)}ä»¶ã®å•é¡Œ",
                "security_issues": security_issues,
                "improvement": 0.05  # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£5%å‘ä¸Š
            }

        except Exception as e:
            return {
                "success": False,
                "error": f"ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¹ã‚­ãƒ£ãƒ³ã‚¨ãƒ©ãƒ¼: {str(e)}"
            }

    def _execute_performance_optimization(self, file_path: str, code_content: str) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–å®Ÿè¡Œ"""

        try:
            perf_suggestions = []

            # åŸºæœ¬çš„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯
            if 'for.*in.*range(len(' in code_content:
                perf_suggestions.append("éåŠ¹ç‡ãªãƒ«ãƒ¼ãƒ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º")

            return {
                "success": True,
                "message": f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æå®Œäº†: {len(perf_suggestions)}ä»¶ã®ææ¡ˆ",
                "suggestions": perf_suggestions,
                "improvement": 0.05  # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹5%å‘ä¸Š
            }

        except Exception as e:
            return {
                "success": False,
                "error": f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}"
            }

    def _execute_refactoring_suggestion(self, file_path: str, code_content: str) -> Dict[str, Any]:
        """ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ææ¡ˆå®Ÿè¡Œ"""

        try:
            refactoring_suggestions = []

            lines = code_content.split('\n')
            if len(lines) > 300:
                refactoring_suggestions.append("ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãã„ãŸã‚åˆ†å‰²ã‚’æ¤œè¨")

            return {
                "success": True,
                "message": f"ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°åˆ†æå®Œäº†: {len(refactoring_suggestions)}ä»¶ã®ææ¡ˆ",
                "suggestions": refactoring_suggestions,
                "improvement": 0.1  # ä¿å®ˆæ€§10%å‘ä¸Š
            }

        except Exception as e:
            return {
                "success": False,
                "error": f"ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°åˆ†æã‚¨ãƒ©ãƒ¼: {str(e)}"
            }

    def _record_action_execution(self, execution_results: Dict[str, Any]) -> None:
        """ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œè¨˜éŒ²"""

        try:
            record = {
                "timestamp": datetime.datetime.now().isoformat(),
                "execution_results": execution_results
            }

            self.action_history.append(record)

            # æœ€æ–°50ä»¶ã®ã¿ä¿æŒ
            if len(self.action_history) > 50:
                self.action_history = self.action_history[-50:]

            # ä¿å­˜
            with open(self.action_history_file, 'w', encoding='utf-8') as f:
                json.dump(self.action_history, f, indent=2, ensure_ascii=False)

            logger.debug("âœ… ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œè¨˜éŒ²å®Œäº†")

        except Exception as e:
            logger.warning(f"âš ï¸ ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œè¨˜éŒ²ã‚¨ãƒ©ãƒ¼: {e}")

    def generate_failsafe_reduction_report(self) -> FailsafeReductionReport:
        """ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•å‰Šæ¸›ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""

        logger.info("ğŸ“Š ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•å‰Šæ¸›ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆé–‹å§‹")

        try:
            # ç¾åœ¨ã®ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ä½¿ç”¨ç‡å–å¾—
            current_rate = self._calculate_current_failsafe_rate()

            # å‰Šæ¸›ç‡è¨ˆç®—
            reduction_percentage = max(0, (self.current_baseline - current_rate) / self.current_baseline * 100)

            # å“è³ªæ”¹å–„åº¦è¨ˆç®—
            quality_improvements = self._calculate_quality_improvements()

            # å®Ÿè¡Œã•ã‚ŒãŸäºˆé˜²çš„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
            preventive_actions = self._get_recent_preventive_actions()

            # æˆåŠŸãƒ¡ãƒˆãƒªã‚¯ã‚¹
            success_metrics = self._calculate_success_metrics(current_rate)

            # æ¨å¥¨äº‹é …
            recommendations = self._generate_reduction_recommendations(current_rate)

            report = FailsafeReductionReport(
                timestamp=datetime.datetime.now().isoformat(),
                baseline_failsafe_rate=self.current_baseline,
                current_failsafe_rate=current_rate,
                reduction_percentage=reduction_percentage,
                quality_improvements=quality_improvements,
                preventive_actions_taken=preventive_actions,
                success_metrics=success_metrics,
                recommendations=recommendations
            )

            # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
            self._save_reduction_report(report)

            logger.info(f"âœ… ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•å‰Šæ¸›ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {reduction_percentage:.1f}%å‰Šæ¸›")

            return report

        except Exception as e:
            logger.error(f"âŒ ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•å‰Šæ¸›ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return self._create_fallback_reduction_report()

    def _calculate_current_failsafe_rate(self) -> float:
        """ç¾åœ¨ã®ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ä½¿ç”¨ç‡è¨ˆç®—"""

        try:
            failsafe_file = self.monitoring_dir / "failsafe_usage.json"

            if not failsafe_file.exists():
                return self.current_baseline

            with open(failsafe_file, 'r', encoding='utf-8') as f:
                failsafe_data = json.load(f)

            if not failsafe_data:
                return self.current_baseline

            # æœ€æ–°ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
            recent_data = failsafe_data[-5:]  # æœ€æ–°5ä»¶

            total_failsafe = sum(entry["failsafe_count"] for entry in recent_data)
            total_files = sum(entry["total_files"] for entry in recent_data)

            if total_files == 0:
                return self.current_baseline

            return total_failsafe / total_files

        except Exception as e:
            logger.warning(f"âš ï¸ ç¾åœ¨ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ç‡è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return self.current_baseline

    def _calculate_quality_improvements(self) -> Dict[str, float]:
        """å“è³ªæ”¹å–„åº¦è¨ˆç®—"""

        improvements = {}

        # äºˆé˜²çš„ãƒã‚§ãƒƒã‚¯å±¥æ­´ã‹ã‚‰æ”¹å–„åº¦è¨ˆç®—
        check_files = list(self.preventive_dir.glob("check_*.json"))

        if len(check_files) >= 2:
            try:
                # æœ€æ–°ã¨éå»ã®ãƒã‚§ãƒƒã‚¯çµæœæ¯”è¼ƒ
                latest_files = sorted(check_files)[-5:]  # æœ€æ–°5ä»¶
                older_files = sorted(check_files)[-10:-5] if len(check_files) >= 10 else []

                latest_scores = []
                older_scores = []

                for file in latest_files:
                    with open(file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        latest_scores.append(data.get("overall_quality_score", 0.0))

                for file in older_files:
                    with open(file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        older_scores.append(data.get("overall_quality_score", 0.0))

                if latest_scores and older_scores:
                    latest_avg = statistics.mean(latest_scores)
                    older_avg = statistics.mean(older_scores)
                    improvements["overall_quality"] = latest_avg - older_avg

            except Exception as e:
                logger.warning(f"âš ï¸ å“è³ªæ”¹å–„åº¦è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ”¹å–„åº¦ï¼ˆæ¨å®šï¼‰
        if not improvements:
            improvements = {
                "overall_quality": 0.05,  # 5%æ”¹å–„æ¨å®š
                "syntax_quality": 0.08,   # 8%æ”¹å–„æ¨å®š
                "type_safety": 0.06      # 6%æ”¹å–„æ¨å®š
            }

        return improvements

    def _get_recent_preventive_actions(self) -> List[str]:
        """æœ€è¿‘ã®äºˆé˜²çš„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å–å¾—"""

        actions = []

        if self.action_history:
            # æœ€æ–°ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å±¥æ­´ã‹ã‚‰æŠ½å‡º
            recent_records = self.action_history[-10:]  # æœ€æ–°10ä»¶

            for record in recent_records:
                execution_results = record.get("execution_results", {})
                executed_actions = execution_results.get("actions_executed", [])

                for action_info in executed_actions:
                    action_name = action_info.get("action", "unknown")
                    actions.append(action_name)

        # é‡è¤‡é™¤å»ã¨çµ±è¨ˆ
        unique_actions = list(set(actions))
        return unique_actions[:10]  # æœ€å¤§10ä»¶

    def _calculate_success_metrics(self, current_rate: float) -> Dict[str, float]:
        """æˆåŠŸãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—"""

        return {
            "failsafe_reduction_rate": max(0, (self.current_baseline - current_rate) / self.current_baseline),
            "target_achievement_rate": min(1.0, (self.current_baseline - current_rate) / (self.current_baseline - self.target_failsafe_rate)),
            "quality_improvement_score": 0.1,  # å“è³ªæ”¹å–„ã‚¹ã‚³ã‚¢ï¼ˆæ¨å®šï¼‰
            "prevention_effectiveness": 0.8   # äºˆé˜²åŠ¹æœï¼ˆæ¨å®šï¼‰
        }

    def _generate_reduction_recommendations(self, current_rate: float) -> List[str]:
        """å‰Šæ¸›æ¨å¥¨äº‹é …ç”Ÿæˆ"""

        recommendations = []

        if current_rate <= self.target_failsafe_rate:
            recommendations.append("âœ… ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ä½¿ç”¨ç‡ãŒç›®æ¨™å€¤ã‚’é”æˆã—ã¦ã„ã¾ã™")
            recommendations.append("ğŸ”„ ç¾åœ¨ã®äºˆé˜²çš„å“è³ªãƒã‚§ãƒƒã‚¯ä½“åˆ¶ã‚’ç¶­æŒã—ã¦ãã ã•ã„")
        else:
            gap = current_rate - self.target_failsafe_rate
            recommendations.append(f"ğŸ“ˆ ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ä½¿ç”¨ç‡ã‚’ã‚ã¨{gap:.1%}å‰Šæ¸›ãŒå¿…è¦ã§ã™")

            if gap > 0.2:
                recommendations.append("ğŸš¨ å¤§å¹…ãªæ”¹å–„ãŒå¿…è¦: äºˆé˜²çš„ãƒã‚§ãƒƒã‚¯å¼·åŒ–ã‚’æ¨å¥¨")
            elif gap > 0.1:
                recommendations.append("âš ï¸ ä¸­ç¨‹åº¦ã®æ”¹å–„ãŒå¿…è¦: ç‰¹å®šé ˜åŸŸã®å“è³ªå‘ä¸Š")
            else:
                recommendations.append("ğŸ¯ è»½å¾®ãªèª¿æ•´ã§ç›®æ¨™é”æˆå¯èƒ½")

        # å…·ä½“çš„ãªæ¨å¥¨
        recommendations.append("ğŸ“ æ§‹æ–‡å“è³ªã®äº‹å‰ãƒã‚§ãƒƒã‚¯å¼·åŒ–")
        recommendations.append("ğŸ·ï¸ å‹æ³¨é‡ˆã‚«ãƒãƒ¬ãƒƒã‚¸ã®å‘ä¸Š")
        recommendations.append("ğŸ”§ è‡ªå‹•ä¿®æ­£æ©Ÿèƒ½ã®æ´»ç”¨ä¿ƒé€²")
        recommendations.append("ğŸ§ª çµ±åˆãƒ†ã‚¹ãƒˆã‚·ã‚¹ãƒ†ãƒ ã¨ã®é€£æºå¼·åŒ–")

        return recommendations

    def _save_reduction_report(self, report: FailsafeReductionReport) -> None:
        """å‰Šæ¸›ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜"""

        try:
            report_file = self.preventive_dir / f"reduction_report_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(asdict(report), f, indent=2, ensure_ascii=False)

            logger.info(f"âœ… ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•å‰Šæ¸›ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜å®Œäº†: {report_file}")

        except Exception as e:
            logger.error(f"âŒ å‰Šæ¸›ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    def _create_fallback_reduction_report(self) -> FailsafeReductionReport:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‰Šæ¸›ãƒ¬ãƒãƒ¼ãƒˆ"""

        return FailsafeReductionReport(
            timestamp=datetime.datetime.now().isoformat(),
            baseline_failsafe_rate=self.current_baseline,
            current_failsafe_rate=self.current_baseline,
            reduction_percentage=0.0,
            quality_improvements={},
            preventive_actions_taken=[],
            success_metrics={},
            recommendations=["ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã«ã‚ˆã‚Šå‰Šæ¸›åŠ¹æœæ¸¬å®šãŒåˆ¶é™ã•ã‚Œã¾ã—ãŸ"]
        )


def main() -> None:
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    checker = PreventiveQualityChecker()

    # ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ã§äºˆé˜²çš„ãƒã‚§ãƒƒã‚¯ãƒ†ã‚¹ãƒˆ
    test_code = '''
def calculate_result(x, y):
    if x > 0:
        result = x + y
        return result
    else:
        return 0

class DataProcessor:
    def __init__(self):
        self.data = []

    def process(self, items):
        for item in items:
            self.data.append(item.upper())
'''

    # äºˆé˜²çš„ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ
    result = checker.run_preventive_check("test.py", test_code)
    print(f"å“è³ªã‚¹ã‚³ã‚¢: {result.overall_quality_score:.3f}")
    print(f"ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«: {result.risk_assessment.value}")
    print(f"ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•å›é¿ç‡: {result.failsafe_prevention_score:.3f}")

    # äºˆé˜²çš„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
    if result.immediate_actions:
        execution_result = checker.execute_preventive_actions(
            result.immediate_actions, "test.py", test_code
        )
        print(f"å®Ÿè¡Œã•ã‚ŒãŸã‚¢ã‚¯ã‚·ãƒ§ãƒ³: {len(execution_result['actions_executed'])}ä»¶")

    # å‰Šæ¸›ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report = checker.generate_failsafe_reduction_report()
    print(f"ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•å‰Šæ¸›ç‡: {report.reduction_percentage:.1f}%")


if __name__ == "__main__":
    main()
