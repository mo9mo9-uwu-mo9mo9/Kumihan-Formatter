#!/usr/bin/env python3
"""
Áµ±ÂêàÂìÅË≥™ÁÆ°ÁêÜ„Ç∑„Çπ„ÉÜ„É†
Claude ‚Üî GeminiÂçîÊ•≠„Åß„ÅÆÂìÅË≥™‰øùË®ºÁµ±‰∏ÄÁÆ°ÁêÜ
"""

import os
import json
import datetime
import subprocess
from typing import Dict, List, Any, Optional, Tuple, Union
from pathlib import Path
from dataclasses import dataclass
from enum import Enum

# Áµ±Âêà„ÉÜ„Çπ„Éà„Ç∑„Çπ„ÉÜ„É†Áµ±Âêà (Issue #859)
try:
    from .integration_test_system import IntegrationTestSystem, IntegrationTestResult
    from .test_generator import TestGeneratorEngine, TestSuite, GenerationStrategy
    INTEGRATION_TEST_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è Áµ±Âêà„ÉÜ„Çπ„Éà„Ç∑„Çπ„ÉÜ„É†„ÅÆ„Ç§„É≥„Éù„Éº„Éà„Å´Â§±Êïó„Åó„Åæ„Åó„Åü")
    INTEGRATION_TEST_AVAILABLE = False

class QualityLevel(Enum):
    """ÂìÅË≥™„É¨„Éô„É´ÂÆöÁæ©"""
    EXCELLENT = "excellent"     # ÂÑ™ÁßÄÔºà95%‰ª•‰∏äÔºâ
    GOOD = "good"              # ËâØÂ•ΩÔºà80-94%Ôºâ
    ACCEPTABLE = "acceptable"   # Ë®±ÂÆπÔºà60-79%Ôºâ
    POOR = "poor"              # ‰∏çËâØÔºà40-59%Ôºâ
    CRITICAL = "critical"       # ÈáçÂ§ßÔºà40%Êú™Ê∫ÄÔºâ

class CheckType(Enum):
    """„ÉÅ„Çß„ÉÉ„ÇØÁ®ÆÂà•"""
    SYNTAX = "syntax"           # ÊßãÊñá„ÉÅ„Çß„ÉÉ„ÇØ
    TYPE_CHECK = "type_check"   # Âûã„ÉÅ„Çß„ÉÉ„ÇØ
    LINT = "lint"              # „É™„É≥„Éà„ÉÅ„Çß„ÉÉ„ÇØ
    FORMAT = "format"          # „Éï„Ç©„Éº„Éû„ÉÉ„Éà„ÉÅ„Çß„ÉÉ„ÇØ
    SECURITY = "security"       # „Çª„Ç≠„É•„É™„ÉÜ„Ç£„ÉÅ„Çß„ÉÉ„ÇØ
    PERFORMANCE = "performance" # „Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„ÉÅ„Çß„ÉÉ„ÇØ
    TEST = "test"              # „ÉÜ„Çπ„Éà„ÉÅ„Çß„ÉÉ„ÇØ

@dataclass
class QualityMetrics:
    """ÂìÅË≥™„É°„Éà„É™„ÇØ„Çπ"""
    overall_score: float        # Á∑èÂêà„Çπ„Ç≥„Ç¢ (0.0-1.0)
    syntax_score: float        # ÊßãÊñáÂìÅË≥™
    type_score: float          # ÂûãÂÆâÂÖ®ÊÄß
    lint_score: float          # „Ç≥„Éº„ÉâÂìÅË≥™
    format_score: float        # „Éï„Ç©„Éº„Éû„ÉÉ„ÉàÂìÅË≥™
    security_score: float      # „Çª„Ç≠„É•„É™„ÉÜ„Ç£ÂìÅË≥™
    performance_score: float   # „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÂìÅË≥™
    test_coverage: float       # „ÉÜ„Çπ„Éà„Ç´„Éê„É¨„ÉÉ„Ç∏

    error_count: int           # „Ç®„É©„ÉºÁ∑èÊï∞
    warning_count: int         # Ë≠¶ÂëäÁ∑èÊï∞
    fixed_count: int           # ‰øÆÊ≠£Ê∏à„ÅøÊï∞

    quality_level: QualityLevel
    improvement_suggestions: List[str]

@dataclass
class QualityResult:
    """ÂìÅË≥™„ÉÅ„Çß„ÉÉ„ÇØÁµêÊûú"""
    check_type: CheckType
    passed: bool
    score: float
    errors: List[str]
    warnings: List[str]
    details: Dict[str, Any]
    execution_time: float

class QualityManager:
    """Áµ±ÂêàÂìÅË≥™ÁÆ°ÁêÜ„Ç∑„Çπ„ÉÜ„É†"""

    def __init__(self) -> None:
        self.standards_path = Path("postbox/quality/standards.json")
        self.results_path = Path("postbox/monitoring/quality_results.json")
        self.history_path = Path("postbox/monitoring/quality_history.json")

        # „Éá„Ç£„É¨„ÇØ„Éà„É™‰ΩúÊàê
        for path in [self.standards_path, self.results_path, self.history_path]:
            path.parent.mkdir(parents=True, exist_ok=True)

        self.standards = self._load_standards()
        self.thresholds = self.standards.get("thresholds", {})

        # Áµ±Âêà„ÉÜ„Çπ„Éà„Ç∑„Çπ„ÉÜ„É†ÂàùÊúüÂåñ (Issue #859)
        if INTEGRATION_TEST_AVAILABLE:
            try:
                self.integration_test_system = IntegrationTestSystem()
                self.test_generator = TestGeneratorEngine()
                print("üß™ Áµ±Âêà„ÉÜ„Çπ„Éà„Ç∑„Çπ„ÉÜ„É†Áµ±ÂêàÂÆå‰∫Ü")
            except Exception as e:
                print(f"‚ö†Ô∏è Áµ±Âêà„ÉÜ„Çπ„Éà„Ç∑„Çπ„ÉÜ„É†ÂàùÊúüÂåñ„Ç®„É©„Éº: {e}")
                self.integration_test_system = None
                self.test_generator = None
        else:
            self.integration_test_system = None
            self.test_generator = None

        print("üéØ QualityManager ÂàùÊúüÂåñÂÆå‰∫Ü")

    def run_comprehensive_check(self, target_files: List[str],
                              ai_agent: str = "claude") -> QualityMetrics:
        """ÂåÖÊã¨ÁöÑÂìÅË≥™„ÉÅ„Çß„ÉÉ„ÇØÂÆüË°å"""

        print(f"üîç ÂåÖÊã¨ÁöÑÂìÅË≥™„ÉÅ„Çß„ÉÉ„ÇØÈñãÂßã ({ai_agent}) - ÂØæË±°: {len(target_files)}„Éï„Ç°„Ç§„É´")

        start_time = datetime.datetime.now()
        results = {}

        # ÂêÑÁ®Æ„ÉÅ„Çß„ÉÉ„ÇØÂÆüË°å
        for check_type in CheckType:
            print(f"  üìã {check_type.value} „ÉÅ„Çß„ÉÉ„ÇØÂÆüË°å‰∏≠...")
            result = self._run_quality_check(check_type, target_files)
            results[check_type.value] = result

        # „É°„Éà„É™„ÇØ„ÇπË®àÁÆó
        metrics = self._calculate_metrics(results, target_files)

        # ÁµêÊûú‰øùÂ≠ò
        execution_time = (datetime.datetime.now() - start_time).total_seconds()
        self._save_quality_result(metrics, ai_agent, execution_time, target_files)

        # ÊîπÂñÑÊèêÊ°àÁîüÊàê
        metrics.improvement_suggestions = self._generate_improvements(metrics, results)

        print(f"‚úÖ ÂìÅË≥™„ÉÅ„Çß„ÉÉ„ÇØÂÆå‰∫Ü: {metrics.quality_level.value} („Çπ„Ç≥„Ç¢: {metrics.overall_score:.3f})")

        return metrics

    def _run_quality_check(self, check_type: CheckType, target_files: List[str]) -> QualityResult:
        """ÂÄãÂà•ÂìÅË≥™„ÉÅ„Çß„ÉÉ„ÇØÂÆüË°å"""

        start_time = datetime.datetime.now()

        try:
            if check_type == CheckType.SYNTAX:
                return self._check_syntax(target_files)
            elif check_type == CheckType.TYPE_CHECK:
                return self._check_types(target_files)
            elif check_type == CheckType.LINT:
                return self._check_lint(target_files)
            elif check_type == CheckType.FORMAT:
                return self._check_format(target_files)
            elif check_type == CheckType.SECURITY:
                return self._check_security(target_files)
            elif check_type == CheckType.PERFORMANCE:
                return self._check_performance(target_files)
            elif check_type == CheckType.TEST:
                return self._check_tests(target_files)
            else:
                raise ValueError(f"Êú™ÂØæÂøú„ÅÆ„ÉÅ„Çß„ÉÉ„ÇØÁ®ÆÂà•: {check_type}")

        except Exception as e:
            execution_time = (datetime.datetime.now() - start_time).total_seconds()
            return QualityResult(
                check_type=check_type,
                passed=False,
                score=0.0,
                errors=[f"„ÉÅ„Çß„ÉÉ„ÇØÂÆüË°å„Ç®„É©„Éº: {str(e)}"],
                warnings=[],
                details={"error": str(e)},
                execution_time=execution_time
            )

    def _check_syntax(self, target_files: List[str]) -> QualityResult:
        """ÊßãÊñá„ÉÅ„Çß„ÉÉ„ÇØ"""
        errors: List[str] = []
        warnings: List[str] = []
        total_files = len(target_files)
        passed_files = 0

        for file_path in target_files:
            try:
                result = subprocess.run(
                    ["python3", "-m", "py_compile", file_path],
                    capture_output=True,
                    text=True
                )

                if result.returncode == 0:
                    passed_files += 1
                else:
                    errors.append(f"{file_path}: {result.stderr.strip()}")

            except Exception as e:
                errors.append(f"{file_path}: ÊßãÊñá„ÉÅ„Çß„ÉÉ„ÇØÂÆüË°å„Ç®„É©„Éº - {str(e)}")

        score = passed_files / total_files if total_files > 0 else 1.0

        return QualityResult(
            check_type=CheckType.SYNTAX,
            passed=len(errors) == 0,
            score=score,
            errors=errors,
            warnings=warnings,
            details={"passed_files": passed_files, "total_files": total_files},
            execution_time=0.0
        )

    def _check_types(self, target_files: List[str]) -> QualityResult:
        """Âûã„ÉÅ„Çß„ÉÉ„ÇØ (mypy)"""
        errors = []
        warnings = []

        try:
            # „Åæ„ÅöÂÄãÂà•„Éï„Ç°„Ç§„É´„Çí„ÉÅ„Çß„ÉÉ„ÇØ
            for file_path in target_files:
                result = subprocess.run(
                    ["python3", "-m", "mypy", "--strict", file_path],
                    capture_output=True,
                    text=True
                )

                for line in result.stdout.split('\n'):
                    if 'error:' in line:
                        errors.append(line.strip())
                    elif 'warning:' in line or 'note:' in line:
                        warnings.append(line.strip())

            # „Çπ„Ç≥„Ç¢Ë®àÁÆó („Ç®„É©„ÉºÁéá„Éô„Éº„Çπ)
            total_lines = sum(self._count_file_lines(f) for f in target_files if os.path.exists(f))
            error_rate = len(errors) / max(total_lines, 1) if total_lines > 0 else 1.0
            score = max(0.0, 1.0 - error_rate)

        except Exception as e:
            errors.append(f"mypyÂÆüË°å„Ç®„É©„Éº: {str(e)}")
            score = 0.0

        return QualityResult(
            check_type=CheckType.TYPE_CHECK,
            passed=len(errors) == 0,
            score=score,
            errors=errors,
            warnings=warnings,
            details={"total_lines": total_lines, "error_rate": error_rate},
            execution_time=0.0
        )

    def _check_lint(self, target_files: List[str]) -> QualityResult:
        """„É™„É≥„Éà„ÉÅ„Çß„ÉÉ„ÇØ (flake8)"""
        errors: List[str] = []
        warnings: List[str] = []

        try:
            for file_path in target_files:
                result = subprocess.run(
                    ["python3", "-m", "flake8", file_path],
                    capture_output=True,
                    text=True
                )

                for line in result.stdout.split('\n'):
                    if line.strip():
                        if any(code in line for code in ['E', 'F']):  # Error codes
                            errors.append(line.strip())
                        else:  # Warnings
                            warnings.append(line.strip())

            # „Çπ„Ç≥„Ç¢Ë®àÁÆó
            total_files = len(target_files)
            error_rate = len(errors) / max(total_files, 1)
            score = max(0.0, 1.0 - error_rate)

        except Exception as e:
            errors.append(f"flake8ÂÆüË°å„Ç®„É©„Éº: {str(e)}")
            score = 0.0

        return QualityResult(
            check_type=CheckType.LINT,
            passed=len(errors) == 0,
            score=score,
            errors=errors,
            warnings=warnings,
            details={"error_rate": error_rate},
            execution_time=0.0
        )

    def _check_format(self, target_files: List[str]) -> QualityResult:
        """„Éï„Ç©„Éº„Éû„ÉÉ„Éà„ÉÅ„Çß„ÉÉ„ÇØ (black)"""
        errors: List[str] = []
        warnings: List[str] = []

        try:
            for file_path in target_files:
                result = subprocess.run(
                    ["python3", "-m", "black", "--check", file_path],
                    capture_output=True,
                    text=True
                )

                if result.returncode != 0:
                    errors.append(f"{file_path}: „Éï„Ç©„Éº„Éû„ÉÉ„Éà‰∏çÈÅ©Âêà")

            score = 1.0 - (len(errors) / max(len(target_files), 1))

        except Exception as e:
            errors.append(f"blackÂÆüË°å„Ç®„É©„Éº: {str(e)}")
            score = 0.0

        return QualityResult(
            check_type=CheckType.FORMAT,
            passed=len(errors) == 0,
            score=score,
            errors=errors,
            warnings=warnings,
            details={},
            execution_time=0.0
        )

    def _check_security(self, target_files: List[str]) -> QualityResult:
        """„Çª„Ç≠„É•„É™„ÉÜ„Ç£„ÉÅ„Çß„ÉÉ„ÇØ (Âü∫Êú¨ÁöÑ„Å™„Éë„Çø„Éº„É≥„Éû„ÉÉ„ÉÅ„É≥„Ç∞)"""
        errors: List[str] = []
        warnings: List[str] = []

        # Âç±Èô∫„Å™„Éë„Çø„Éº„É≥
        dangerous_patterns = [
            r"eval\s*\(",
            r"exec\s*\(",
            r"subprocess\.call\s*\(",
            r"os\.system\s*\(",
            r"shell=True",
            r"password\s*=\s*[\"']",
            r"secret\s*=\s*[\"']",
            r"api_key\s*=\s*[\"']"
        ]

        import re

        for file_path in target_files:
            if os.path.exists(file_path):
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()

                    for pattern in dangerous_patterns:
                        matches = re.findall(pattern, content, re.IGNORECASE)
                        if matches:
                            warnings.append(f"{file_path}: ÊΩúÂú®ÁöÑ„Çª„Ç≠„É•„É™„ÉÜ„Ç£„É™„Çπ„ÇØ - {pattern}")

                except Exception as e:
                    errors.append(f"{file_path}: „Çª„Ç≠„É•„É™„ÉÜ„Ç£„ÉÅ„Çß„ÉÉ„ÇØ„Ç®„É©„Éº - {str(e)}")

        # „Çπ„Ç≥„Ç¢Ë®àÁÆó
        total_issues = len(errors) + len(warnings)
        score = max(0.0, 1.0 - (total_issues / max(len(target_files), 1)))

        return QualityResult(
            check_type=CheckType.SECURITY,
            passed=len(errors) == 0,
            score=score,
            errors=errors,
            warnings=warnings,
            details={"total_issues": total_issues},
            execution_time=0.0
        )

    def _check_performance(self, target_files: List[str]) -> QualityResult:
        """„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„ÉÅ„Çß„ÉÉ„ÇØ (Âü∫Êú¨ÁöÑ„Å™ÊúÄÈÅ©Âåñ„Éë„Çø„Éº„É≥)"""
        warnings: List[str] = []
        errors: List[str] = []

        performance_issues = [
            r"for\s+\w+\s+in\s+range\(len\(",  # range(len()) „Éë„Çø„Éº„É≥
            r"\.append\s*\(\s*\)\s*\n.*for",   # ÈùûÂäπÁéá„Å™„É´„Éº„Éó
            r"time\.sleep\s*\(\s*[0-9]+\s*\)"  # Èï∑„ÅÑsleep
        ]

        import re

        for file_path in target_files:
            if os.path.exists(file_path):
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()

                    for pattern in performance_issues:
                        if re.search(pattern, content):
                            warnings.append(f"{file_path}: „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊîπÂñÑÂèØËÉΩ - {pattern}")

                except Exception as e:
                    errors.append(f"{file_path}: „Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„ÉÅ„Çß„ÉÉ„ÇØ„Ç®„É©„Éº - {str(e)}")

        score = max(0.0, 1.0 - (len(warnings) / max(len(target_files), 1)))

        return QualityResult(
            check_type=CheckType.PERFORMANCE,
            passed=len(errors) == 0,
            score=score,
            errors=errors,
            warnings=warnings,
            details={},
            execution_time=0.0
        )

    def _check_tests(self, target_files: List[str]) -> QualityResult:
        """„ÉÜ„Çπ„Éà„ÉÅ„Çß„ÉÉ„ÇØ"""
        errors: List[str] = []
        warnings: List[str] = []

        # „ÉÜ„Çπ„Éà„Éï„Ç°„Ç§„É´Â≠òÂú®„ÉÅ„Çß„ÉÉ„ÇØ
        test_files = []
        source_files = []

        for file_path in target_files:
            if 'test' in file_path.lower():
                test_files.append(file_path)
            else:
                source_files.append(file_path)

        # „ÉÜ„Çπ„Éà„Ç´„Éê„É¨„ÉÉ„Ç∏Ë®àÁÆó
        coverage_ratio = len(test_files) / max(len(source_files), 1) if source_files else 1.0

        if coverage_ratio < 0.3:
            warnings.append(f"„ÉÜ„Çπ„Éà„Ç´„Éê„É¨„ÉÉ„Ç∏‰∏çË∂≥: {coverage_ratio:.1%}")

        score = min(1.0, coverage_ratio)

        return QualityResult(
            check_type=CheckType.TEST,
            passed=coverage_ratio >= 0.5,
            score=score,
            errors=errors,
            warnings=warnings,
            details={"coverage_ratio": coverage_ratio, "test_files": len(test_files)},
            execution_time=0.0
        )

    def _calculate_metrics(self, results: Dict[str, QualityResult],
                          target_files: List[str]) -> QualityMetrics:
        """ÂìÅË≥™„É°„Éà„É™„ÇØ„ÇπË®àÁÆó"""

        # ÂêÑÁ®Æ„Çπ„Ç≥„Ç¢ÈõÜË®à
        scores = {}
        total_errors = 0
        total_warnings = 0

        for check_type, result in results.items():
            scores[check_type] = result.score
            total_errors += len(result.errors)
            total_warnings += len(result.warnings)

        # Á∑èÂêà„Çπ„Ç≥„Ç¢Ë®àÁÆó (Èáç„Åø‰ªò„ÅçÂπ≥Âùá)
        weights = {
            "syntax": 0.2,
            "type_check": 0.25,
            "lint": 0.2,
            "format": 0.1,
            "security": 0.15,
            "performance": 0.05,
            "test": 0.05
        }

        overall_score = sum(scores.get(k, 0) * w for k, w in weights.items())

        # ÂìÅË≥™„É¨„Éô„É´Ê±∫ÂÆö
        if overall_score >= 0.95:
            quality_level = QualityLevel.EXCELLENT
        elif overall_score >= 0.80:
            quality_level = QualityLevel.GOOD
        elif overall_score >= 0.60:
            quality_level = QualityLevel.ACCEPTABLE
        elif overall_score >= 0.40:
            quality_level = QualityLevel.POOR
        else:
            quality_level = QualityLevel.CRITICAL

        return QualityMetrics(
            overall_score=overall_score,
            syntax_score=scores.get("syntax", 0),
            type_score=scores.get("type_check", 0),
            lint_score=scores.get("lint", 0),
            format_score=scores.get("format", 0),
            security_score=scores.get("security", 0),
            performance_score=scores.get("performance", 0),
            test_coverage=scores.get("test", 0),
            error_count=total_errors,
            warning_count=total_warnings,
            fixed_count=0,  # ‰øÆÊ≠£„Ç∑„Çπ„ÉÜ„É†„Å®ÈÄ£Êê∫ÊôÇ„Å´Êõ¥Êñ∞
            quality_level=quality_level,
            improvement_suggestions=[]
        )

    def _generate_improvements(self, metrics: QualityMetrics,
                             results: Dict[str, QualityResult]) -> List[str]:
        """ÊîπÂñÑÊèêÊ°àÁîüÊàê"""

        suggestions = []

        # „Çπ„Ç≥„Ç¢Âà•ÊîπÂñÑÊèêÊ°à
        if metrics.syntax_score < 0.8:
            suggestions.append("üîß ÊßãÊñá„Ç®„É©„Éº„ÅÆ‰øÆÊ≠£„ÅåÂøÖË¶Å„Åß„Åô")

        if metrics.type_score < 0.7:
            suggestions.append("üìù ÂûãÊ≥®Èáà„ÅÆËøΩÂä†„Éª‰øÆÊ≠£„ÇíÊé®Â•®„Åó„Åæ„Åô")

        if metrics.lint_score < 0.8:
            suggestions.append("üéØ „Ç≥„Éº„ÉâÂìÅË≥™„ÅÆÊîπÂñÑ„ÅåÂøÖË¶Å„Åß„Åô (flake8)")

        if metrics.format_score < 0.9:
            suggestions.append("üíÖ „Ç≥„Éº„Éâ„Éï„Ç©„Éº„Éû„ÉÉ„Éà„ÅÆÁµ±‰∏Ä„ÅåÂøÖË¶Å„Åß„Åô (black)")

        if metrics.security_score < 0.9:
            suggestions.append("üîí „Çª„Ç≠„É•„É™„ÉÜ„Ç£„É™„Çπ„ÇØ„ÅÆÁ¢∫Ë™ç„Éª‰øÆÊ≠£„ÇíÊé®Â•®„Åó„Åæ„Åô")

        if metrics.performance_score < 0.8:
            suggestions.append("‚ö° „Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊúÄÈÅ©Âåñ„ÇíÊ§úË®é„Åó„Å¶„Åè„Å†„Åï„ÅÑ")

        if metrics.test_coverage < 0.5:
            suggestions.append("üß™ „ÉÜ„Çπ„Éà„Ç´„Éê„É¨„ÉÉ„Ç∏„ÅÆÂêë‰∏ä„ÅåÂøÖË¶Å„Åß„Åô")

        # Á∑èÂêàÁöÑ„Å™ÊèêÊ°à
        if metrics.overall_score < 0.6:
            suggestions.append("üö® Á∑èÂêàÂìÅË≥™„ÅåÂü∫Ê∫ñ„Çí‰∏ãÂõû„Å£„Å¶„ÅÑ„Åæ„Åô„ÄÇÂÑ™ÂÖàÁöÑ„Å™ÊîπÂñÑ„ÅåÂøÖË¶Å„Åß„Åô")

        return suggestions

    def _save_quality_result(self, metrics: QualityMetrics, ai_agent: str,
                           execution_time: float, target_files: List[str]) -> None:
        """ÂìÅË≥™ÁµêÊûú‰øùÂ≠ò"""

        result_entry = {
            "timestamp": datetime.datetime.now().isoformat(),
            "ai_agent": ai_agent,
            "execution_time": execution_time,
            "target_files": target_files,
            "metrics": {
                "overall_score": metrics.overall_score,
                "quality_level": metrics.quality_level.value,
                "error_count": metrics.error_count,
                "warning_count": metrics.warning_count,
                "scores": {
                    "syntax": metrics.syntax_score,
                    "type_check": metrics.type_score,
                    "lint": metrics.lint_score,
                    "format": metrics.format_score,
                    "security": metrics.security_score,
                    "performance": metrics.performance_score,
                    "test": metrics.test_coverage
                }
            }
        }

        # Â±•Ê≠¥Êõ¥Êñ∞
        history = []
        if self.history_path.exists():
            try:
                with open(self.history_path, 'r', encoding='utf-8') as f:
                    history = json.load(f)
            except:
                history = []

        history.append(result_entry)

        # Â±•Ê≠¥„Çµ„Ç§„Ç∫Âà∂Èôê
        if len(history) > 50:
            history = history[-50:]

        with open(self.history_path, 'w', encoding='utf-8') as f:
            json.dump(history, f, indent=2, ensure_ascii=False)

    def _count_file_lines(self, file_path: str) -> int:
        """„Éï„Ç°„Ç§„É´Ë°åÊï∞„Ç´„Ç¶„É≥„Éà"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return len(f.readlines())
        except:
            return 0

    def _load_standards(self) -> Dict[str, Any]:
        """ÂìÅË≥™Âü∫Ê∫ñË™≠„ÅøËæº„Åø"""

        default_standards = {
            "thresholds": {
                "minimum_overall_score": 0.7,
                "minimum_type_score": 0.8,
                "minimum_lint_score": 0.8,
                "maximum_error_count": 10,
                "minimum_test_coverage": 0.5
            },
            "weights": {
                "syntax": 0.2,
                "type_check": 0.25,
                "lint": 0.2,
                "format": 0.1,
                "security": 0.15,
                "performance": 0.05,
                "test": 0.05
            },
            "automation": {
                "auto_fix_format": True,
                "auto_fix_simple_types": True,
                "auto_fix_imports": True
            }
        }

        if self.standards_path.exists():
            try:
                with open(self.standards_path, 'r', encoding='utf-8') as f:
                    user_standards = json.load(f)
                default_standards.update(user_standards)
            except:
                pass
        else:
            # „Éá„Éï„Ç©„É´„ÉàË®≠ÂÆö‰øùÂ≠ò
            with open(self.standards_path, 'w', encoding='utf-8') as f:
                json.dump(default_standards, f, indent=2, ensure_ascii=False)

        return default_standards

    def comprehensive_quality_check(self, target_files: List[str],
                                   ai_agent: str = "claude",
                                   context: Optional[Dict[str, Any]] = None) -> QualityMetrics:
        """ÂåÖÊã¨ÁöÑÂìÅË≥™„ÉÅ„Çß„ÉÉ„ÇØÔºà3Â±§Ê§úË®º‰ΩìÂà∂ÂØæÂøúÁâàÔºâ

        run_comprehensive_check„ÅÆ„Ç®„Ç§„É™„Ç¢„Çπ + 3Â±§Ê§úË®º„Ç≥„É≥„ÉÜ„Ç≠„Çπ„ÉàÂØæÂøú
        """

        print(f"üîç ÂåÖÊã¨ÁöÑÂìÅË≥™„ÉÅ„Çß„ÉÉ„ÇØÈñãÂßã - 3Â±§Ê§úË®º‰ΩìÂà∂ÂØæÂøúÁâà")

        context = context or {}

        # Êó¢Â≠ò„ÅÆrun_comprehensive_check„ÇíÂÆüË°å
        base_metrics = self.run_comprehensive_check(target_files, ai_agent)

        # 3Â±§Ê§úË®º„Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà„Åß„ÅÆËøΩÂä†ÊÉÖÂ†±„Çí‰ªò‰∏é
        if context.get("layer_context"):
            layer_info = context["layer_context"]

            # Layer 1„ÅÆÁµêÊûú„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„ÇãÂ†¥Âêà
            if layer_info.get("layer1_result"):
                layer1_result = layer_info["layer1_result"]
                if not layer1_result.get("passed"):
                    print("‚ö†Ô∏è Layer 1ÊßãÊñáÊ§úË®ºÊú™ÈÄöÈÅé - ÂìÅË≥™„Çπ„Ç≥„Ç¢Ë™øÊï¥")
                    # ÊßãÊñá„Ç®„É©„Éº„Åå„ÅÇ„ÇãÂ†¥Âêà„ÅØ„Çπ„Ç≥„Ç¢„Çí‰∏ãÊñπ‰øÆÊ≠£
                    base_metrics.overall_score *= 0.7

            # Layer 2„ÅÆÁµêÊûú„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„ÇãÂ†¥Âêà
            if layer_info.get("layer2_result"):
                layer2_result = layer_info["layer2_result"]
                if not layer2_result.get("passed"):
                    print("‚ö†Ô∏è Layer 2ÂìÅË≥™Ê§úË®ºÊú™ÈÄöÈÅé - ÂìÅË≥™„É¨„Éô„É´Ë™øÊï¥")
                    # ÂìÅË≥™Ê§úË®ºÊú™ÈÄöÈÅé„ÅÆÂ†¥Âêà„ÅØ„É¨„Éô„É´„Çí‰∏ãÊñπ‰øÆÊ≠£
                    if base_metrics.quality_level == QualityLevel.EXCELLENT:
                        base_metrics.quality_level = QualityLevel.GOOD
                    elif base_metrics.quality_level == QualityLevel.GOOD:
                        base_metrics.quality_level = QualityLevel.ACCEPTABLE

        # 3Â±§Ê§úË®ºÂ∞ÇÁî®„ÅÆÊîπÂñÑÊèêÊ°à„ÇíËøΩÂä†
        three_layer_suggestions = [
            "3Â±§Ê§úË®º‰ΩìÂà∂„Åß„ÅÆÂìÅË≥™„ÉÅ„Çß„ÉÉ„ÇØÂÆå‰∫Ü",
            "Layer 1ÔºàÊßãÊñáÔºâ‚Üí Layer 2ÔºàÂìÅË≥™Ôºâ‚Üí Layer 3ÔºàClaudeÊâøË™çÔºâ„ÅÆÈ†ÜÂ∫è„ÅßÂÆüË°åÊé®Â•®"
        ]

        base_metrics.improvement_suggestions.extend(three_layer_suggestions)

        print(f"‚úÖ ÂåÖÊã¨ÁöÑÂìÅË≥™„ÉÅ„Çß„ÉÉ„ÇØÂÆå‰∫ÜÔºà3Â±§Ê§úË®ºÂØæÂøúÁâàÔºâ")

        return base_metrics

    def get_quality_report(self) -> Dict[str, Any]:
        """ÂìÅË≥™„É¨„Éù„Éº„ÉàÁîüÊàê"""

        if not self.history_path.exists():
            return {"error": "ÂìÅË≥™Â±•Ê≠¥„Éá„Éº„Çø„Å™„Åó"}

        try:
            with open(self.history_path, 'r', encoding='utf-8') as f:
                history = json.load(f)

            if not history:
                return {"error": "ÂìÅË≥™Â±•Ê≠¥„Éá„Éº„Çø„Å™„Åó"}

            # Áµ±Ë®àË®àÁÆó
            recent_results = history[-10:]  # ÊúÄÊñ∞10‰ª∂

            avg_overall = sum(r["metrics"]["overall_score"] for r in recent_results) / len(recent_results)
            avg_errors = sum(r["metrics"]["error_count"] for r in recent_results) / len(recent_results)

            quality_trend = []
            if len(history) >= 5:
                for i in range(len(history) - 4):
                    batch = history[i:i+5]
                    batch_avg = sum(r["metrics"]["overall_score"] for r in batch) / 5
                    quality_trend.append(batch_avg)

            return {
                "current_quality": {
                    "overall_score": recent_results[-1]["metrics"]["overall_score"],
                    "quality_level": recent_results[-1]["metrics"]["quality_level"],
                    "error_count": recent_results[-1]["metrics"]["error_count"]
                },
                "trends": {
                    "average_overall_score": avg_overall,
                    "average_error_count": avg_errors,
                    "quality_trend": quality_trend
                },
                "recommendations": self._generate_recommendations(recent_results),
                "total_checks": len(history)
            }

        except Exception as e:
            return {"error": f"„É¨„Éù„Éº„ÉàÁîüÊàê„Ç®„É©„Éº: {str(e)}"}

    def _generate_recommendations(self, recent_results: List[Dict[str, Any]]) -> List[str]:
        """ÊîπÂñÑÊé®Â•®‰∫ãÈ†ÖÁîüÊàê"""

        recommendations = []

        # ÂÇæÂêëÂàÜÊûê
        if len(recent_results) >= 3:
            scores = [r["metrics"]["overall_score"] for r in recent_results[-3:]]
            if all(scores[i] > scores[i+1] for i in range(len(scores)-1)):
                recommendations.append("üìâ ÂìÅË≥™„Çπ„Ç≥„Ç¢„Åå‰Ωé‰∏ãÂÇæÂêë„Å´„ÅÇ„Çä„Åæ„Åô„ÄÇÊîπÂñÑ„ÅåÂøÖË¶Å„Åß„Åô")
            elif all(scores[i] < scores[i+1] for i in range(len(scores)-1)):
                recommendations.append("üìà ÂìÅË≥™„Çπ„Ç≥„Ç¢„ÅåÂêë‰∏ä„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇÂºï„ÅçÁ∂ö„ÅçÁ∂≠ÊåÅ„Åó„Å¶„Åè„Å†„Åï„ÅÑ")

        # „Ç®„É©„ÉºÁéáÂàÜÊûê
        recent_errors = [r["metrics"]["error_count"] for r in recent_results]
        avg_errors = sum(recent_errors) / len(recent_errors)

        if avg_errors > 20:
            recommendations.append("üö® „Ç®„É©„ÉºÊï∞„ÅåÂ§ö„Åô„Åé„Åæ„Åô„ÄÇÂü∫Êú¨ÁöÑ„Å™ÂìÅË≥™„ÉÅ„Çß„ÉÉ„ÇØ„ÇíÂº∑Âåñ„Åó„Å¶„Åè„Å†„Åï„ÅÑ")
        elif avg_errors > 10:
            recommendations.append("‚ö†Ô∏è „Ç®„É©„ÉºÊï∞„Åå„ÇÑ„ÇÑÂ§ö„ÇÅ„Åß„Åô„ÄÇÂÆöÊúüÁöÑ„Å™ÂìÅË≥™„ÉÅ„Çß„ÉÉ„ÇØ„ÇíÊé®Â•®„Åó„Åæ„Åô")

        return recommendations

    # =========================
    # 3Â±§Ê§úË®º‰ΩìÂà∂Â∞ÇÁî®„É°„ÇΩ„ÉÉ„Éâ
    # =========================

    def validate_syntax(self, target_files: List[str]) -> Dict[str, Any]:
        """Layer 1: ÊßãÊñáÊ§úË®ºÔºà3Â±§Ê§úË®º‰ΩìÂà∂Ôºâ"""

        print("üîç Layer 1: ÊßãÊñáÊ§úË®ºÈñãÂßã...")

        # ÊßãÊñá„ÉÅ„Çß„ÉÉ„ÇØÂÆüË°å
        syntax_result = self._check_syntax(target_files)

        # Âü∫Êú¨ÂìÅË≥™„ÉÅ„Çß„ÉÉ„ÇØ
        type_result = self._check_types(target_files)

        # Ê§úË®ºÁµêÊûúÁµ±Âêà
        validation_passed = (
            syntax_result.passed and
            type_result.score >= 0.7  # Âûã„ÉÅ„Çß„ÉÉ„ÇØ70%‰ª•‰∏ä
        )

        result = {
            "layer": 1,
            "validation_type": "syntax_validation",
            "passed": validation_passed,
            "syntax_check": {
                "passed": syntax_result.passed,
                "score": syntax_result.score,
                "errors": len(syntax_result.errors),
                "details": syntax_result.details
            },
            "type_check": {
                "passed": type_result.passed,
                "score": type_result.score,
                "errors": len(type_result.errors),
                "details": type_result.details
            },
            "summary": {
                "total_files": len(target_files),
                "syntax_errors": len(syntax_result.errors),
                "type_errors": len(type_result.errors),
                "validation_passed": validation_passed
            },
            "next_layer_recommended": validation_passed,
            "timestamp": datetime.datetime.now().isoformat()
        }

        print(f"‚úÖ Layer 1ÂÆå‰∫Ü: {'PASS' if validation_passed else 'FAIL'}")
        print(f"   ÊßãÊñá„Ç®„É©„Éº: {len(syntax_result.errors)}‰ª∂")
        print(f"   Âûã„Ç®„É©„Éº: {len(type_result.errors)}‰ª∂")

        return result

    def check_code_quality(self, target_files: List[str], layer1_result: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Layer 2: ÂìÅË≥™Ê§úË®ºÔºà3Â±§Ê§úË®º‰ΩìÂà∂Ôºâ"""

        print("üõ°Ô∏è Layer 2: ÂìÅË≥™Ê§úË®ºÈñãÂßã...")

        # Layer 1„ÅÆÁµêÊûúÁ¢∫Ë™ç
        if layer1_result and not layer1_result.get("passed", False):
            return {
                "layer": 2,
                "validation_type": "quality_validation",
                "passed": False,
                "skipped": True,
                "reason": "Layer 1ÊßãÊñáÊ§úË®º„ÅåÂ§±Êïó„Åó„Åü„Åü„ÇÅÂìÅË≥™Ê§úË®º„Çí„Çπ„Ç≠„ÉÉ„Éó",
                "timestamp": datetime.datetime.now().isoformat()
            }

        # ÂåÖÊã¨ÁöÑÂìÅË≥™„ÉÅ„Çß„ÉÉ„ÇØÂÆüË°å
        lint_result = self._check_lint(target_files)
        format_result = self._check_format(target_files)
        security_result = self._check_security(target_files)
        performance_result = self._check_performance(target_files)
        test_result = self._check_tests(target_files)

        # ÂìÅË≥™Âü∫Ê∫ñÂà§ÂÆö
        quality_scores = {
            "lint": lint_result.score,
            "format": format_result.score,
            "security": security_result.score,
            "performance": performance_result.score,
            "test": test_result.score
        }

        overall_quality_score = sum(quality_scores.values()) / len(quality_scores)
        quality_passed = overall_quality_score >= 0.75  # 75%‰ª•‰∏ä„ÅßÂêàÊ†º

        result = {
            "layer": 2,
            "validation_type": "quality_validation",
            "passed": quality_passed,
            "overall_quality_score": overall_quality_score,
            "quality_checks": {
                "lint": {
                    "passed": lint_result.passed,
                    "score": lint_result.score,
                    "warnings": len(lint_result.warnings)
                },
                "format": {
                    "passed": format_result.passed,
                    "score": format_result.score,
                    "issues": len(format_result.errors)
                },
                "security": {
                    "passed": security_result.passed,
                    "score": security_result.score,
                    "vulnerabilities": len(security_result.errors)
                },
                "performance": {
                    "passed": performance_result.passed,
                    "score": performance_result.score,
                    "bottlenecks": len(performance_result.warnings)
                },
                "test": {
                    "passed": test_result.passed,
                    "score": test_result.score,
                    "coverage": test_result.score * 100
                }
            },
            "summary": {
                "total_files": len(target_files),
                "quality_level": self._determine_quality_level(overall_quality_score),
                "claude_review_recommended": quality_passed
            },
            "next_layer_recommended": quality_passed,
            "timestamp": datetime.datetime.now().isoformat()
        }

        print(f"‚úÖ Layer 2ÂÆå‰∫Ü: {'PASS' if quality_passed else 'FAIL'}")
        print(f"   Á∑èÂêàÂìÅË≥™„Çπ„Ç≥„Ç¢: {overall_quality_score:.3f}")
        print(f"   ÂìÅË≥™„É¨„Éô„É´: {self._determine_quality_level(overall_quality_score).value}")

        return result

    def claude_final_approval(self, target_files: List[str], layer1_result: Optional[Dict[str, Any]] = None,
                            layer2_result: Optional[Dict[str, Any]] = None, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Layer 3: ClaudeÊúÄÁµÇÊâøË™çÔºà3Â±§Ê§úË®º‰ΩìÂà∂Ôºâ"""

        print("üë®‚Äçüíª Layer 3: ClaudeÊúÄÁµÇÊâøË™çÈñãÂßã...")

        context = context or {}

        # ÂâçÂ±§„ÅÆÁµêÊûúÁ¢∫Ë™ç
        layer1_passed = layer1_result.get("passed", False) if layer1_result else False
        layer2_passed = layer2_result.get("passed", False) if layer2_result else False

        if not (layer1_passed and layer2_passed):
            return {
                "layer": 3,
                "validation_type": "claude_final_approval",
                "approved": False,
                "skipped": True,
                "reason": "ÂâçÂ±§„ÅÆÊ§úË®º„ÅåÊú™ÂÆå‰∫Ü„Åæ„Åü„ÅØÂ§±Êïó„ÅÆ„Åü„ÇÅÊúÄÁµÇÊâøË™ç„Çí„Çπ„Ç≠„ÉÉ„Éó",
                "layer1_passed": layer1_passed,
                "layer2_passed": layer2_passed,
                "timestamp": datetime.datetime.now().isoformat()
            }

        # ClaudeÂìÅË≥™Âü∫Ê∫ñ„Å´„Çà„ÇãÊúÄÁµÇ„ÉÅ„Çß„ÉÉ„ÇØ
        final_metrics = self.run_comprehensive_check(target_files, "claude")

        # ÊúÄÁµÇÊâøË™çÂà§ÂÆöÂü∫Ê∫ñ
        approval_criteria = {
            "minimum_overall_score": 0.80,  # 80%‰ª•‰∏ä
            "maximum_critical_errors": 0,   # ÈáçÂ§ß„Ç®„É©„Éº0‰ª∂
            "minimum_test_coverage": 0.70,  # „ÉÜ„Çπ„Éà„Ç´„Éê„É¨„ÉÉ„Ç∏70%‰ª•‰∏ä
            "maximum_security_issues": 0    # „Çª„Ç≠„É•„É™„ÉÜ„Ç£ÂïèÈ°å0‰ª∂
        }

        # Âà§ÂÆöÂÆüË°å
        approval_checks = {
            "overall_score_check": final_metrics.overall_score >= approval_criteria["minimum_overall_score"],
            "critical_errors_check": final_metrics.error_count <= approval_criteria["maximum_critical_errors"],
            "test_coverage_check": final_metrics.test_coverage >= approval_criteria["minimum_test_coverage"],
            "security_check": final_metrics.security_score >= 0.95  # „Çª„Ç≠„É•„É™„ÉÜ„Ç£„ÅØ95%‰ª•‰∏ä
        }

        final_approved = all(approval_checks.values())

        # „Ç≥„É≥„ÉÜ„Ç≠„Çπ„ÉàËÄÉÊÖÆÔºàÊñ∞Ë¶èÂÆüË£Ö„ÅÆÂ†¥Âêà„ÅÆÁâπÂà•Âü∫Ê∫ñÔºâ
        if context.get("task_type") in ["new_implementation", "hybrid_implementation", "new_feature_development"]:
            # Êñ∞Ë¶èÂÆüË£Ö„ÅØÂü∫Ê∫ñ„ÇíËã•Âπ≤Á∑©Âíå
            if final_metrics.overall_score >= 0.75 and final_metrics.error_count <= 2:
                final_approved = True
                approval_checks["new_implementation_exception"] = True

        result = {
            "layer": 3,
            "validation_type": "claude_final_approval",
            "approved": final_approved,
            "final_metrics": {
                "overall_score": final_metrics.overall_score,
                "quality_level": final_metrics.quality_level.value,
                "error_count": final_metrics.error_count,
                "warning_count": final_metrics.warning_count,
                "test_coverage": final_metrics.test_coverage,
                "security_score": final_metrics.security_score
            },
            "approval_criteria": approval_criteria,
            "approval_checks": approval_checks,
            "context_considerations": {
                "task_type": context.get("task_type", "unknown"),
                "special_criteria_applied": "new_implementation_exception" in approval_checks
            },
            "recommendations": final_metrics.improvement_suggestions,
            "summary": {
                "layer1_passed": layer1_passed,
                "layer2_passed": layer2_passed,
                "layer3_approved": final_approved,
                "three_layer_verification_complete": final_approved
            },
            "timestamp": datetime.datetime.now().isoformat()
        }

        print(f"‚úÖ Layer 3ÂÆå‰∫Ü: {'APPROVED' if final_approved else 'REJECTED'}")
        print(f"   ÊúÄÁµÇÂìÅË≥™„Çπ„Ç≥„Ç¢: {final_metrics.overall_score:.3f}")
        print(f"   3Â±§Ê§úË®ºÁµêÊûú: {'ÂÆåÂÖ®ÈÄöÈÅé' if final_approved else 'Ë¶ÅÊîπÂñÑ'}")

        return result

    def evaluate_integration_quality(self, target_files: List[str],
                                    layer2_result: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Áµ±ÂêàÂìÅË≥™Ë©ï‰æ°Ôºà3Â±§Ê§úË®º‰ΩìÂà∂Â∞ÇÁî®Ôºâ"""

        print("üîó Áµ±ÂêàÂìÅË≥™Ë©ï‰æ°ÈñãÂßã...")

        # Layer 2„ÅÆÁµêÊûúÁ¢∫Ë™ç
        if layer2_result and not layer2_result.get("passed", False):
            return {
                "layer": "integration",
                "validation_type": "integration_quality_evaluation",
                "passed": False,
                "skipped": True,
                "reason": "Layer 2ÂìÅË≥™Ê§úË®º„ÅåÂ§±Êïó„Åó„Åü„Åü„ÇÅÁµ±ÂêàÂìÅË≥™Ë©ï‰æ°„Çí„Çπ„Ç≠„ÉÉ„Éó",
                "timestamp": datetime.datetime.now().isoformat()
            }

        try:
            # ComprehensiveQualityValidator„ÇíÂãïÁöÑ„Ç§„É≥„Éù„Éº„Éà„Éª‰ΩøÁî®
            try:
                from .comprehensive_validator import (  # type: ignore
                    ComprehensiveQualityValidator,
                    ValidationCategory
                )
            except ImportError:
                print("‚ö†Ô∏è ComprehensiveQualityValidator „Ç§„É≥„Éù„Éº„Éà„Ç®„É©„Éº - „Éï„Çß„Ç§„É´„Çª„Éº„Éï„É¢„Éº„ÉâÂÆüË°å")
                return self._fallback_integration_evaluation(target_files)

            comprehensive_validator = ComprehensiveQualityValidator()

            # Áµ±ÂêàÂìÅË≥™Ê§úË®ºÂÆüË°å
            integration_categories = [
                ValidationCategory.INTEGRATION,
                ValidationCategory.RELIABILITY,
                ValidationCategory.SCALABILITY
            ]

            validation_result = comprehensive_validator.validate_comprehensive_quality(
                target_files,
                enterprise_mode=False,
                categories=integration_categories
            )

            summary = validation_result["summary"]

            # Áµ±ÂêàÂìÅË≥™Âà§ÂÆöÂü∫Ê∫ñ (ÊÆµÈöéÁöÑÊîπÂñÑ„Ç¢„Éó„É≠„Éº„ÉÅ)
            integration_score = summary.get("overall_score", 0.0)
            # ÂàùÊúüÊÆµÈöé: 50%‰ª•‰∏ä„ÅßÂêàÊ†ºÔºàÊÆµÈöéÁöÑ„Å´95%„Åæ„ÅßÂºï„Åç‰∏ä„Åí‰∫àÂÆöÔºâ
            integration_passed = integration_score >= 0.50

            # Ë©≥Á¥∞ÂàÜÊûê
            integration_results = validation_result.get("results_by_category", {}).get("integration", [])
            reliability_results = validation_result.get("results_by_category", {}).get("reliability", [])
            scalability_results = validation_result.get("results_by_category", {}).get("scalability", [])

            # Áµ±Âêà„ÉÜ„Çπ„ÉàÁµêÊûúÂàÜÊûê
            integration_test_score = 0.0
            integration_findings = []

            if integration_results:
                test_scores = [r.score for r in integration_results]
                integration_test_score = sum(test_scores) / len(test_scores)
                integration_findings = [finding for r in integration_results for finding in r.findings]

            result = {
                "layer": "integration",
                "validation_type": "integration_quality_evaluation",
                "passed": integration_passed,
                "overall_integration_score": integration_score,
                "detailed_scores": {
                    "integration_test_score": integration_test_score,
                    "reliability_score": summary.get("category_scores", {}).get("reliability", 0.0),
                    "scalability_score": summary.get("category_scores", {}).get("scalability", 0.0)
                },
                "integration_analysis": {
                    "total_files": len(target_files),
                    "integration_tests_generated": len(integration_results),
                    "integration_findings": integration_findings[:5],  # ÊúÄÂ§ß5‰ª∂
                    "reliability_issues": len(reliability_results),
                    "scalability_concerns": len(scalability_results)
                },
                "quality_assessment": {
                    "integration_readiness": integration_passed,
                    "recommended_next_steps": self._generate_integration_recommendations(
                        integration_score, integration_findings
                    ),
                    "risk_level": "low" if integration_score >= 0.7 else "medium" if integration_score >= 0.5 else "high"
                },
                "comprehensive_validation_summary": summary,
                "timestamp": datetime.datetime.now().isoformat()
            }

            print(f"‚úÖ Áµ±ÂêàÂìÅË≥™Ë©ï‰æ°ÂÆå‰∫Ü: {'PASS' if integration_passed else 'FAIL'}")
            print(f"   Áµ±Âêà„Çπ„Ç≥„Ç¢: {integration_score:.3f}")
            print(f"   „ÉÜ„Çπ„ÉàÁîüÊàêÊï∞: {len(integration_results)}‰ª∂")
            print(f"   „É™„Çπ„ÇØ„É¨„Éô„É´: {result['quality_assessment']['risk_level']}")

            return result

        except ImportError as e:
            print(f"‚ö†Ô∏è ComprehensiveQualityValidator „Ç§„É≥„Éù„Éº„Éà„Ç®„É©„Éº: {e}")
            return self._fallback_integration_evaluation(target_files)
        except Exception as e:
            print(f"‚ùå Áµ±ÂêàÂìÅË≥™Ë©ï‰æ°„Ç®„É©„Éº: {e}")
            return {
                "layer": "integration",
                "validation_type": "integration_quality_evaluation",
                "passed": False,
                "error": True,
                "error_message": str(e),
                "fallback_used": True,
                "timestamp": datetime.datetime.now().isoformat()
            }

    def _generate_integration_recommendations(self, score: float, findings: List[str]) -> List[str]:
        """Áµ±ÂêàÂìÅË≥™ÊîπÂñÑÊé®Â•®‰∫ãÈ†ÖÁîüÊàê"""

        recommendations = []

        if score < 0.5:
            recommendations.extend([
                "Áµ±ÂêàÂìÅË≥™„ÅåÂü∫Ê∫ñ„ÇíÂ§ßÂπÖ„Å´‰∏ãÂõû„Å£„Å¶„ÅÑ„Åæ„Åô„ÄÇÂü∫Êú¨ÁöÑ„Å™„É¢„Ç∏„É•„Éº„É´ÊßãÈÄ†„ÇíË¶ãÁõ¥„Åó„Å¶„Åè„Å†„Åï„ÅÑ",
                "Áµ±Âêà„ÉÜ„Çπ„Éà„ÅåÂ§öÊï∞Â§±Êïó„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ„Ç≥„É≥„Éù„Éº„Éç„É≥„ÉàÈñì„ÅÆ‰æùÂ≠òÈñ¢‰øÇ„ÇíÁ¢∫Ë™ç„Åó„Å¶„Åè„Å†„Åï„ÅÑ",
                "„É¢„Ç∏„É•„Éº„É´ÂàÜÈõ¢„ÉªÁñéÁµêÂêàË®≠Ë®à„ÅÆÂ∞éÂÖ•„ÇíÊ§úË®é„Åó„Å¶„Åè„Å†„Åï„ÅÑ"
            ])
        elif score < 0.7:
            recommendations.extend([
                "Áµ±ÂêàÂìÅË≥™„ÅÆÊîπÂñÑ„ÅåÂøÖË¶Å„Åß„Åô„ÄÇ„Ç§„É≥„Çø„Éº„Éï„Çß„Éº„ÇπË®≠Ë®à„ÇíË¶ãÁõ¥„Åó„Å¶„Åè„Å†„Åï„ÅÑ",
                "Áµ±Âêà„ÉÜ„Çπ„Éà„ÅÆËøΩÂä†„ÉªÊîπÂñÑ„ÇíÊ§úË®é„Åó„Å¶„Åè„Å†„Åï„ÅÑ",
                "„Ç®„É©„Éº„Éè„É≥„Éâ„É™„É≥„Ç∞„ÅÆÂº∑Âåñ„ÇíÊé®Â•®„Åó„Åæ„Åô"
            ])
        elif score < 0.85:
            recommendations.extend([
                "Âü∫Êú¨ÁöÑ„Å™Áµ±ÂêàÂìÅË≥™„ÅØÁ¢∫‰øù„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇÁ¥∞„Åã„ÅÑÊîπÂñÑ„ÇíÁ∂ôÁ∂ö„Åó„Å¶„Åè„Å†„Åï„ÅÑ",
                "Áµ±Âêà„ÉÜ„Çπ„Éà„Ç´„Éê„É¨„ÉÉ„Ç∏„ÅÆÂêë‰∏ä„ÇíÊ§úË®é„Åó„Å¶„Åè„Å†„Åï„ÅÑ",
                "„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊúÄÈÅ©Âåñ„ÇíÊ§úË®é„Åó„Å¶„Åè„Å†„Åï„ÅÑ"
            ])
        else:
            recommendations.append("ÂÑ™ÁßÄ„Å™Áµ±ÂêàÂìÅË≥™„É¨„Éô„É´„Åß„Åô„ÄÇÁèæÂú®„ÅÆÊ∞¥Ê∫ñ„ÇíÁ∂≠ÊåÅ„Åó„Å¶„Åè„Å†„Åï„ÅÑ")

        # ÂÖ∑‰ΩìÁöÑ„Å™ÂïèÈ°å„Å´Âü∫„Å•„ÅèÊé®Â•®‰∫ãÈ†Ö
        if findings:
            recommendations.append(f"Áô∫Ë¶ã„Åï„Çå„ÅüÂïèÈ°åÔºà{len(findings)}‰ª∂Ôºâ„ÅÆÂÑ™ÂÖàÁöÑ„Å™Ëß£Ê±∫„ÅåÂøÖË¶Å„Åß„Åô")

        return recommendations

    def _fallback_integration_evaluation(self, target_files: List[str]) -> Dict[str, Any]:
        """„Éï„Çß„Ç§„É´„Çª„Éº„ÉïÁî®„ÅÆÂü∫Êú¨ÁöÑÁµ±ÂêàÂìÅË≥™Ë©ï‰æ°"""

        print("üîÑ „Éï„Çß„Ç§„É´„Çª„Éº„Éï„É¢„Éº„Éâ: Âü∫Êú¨ÁöÑÁµ±ÂêàÂìÅË≥™Ë©ï‰æ°„ÇíÂÆüË°å")

        # Âü∫Êú¨ÁöÑ„Å™„Éï„Ç°„Ç§„É´Èñ¢ÈÄ£ÊÄß„ÉÅ„Çß„ÉÉ„ÇØ
        integration_issues = []

        for file_path in target_files:
            if not os.path.exists(file_path):
                integration_issues.append(f"„Éï„Ç°„Ç§„É´Êú™Â≠òÂú®: {file_path}")
                continue

            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                # Âü∫Êú¨ÁöÑ„Å™Áµ±ÂêàÂïèÈ°å„Éë„Çø„Éº„É≥„ÉÅ„Çß„ÉÉ„ÇØ
                if 'import' not in content:
                    integration_issues.append(f"„Ç§„É≥„Éù„Éº„ÉàÊñá„Å™„Åó: {file_path}")

                if len(content.strip()) == 0:
                    integration_issues.append(f"Á©∫„Éï„Ç°„Ç§„É´: {file_path}")

            except Exception as e:
                integration_issues.append(f"„Éï„Ç°„Ç§„É´Ë™≠„ÅøËæº„Åø„Ç®„É©„Éº: {file_path} - {str(e)}")

        # „Çπ„Ç≥„Ç¢Ë®àÁÆóÔºàÁ∞°ÊòìÁâàÔºâ
        total_files = len(target_files)
        issues_count = len(integration_issues)
        fallback_score = max(0.0, 1.0 - (issues_count / max(total_files, 1)))

        return {
            "layer": "integration",
            "validation_type": "integration_quality_evaluation",
            "passed": fallback_score >= 0.7,
            "overall_integration_score": fallback_score,
            "fallback_mode": True,
            "integration_analysis": {
                "total_files": total_files,
                "issues_found": issues_count,
                "integration_findings": integration_issues[:10]  # ÊúÄÂ§ß10‰ª∂
            },
            "quality_assessment": {
                "integration_readiness": fallback_score >= 0.7,
                "recommended_next_steps": [
                    "ComprehensiveQualityValidator„ÅÆË®≠ÂÆö„ÇíÁ¢∫Ë™ç„Åó„Å¶„Åè„Å†„Åï„ÅÑ",
                    "Ë©≥Á¥∞„Å™Áµ±ÂêàÂìÅË≥™Ë©ï‰æ°„Ç∑„Çπ„ÉÜ„É†„ÅÆÂæ©Êóß„ÅåÂøÖË¶Å„Åß„Åô"
                ],
                "risk_level": "high" if fallback_score < 0.7 else "medium"
            },
            "timestamp": datetime.datetime.now().isoformat()
        }

    def run_three_layer_verification_with_failsafe(self, target_files: List[str],
                                                  context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """3Â±§Ê§úË®º‰ΩìÂà∂ÂÆåÂÖ®ÁâàÔºà„Éï„Çß„Ç§„É´„Çª„Éº„ÉïÊ©üËÉΩ‰ªò„ÅçÔºâ"""

        print("üõ°Ô∏è 3Â±§Ê§úË®º‰ΩìÂà∂ÈñãÂßãÔºà„Éï„Çß„Ç§„É´„Çª„Éº„ÉïÊ©üËÉΩ‰ªò„ÅçÔºâ")

        context = context or {}
        verification_results: Dict[str, Any] = {
            "layer1_result": None,
            "layer2_result": None,
            "layer3_result": None,
            "integration_evaluation": None,
            "failsafe_activated": [],
            "overall_status": "pending",
            "total_execution_time": 0.0,
            "timestamp": datetime.datetime.now().isoformat()
        }

        start_time = datetime.datetime.now()

        try:
            # ========== Layer 1: ÊßãÊñáÊ§úË®º ==========
            print("üîç Layer 1: ÊßãÊñáÊ§úË®ºÂÆüË°å‰∏≠...")
            try:
                layer1_result = self.validate_syntax(target_files)
                verification_results["layer1_result"] = layer1_result

                if not layer1_result.get("passed", False):
                    print("‚ö†Ô∏è Layer 1Â§±Êïó - „Éï„Çß„Ç§„É´„Çª„Éº„Éï„É¢„Éº„Éâ: Âü∫Êú¨ÊßãÊñá„ÉÅ„Çß„ÉÉ„ÇØÂÆüË°å")
                    verification_results["failsafe_activated"].append("layer1_basic_syntax_check")

                    # „Éï„Çß„Ç§„É´„Çª„Éº„Éï: Âü∫Êú¨ÁöÑ„Å™ÊßãÊñá„ÉÅ„Çß„ÉÉ„ÇØ
                    layer1_result = self._failsafe_basic_syntax_check(target_files)
                    verification_results["layer1_result"] = layer1_result

            except Exception as e:
                print(f"‚ùå Layer 1„Ç®„É©„Éº - „Éï„Çß„Ç§„É´„Çª„Éº„ÉïÂÆüË°å: {e}")
                verification_results["failsafe_activated"].append("layer1_exception_handling")
                layer1_result = self._failsafe_basic_syntax_check(target_files)
                verification_results["layer1_result"] = layer1_result

            # ========== Layer 2: ÂìÅË≥™Ê§úË®º ==========
            print("üõ°Ô∏è Layer 2: ÂìÅË≥™Ê§úË®ºÂÆüË°å‰∏≠...")
            try:
                layer2_result = self.check_code_quality(target_files, verification_results["layer1_result"])
                verification_results["layer2_result"] = layer2_result

                if not layer2_result.get("passed", False):
                    print("‚ö†Ô∏è Layer 2Â§±Êïó - „Éï„Çß„Ç§„É´„Çª„Éº„Éï„É¢„Éº„Éâ: Âü∫Êú¨ÂìÅË≥™„ÉÅ„Çß„ÉÉ„ÇØÂÆüË°å")
                    verification_results["failsafe_activated"].append("layer2_basic_quality_check")

                    # „Éï„Çß„Ç§„É´„Çª„Éº„Éï: Âü∫Êú¨ÁöÑ„Å™ÂìÅË≥™„ÉÅ„Çß„ÉÉ„ÇØ
                    layer2_result = self._failsafe_basic_quality_check(target_files)
                    verification_results["layer2_result"] = layer2_result

            except Exception as e:
                print(f"‚ùå Layer 2„Ç®„É©„Éº - „Éï„Çß„Ç§„É´„Çª„Éº„ÉïÂÆüË°å: {e}")
                verification_results["failsafe_activated"].append("layer2_exception_handling")
                layer2_result = self._failsafe_basic_quality_check(target_files)
                verification_results["layer2_result"] = layer2_result

            # ========== Áµ±ÂêàÂìÅË≥™Ë©ï‰æ° ==========
            print("üîó Áµ±ÂêàÂìÅË≥™Ë©ï‰æ°ÂÆüË°å‰∏≠...")
            try:
                integration_result = self.evaluate_integration_quality(target_files, verification_results["layer2_result"])
                verification_results["integration_evaluation"] = integration_result

            except Exception as e:
                print(f"‚ùå Áµ±ÂêàÂìÅË≥™Ë©ï‰æ°„Ç®„É©„Éº - „Éï„Çß„Ç§„É´„Çª„Éº„ÉïÂÆüË°å: {e}")
                verification_results["failsafe_activated"].append("integration_evaluation_exception")
                integration_result = self._fallback_integration_evaluation(target_files)
                verification_results["integration_evaluation"] = integration_result

            # ========== Layer 3: ClaudeÊúÄÁµÇÊâøË™ç ==========
            print("üë®‚Äçüíª Layer 3: ClaudeÊúÄÁµÇÊâøË™çÂÆüË°å‰∏≠...")
            try:
                layer3_result = self.claude_final_approval(
                    target_files,
                    verification_results["layer1_result"],
                    verification_results["layer2_result"],
                    context
                )
                verification_results["layer3_result"] = layer3_result

                if not layer3_result.get("approved", False):
                    print("‚ö†Ô∏è Layer 3ÊâøË™çÊú™ÂèñÂæó - „Éï„Çß„Ç§„É´„Çª„Éº„Éï„É¢„Éº„Éâ: Âü∫Êú¨ÊâøË™ç„ÉÅ„Çß„ÉÉ„ÇØÂÆüË°å")
                    verification_results["failsafe_activated"].append("layer3_basic_approval")

                    # „Éï„Çß„Ç§„É´„Çª„Éº„Éï: Âü∫Êú¨ÊâøË™ç„ÉÅ„Çß„ÉÉ„ÇØ
                    layer3_result = self._failsafe_basic_approval_check(
                        target_files,
                        verification_results["layer1_result"],
                        verification_results["layer2_result"]
                    )
                    verification_results["layer3_result"] = layer3_result

            except Exception as e:
                print(f"‚ùå Layer 3„Ç®„É©„Éº - „Éï„Çß„Ç§„É´„Çª„Éº„ÉïÂÆüË°å: {e}")
                verification_results["failsafe_activated"].append("layer3_exception_handling")
                layer3_result = self._failsafe_basic_approval_check(
                    target_files,
                    verification_results["layer1_result"],
                    verification_results["layer2_result"]
                )
                verification_results["layer3_result"] = layer3_result

            # ========== ÊúÄÁµÇÂà§ÂÆö ==========
            total_execution_time = (datetime.datetime.now() - start_time).total_seconds()
            verification_results["total_execution_time"] = total_execution_time

            # ÂÖ®Â±§„ÅÆÁµêÊûú„Åã„ÇâÊúÄÁµÇ„Çπ„ÉÜ„Éº„Çø„ÇπÊ±∫ÂÆö
            layer1_passed = verification_results["layer1_result"].get("passed", False)
            layer2_passed = verification_results["layer2_result"].get("passed", False)
            layer3_approved = verification_results["layer3_result"].get("approved", False)
            integration_passed = verification_results["integration_evaluation"].get("passed", False)

            overall_passed = layer1_passed and layer2_passed and layer3_approved and integration_passed

            if overall_passed:
                verification_results["overall_status"] = "fully_verified"
            elif layer3_approved:
                verification_results["overall_status"] = "approved_with_conditions"
            elif layer2_passed:
                verification_results["overall_status"] = "quality_verified"
            elif layer1_passed:
                verification_results["overall_status"] = "syntax_verified"
            else:
                verification_results["overall_status"] = "verification_failed"

            # „Éï„Çß„Ç§„É´„Çª„Éº„Éï‰ΩøÁî®„ÅÆË®òÈå≤„Å®„É≠„Ç∞
            if verification_results["failsafe_activated"]:
                self._log_failsafe_usage(verification_results["failsafe_activated"], target_files)
                print(f"üîÑ „Éï„Çß„Ç§„É´„Çª„Éº„ÉïÂÆüË°åÂõûÊï∞: {len(verification_results['failsafe_activated'])}Âõû")

            print(f"‚úÖ 3Â±§Ê§úË®º‰ΩìÂà∂ÂÆå‰∫Ü: {verification_results['overall_status']}")
            print(f"‚è±Ô∏è Á∑èÂÆüË°åÊôÇÈñì: {total_execution_time:.2f}Áßí")

            return verification_results

        except Exception as e:
            total_execution_time = (datetime.datetime.now() - start_time).total_seconds()
            print(f"üö® 3Â±§Ê§úË®º‰ΩìÂà∂ÈáçÂ§ß„Ç®„É©„Éº: {e}")

            # ÂÆåÂÖ®„Éï„Çß„Ç§„É´„Çª„Éº„Éï„É¢„Éº„Éâ
            return self._complete_failsafe_verification(target_files, str(e), total_execution_time)

    def _failsafe_basic_syntax_check(self, target_files: List[str]) -> Dict[str, Any]:
        """„Éï„Çß„Ç§„É´„Çª„Éº„Éï: Âü∫Êú¨ÊßãÊñá„ÉÅ„Çß„ÉÉ„ÇØ"""

        print("üîÑ „Éï„Çß„Ç§„É´„Çª„Éº„Éï: Âü∫Êú¨ÊßãÊñá„ÉÅ„Çß„ÉÉ„ÇØÂÆüË°å")

        syntax_errors = []
        for file_path in target_files:
            if not os.path.exists(file_path):
                syntax_errors.append(f"„Éï„Ç°„Ç§„É´Êú™Â≠òÂú®: {file_path}")
                continue

            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                # Âü∫Êú¨ÁöÑ„Å™ÊßãÊñá„Ç®„É©„Éº„Éë„Çø„Éº„É≥„ÉÅ„Çß„ÉÉ„ÇØ
                if not content.strip():
                    syntax_errors.append(f"Á©∫„Éï„Ç°„Ç§„É´: {file_path}")
                elif 'SyntaxError' in content:
                    syntax_errors.append(f"ÊßãÊñá„Ç®„É©„ÉºÂê´Êúâ: {file_path}")

            except Exception as e:
                syntax_errors.append(f"Ë™≠„ÅøËæº„Åø„Ç®„É©„Éº: {file_path} - {str(e)}")

        passed = len(syntax_errors) == 0

        return {
            "layer": 1,
            "validation_type": "failsafe_syntax_validation",
            "passed": passed,
            "failsafe_mode": True,
            "syntax_errors": syntax_errors,
            "files_checked": len(target_files),
            "timestamp": datetime.datetime.now().isoformat()
        }

    def _failsafe_basic_quality_check(self, target_files: List[str]) -> Dict[str, Any]:
        """„Éï„Çß„Ç§„É´„Çª„Éº„Éï: Âü∫Êú¨ÂìÅË≥™„ÉÅ„Çß„ÉÉ„ÇØ"""

        print("üîÑ „Éï„Çß„Ç§„É´„Çª„Éº„Éï: Âü∫Êú¨ÂìÅË≥™„ÉÅ„Çß„ÉÉ„ÇØÂÆüË°å")

        quality_issues = []
        for file_path in target_files:
            if not os.path.exists(file_path):
                continue

            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                lines = content.split('\n')

                # Âü∫Êú¨ÂìÅË≥™„ÉÅ„Çß„ÉÉ„ÇØ
                if len(lines) > 1000:
                    quality_issues.append(f"Â§ß„Åç„Åô„Åé„Çã„Éï„Ç°„Ç§„É´: {file_path} ({len(lines)}Ë°å)")

                if 'TODO' in content or 'FIXME' in content:
                    quality_issues.append(f"Êú™Ëß£Ê±∫TODO/FIXME: {file_path}")

            except Exception as e:
                quality_issues.append(f"ÂìÅË≥™„ÉÅ„Çß„ÉÉ„ÇØ„Ç®„É©„Éº: {file_path} - {str(e)}")

        passed = len(quality_issues) <= 2  # ËªΩÂæÆ„Å™ÂïèÈ°å„ÅØË®±ÂÆπ

        return {
            "layer": 2,
            "validation_type": "failsafe_quality_validation",
            "passed": passed,
            "failsafe_mode": True,
            "quality_issues": quality_issues,
            "files_checked": len(target_files),
            "timestamp": datetime.datetime.now().isoformat()
        }

    def _failsafe_basic_approval_check(self, target_files: List[str],
                                     layer1_result: Optional[Dict[str, Any]] = None,
                                     layer2_result: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """„Éï„Çß„Ç§„É´„Çª„Éº„Éï: Âü∫Êú¨ÊâøË™ç„ÉÅ„Çß„ÉÉ„ÇØ"""

        print("üîÑ „Éï„Çß„Ç§„É´„Çª„Éº„Éï: Âü∫Êú¨ÊâøË™ç„ÉÅ„Çß„ÉÉ„ÇØÂÆüË°å")

        # ÊúÄ‰ΩéÈôê„ÅÆÊâøË™çÊù°‰ª∂
        layer1_ok = layer1_result and layer1_result.get("passed", False)
        layer2_ok = layer2_result and layer2_result.get("passed", False)

        # „Éï„Çß„Ç§„É´„Çª„Éº„Éï„Åß„ÅØÁ∑©„ÅÑÂü∫Ê∫ñ„ÅßÊâøË™ç
        basic_approved = layer1_ok or (len(target_files) > 0 and all(os.path.exists(f) for f in target_files))

        return {
            "layer": 3,
            "validation_type": "failsafe_claude_approval",
            "approved": basic_approved,
            "failsafe_mode": True,
            "approval_basis": "basic_file_existence_check" if basic_approved else "no_valid_files",
            "files_checked": len(target_files),
            "layer1_input": layer1_ok,
            "layer2_input": layer2_ok,
            "timestamp": datetime.datetime.now().isoformat()
        }

    def _complete_failsafe_verification(self, target_files: List[str],
                                      error_message: str, execution_time: float) -> Dict[str, Any]:
        """ÂÆåÂÖ®„Éï„Çß„Ç§„É´„Çª„Éº„ÉïÊ§úË®º"""

        print("üÜò ÂÆåÂÖ®„Éï„Çß„Ç§„É´„Çª„Éº„Éï„É¢„Éº„ÉâÂÆüË°å")

        return {
            "layer1_result": {"passed": False, "failsafe_mode": True},
            "layer2_result": {"passed": False, "failsafe_mode": True},
            "layer3_result": {"approved": False, "failsafe_mode": True},
            "integration_evaluation": {"passed": False, "failsafe_mode": True},
            "failsafe_activated": ["complete_system_failure"],
            "overall_status": "complete_failsafe_mode",
            "error_message": error_message,
            "total_execution_time": execution_time,
            "files_attempted": len(target_files),
            "recommendations": [
                "„Ç∑„Çπ„ÉÜ„É†Ë®≠ÂÆö„ÅÆÁ¢∫Ë™ç„ÅåÂøÖË¶Å„Åß„Åô",
                "‰æùÂ≠òÈñ¢‰øÇ„ÅÆÁ¢∫Ë™ç„ÇíË°å„Å£„Å¶„Åè„Å†„Åï„ÅÑ",
                "„É≠„Ç∞„Éï„Ç°„Ç§„É´„ÇíÁ¢∫Ë™ç„Åó„Å¶Ë©≥Á¥∞„Å™„Ç®„É©„ÉºÊÉÖÂ†±„ÇíÂèñÂæó„Åó„Å¶„Åè„Å†„Åï„ÅÑ"
            ],
            "timestamp": datetime.datetime.now().isoformat()
        }

    def _log_failsafe_usage(self, failsafe_types: List[str], target_files: List[str]) -> None:
        """„Éï„Çß„Ç§„É´„Çª„Éº„Éï‰ΩøÁî®„ÅÆ„É≠„Ç∞Ë®òÈå≤"""

        log_entry = {
            "timestamp": datetime.datetime.now().isoformat(),
            "failsafe_types": failsafe_types,
            "target_files": target_files,
            "total_files": len(target_files),
            "failsafe_count": len(failsafe_types)
        }

        # „Éï„Çß„Ç§„É´„Çª„Éº„Éï„É≠„Ç∞„Éï„Ç°„Ç§„É´„Å´Ë®òÈå≤
        failsafe_log_path = Path("postbox/monitoring/failsafe_usage.json")
        failsafe_log_path.parent.mkdir(parents=True, exist_ok=True)

        try:
            failsafe_logs = []
            if failsafe_log_path.exists():
                with open(failsafe_log_path, 'r', encoding='utf-8') as f:
                    failsafe_logs = json.load(f)

            failsafe_logs.append(log_entry)

            # „É≠„Ç∞„Çµ„Ç§„Ç∫Âà∂ÈôêÔºàÊúÄÊñ∞100‰ª∂Ôºâ
            if len(failsafe_logs) > 100:
                failsafe_logs = failsafe_logs[-100:]

            with open(failsafe_log_path, 'w', encoding='utf-8') as f:
                json.dump(failsafe_logs, f, indent=2, ensure_ascii=False)

            print(f"üìù „Éï„Çß„Ç§„É´„Çª„Éº„Éï„É≠„Ç∞Ë®òÈå≤: {failsafe_log_path}")

        except Exception as e:
            print(f"‚ö†Ô∏è „Éï„Çß„Ç§„É´„Çª„Éº„Éï„É≠„Ç∞Ë®òÈå≤„Ç®„É©„Éº: {e}")

    def execute_complete_three_layer_verification(self, target_files: List[str],
                                                context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """ÂÆåÂÖ®3Â±§Ê§úË®ºÂÆüË°åÔºàÁµ±Âêà„Ç§„É≥„Çø„Éº„Éï„Çß„Éº„ÇπÔºâ"""

        print("üéØ ÂÆåÂÖ®3Â±§Ê§úË®º„Ç∑„Çπ„ÉÜ„É†ÂÆüË°åÈñãÂßã")
        print(f"üìÅ ÂØæË±°„Éï„Ç°„Ç§„É´: {len(target_files)}‰ª∂")

        context = context or {}

        # „Éï„Çß„Ç§„É´„Çª„Éº„Éï‰ªò„Åç3Â±§Ê§úË®º„ÇíÂÆüË°å
        verification_result = self.run_three_layer_verification_with_failsafe(target_files, context)

        # ÁµêÊûú„ÅÆË©≥Á¥∞ÂàÜÊûê„Å®„É¨„Éù„Éº„ÉàÁîüÊàê
        enhanced_result = self._enhance_verification_result(verification_result, target_files, context)

        # ÊúÄÁµÇ„É¨„Éù„Éº„ÉàÂá∫Âäõ
        self._print_verification_summary(enhanced_result)

        # Áõ£Ë¶ñ„Ç∑„Çπ„ÉÜ„É†„Å∏„ÅÆÁµêÊûúË®òÈå≤
        self._record_verification_metrics(enhanced_result)

        return enhanced_result

    def _enhance_verification_result(self, verification_result: Dict[str, Any],
                                   target_files: List[str], context: Dict[str, Any]) -> Dict[str, Any]:
        """Ê§úË®ºÁµêÊûú„ÅÆÊã°Âºµ„ÉªÂàÜÊûê"""

        enhanced = verification_result.copy()

        # Ë©≥Á¥∞Áµ±Ë®à„ÅÆËøΩÂä†
        enhanced["detailed_statistics"] = {
            "total_files_processed": len(target_files),
            "layer1_success_rate": 1.0 if verification_result["layer1_result"].get("passed") else 0.0,
            "layer2_success_rate": 1.0 if verification_result["layer2_result"].get("passed") else 0.0,
            "layer3_approval_rate": 1.0 if verification_result["layer3_result"].get("approved") else 0.0,
            "integration_success_rate": 1.0 if verification_result["integration_evaluation"].get("passed") else 0.0,
            "overall_success_rate": self._calculate_overall_success_rate(verification_result),
            "failsafe_usage_rate": len(verification_result["failsafe_activated"]) / 4.0  # 4Â±§„Åß„ÅÆ‰ΩøÁî®Áéá
        }

        # ÊîπÂñÑÊé®Â•®‰∫ãÈ†Ö„ÅÆÁîüÊàê
        enhanced["improvement_roadmap"] = self._generate_improvement_roadmap(verification_result, context)

        # Ê¨°ÂõûÂÆüË°å„Å∏„ÅÆÊèêÊ°à
        enhanced["next_execution_suggestions"] = self._generate_next_execution_suggestions(verification_result)

        # ÂìÅË≥™„Éà„É¨„É≥„Éâ„ÅÆ‰∫àÊ∏¨
        enhanced["quality_trend_prediction"] = self._predict_quality_trend(verification_result, target_files)

        return enhanced

    def _calculate_overall_success_rate(self, verification_result: Dict[str, Any]) -> float:
        """Á∑èÂêàÊàêÂäüÁéáË®àÁÆó"""

        success_scores = []

        # Layer 1
        if verification_result["layer1_result"].get("passed"):
            success_scores.append(1.0)
        else:
            success_scores.append(0.0)

        # Layer 2
        if verification_result["layer2_result"].get("passed"):
            success_scores.append(1.0)
        else:
            success_scores.append(0.0)

        # Layer 3
        if verification_result["layer3_result"].get("approved"):
            success_scores.append(1.0)
        else:
            success_scores.append(0.0)

        # Integration
        if verification_result["integration_evaluation"].get("passed"):
            success_scores.append(1.0)
        else:
            success_scores.append(0.0)

        return sum(success_scores) / len(success_scores) if success_scores else 0.0

    def _generate_improvement_roadmap(self, verification_result: Dict[str, Any],
                                    context: Dict[str, Any]) -> Dict[str, Any]:
        """ÊîπÂñÑ„É≠„Éº„Éâ„Éû„ÉÉ„ÉóÁîüÊàê"""

        roadmap: Dict[str, Any] = {
            "immediate_actions": [],
            "short_term_goals": [],
            "long_term_improvements": [],
            "priority_order": []
        }

        # Layer 1„ÅåÂ§±Êïó„Åó„ÅüÂ†¥Âêà
        if not verification_result["layer1_result"].get("passed"):
            roadmap["immediate_actions"].extend([
                "ÊßãÊñá„Ç®„É©„Éº„ÅÆ‰øÆÊ≠£",
                "Âü∫Êú¨ÁöÑ„Å™PythonÊñáÊ≥ï„ÉÅ„Çß„ÉÉ„ÇØ",
                "„Ç§„É≥„Éù„Éº„ÉàÊñá„ÅÆÁ¢∫Ë™ç"
            ])
            roadmap["priority_order"].append("syntax_fixes")

        # Layer 2„ÅåÂ§±Êïó„Åó„ÅüÂ†¥Âêà
        if not verification_result["layer2_result"].get("passed"):
            roadmap["short_term_goals"].extend([
                "„Ç≥„Éº„ÉâÂìÅË≥™Âü∫Ê∫ñ„ÅÆÈÅµÂÆà",
                "„É™„É≥„ÉàÊåáÊëò‰∫ãÈ†Ö„ÅÆËß£Ê±∫",
                "„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÁµ±‰∏Ä"
            ])
            roadmap["priority_order"].append("quality_improvements")

        # Layer 3„ÅåÊâøË™ç„Åï„Çå„Å™„Åã„Å£„ÅüÂ†¥Âêà
        if not verification_result["layer3_result"].get("approved"):
            roadmap["short_term_goals"].extend([
                "ClaudeÊâøË™çÂü∫Ê∫ñ„ÅÆÁ¢∫Ë™ç",
                "ÂåÖÊã¨ÁöÑÂìÅË≥™Âêë‰∏ä",
                "„ÉÜ„Çπ„Éà„Ç´„Éê„É¨„ÉÉ„Ç∏ÊîπÂñÑ"
            ])
            roadmap["priority_order"].append("approval_requirements")

        # Áµ±ÂêàË©ï‰æ°„ÅåÂ§±Êïó„Åó„ÅüÂ†¥Âêà
        if not verification_result["integration_evaluation"].get("passed"):
            roadmap["long_term_improvements"].extend([
                "„É¢„Ç∏„É•„Éº„É´ÊßãÈÄ†„ÅÆÊîπÂñÑ",
                "Áµ±Âêà„ÉÜ„Çπ„Éà„ÅÆËøΩÂä†",
                "„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£„ÅÆË¶ãÁõ¥„Åó"
            ])
            roadmap["priority_order"].append("architecture_improvements")

        # „Éï„Çß„Ç§„É´„Çª„Éº„Éï„Åå‰ΩøÁî®„Åï„Çå„ÅüÂ†¥Âêà
        if verification_result["failsafe_activated"]:
            roadmap["immediate_actions"].append("„Ç∑„Çπ„ÉÜ„É†Ë®≠ÂÆö„Éª‰æùÂ≠òÈñ¢‰øÇ„ÅÆÁ¢∫Ë™ç")
            roadmap["priority_order"].insert(0, "system_stability")

        return roadmap

    def _generate_next_execution_suggestions(self, verification_result: Dict[str, Any]) -> List[str]:
        """Ê¨°ÂõûÂÆüË°åÊèêÊ°àÁîüÊàê"""

        suggestions = []

        overall_status = verification_result.get("overall_status", "unknown")

        if overall_status == "fully_verified":
            suggestions.extend([
                "ÂìÅË≥™„É¨„Éô„É´„ÅåÈ´ò„ÅÑ„Åü„ÇÅ„ÄÅÂÆöÊúüÁöÑ„Å™„É°„É≥„ÉÜ„Éä„É≥„Çπ„ÉÅ„Çß„ÉÉ„ÇØÊé®Â•®",
                "Êñ∞Ê©üËÉΩËøΩÂä†ÊôÇ„ÅÆÁ∂ôÁ∂öÁöÑÂìÅË≥™Áõ£Ë¶ñ„ÇíÊé®Â•®",
                "ÁèæÂú®„ÅÆÂìÅË≥™„É¨„Éô„É´Á∂≠ÊåÅ„ÅÆ„Åü„ÇÅ„ÅÆÂÆöÊúü„ÉÅ„Çß„ÉÉ„ÇØ"
            ])
        elif overall_status == "approved_with_conditions":
            suggestions.extend([
                "Êù°‰ª∂‰ªò„ÅçÊâøË™ç„ÅÆ„Åü„ÇÅ„ÄÅÊåáÊëò‰∫ãÈ†Ö„ÅÆ‰øÆÊ≠£Âæå„Å´ÂÜçÊ§úË®ºÊé®Â•®",
                "ÈÉ®ÂàÜÁöÑÊîπÂñÑÂæå„ÅÆÊÆµÈöéÁöÑÂÜç„ÉÅ„Çß„ÉÉ„ÇØ",
                "ÁâπÂÆöLayerÂÜçÂÆüË°å„Åß„ÅÆ„Éî„É≥„Éù„Ç§„É≥„ÉàÊîπÂñÑ"
            ])
        elif overall_status in ["quality_verified", "syntax_verified"]:
            suggestions.extend([
                "Êú™ÈÄöÈÅéLayer„ÅÆÈõÜ‰∏≠ÁöÑ„Å™ÊîπÂñÑ„ÅåÂøÖË¶Å",
                "ÊÆµÈöéÁöÑÂìÅË≥™Âêë‰∏ä„Ç¢„Éó„É≠„Éº„ÉÅÊé®Â•®",
                "ÁâπÂåñÂûãÂìÅË≥™ÊîπÂñÑ„ÉÑ„Éº„É´„ÅÆÊ¥ªÁî®Ê§úË®é"
            ])
        else:
            suggestions.extend([
                "Âü∫Êú¨ÁöÑ„Å™ÂìÅË≥™Á¢∫‰øù„Åã„ÇâÈñãÂßã„Åô„ÇãÊÆµÈöéÁöÑ„Ç¢„Éó„É≠„Éº„ÉÅ„ÅåÂøÖË¶Å",
                "Ëá™Âãï‰øÆÊ≠£„ÉÑ„Éº„É´„ÅÆÁ©çÊ•µÊ¥ªÁî®„ÇíÊé®Â•®",
                "Â§ñÈÉ®ÂìÅË≥™Áõ£Êüª„ÉÑ„Éº„É´„Å®„ÅÆÈÄ£Êê∫Ê§úË®é"
            ])

        return suggestions

    def _predict_quality_trend(self, verification_result: Dict[str, Any],
                             target_files: List[str]) -> Dict[str, Any]:
        """ÂìÅË≥™„Éà„É¨„É≥„Éâ‰∫àÊ∏¨"""

        return {
            "current_quality_level": verification_result.get("overall_status", "unknown"),
            "predicted_improvement_time": "1-2ÈÄ±Èñì" if verification_result.get("overall_status") in [
                "quality_verified", "approved_with_conditions"
            ] else "1-2„É∂Êúà",
            "improvement_confidence": 0.8 if not verification_result["failsafe_activated"] else 0.6,
            "recommended_check_frequency": "ÈÄ±1Âõû" if verification_result.get("overall_status") == "fully_verified" else "ÊØéÊó•",
            "risk_factors": [
                "„Éï„Çß„Ç§„É´„Çª„Éº„Éï‰ΩøÁî®" if verification_result["failsafe_activated"] else "ÂÆâÂÆöÂãï‰Ωú",
                f"ÂØæË±°„Éï„Ç°„Ç§„É´Êï∞: {len(target_files)}",
                f"ÂÆüË°åÊôÇÈñì: {verification_result.get('total_execution_time', 0):.1f}Áßí"
            ]
        }

    def _print_verification_summary(self, enhanced_result: Dict[str, Any]) -> None:
        """Ê§úË®ºÁµêÊûú„Çµ„Éû„É™„ÉºÂá∫Âäõ"""

        print("\n" + "="*60)
        print("üìä 3Â±§Ê§úË®º‰ΩìÂà∂ ÂÆåÂÖ®ÂÆüË°åÁµêÊûú„Çµ„Éû„É™„Éº")
        print("="*60)

        # Âü∫Êú¨„Çπ„ÉÜ„Éº„Çø„Çπ
        overall_status = enhanced_result.get("overall_status", "unknown")
        print(f"üéØ Á∑èÂêà„Çπ„ÉÜ„Éº„Çø„Çπ: {overall_status}")
        print(f"‚è±Ô∏è  ÂÆüË°åÊôÇÈñì: {enhanced_result.get('total_execution_time', 0):.2f}Áßí")

        # ÂêÑÂ±§„ÅÆÁµêÊûú
        print(f"\nüìã ÂêÑÂ±§„ÅÆÁµêÊûú:")
        layer1_status = "‚úÖ PASS" if enhanced_result["layer1_result"].get("passed") else "‚ùå FAIL"
        layer2_status = "‚úÖ PASS" if enhanced_result["layer2_result"].get("passed") else "‚ùå FAIL"
        layer3_status = "‚úÖ APPROVED" if enhanced_result["layer3_result"].get("approved") else "‚ùå REJECTED"
        integration_status = "‚úÖ PASS" if enhanced_result["integration_evaluation"].get("passed") else "‚ùå FAIL"

        print(f"  üîç Layer 1 (ÊßãÊñáÊ§úË®º): {layer1_status}")
        print(f"  üõ°Ô∏è  Layer 2 (ÂìÅË≥™Ê§úË®º): {layer2_status}")
        print(f"  üë®‚Äçüíª Layer 3 (ClaudeÊâøË™ç): {layer3_status}")
        print(f"  üîó Áµ±ÂêàÂìÅË≥™Ë©ï‰æ°: {integration_status}")

        # Áµ±Ë®àÊÉÖÂ†±
        stats = enhanced_result.get("detailed_statistics", {})
        print(f"\nüìà Ë©≥Á¥∞Áµ±Ë®à:")
        print(f"  Á∑èÂêàÊàêÂäüÁéá: {stats.get('overall_success_rate', 0):.1%}")
        print(f"  „Éï„Çß„Ç§„É´„Çª„Éº„Éï‰ΩøÁî®Áéá: {stats.get('failsafe_usage_rate', 0):.1%}")

        # „Éï„Çß„Ç§„É´„Çª„Éº„ÉïÊÉÖÂ†±
        if enhanced_result.get("failsafe_activated"):
            print(f"\nüîÑ „Éï„Çß„Ç§„É´„Çª„Éº„ÉïÂÆüË°å: {len(enhanced_result['failsafe_activated'])}Âõû")
            for i, fs_type in enumerate(enhanced_result['failsafe_activated'], 1):
                print(f"  {i}. {fs_type}")

        # ÊîπÂñÑÊèêÊ°à
        roadmap = enhanced_result.get("improvement_roadmap", {})
        if roadmap.get("immediate_actions"):
            print(f"\nüöÄ Á∑äÊÄ•ÊîπÂñÑ‰∫ãÈ†Ö:")
            for action in roadmap["immediate_actions"][:3]:
                print(f"  ‚Ä¢ {action}")

        print("="*60)

    def _record_verification_metrics(self, enhanced_result: Dict[str, Any]) -> None:
        """Ê§úË®º„É°„Éà„É™„ÇØ„ÇπË®òÈå≤"""

        metrics_entry = {
            "timestamp": datetime.datetime.now().isoformat(),
            "overall_status": enhanced_result.get("overall_status"),
            "execution_time": enhanced_result.get("total_execution_time"),
            "success_rates": enhanced_result.get("detailed_statistics", {}),
            "failsafe_usage": len(enhanced_result.get("failsafe_activated", [])),
            "layer_results": {
                "layer1_passed": enhanced_result["layer1_result"].get("passed"),
                "layer2_passed": enhanced_result["layer2_result"].get("passed"),
                "layer3_approved": enhanced_result["layer3_result"].get("approved"),
                "integration_passed": enhanced_result["integration_evaluation"].get("passed")
            }
        }

        # „É°„Éà„É™„ÇØ„Çπ„Éï„Ç°„Ç§„É´„Å´Ë®òÈå≤
        metrics_path = Path("postbox/monitoring/three_layer_verification_metrics.json")
        metrics_path.parent.mkdir(parents=True, exist_ok=True)

        try:
            metrics_history = []
            if metrics_path.exists():
                with open(metrics_path, 'r', encoding='utf-8') as f:
                    metrics_history = json.load(f)

            metrics_history.append(metrics_entry)

            # Â±•Ê≠¥„Çµ„Ç§„Ç∫Âà∂ÈôêÔºàÊúÄÊñ∞200‰ª∂Ôºâ
            if len(metrics_history) > 200:
                metrics_history = metrics_history[-200:]

            with open(metrics_path, 'w', encoding='utf-8') as f:
                json.dump(metrics_history, f, indent=2, ensure_ascii=False)

            print(f"üìä Ê§úË®º„É°„Éà„É™„ÇØ„ÇπË®òÈå≤: {metrics_path}")

        except Exception as e:
            print(f"‚ö†Ô∏è „É°„Éà„É™„ÇØ„ÇπË®òÈå≤„Ç®„É©„Éº: {e}")

    def _determine_quality_level(self, score: float) -> QualityLevel:
        """„Çπ„Ç≥„Ç¢„Åã„ÇâÂìÅË≥™„É¨„Éô„É´„ÇíÂà§ÂÆö"""
        if score >= 0.95:
            return QualityLevel.EXCELLENT
        elif score >= 0.80:
            return QualityLevel.GOOD
        elif score >= 0.60:
            return QualityLevel.ACCEPTABLE
        elif score >= 0.40:
            return QualityLevel.POOR
        else:
            return QualityLevel.CRITICAL

    # ========== Áµ±Âêà„ÉÜ„Çπ„Éà„Ç∑„Çπ„ÉÜ„É†Áµ±Âêà„É°„ÇΩ„ÉÉ„Éâ (Issue #859) ==========

    def run_integration_test_suite(self, target_files: List[str],
                                  context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Áµ±Âêà„ÉÜ„Çπ„Éà„Çπ„Ç§„Éº„ÉàÂÆüË°å

        Args:
            target_files: „ÉÜ„Çπ„ÉàÂØæË±°„Éï„Ç°„Ç§„É´„É™„Çπ„Éà
            context: „ÉÜ„Çπ„ÉàÂÆüË°å„Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà

        Returns:
            Dict[str, Any]: Áµ±Âêà„ÉÜ„Çπ„ÉàÁµêÊûú
        """

        print("üß™ Áµ±Âêà„ÉÜ„Çπ„Éà„Çπ„Ç§„Éº„ÉàÂÆüË°åÈñãÂßã")

        if not INTEGRATION_TEST_AVAILABLE or not self.integration_test_system:
            return {
                "error": "Áµ±Âêà„ÉÜ„Çπ„Éà„Ç∑„Çπ„ÉÜ„É†„ÅåÂà©Áî®„Åß„Åç„Åæ„Åõ„Çì",
                "available": False,
                "timestamp": datetime.datetime.now().isoformat()
            }

        context = context or {}

        try:
            integration_results = []

            for file_path in target_files:
                if not os.path.exists(file_path):
                    print(f"‚ö†Ô∏è „Éï„Ç°„Ç§„É´„ÅåÂ≠òÂú®„Åó„Åæ„Åõ„Çì: {file_path}")
                    continue

                # Êñ∞Ë¶èÂÆüË£ÖÁµ±Âêà„ÉÜ„Çπ„ÉàÂÆüË°å
                result = self.integration_test_system.test_new_implementation(
                    file_path, context
                )
                integration_results.append(result)

            # ‰∫íÊèõÊÄßÊ§úË®º
            compatibility_result = self.integration_test_system.verify_existing_compatibility(
                target_files
            )

            # ÂìÅË≥™Âü∫Ê∫ñÈÅ©ÂêàÁ¢∫Ë™ç
            quality_results = []
            for file_path in target_files:
                if os.path.exists(file_path):
                    quality_result = self.integration_test_system.validate_quality_standards(
                        file_path
                    )
                    quality_results.append(quality_result)

            # Á∑èÂêàË©ï‰æ°
            overall_score = self._calculate_integration_overall_score(
                integration_results, compatibility_result, quality_results
            )

            suite_result = {
                "integration_tests": [
                    {
                        "test_id": r.test_id,
                        "status": r.status.value,
                        "score": r.score,
                        "target_file": r.target_files[0] if r.target_files else "",
                        "execution_time": r.execution_time,
                        "recommendations": r.recommendations
                    } for r in integration_results
                ],
                "compatibility_check": {
                    "compatibility_score": compatibility_result.compatibility_score,
                    "backward_compatible": compatibility_result.backward_compatible,
                    "forward_compatible": compatibility_result.forward_compatible,
                    "breaking_changes": compatibility_result.breaking_changes,
                    "integration_issues": compatibility_result.integration_issues
                },
                "quality_validation": [
                    {
                        "file": target_files[i] if i < len(target_files) else "unknown",
                        "overall_score": qr.overall_quality_score,
                        "quality_level": qr.quality_level,
                        "test_coverage": qr.test_coverage,
                        "improvement_suggestions": qr.improvement_suggestions
                    } for i, qr in enumerate(quality_results)
                ],
                "overall_assessment": {
                    "overall_score": overall_score,
                    "total_tests_run": len(integration_results),
                    "tests_passed": len([r for r in integration_results if r.status.value == "pass"]),
                    "tests_failed": len([r for r in integration_results if r.status.value == "fail"]),
                    "compatibility_passed": compatibility_result.compatibility_score >= 0.75,
                    "quality_standards_met": all(qr.overall_quality_score >= 0.70 for qr in quality_results)
                },
                "timestamp": datetime.datetime.now().isoformat()
            }

            print(f"‚úÖ Áµ±Âêà„ÉÜ„Çπ„Éà„Çπ„Ç§„Éº„ÉàÂÆüË°åÂÆå‰∫Ü: Á∑èÂêà„Çπ„Ç≥„Ç¢ {overall_score:.3f}")

            return suite_result

        except Exception as e:
            print(f"‚ùå Áµ±Âêà„ÉÜ„Çπ„Éà„Çπ„Ç§„Éº„ÉàÂÆüË°å„Ç®„É©„Éº: {e}")
            return {
                "error": str(e),
                "available": True,
                "execution_failed": True,
                "timestamp": datetime.datetime.now().isoformat()
            }

    def generate_test_coverage_report(self, target_files: List[str]) -> Dict[str, Any]:
        """„ÉÜ„Çπ„Éà„Ç´„Éê„É¨„ÉÉ„Ç∏„É¨„Éù„Éº„ÉàÁîüÊàê

        Args:
            target_files: ÂØæË±°„Éï„Ç°„Ç§„É´„É™„Çπ„Éà

        Returns:
            Dict[str, Any]: „Ç´„Éê„É¨„ÉÉ„Ç∏„É¨„Éù„Éº„Éà
        """

        print("üìä „ÉÜ„Çπ„Éà„Ç´„Éê„É¨„ÉÉ„Ç∏„É¨„Éù„Éº„ÉàÁîüÊàêÈñãÂßã")

        if not INTEGRATION_TEST_AVAILABLE or not self.test_generator:
            return {
                "error": "„ÉÜ„Çπ„ÉàÁîüÊàê„Ç®„É≥„Ç∏„É≥„ÅåÂà©Áî®„Åß„Åç„Åæ„Åõ„Çì",
                "available": False
            }

        try:
            # ÂåÖÊã¨ÁöÑ„ÉÜ„Çπ„Éà„Çπ„Ç§„Éº„ÉàÁîüÊàê
            test_suite = self.test_generator.generate_comprehensive_test_suite(
                target_files, GenerationStrategy.COVERAGE_DRIVEN
            )

            # „ÉÜ„Çπ„Éà„Éï„Ç°„Ç§„É´ÁîüÊàê
            test_file_path = self.test_generator.generate_test_file(test_suite)

            coverage_report = {
                "test_suite_info": {
                    "suite_id": test_suite.suite_id,
                    "total_tests": test_suite.total_tests,
                    "estimated_coverage": test_suite.estimated_coverage,
                    "generation_strategy": test_suite.generation_strategy.value,
                    "target_modules": test_suite.target_module
                },
                "coverage_analysis": {
                    "files_analyzed": len(target_files),
                    "test_cases_generated": len(test_suite.test_cases),
                    "coverage_gaps": self._identify_coverage_gaps(target_files, test_suite),
                    "improvement_opportunities": self._suggest_coverage_improvements(test_suite)
                },
                "generated_test_file": test_file_path,
                "recommendations": [
                    "ÁîüÊàê„Åï„Çå„Åü„ÉÜ„Çπ„Éà„Éï„Ç°„Ç§„É´„ÇíÂÆüË°å„Åó„Å¶„Ç´„Éê„É¨„ÉÉ„Ç∏„ÇíÁ¢∫Ë™ç„Åó„Å¶„Åè„Å†„Åï„ÅÑ",
                    "„Ç´„Éê„É¨„ÉÉ„Ç∏„ÇÆ„É£„ÉÉ„Éó„ÇíÊâãÂãï„ÅßË£úÂÆå„Åô„Çã„Åì„Å®„ÇíÊ§úË®é„Åó„Å¶„Åè„Å†„Åï„ÅÑ",
                    "ÂÆöÊúüÁöÑ„Å´„Ç´„Éê„É¨„ÉÉ„Ç∏„É¨„Éù„Éº„Éà„ÇíÊõ¥Êñ∞„Åó„Å¶„Åè„Å†„Åï„ÅÑ"
                ],
                "timestamp": datetime.datetime.now().isoformat()
            }

            print(f"‚úÖ „ÉÜ„Çπ„Éà„Ç´„Éê„É¨„ÉÉ„Ç∏„É¨„Éù„Éº„ÉàÁîüÊàêÂÆå‰∫Ü: {test_suite.total_tests}„ÉÜ„Çπ„Éà„Ç±„Éº„Çπ")

            return coverage_report

        except Exception as e:
            print(f"‚ùå „ÉÜ„Çπ„Éà„Ç´„Éê„É¨„ÉÉ„Ç∏„É¨„Éù„Éº„ÉàÁîüÊàê„Ç®„É©„Éº: {e}")
            return {
                "error": str(e),
                "available": True,
                "generation_failed": True,
                "timestamp": datetime.datetime.now().isoformat()
            }

    def run_new_implementation_quality_check(self, implementation_path: str,
                                           context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Êñ∞Ë¶èÂÆüË£ÖÂêë„ÅëÂìÅË≥™„ÉÅ„Çß„ÉÉ„ÇØÔºàÁµ±Âêà„ÉÜ„Çπ„ÉàÂê´„ÇÄÔºâ

        Args:
            implementation_path: Êñ∞Ë¶èÂÆüË£Ö„Éï„Ç°„Ç§„É´„Éë„Çπ
            context: ÂÆüË£Ö„Ç≥„É≥„ÉÜ„Ç≠„Çπ„ÉàÔºà„Çø„Çπ„ÇØ„Çø„Ç§„Éó„ÄÅË¶Å‰ª∂Á≠âÔºâ

        Returns:
            Dict[str, Any]: ÂìÅË≥™„ÉÅ„Çß„ÉÉ„ÇØÁµêÊûú
        """

        print(f"üîç Êñ∞Ë¶èÂÆüË£ÖÂêë„ÅëÂìÅË≥™„ÉÅ„Çß„ÉÉ„ÇØÈñãÂßã: {implementation_path}")

        context = context or {}

        # 1. Âü∫Êú¨ÂìÅË≥™„ÉÅ„Çß„ÉÉ„ÇØ
        basic_quality = self.run_comprehensive_check([implementation_path], "claude")

        # 2. Áµ±Âêà„ÉÜ„Çπ„ÉàÂÆüË°å
        integration_result = None
        if INTEGRATION_TEST_AVAILABLE and self.integration_test_system:
            try:
                integration_result = self.integration_test_system.test_new_implementation(
                    implementation_path, context
                )
            except Exception as e:
                print(f"‚ö†Ô∏è Áµ±Âêà„ÉÜ„Çπ„ÉàÂÆüË°å„Ç®„É©„Éº: {e}")

        # 3. „ÉÜ„Çπ„ÉàÁîüÊàê
        test_generation_result = None
        if INTEGRATION_TEST_AVAILABLE and self.test_generator:
            try:
                test_generation_result = self.test_generator.generate_comprehensive_test_suite(
                    [implementation_path], GenerationStrategy.COMPREHENSIVE
                )
            except Exception as e:
                print(f"‚ö†Ô∏è „ÉÜ„Çπ„ÉàÁîüÊàê„Ç®„É©„Éº: {e}")

        # 4. Á∑èÂêàË©ï‰æ°
        comprehensive_result = {
            "basic_quality": {
                "overall_score": basic_quality.overall_score,
                "quality_level": basic_quality.quality_level.value,
                "error_count": basic_quality.error_count,
                "warning_count": basic_quality.warning_count,
                "improvement_suggestions": basic_quality.improvement_suggestions
            },
            "integration_test": {
                "available": integration_result is not None,
                "test_id": integration_result.test_id if integration_result else None,
                "status": integration_result.status.value if integration_result else "not_run",
                "score": integration_result.score if integration_result else 0.0,
                "execution_time": integration_result.execution_time if integration_result else 0.0,
                "recommendations": integration_result.recommendations if integration_result else []
            },
            "test_generation": {
                "available": test_generation_result is not None,
                "suite_id": test_generation_result.suite_id if test_generation_result else None,
                "total_tests": test_generation_result.total_tests if test_generation_result else 0,
                "estimated_coverage": test_generation_result.estimated_coverage if test_generation_result else 0.0
            },
            "overall_assessment": self._calculate_new_implementation_assessment(
                basic_quality, integration_result, test_generation_result, context
            ),
            "timestamp": datetime.datetime.now().isoformat()
        }

        print(f"‚úÖ Êñ∞Ë¶èÂÆüË£ÖÂêë„ÅëÂìÅË≥™„ÉÅ„Çß„ÉÉ„ÇØÂÆå‰∫Ü")

        return comprehensive_result

    def _calculate_integration_overall_score(self, integration_results: List[Any],
                                           compatibility_result: Any,
                                           quality_results: List[Any]) -> float:
        """Áµ±Âêà„ÉÜ„Çπ„ÉàÁ∑èÂêà„Çπ„Ç≥„Ç¢Ë®àÁÆó"""

        scores = []

        # Áµ±Âêà„ÉÜ„Çπ„Éà„Çπ„Ç≥„Ç¢
        if integration_results:
            integration_avg = sum(r.score for r in integration_results) / len(integration_results)
            scores.append(integration_avg * 0.4)  # 40%„ÅÆÈáç„Åø

        # ‰∫íÊèõÊÄß„Çπ„Ç≥„Ç¢
        if compatibility_result:
            scores.append(compatibility_result.compatibility_score * 0.3)  # 30%„ÅÆÈáç„Åø

        # ÂìÅË≥™„Çπ„Ç≥„Ç¢
        if quality_results:
            quality_avg = sum(qr.overall_quality_score for qr in quality_results) / len(quality_results)
            scores.append(quality_avg * 0.3)  # 30%„ÅÆÈáç„Åø

        return sum(scores) if scores else 0.0

    def _identify_coverage_gaps(self, target_files: List[str], test_suite: Any) -> List[str]:
        """„Ç´„Éê„É¨„ÉÉ„Ç∏„ÇÆ„É£„ÉÉ„ÉóÁâπÂÆö"""

        gaps = []

        for file_path in target_files:
            if not os.path.exists(file_path):
                gaps.append(f"„Éï„Ç°„Ç§„É´Êú™Â≠òÂú®: {file_path}")
                continue

            # Á∞°Êòì„ÇÆ„É£„ÉÉ„ÉóÂàÜÊûê
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                # Èñ¢Êï∞„Éª„ÇØ„É©„ÇπÊï∞„Å®„ÉÜ„Çπ„Éà„Ç±„Éº„ÇπÊï∞„ÅÆÊØîËºÉ
                function_count = content.count("def ")
                class_count = content.count("class ")

                estimated_testable_units = function_count + class_count
                actual_test_cases = len(test_suite.test_cases) if test_suite else 0

                if actual_test_cases < estimated_testable_units:
                    gaps.append(f"{file_path}: Êé®ÂÆö{estimated_testable_units}Âçò‰Ωç„Å´ÂØæ„Åó{actual_test_cases}„ÉÜ„Çπ„Éà")

            except Exception as e:
                gaps.append(f"{file_path}: ÂàÜÊûê„Ç®„É©„Éº - {str(e)}")

        return gaps

    def _suggest_coverage_improvements(self, test_suite: Any) -> List[str]:
        """„Ç´„Éê„É¨„ÉÉ„Ç∏ÊîπÂñÑÊèêÊ°à"""

        suggestions = []

        if not test_suite:
            suggestions.append("„ÉÜ„Çπ„Éà„Çπ„Ç§„Éº„Éà„ÅåÁîüÊàê„Åï„Çå„Åæ„Åõ„Çì„Åß„Åó„Åü")
            return suggestions

        if test_suite.estimated_coverage < 0.8:
            suggestions.append("Êé®ÂÆö„Ç´„Éê„É¨„ÉÉ„Ç∏„Åå80%Êú™Ê∫Ä„Åß„Åô„ÄÇËøΩÂä†„ÅÆ„ÉÜ„Çπ„Éà„Ç±„Éº„Çπ„ÇíÊ§úË®é„Åó„Å¶„Åè„Å†„Åï„ÅÑ")

        if test_suite.total_tests < 5:
            suggestions.append("„ÉÜ„Çπ„Éà„Ç±„Éº„ÇπÊï∞„ÅåÂ∞ë„Å™„Åô„Åé„Åæ„Åô„ÄÇÂ¢ÉÁïåÂÄ§„ÉÜ„Çπ„Éà„ÇÑ„Ç®„ÉÉ„Ç∏„Ç±„Éº„Çπ„ÅÆËøΩÂä†„ÇíÊé®Â•®„Åó„Åæ„Åô")

        # „ÉÜ„Çπ„Éà„Çø„Ç§„ÉóÂà•„ÅÆÊèêÊ°à
        test_types = set()
        for test_case in test_suite.test_cases:
            test_types.add(test_case.test_type.value)

        if "boundary" not in test_types:
            suggestions.append("Â¢ÉÁïåÂÄ§„ÉÜ„Çπ„Éà„ÅÆËøΩÂä†„ÇíÊé®Â•®„Åó„Åæ„Åô")

        if "integration" not in test_types:
            suggestions.append("Áµ±Âêà„ÉÜ„Çπ„Éà„ÅÆËøΩÂä†„ÇíÊé®Â•®„Åó„Åæ„Åô")

        if not suggestions:
            suggestions.append("„ÉÜ„Çπ„Éà„Ç´„Éê„É¨„ÉÉ„Ç∏„ÅØÈÅ©Âàá„Åß„Åô")

        return suggestions

    def _calculate_new_implementation_assessment(self, basic_quality: Any,
                                               integration_result: Any,
                                               test_generation_result: Any,
                                               context: Dict[str, Any]) -> Dict[str, Any]:
        """Êñ∞Ë¶èÂÆüË£ÖÁ∑èÂêàË©ï‰æ°Ë®àÁÆó"""

        # „Çπ„Ç≥„Ç¢Ë®àÁÆó
        quality_score = basic_quality.overall_score
        integration_score = integration_result.score if integration_result else 0.0
        test_generation_score = (test_generation_result.estimated_coverage
                               if test_generation_result else 0.0)

        # Èáç„Åø‰ªò„ÅçÂπ≥Âùá
        weights = {"quality": 0.5, "integration": 0.3, "test_generation": 0.2}

        overall_score = (
            quality_score * weights["quality"] +
            integration_score * weights["integration"] +
            test_generation_score * weights["test_generation"]
        )

        # Ë©ï‰æ°Âà§ÂÆö
        if overall_score >= 0.8:
            assessment = "EXCELLENT"
            recommendation = "Êñ∞Ë¶èÂÆüË£Ö„ÅØÈ´òÂìÅË≥™„Åß„Åô„ÄÇÊú¨Áï™„Éá„Éó„É≠„Ç§„ÅÆÊ∫ñÂÇô„Åå„Åß„Åç„Å¶„ÅÑ„Åæ„Åô"
        elif overall_score >= 0.7:
            assessment = "GOOD"
            recommendation = "Êñ∞Ë¶èÂÆüË£Ö„ÅØËâØÂ•Ω„Å™ÂìÅË≥™„Åß„Åô„ÄÇËªΩÂæÆ„Å™ÊîπÂñÑÂæå„Å´„Éá„Éó„É≠„Ç§ÂèØËÉΩ„Åß„Åô"
        elif overall_score >= 0.6:
            recommendation = "Êñ∞Ë¶èÂÆüË£Ö„ÅØË®±ÂÆπÁØÑÂõ≤ÂÜÖ„Åß„Åô„Åå„ÄÅÊîπÂñÑ„ÅÆ‰ΩôÂú∞„Åå„ÅÇ„Çä„Åæ„Åô"
            assessment = "ACCEPTABLE"
        else:
            assessment = "POOR"
            recommendation = "Êñ∞Ë¶èÂÆüË£Ö„ÅØÂìÅË≥™Âü∫Ê∫ñ„Çí‰∏ãÂõû„Å£„Å¶„ÅÑ„Åæ„Åô„ÄÇÂ§ßÂπÖ„Å™ÊîπÂñÑ„ÅåÂøÖË¶Å„Åß„Åô"

        # „Ç≥„É≥„ÉÜ„Ç≠„Çπ„ÉàËÄÉÊÖÆ
        task_type = context.get("task_type", "unknown")
        if task_type in ["new_implementation", "hybrid_implementation"]:
            # Êñ∞Ë¶èÂÆüË£Ö„ÅØËã•Âπ≤Âü∫Ê∫ñ„ÇíÁ∑©Âíå
            if overall_score >= 0.65:
                assessment = "ACCEPTABLE_FOR_NEW_IMPLEMENTATION"

        return {
            "overall_score": overall_score,
            "assessment": assessment,
            "recommendation": recommendation,
            "score_breakdown": {
                "quality": quality_score,
                "integration": integration_score,
                "test_generation": test_generation_score
            },
            "context_applied": task_type,
            "ready_for_deployment": overall_score >= 0.7
        }

def main() -> None:
    """„ÉÜ„Çπ„ÉàÂÆüË°å"""
    qm = QualityManager()

    test_files = [
        "kumihan_formatter/core/utilities/logger.py",
        "kumihan_formatter/config/base_config.py"
    ]

    metrics = qm.run_comprehensive_check(test_files, "claude")

    print(f"\nüìä ÂìÅË≥™„ÉÅ„Çß„ÉÉ„ÇØÁµêÊûú:")
    print(f"Á∑èÂêà„Çπ„Ç≥„Ç¢: {metrics.overall_score:.3f}")
    print(f"ÂìÅË≥™„É¨„Éô„É´: {metrics.quality_level.value}")
    print(f"„Ç®„É©„ÉºÊï∞: {metrics.error_count}")
    print(f"Ë≠¶ÂëäÊï∞: {metrics.warning_count}")

    if metrics.improvement_suggestions:
        print(f"\nüí° ÊîπÂñÑÊèêÊ°à:")
        for suggestion in metrics.improvement_suggestions:
            print(f"  {suggestion}")

if __name__ == "__main__":
    main()
