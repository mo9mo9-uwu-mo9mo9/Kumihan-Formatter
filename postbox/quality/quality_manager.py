#!/usr/bin/env python3
"""
çµ±åˆå“è³ªç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 
Claude â†” Geminiå”æ¥­ã§ã®å“è³ªä¿è¨¼çµ±ä¸€ç®¡ç†
"""

import os
import json
import datetime
import subprocess
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
from dataclasses import dataclass
from enum import Enum

class QualityLevel(Enum):
    """å“è³ªãƒ¬ãƒ™ãƒ«å®šç¾©"""
    EXCELLENT = "excellent"     # å„ªç§€ï¼ˆ95%ä»¥ä¸Šï¼‰
    GOOD = "good"              # è‰¯å¥½ï¼ˆ80-94%ï¼‰
    ACCEPTABLE = "acceptable"   # è¨±å®¹ï¼ˆ60-79%ï¼‰
    POOR = "poor"              # ä¸è‰¯ï¼ˆ40-59%ï¼‰
    CRITICAL = "critical"       # é‡å¤§ï¼ˆ40%æœªæº€ï¼‰

class CheckType(Enum):
    """ãƒã‚§ãƒƒã‚¯ç¨®åˆ¥"""
    SYNTAX = "syntax"           # æ§‹æ–‡ãƒã‚§ãƒƒã‚¯
    TYPE_CHECK = "type_check"   # å‹ãƒã‚§ãƒƒã‚¯
    LINT = "lint"              # ãƒªãƒ³ãƒˆãƒã‚§ãƒƒã‚¯
    FORMAT = "format"          # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãƒã‚§ãƒƒã‚¯
    SECURITY = "security"       # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒã‚§ãƒƒã‚¯
    PERFORMANCE = "performance" # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒã‚§ãƒƒã‚¯
    TEST = "test"              # ãƒ†ã‚¹ãƒˆãƒã‚§ãƒƒã‚¯

@dataclass
class QualityMetrics:
    """å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    overall_score: float        # ç·åˆã‚¹ã‚³ã‚¢ (0.0-1.0)
    syntax_score: float        # æ§‹æ–‡å“è³ª
    type_score: float          # å‹å®‰å…¨æ€§
    lint_score: float          # ã‚³ãƒ¼ãƒ‰å“è³ª
    format_score: float        # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå“è³ª
    security_score: float      # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å“è³ª
    performance_score: float   # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å“è³ª
    test_coverage: float       # ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸

    error_count: int           # ã‚¨ãƒ©ãƒ¼ç·æ•°
    warning_count: int         # è­¦å‘Šç·æ•°
    fixed_count: int           # ä¿®æ­£æ¸ˆã¿æ•°

    quality_level: QualityLevel
    improvement_suggestions: List[str]

@dataclass
class QualityResult:
    """å“è³ªãƒã‚§ãƒƒã‚¯çµæœ"""
    check_type: CheckType
    passed: bool
    score: float
    errors: List[str]
    warnings: List[str]
    details: Dict[str, Any]
    execution_time: float

class QualityManager:
    """çµ±åˆå“è³ªç®¡ç†ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        self.standards_path = Path("postbox/quality/standards.json")
        self.results_path = Path("postbox/monitoring/quality_results.json")
        self.history_path = Path("postbox/monitoring/quality_history.json")

        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        for path in [self.standards_path, self.results_path, self.history_path]:
            path.parent.mkdir(parents=True, exist_ok=True)

        self.standards = self._load_standards()
        self.thresholds = self.standards.get("thresholds", {})

        print("ğŸ¯ QualityManager åˆæœŸåŒ–å®Œäº†")

    def run_comprehensive_check(self, target_files: List[str],
                              ai_agent: str = "claude") -> QualityMetrics:
        """åŒ…æ‹¬çš„å“è³ªãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ"""

        print(f"ğŸ” åŒ…æ‹¬çš„å“è³ªãƒã‚§ãƒƒã‚¯é–‹å§‹ ({ai_agent}) - å¯¾è±¡: {len(target_files)}ãƒ•ã‚¡ã‚¤ãƒ«")

        start_time = datetime.datetime.now()
        results = {}

        # å„ç¨®ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ
        for check_type in CheckType:
            print(f"  ğŸ“‹ {check_type.value} ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œä¸­...")
            result = self._run_quality_check(check_type, target_files)
            results[check_type.value] = result

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        metrics = self._calculate_metrics(results, target_files)

        # çµæœä¿å­˜
        execution_time = (datetime.datetime.now() - start_time).total_seconds()
        self._save_quality_result(metrics, ai_agent, execution_time, target_files)

        # æ”¹å–„ææ¡ˆç”Ÿæˆ
        metrics.improvement_suggestions = self._generate_improvements(metrics, results)

        print(f"âœ… å“è³ªãƒã‚§ãƒƒã‚¯å®Œäº†: {metrics.quality_level.value} (ã‚¹ã‚³ã‚¢: {metrics.overall_score:.3f})")

        return metrics

    def _run_quality_check(self, check_type: CheckType, target_files: List[str]) -> QualityResult:
        """å€‹åˆ¥å“è³ªãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ"""

        start_time = datetime.datetime.now()

        try:
            if check_type == CheckType.SYNTAX:
                return self._check_syntax(target_files)
            elif check_type == CheckType.TYPE_CHECK:
                return self._check_types(target_files)
            elif check_type == CheckType.LINT:
                return self._check_lint(target_files)
            elif check_type == CheckType.FORMAT:
                return self._check_format(target_files)
            elif check_type == CheckType.SECURITY:
                return self._check_security(target_files)
            elif check_type == CheckType.PERFORMANCE:
                return self._check_performance(target_files)
            elif check_type == CheckType.TEST:
                return self._check_tests(target_files)
            else:
                raise ValueError(f"æœªå¯¾å¿œã®ãƒã‚§ãƒƒã‚¯ç¨®åˆ¥: {check_type}")

        except Exception as e:
            execution_time = (datetime.datetime.now() - start_time).total_seconds()
            return QualityResult(
                check_type=check_type,
                passed=False,
                score=0.0,
                errors=[f"ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}"],
                warnings=[],
                details={"error": str(e)},
                execution_time=execution_time
            )

    def _check_syntax(self, target_files: List[str]) -> QualityResult:
        """æ§‹æ–‡ãƒã‚§ãƒƒã‚¯"""
        errors = []
        warnings = []
        total_files = len(target_files)
        passed_files = 0

        for file_path in target_files:
            try:
                result = subprocess.run(
                    ["python3", "-m", "py_compile", file_path],
                    capture_output=True,
                    text=True
                )

                if result.returncode == 0:
                    passed_files += 1
                else:
                    errors.append(f"{file_path}: {result.stderr.strip()}")

            except Exception as e:
                errors.append(f"{file_path}: æ§‹æ–‡ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œã‚¨ãƒ©ãƒ¼ - {str(e)}")

        score = passed_files / total_files if total_files > 0 else 1.0

        return QualityResult(
            check_type=CheckType.SYNTAX,
            passed=len(errors) == 0,
            score=score,
            errors=errors,
            warnings=warnings,
            details={"passed_files": passed_files, "total_files": total_files},
            execution_time=0.0
        )

    def _check_types(self, target_files: List[str]) -> QualityResult:
        """å‹ãƒã‚§ãƒƒã‚¯ (mypy)"""
        errors = []
        warnings = []

        try:
            # ã¾ãšå€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯
            for file_path in target_files:
                result = subprocess.run(
                    ["python3", "-m", "mypy", "--strict", file_path],
                    capture_output=True,
                    text=True
                )

                for line in result.stdout.split('\n'):
                    if 'error:' in line:
                        errors.append(line.strip())
                    elif 'warning:' in line or 'note:' in line:
                        warnings.append(line.strip())

            # ã‚¹ã‚³ã‚¢è¨ˆç®— (ã‚¨ãƒ©ãƒ¼ç‡ãƒ™ãƒ¼ã‚¹)
            total_lines = sum(self._count_file_lines(f) for f in target_files if os.path.exists(f))
            error_rate = len(errors) / max(total_lines, 1) if total_lines > 0 else 1.0
            score = max(0.0, 1.0 - error_rate)

        except Exception as e:
            errors.append(f"mypyå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}")
            score = 0.0

        return QualityResult(
            check_type=CheckType.TYPE_CHECK,
            passed=len(errors) == 0,
            score=score,
            errors=errors,
            warnings=warnings,
            details={"total_lines": total_lines, "error_rate": error_rate},
            execution_time=0.0
        )

    def _check_lint(self, target_files: List[str]) -> QualityResult:
        """ãƒªãƒ³ãƒˆãƒã‚§ãƒƒã‚¯ (flake8)"""
        errors = []
        warnings = []

        try:
            for file_path in target_files:
                result = subprocess.run(
                    ["python3", "-m", "flake8", file_path],
                    capture_output=True,
                    text=True
                )

                for line in result.stdout.split('\n'):
                    if line.strip():
                        if any(code in line for code in ['E', 'F']):  # Error codes
                            errors.append(line.strip())
                        else:  # Warnings
                            warnings.append(line.strip())

            # ã‚¹ã‚³ã‚¢è¨ˆç®—
            total_files = len(target_files)
            error_rate = len(errors) / max(total_files, 1)
            score = max(0.0, 1.0 - error_rate)

        except Exception as e:
            errors.append(f"flake8å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}")
            score = 0.0

        return QualityResult(
            check_type=CheckType.LINT,
            passed=len(errors) == 0,
            score=score,
            errors=errors,
            warnings=warnings,
            details={"error_rate": error_rate},
            execution_time=0.0
        )

    def _check_format(self, target_files: List[str]) -> QualityResult:
        """ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãƒã‚§ãƒƒã‚¯ (black)"""
        errors = []
        warnings = []

        try:
            for file_path in target_files:
                result = subprocess.run(
                    ["python3", "-m", "black", "--check", file_path],
                    capture_output=True,
                    text=True
                )

                if result.returncode != 0:
                    errors.append(f"{file_path}: ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆä¸é©åˆ")

            score = 1.0 - (len(errors) / max(len(target_files), 1))

        except Exception as e:
            errors.append(f"blackå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}")
            score = 0.0

        return QualityResult(
            check_type=CheckType.FORMAT,
            passed=len(errors) == 0,
            score=score,
            errors=errors,
            warnings=warnings,
            details={},
            execution_time=0.0
        )

    def _check_security(self, target_files: List[str]) -> QualityResult:
        """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒã‚§ãƒƒã‚¯ (åŸºæœ¬çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°)"""
        errors = []
        warnings = []

        # å±é™ºãªãƒ‘ã‚¿ãƒ¼ãƒ³
        dangerous_patterns = [
            r"eval\s*\(",
            r"exec\s*\(",
            r"subprocess\.call\s*\(",
            r"os\.system\s*\(",
            r"shell=True",
            r"password\s*=\s*[\"']",
            r"secret\s*=\s*[\"']",
            r"api_key\s*=\s*[\"']"
        ]

        import re

        for file_path in target_files:
            if os.path.exists(file_path):
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()

                    for pattern in dangerous_patterns:
                        matches = re.findall(pattern, content, re.IGNORECASE)
                        if matches:
                            warnings.append(f"{file_path}: æ½œåœ¨çš„ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒªã‚¹ã‚¯ - {pattern}")

                except Exception as e:
                    errors.append(f"{file_path}: ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼ - {str(e)}")

        # ã‚¹ã‚³ã‚¢è¨ˆç®—
        total_issues = len(errors) + len(warnings)
        score = max(0.0, 1.0 - (total_issues / max(len(target_files), 1)))

        return QualityResult(
            check_type=CheckType.SECURITY,
            passed=len(errors) == 0,
            score=score,
            errors=errors,
            warnings=warnings,
            details={"total_issues": total_issues},
            execution_time=0.0
        )

    def _check_performance(self, target_files: List[str]) -> QualityResult:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒã‚§ãƒƒã‚¯ (åŸºæœ¬çš„ãªæœ€é©åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³)"""
        warnings = []
        errors = []

        performance_issues = [
            r"for\s+\w+\s+in\s+range\(len\(",  # range(len()) ãƒ‘ã‚¿ãƒ¼ãƒ³
            r"\.append\s*\(\s*\)\s*\n.*for",   # éåŠ¹ç‡ãªãƒ«ãƒ¼ãƒ—
            r"time\.sleep\s*\(\s*[0-9]+\s*\)"  # é•·ã„sleep
        ]

        import re

        for file_path in target_files:
            if os.path.exists(file_path):
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()

                    for pattern in performance_issues:
                        if re.search(pattern, content):
                            warnings.append(f"{file_path}: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„å¯èƒ½ - {pattern}")

                except Exception as e:
                    errors.append(f"{file_path}: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼ - {str(e)}")

        score = max(0.0, 1.0 - (len(warnings) / max(len(target_files), 1)))

        return QualityResult(
            check_type=CheckType.PERFORMANCE,
            passed=len(errors) == 0,
            score=score,
            errors=errors,
            warnings=warnings,
            details={},
            execution_time=0.0
        )

    def _check_tests(self, target_files: List[str]) -> QualityResult:
        """ãƒ†ã‚¹ãƒˆãƒã‚§ãƒƒã‚¯"""
        errors = []
        warnings = []

        # ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        test_files = []
        source_files = []

        for file_path in target_files:
            if 'test' in file_path.lower():
                test_files.append(file_path)
            else:
                source_files.append(file_path)

        # ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸è¨ˆç®—
        coverage_ratio = len(test_files) / max(len(source_files), 1) if source_files else 1.0

        if coverage_ratio < 0.3:
            warnings.append(f"ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ä¸è¶³: {coverage_ratio:.1%}")

        score = min(1.0, coverage_ratio)

        return QualityResult(
            check_type=CheckType.TEST,
            passed=coverage_ratio >= 0.5,
            score=score,
            errors=errors,
            warnings=warnings,
            details={"coverage_ratio": coverage_ratio, "test_files": len(test_files)},
            execution_time=0.0
        )

    def _calculate_metrics(self, results: Dict[str, QualityResult],
                          target_files: List[str]) -> QualityMetrics:
        """å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—"""

        # å„ç¨®ã‚¹ã‚³ã‚¢é›†è¨ˆ
        scores = {}
        total_errors = 0
        total_warnings = 0

        for check_type, result in results.items():
            scores[check_type] = result.score
            total_errors += len(result.errors)
            total_warnings += len(result.warnings)

        # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®— (é‡ã¿ä»˜ãå¹³å‡)
        weights = {
            "syntax": 0.2,
            "type_check": 0.25,
            "lint": 0.2,
            "format": 0.1,
            "security": 0.15,
            "performance": 0.05,
            "test": 0.05
        }

        overall_score = sum(scores.get(k, 0) * w for k, w in weights.items())

        # å“è³ªãƒ¬ãƒ™ãƒ«æ±ºå®š
        if overall_score >= 0.95:
            quality_level = QualityLevel.EXCELLENT
        elif overall_score >= 0.80:
            quality_level = QualityLevel.GOOD
        elif overall_score >= 0.60:
            quality_level = QualityLevel.ACCEPTABLE
        elif overall_score >= 0.40:
            quality_level = QualityLevel.POOR
        else:
            quality_level = QualityLevel.CRITICAL

        return QualityMetrics(
            overall_score=overall_score,
            syntax_score=scores.get("syntax", 0),
            type_score=scores.get("type_check", 0),
            lint_score=scores.get("lint", 0),
            format_score=scores.get("format", 0),
            security_score=scores.get("security", 0),
            performance_score=scores.get("performance", 0),
            test_coverage=scores.get("test", 0),
            error_count=total_errors,
            warning_count=total_warnings,
            fixed_count=0,  # ä¿®æ­£ã‚·ã‚¹ãƒ†ãƒ ã¨é€£æºæ™‚ã«æ›´æ–°
            quality_level=quality_level,
            improvement_suggestions=[]
        )

    def _generate_improvements(self, metrics: QualityMetrics,
                             results: Dict[str, QualityResult]) -> List[str]:
        """æ”¹å–„ææ¡ˆç”Ÿæˆ"""

        suggestions = []

        # ã‚¹ã‚³ã‚¢åˆ¥æ”¹å–„ææ¡ˆ
        if metrics.syntax_score < 0.8:
            suggestions.append("ğŸ”§ æ§‹æ–‡ã‚¨ãƒ©ãƒ¼ã®ä¿®æ­£ãŒå¿…è¦ã§ã™")

        if metrics.type_score < 0.7:
            suggestions.append("ğŸ“ å‹æ³¨é‡ˆã®è¿½åŠ ãƒ»ä¿®æ­£ã‚’æ¨å¥¨ã—ã¾ã™")

        if metrics.lint_score < 0.8:
            suggestions.append("ğŸ¯ ã‚³ãƒ¼ãƒ‰å“è³ªã®æ”¹å–„ãŒå¿…è¦ã§ã™ (flake8)")

        if metrics.format_score < 0.9:
            suggestions.append("ğŸ’… ã‚³ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®çµ±ä¸€ãŒå¿…è¦ã§ã™ (black)")

        if metrics.security_score < 0.9:
            suggestions.append("ğŸ”’ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒªã‚¹ã‚¯ã®ç¢ºèªãƒ»ä¿®æ­£ã‚’æ¨å¥¨ã—ã¾ã™")

        if metrics.performance_score < 0.8:
            suggestions.append("âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚’æ¤œè¨ã—ã¦ãã ã•ã„")

        if metrics.test_coverage < 0.5:
            suggestions.append("ğŸ§ª ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ã®å‘ä¸ŠãŒå¿…è¦ã§ã™")

        # ç·åˆçš„ãªææ¡ˆ
        if metrics.overall_score < 0.6:
            suggestions.append("ğŸš¨ ç·åˆå“è³ªãŒåŸºæº–ã‚’ä¸‹å›ã£ã¦ã„ã¾ã™ã€‚å„ªå…ˆçš„ãªæ”¹å–„ãŒå¿…è¦ã§ã™")

        return suggestions

    def _save_quality_result(self, metrics: QualityMetrics, ai_agent: str,
                           execution_time: float, target_files: List[str]) -> None:
        """å“è³ªçµæœä¿å­˜"""

        result_entry = {
            "timestamp": datetime.datetime.now().isoformat(),
            "ai_agent": ai_agent,
            "execution_time": execution_time,
            "target_files": target_files,
            "metrics": {
                "overall_score": metrics.overall_score,
                "quality_level": metrics.quality_level.value,
                "error_count": metrics.error_count,
                "warning_count": metrics.warning_count,
                "scores": {
                    "syntax": metrics.syntax_score,
                    "type_check": metrics.type_score,
                    "lint": metrics.lint_score,
                    "format": metrics.format_score,
                    "security": metrics.security_score,
                    "performance": metrics.performance_score,
                    "test": metrics.test_coverage
                }
            }
        }

        # å±¥æ­´æ›´æ–°
        history = []
        if self.history_path.exists():
            try:
                with open(self.history_path, 'r', encoding='utf-8') as f:
                    history = json.load(f)
            except:
                history = []

        history.append(result_entry)

        # å±¥æ­´ã‚µã‚¤ã‚ºåˆ¶é™
        if len(history) > 50:
            history = history[-50:]

        with open(self.history_path, 'w', encoding='utf-8') as f:
            json.dump(history, f, indent=2, ensure_ascii=False)

    def _count_file_lines(self, file_path: str) -> int:
        """ãƒ•ã‚¡ã‚¤ãƒ«è¡Œæ•°ã‚«ã‚¦ãƒ³ãƒˆ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return len(f.readlines())
        except:
            return 0

    def _load_standards(self) -> Dict[str, Any]:
        """å“è³ªåŸºæº–èª­ã¿è¾¼ã¿"""

        default_standards = {
            "thresholds": {
                "minimum_overall_score": 0.7,
                "minimum_type_score": 0.8,
                "minimum_lint_score": 0.8,
                "maximum_error_count": 10,
                "minimum_test_coverage": 0.5
            },
            "weights": {
                "syntax": 0.2,
                "type_check": 0.25,
                "lint": 0.2,
                "format": 0.1,
                "security": 0.15,
                "performance": 0.05,
                "test": 0.05
            },
            "automation": {
                "auto_fix_format": True,
                "auto_fix_simple_types": True,
                "auto_fix_imports": True
            }
        }

        if self.standards_path.exists():
            try:
                with open(self.standards_path, 'r', encoding='utf-8') as f:
                    user_standards = json.load(f)
                default_standards.update(user_standards)
            except:
                pass
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šä¿å­˜
            with open(self.standards_path, 'w', encoding='utf-8') as f:
                json.dump(default_standards, f, indent=2, ensure_ascii=False)

        return default_standards

    def get_quality_report(self) -> Dict[str, Any]:
        """å“è³ªãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""

        if not self.history_path.exists():
            return {"error": "å“è³ªå±¥æ­´ãƒ‡ãƒ¼ã‚¿ãªã—"}

        try:
            with open(self.history_path, 'r', encoding='utf-8') as f:
                history = json.load(f)

            if not history:
                return {"error": "å“è³ªå±¥æ­´ãƒ‡ãƒ¼ã‚¿ãªã—"}

            # çµ±è¨ˆè¨ˆç®—
            recent_results = history[-10:]  # æœ€æ–°10ä»¶

            avg_overall = sum(r["metrics"]["overall_score"] for r in recent_results) / len(recent_results)
            avg_errors = sum(r["metrics"]["error_count"] for r in recent_results) / len(recent_results)

            quality_trend = []
            if len(history) >= 5:
                for i in range(len(history) - 4):
                    batch = history[i:i+5]
                    batch_avg = sum(r["metrics"]["overall_score"] for r in batch) / 5
                    quality_trend.append(batch_avg)

            return {
                "current_quality": {
                    "overall_score": recent_results[-1]["metrics"]["overall_score"],
                    "quality_level": recent_results[-1]["metrics"]["quality_level"],
                    "error_count": recent_results[-1]["metrics"]["error_count"]
                },
                "trends": {
                    "average_overall_score": avg_overall,
                    "average_error_count": avg_errors,
                    "quality_trend": quality_trend
                },
                "recommendations": self._generate_recommendations(recent_results),
                "total_checks": len(history)
            }

        except Exception as e:
            return {"error": f"ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}"}

    def _generate_recommendations(self, recent_results: List[Dict]) -> List[str]:
        """æ”¹å–„æ¨å¥¨äº‹é …ç”Ÿæˆ"""

        recommendations = []

        # å‚¾å‘åˆ†æ
        if len(recent_results) >= 3:
            scores = [r["metrics"]["overall_score"] for r in recent_results[-3:]]
            if all(scores[i] > scores[i+1] for i in range(len(scores)-1)):
                recommendations.append("ğŸ“‰ å“è³ªã‚¹ã‚³ã‚¢ãŒä½ä¸‹å‚¾å‘ã«ã‚ã‚Šã¾ã™ã€‚æ”¹å–„ãŒå¿…è¦ã§ã™")
            elif all(scores[i] < scores[i+1] for i in range(len(scores)-1)):
                recommendations.append("ğŸ“ˆ å“è³ªã‚¹ã‚³ã‚¢ãŒå‘ä¸Šã—ã¦ã„ã¾ã™ã€‚å¼•ãç¶šãç¶­æŒã—ã¦ãã ã•ã„")

        # ã‚¨ãƒ©ãƒ¼ç‡åˆ†æ
        recent_errors = [r["metrics"]["error_count"] for r in recent_results]
        avg_errors = sum(recent_errors) / len(recent_errors)

        if avg_errors > 20:
            recommendations.append("ğŸš¨ ã‚¨ãƒ©ãƒ¼æ•°ãŒå¤šã™ãã¾ã™ã€‚åŸºæœ¬çš„ãªå“è³ªãƒã‚§ãƒƒã‚¯ã‚’å¼·åŒ–ã—ã¦ãã ã•ã„")
        elif avg_errors > 10:
            recommendations.append("âš ï¸ ã‚¨ãƒ©ãƒ¼æ•°ãŒã‚„ã‚„å¤šã‚ã§ã™ã€‚å®šæœŸçš„ãªå“è³ªãƒã‚§ãƒƒã‚¯ã‚’æ¨å¥¨ã—ã¾ã™")

        return recommendations

    # =========================
    # 3å±¤æ¤œè¨¼ä½“åˆ¶å°‚ç”¨ãƒ¡ã‚½ãƒƒãƒ‰
    # =========================

    def validate_syntax(self, target_files: List[str]) -> Dict[str, Any]:
        """Layer 1: æ§‹æ–‡æ¤œè¨¼ï¼ˆ3å±¤æ¤œè¨¼ä½“åˆ¶ï¼‰"""

        print("ğŸ” Layer 1: æ§‹æ–‡æ¤œè¨¼é–‹å§‹...")

        # æ§‹æ–‡ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ
        syntax_result = self._check_syntax(target_files)

        # åŸºæœ¬å“è³ªãƒã‚§ãƒƒã‚¯
        type_result = self._check_types(target_files)

        # æ¤œè¨¼çµæœçµ±åˆ
        validation_passed = (
            syntax_result.passed and
            type_result.score >= 0.7  # å‹ãƒã‚§ãƒƒã‚¯70%ä»¥ä¸Š
        )

        result = {
            "layer": 1,
            "validation_type": "syntax_validation",
            "passed": validation_passed,
            "syntax_check": {
                "passed": syntax_result.passed,
                "score": syntax_result.score,
                "errors": syntax_result.error_count,
                "details": syntax_result.details
            },
            "type_check": {
                "passed": type_result.passed,
                "score": type_result.score,
                "errors": type_result.error_count,
                "details": type_result.details
            },
            "summary": {
                "total_files": len(target_files),
                "syntax_errors": syntax_result.error_count,
                "type_errors": type_result.error_count,
                "validation_passed": validation_passed
            },
            "next_layer_recommended": validation_passed,
            "timestamp": datetime.datetime.now().isoformat()
        }

        print(f"âœ… Layer 1å®Œäº†: {'PASS' if validation_passed else 'FAIL'}")
        print(f"   æ§‹æ–‡ã‚¨ãƒ©ãƒ¼: {syntax_result.error_count}ä»¶")
        print(f"   å‹ã‚¨ãƒ©ãƒ¼: {type_result.error_count}ä»¶")

        return result

    def check_code_quality(self, target_files: List[str], layer1_result: Dict[str, Any] = None) -> Dict[str, Any]:
        """Layer 2: å“è³ªæ¤œè¨¼ï¼ˆ3å±¤æ¤œè¨¼ä½“åˆ¶ï¼‰"""

        print("ğŸ›¡ï¸ Layer 2: å“è³ªæ¤œè¨¼é–‹å§‹...")

        # Layer 1ã®çµæœç¢ºèª
        if layer1_result and not layer1_result.get("passed", False):
            return {
                "layer": 2,
                "validation_type": "quality_validation",
                "passed": False,
                "skipped": True,
                "reason": "Layer 1æ§‹æ–‡æ¤œè¨¼ãŒå¤±æ•—ã—ãŸãŸã‚å“è³ªæ¤œè¨¼ã‚’ã‚¹ã‚­ãƒƒãƒ—",
                "timestamp": datetime.datetime.now().isoformat()
            }

        # åŒ…æ‹¬çš„å“è³ªãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ
        lint_result = self._check_lint(target_files)
        format_result = self._check_format(target_files)
        security_result = self._check_security(target_files)
        performance_result = self._check_performance(target_files)
        test_result = self._check_tests(target_files)

        # å“è³ªåŸºæº–åˆ¤å®š
        quality_scores = {
            "lint": lint_result.score,
            "format": format_result.score,
            "security": security_result.score,
            "performance": performance_result.score,
            "test": test_result.score
        }

        overall_quality_score = sum(quality_scores.values()) / len(quality_scores)
        quality_passed = overall_quality_score >= 0.75  # 75%ä»¥ä¸Šã§åˆæ ¼

        result = {
            "layer": 2,
            "validation_type": "quality_validation",
            "passed": quality_passed,
            "overall_quality_score": overall_quality_score,
            "quality_checks": {
                "lint": {
                    "passed": lint_result.passed,
                    "score": lint_result.score,
                    "warnings": lint_result.warning_count
                },
                "format": {
                    "passed": format_result.passed,
                    "score": format_result.score,
                    "issues": format_result.error_count
                },
                "security": {
                    "passed": security_result.passed,
                    "score": security_result.score,
                    "vulnerabilities": security_result.error_count
                },
                "performance": {
                    "passed": performance_result.passed,
                    "score": performance_result.score,
                    "bottlenecks": performance_result.warning_count
                },
                "test": {
                    "passed": test_result.passed,
                    "score": test_result.score,
                    "coverage": test_result.score * 100
                }
            },
            "summary": {
                "total_files": len(target_files),
                "quality_level": self._determine_quality_level(overall_quality_score),
                "claude_review_recommended": quality_passed
            },
            "next_layer_recommended": quality_passed,
            "timestamp": datetime.datetime.now().isoformat()
        }

        print(f"âœ… Layer 2å®Œäº†: {'PASS' if quality_passed else 'FAIL'}")
        print(f"   ç·åˆå“è³ªã‚¹ã‚³ã‚¢: {overall_quality_score:.3f}")
        print(f"   å“è³ªãƒ¬ãƒ™ãƒ«: {self._determine_quality_level(overall_quality_score).value}")

        return result

    def claude_final_approval(self, target_files: List[str], layer1_result: Dict[str, Any] = None,
                            layer2_result: Dict[str, Any] = None, context: Dict[str, Any] = None) -> Dict[str, Any]:
        """Layer 3: Claudeæœ€çµ‚æ‰¿èªï¼ˆ3å±¤æ¤œè¨¼ä½“åˆ¶ï¼‰"""

        print("ğŸ‘¨â€ğŸ’» Layer 3: Claudeæœ€çµ‚æ‰¿èªé–‹å§‹...")

        context = context or {}

        # å‰å±¤ã®çµæœç¢ºèª
        layer1_passed = layer1_result.get("passed", False) if layer1_result else False
        layer2_passed = layer2_result.get("passed", False) if layer2_result else False

        if not (layer1_passed and layer2_passed):
            return {
                "layer": 3,
                "validation_type": "claude_final_approval",
                "approved": False,
                "skipped": True,
                "reason": "å‰å±¤ã®æ¤œè¨¼ãŒæœªå®Œäº†ã¾ãŸã¯å¤±æ•—ã®ãŸã‚æœ€çµ‚æ‰¿èªã‚’ã‚¹ã‚­ãƒƒãƒ—",
                "layer1_passed": layer1_passed,
                "layer2_passed": layer2_passed,
                "timestamp": datetime.datetime.now().isoformat()
            }

        # Claudeå“è³ªåŸºæº–ã«ã‚ˆã‚‹æœ€çµ‚ãƒã‚§ãƒƒã‚¯
        final_metrics = self.run_comprehensive_check(target_files, "claude")

        # æœ€çµ‚æ‰¿èªåˆ¤å®šåŸºæº–
        approval_criteria = {
            "minimum_overall_score": 0.80,  # 80%ä»¥ä¸Š
            "maximum_critical_errors": 0,   # é‡å¤§ã‚¨ãƒ©ãƒ¼0ä»¶
            "minimum_test_coverage": 0.70,  # ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸70%ä»¥ä¸Š
            "maximum_security_issues": 0    # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å•é¡Œ0ä»¶
        }

        # åˆ¤å®šå®Ÿè¡Œ
        approval_checks = {
            "overall_score_check": final_metrics.overall_score >= approval_criteria["minimum_overall_score"],
            "critical_errors_check": final_metrics.error_count <= approval_criteria["maximum_critical_errors"],
            "test_coverage_check": final_metrics.test_coverage >= approval_criteria["minimum_test_coverage"],
            "security_check": final_metrics.security_score >= 0.95  # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã¯95%ä»¥ä¸Š
        }

        final_approved = all(approval_checks.values())

        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè€ƒæ…®ï¼ˆæ–°è¦å®Ÿè£…ã®å ´åˆã®ç‰¹åˆ¥åŸºæº–ï¼‰
        if context.get("task_type") in ["new_implementation", "hybrid_implementation", "new_feature_development"]:
            # æ–°è¦å®Ÿè£…ã¯åŸºæº–ã‚’è‹¥å¹²ç·©å’Œ
            if final_metrics.overall_score >= 0.75 and final_metrics.error_count <= 2:
                final_approved = True
                approval_checks["new_implementation_exception"] = True

        result = {
            "layer": 3,
            "validation_type": "claude_final_approval",
            "approved": final_approved,
            "final_metrics": {
                "overall_score": final_metrics.overall_score,
                "quality_level": final_metrics.quality_level.value,
                "error_count": final_metrics.error_count,
                "warning_count": final_metrics.warning_count,
                "test_coverage": final_metrics.test_coverage,
                "security_score": final_metrics.security_score
            },
            "approval_criteria": approval_criteria,
            "approval_checks": approval_checks,
            "context_considerations": {
                "task_type": context.get("task_type", "unknown"),
                "special_criteria_applied": "new_implementation_exception" in approval_checks
            },
            "recommendations": final_metrics.improvement_suggestions,
            "summary": {
                "layer1_passed": layer1_passed,
                "layer2_passed": layer2_passed,
                "layer3_approved": final_approved,
                "three_layer_verification_complete": final_approved
            },
            "timestamp": datetime.datetime.now().isoformat()
        }

        print(f"âœ… Layer 3å®Œäº†: {'APPROVED' if final_approved else 'REJECTED'}")
        print(f"   æœ€çµ‚å“è³ªã‚¹ã‚³ã‚¢: {final_metrics.overall_score:.3f}")
        print(f"   3å±¤æ¤œè¨¼çµæœ: {'å®Œå…¨é€šé' if final_approved else 'è¦æ”¹å–„'}")

        return result

    def _determine_quality_level(self, score: float) -> QualityLevel:
        """ã‚¹ã‚³ã‚¢ã‹ã‚‰å“è³ªãƒ¬ãƒ™ãƒ«ã‚’åˆ¤å®š"""
        if score >= 0.95:
            return QualityLevel.EXCELLENT
        elif score >= 0.80:
            return QualityLevel.GOOD
        elif score >= 0.60:
            return QualityLevel.ACCEPTABLE
        elif score >= 0.40:
            return QualityLevel.POOR
        else:
            return QualityLevel.CRITICAL

def main():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    qm = QualityManager()

    test_files = [
        "kumihan_formatter/core/utilities/logger.py",
        "kumihan_formatter/config/base_config.py"
    ]

    metrics = qm.run_comprehensive_check(test_files, "claude")

    print(f"\nğŸ“Š å“è³ªãƒã‚§ãƒƒã‚¯çµæœ:")
    print(f"ç·åˆã‚¹ã‚³ã‚¢: {metrics.overall_score:.3f}")
    print(f"å“è³ªãƒ¬ãƒ™ãƒ«: {metrics.quality_level.value}")
    print(f"ã‚¨ãƒ©ãƒ¼æ•°: {metrics.error_count}")
    print(f"è­¦å‘Šæ•°: {metrics.warning_count}")

    if metrics.improvement_suggestions:
        print(f"\nğŸ’¡ æ”¹å–„ææ¡ˆ:")
        for suggestion in metrics.improvement_suggestions:
            print(f"  {suggestion}")

if __name__ == "__main__":
    main()
