#!/usr/bin/env python3
"""
çµ±åˆå“è³ªç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 
Claude â†” Geminiå”æ¥­ã§ã®å“è³ªä¿è¨¼çµ±ä¸€ç®¡ç†
"""

import os
import json
import datetime
import subprocess
from typing import Dict, List, Any, Optional, Tuple, Union
from pathlib import Path
from dataclasses import dataclass
from enum import Enum

# çµ±åˆãƒ†ã‚¹ãƒˆã‚·ã‚¹ãƒ†ãƒ çµ±åˆ (Issue #859)
try:
    from .integration_test_system import IntegrationTestSystem, IntegrationTestResult
    from .test_generator import TestGeneratorEngine, TestSuite, GenerationStrategy
    INTEGRATION_TEST_AVAILABLE = True
except ImportError:
    print("âš ï¸ çµ±åˆãƒ†ã‚¹ãƒˆã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ")
    INTEGRATION_TEST_AVAILABLE = False

class QualityLevel(Enum):
    """å“è³ªãƒ¬ãƒ™ãƒ«å®šç¾©"""
    EXCELLENT = "excellent"     # å„ªç§€ï¼ˆ95%ä»¥ä¸Šï¼‰
    GOOD = "good"              # è‰¯å¥½ï¼ˆ80-94%ï¼‰
    ACCEPTABLE = "acceptable"   # è¨±å®¹ï¼ˆ60-79%ï¼‰
    POOR = "poor"              # ä¸è‰¯ï¼ˆ40-59%ï¼‰
    CRITICAL = "critical"       # é‡å¤§ï¼ˆ40%æœªæº€ï¼‰

class CheckType(Enum):
    """ãƒã‚§ãƒƒã‚¯ç¨®åˆ¥"""
    SYNTAX = "syntax"           # æ§‹æ–‡ãƒã‚§ãƒƒã‚¯
    TYPE_CHECK = "type_check"   # å‹ãƒã‚§ãƒƒã‚¯
    LINT = "lint"              # ãƒªãƒ³ãƒˆãƒã‚§ãƒƒã‚¯
    FORMAT = "format"          # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãƒã‚§ãƒƒã‚¯
    SECURITY = "security"       # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒã‚§ãƒƒã‚¯
    PERFORMANCE = "performance" # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒã‚§ãƒƒã‚¯
    TEST = "test"              # ãƒ†ã‚¹ãƒˆãƒã‚§ãƒƒã‚¯

@dataclass
class QualityMetrics:
    """å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    overall_score: float        # ç·åˆã‚¹ã‚³ã‚¢ (0.0-1.0)
    syntax_score: float        # æ§‹æ–‡å“è³ª
    type_score: float          # å‹å®‰å…¨æ€§
    lint_score: float          # ã‚³ãƒ¼ãƒ‰å“è³ª
    format_score: float        # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå“è³ª
    security_score: float      # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å“è³ª
    performance_score: float   # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å“è³ª
    test_coverage: float       # ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸

    error_count: int           # ã‚¨ãƒ©ãƒ¼ç·æ•°
    warning_count: int         # è­¦å‘Šç·æ•°
    fixed_count: int           # ä¿®æ­£æ¸ˆã¿æ•°

    quality_level: QualityLevel
    improvement_suggestions: List[str]

@dataclass
class QualityResult:
    """å“è³ªãƒã‚§ãƒƒã‚¯çµæœ"""
    check_type: CheckType
    passed: bool
    score: float
    errors: List[str]
    warnings: List[str]
    details: Dict[str, Any]
    execution_time: float

class QualityManager:
    """çµ±åˆå“è³ªç®¡ç†ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self) -> None:
        self.standards_path = Path("postbox/quality/standards.json")
        self.results_path = Path("postbox/monitoring/quality_results.json")
        self.history_path = Path("postbox/monitoring/quality_history.json")

        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        for path in [self.standards_path, self.results_path, self.history_path]:
            path.parent.mkdir(parents=True, exist_ok=True)

        self.standards = self._load_standards()
        self.thresholds = self.standards.get("thresholds", {})

        # çµ±åˆãƒ†ã‚¹ãƒˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ– (Issue #859)
        if INTEGRATION_TEST_AVAILABLE:
            try:
                self.integration_test_system = IntegrationTestSystem()
                self.test_generator = TestGeneratorEngine()
                print("ğŸ§ª çµ±åˆãƒ†ã‚¹ãƒˆã‚·ã‚¹ãƒ†ãƒ çµ±åˆå®Œäº†")
            except Exception as e:
                print(f"âš ï¸ çµ±åˆãƒ†ã‚¹ãƒˆã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
                self.integration_test_system = None
                self.test_generator = None
        else:
            self.integration_test_system = None
            self.test_generator = None

        print("ğŸ¯ QualityManager åˆæœŸåŒ–å®Œäº†")

    def run_comprehensive_check(self, target_files: List[str],
                              ai_agent: str = "claude") -> QualityMetrics:
        """åŒ…æ‹¬çš„å“è³ªãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ"""

        print(f"ğŸ” åŒ…æ‹¬çš„å“è³ªãƒã‚§ãƒƒã‚¯é–‹å§‹ ({ai_agent}) - å¯¾è±¡: {len(target_files)}ãƒ•ã‚¡ã‚¤ãƒ«")

        start_time = datetime.datetime.now()
        results = {}

        # å„ç¨®ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ
        for check_type in CheckType:
            print(f"  ğŸ“‹ {check_type.value} ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œä¸­...")
            result = self._run_quality_check(check_type, target_files)
            results[check_type.value] = result

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        metrics = self._calculate_metrics(results, target_files)

        # çµæœä¿å­˜
        execution_time = (datetime.datetime.now() - start_time).total_seconds()
        self._save_quality_result(metrics, ai_agent, execution_time, target_files)

        # æ”¹å–„ææ¡ˆç”Ÿæˆ
        metrics.improvement_suggestions = self._generate_improvements(metrics, results)

        print(f"âœ… å“è³ªãƒã‚§ãƒƒã‚¯å®Œäº†: {metrics.quality_level.value} (ã‚¹ã‚³ã‚¢: {metrics.overall_score:.3f})")

        return metrics

    def _run_quality_check(self, check_type: CheckType, target_files: List[str]) -> QualityResult:
        """å€‹åˆ¥å“è³ªãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ"""

        start_time = datetime.datetime.now()

        try:
            if check_type == CheckType.SYNTAX:
                return self._check_syntax(target_files)
            elif check_type == CheckType.TYPE_CHECK:
                return self._check_types(target_files)
            elif check_type == CheckType.LINT:
                return self._check_lint(target_files)
            elif check_type == CheckType.FORMAT:
                return self._check_format(target_files)
            elif check_type == CheckType.SECURITY:
                return self._check_security(target_files)
            elif check_type == CheckType.PERFORMANCE:
                return self._check_performance(target_files)
            elif check_type == CheckType.TEST:
                return self._check_tests(target_files)
            else:
                raise ValueError(f"æœªå¯¾å¿œã®ãƒã‚§ãƒƒã‚¯ç¨®åˆ¥: {check_type}")

        except Exception as e:
            execution_time = (datetime.datetime.now() - start_time).total_seconds()
            return QualityResult(
                check_type=check_type,
                passed=False,
                score=0.0,
                errors=[f"ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}"],
                warnings=[],
                details={"error": str(e)},
                execution_time=execution_time
            )

    def _check_syntax(self, target_files: List[str]) -> QualityResult:
        """æ§‹æ–‡ãƒã‚§ãƒƒã‚¯"""
        errors: List[str] = []
        warnings: List[str] = []
        total_files = len(target_files)
        passed_files = 0

        for file_path in target_files:
            try:
                result = subprocess.run(
                    ["python3", "-m", "py_compile", file_path],
                    capture_output=True,
                    text=True
                )

                if result.returncode == 0:
                    passed_files += 1
                else:
                    errors.append(f"{file_path}: {result.stderr.strip()}")

            except Exception as e:
                errors.append(f"{file_path}: æ§‹æ–‡ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œã‚¨ãƒ©ãƒ¼ - {str(e)}")

        score = passed_files / total_files if total_files > 0 else 1.0

        return QualityResult(
            check_type=CheckType.SYNTAX,
            passed=len(errors) == 0,
            score=score,
            errors=errors,
            warnings=warnings,
            details={"passed_files": passed_files, "total_files": total_files},
            execution_time=0.0
        )

    def _check_types(self, target_files: List[str]) -> QualityResult:
        """å‹ãƒã‚§ãƒƒã‚¯ (mypy)"""
        errors = []
        warnings = []

        try:
            # ã¾ãšå€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯
            for file_path in target_files:
                result = subprocess.run(
                    ["python3", "-m", "mypy", "--strict", file_path],
                    capture_output=True,
                    text=True
                )

                for line in result.stdout.split('\n'):
                    if 'error:' in line:
                        errors.append(line.strip())
                    elif 'warning:' in line or 'note:' in line:
                        warnings.append(line.strip())

            # ã‚¹ã‚³ã‚¢è¨ˆç®— (ã‚¨ãƒ©ãƒ¼ç‡ãƒ™ãƒ¼ã‚¹)
            total_lines = sum(self._count_file_lines(f) for f in target_files if os.path.exists(f))
            error_rate = len(errors) / max(total_lines, 1) if total_lines > 0 else 1.0
            score = max(0.0, 1.0 - error_rate)

        except Exception as e:
            errors.append(f"mypyå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}")
            score = 0.0

        return QualityResult(
            check_type=CheckType.TYPE_CHECK,
            passed=len(errors) == 0,
            score=score,
            errors=errors,
            warnings=warnings,
            details={"total_lines": total_lines, "error_rate": error_rate},
            execution_time=0.0
        )

    def _check_lint(self, target_files: List[str]) -> QualityResult:
        """ãƒªãƒ³ãƒˆãƒã‚§ãƒƒã‚¯ (flake8)"""
        errors: List[str] = []
        warnings: List[str] = []

        try:
            for file_path in target_files:
                result = subprocess.run(
                    ["python3", "-m", "flake8", file_path],
                    capture_output=True,
                    text=True
                )

                for line in result.stdout.split('\n'):
                    if line.strip():
                        if any(code in line for code in ['E', 'F']):  # Error codes
                            errors.append(line.strip())
                        else:  # Warnings
                            warnings.append(line.strip())

            # ã‚¹ã‚³ã‚¢è¨ˆç®—
            total_files = len(target_files)
            error_rate = len(errors) / max(total_files, 1)
            score = max(0.0, 1.0 - error_rate)

        except Exception as e:
            errors.append(f"flake8å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}")
            score = 0.0

        return QualityResult(
            check_type=CheckType.LINT,
            passed=len(errors) == 0,
            score=score,
            errors=errors,
            warnings=warnings,
            details={"error_rate": error_rate},
            execution_time=0.0
        )

    def _check_format(self, target_files: List[str]) -> QualityResult:
        """ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãƒã‚§ãƒƒã‚¯ (black)"""
        errors: List[str] = []
        warnings: List[str] = []

        try:
            for file_path in target_files:
                result = subprocess.run(
                    ["python3", "-m", "black", "--check", file_path],
                    capture_output=True,
                    text=True
                )

                if result.returncode != 0:
                    errors.append(f"{file_path}: ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆä¸é©åˆ")

            score = 1.0 - (len(errors) / max(len(target_files), 1))

        except Exception as e:
            errors.append(f"blackå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}")
            score = 0.0

        return QualityResult(
            check_type=CheckType.FORMAT,
            passed=len(errors) == 0,
            score=score,
            errors=errors,
            warnings=warnings,
            details={},
            execution_time=0.0
        )

    def _check_security(self, target_files: List[str]) -> QualityResult:
        """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒã‚§ãƒƒã‚¯ (åŸºæœ¬çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°)"""
        errors: List[str] = []
        warnings: List[str] = []

        # å±é™ºãªãƒ‘ã‚¿ãƒ¼ãƒ³
        dangerous_patterns = [
            r"eval\s*\(",
            r"exec\s*\(",
            r"subprocess\.call\s*\(",
            r"os\.system\s*\(",
            r"shell=True",
            r"password\s*=\s*[\"']",
            r"secret\s*=\s*[\"']",
            r"api_key\s*=\s*[\"']"
        ]

        import re

        for file_path in target_files:
            if os.path.exists(file_path):
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()

                    for pattern in dangerous_patterns:
                        matches = re.findall(pattern, content, re.IGNORECASE)
                        if matches:
                            warnings.append(f"{file_path}: æ½œåœ¨çš„ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒªã‚¹ã‚¯ - {pattern}")

                except Exception as e:
                    errors.append(f"{file_path}: ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼ - {str(e)}")

        # ã‚¹ã‚³ã‚¢è¨ˆç®—
        total_issues = len(errors) + len(warnings)
        score = max(0.0, 1.0 - (total_issues / max(len(target_files), 1)))

        return QualityResult(
            check_type=CheckType.SECURITY,
            passed=len(errors) == 0,
            score=score,
            errors=errors,
            warnings=warnings,
            details={"total_issues": total_issues},
            execution_time=0.0
        )

    def _check_performance(self, target_files: List[str]) -> QualityResult:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒã‚§ãƒƒã‚¯ (åŸºæœ¬çš„ãªæœ€é©åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³)"""
        warnings: List[str] = []
        errors: List[str] = []

        performance_issues = [
            r"for\s+\w+\s+in\s+range\(len\(",  # range(len()) ãƒ‘ã‚¿ãƒ¼ãƒ³
            r"\.append\s*\(\s*\)\s*\n.*for",   # éåŠ¹ç‡ãªãƒ«ãƒ¼ãƒ—
            r"time\.sleep\s*\(\s*[0-9]+\s*\)"  # é•·ã„sleep
        ]

        import re

        for file_path in target_files:
            if os.path.exists(file_path):
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()

                    for pattern in performance_issues:
                        if re.search(pattern, content):
                            warnings.append(f"{file_path}: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„å¯èƒ½ - {pattern}")

                except Exception as e:
                    errors.append(f"{file_path}: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼ - {str(e)}")

        score = max(0.0, 1.0 - (len(warnings) / max(len(target_files), 1)))

        return QualityResult(
            check_type=CheckType.PERFORMANCE,
            passed=len(errors) == 0,
            score=score,
            errors=errors,
            warnings=warnings,
            details={},
            execution_time=0.0
        )

    def _check_tests(self, target_files: List[str]) -> QualityResult:
        """ãƒ†ã‚¹ãƒˆãƒã‚§ãƒƒã‚¯"""
        errors: List[str] = []
        warnings: List[str] = []

        # ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        test_files = []
        source_files = []

        for file_path in target_files:
            if 'test' in file_path.lower():
                test_files.append(file_path)
            else:
                source_files.append(file_path)

        # ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸è¨ˆç®—
        coverage_ratio = len(test_files) / max(len(source_files), 1) if source_files else 1.0

        if coverage_ratio < 0.3:
            warnings.append(f"ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ä¸è¶³: {coverage_ratio:.1%}")

        score = min(1.0, coverage_ratio)

        return QualityResult(
            check_type=CheckType.TEST,
            passed=coverage_ratio >= 0.5,
            score=score,
            errors=errors,
            warnings=warnings,
            details={"coverage_ratio": coverage_ratio, "test_files": len(test_files)},
            execution_time=0.0
        )

    def _calculate_metrics(self, results: Dict[str, QualityResult],
                          target_files: List[str]) -> QualityMetrics:
        """å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—"""

        # å„ç¨®ã‚¹ã‚³ã‚¢é›†è¨ˆ
        scores = {}
        total_errors = 0
        total_warnings = 0

        for check_type, result in results.items():
            scores[check_type] = result.score
            total_errors += len(result.errors)
            total_warnings += len(result.warnings)

        # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®— (é‡ã¿ä»˜ãå¹³å‡)
        weights = {
            "syntax": 0.2,
            "type_check": 0.25,
            "lint": 0.2,
            "format": 0.1,
            "security": 0.15,
            "performance": 0.05,
            "test": 0.05
        }

        overall_score = sum(scores.get(k, 0) * w for k, w in weights.items())

        # å“è³ªãƒ¬ãƒ™ãƒ«æ±ºå®š
        if overall_score >= 0.95:
            quality_level = QualityLevel.EXCELLENT
        elif overall_score >= 0.80:
            quality_level = QualityLevel.GOOD
        elif overall_score >= 0.60:
            quality_level = QualityLevel.ACCEPTABLE
        elif overall_score >= 0.40:
            quality_level = QualityLevel.POOR
        else:
            quality_level = QualityLevel.CRITICAL

        return QualityMetrics(
            overall_score=overall_score,
            syntax_score=scores.get("syntax", 0),
            type_score=scores.get("type_check", 0),
            lint_score=scores.get("lint", 0),
            format_score=scores.get("format", 0),
            security_score=scores.get("security", 0),
            performance_score=scores.get("performance", 0),
            test_coverage=scores.get("test", 0),
            error_count=total_errors,
            warning_count=total_warnings,
            fixed_count=0,  # ä¿®æ­£ã‚·ã‚¹ãƒ†ãƒ ã¨é€£æºæ™‚ã«æ›´æ–°
            quality_level=quality_level,
            improvement_suggestions=[]
        )

    def _generate_improvements(self, metrics: QualityMetrics,
                             results: Dict[str, QualityResult]) -> List[str]:
        """æ”¹å–„ææ¡ˆç”Ÿæˆ"""

        suggestions = []

        # ã‚¹ã‚³ã‚¢åˆ¥æ”¹å–„ææ¡ˆ
        if metrics.syntax_score < 0.8:
            suggestions.append("ğŸ”§ æ§‹æ–‡ã‚¨ãƒ©ãƒ¼ã®ä¿®æ­£ãŒå¿…è¦ã§ã™")

        if metrics.type_score < 0.7:
            suggestions.append("ğŸ“ å‹æ³¨é‡ˆã®è¿½åŠ ãƒ»ä¿®æ­£ã‚’æ¨å¥¨ã—ã¾ã™")

        if metrics.lint_score < 0.8:
            suggestions.append("ğŸ¯ ã‚³ãƒ¼ãƒ‰å“è³ªã®æ”¹å–„ãŒå¿…è¦ã§ã™ (flake8)")

        if metrics.format_score < 0.9:
            suggestions.append("ğŸ’… ã‚³ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®çµ±ä¸€ãŒå¿…è¦ã§ã™ (black)")

        if metrics.security_score < 0.9:
            suggestions.append("ğŸ”’ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒªã‚¹ã‚¯ã®ç¢ºèªãƒ»ä¿®æ­£ã‚’æ¨å¥¨ã—ã¾ã™")

        if metrics.performance_score < 0.8:
            suggestions.append("âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚’æ¤œè¨ã—ã¦ãã ã•ã„")

        if metrics.test_coverage < 0.5:
            suggestions.append("ğŸ§ª ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ã®å‘ä¸ŠãŒå¿…è¦ã§ã™")

        # ç·åˆçš„ãªææ¡ˆ
        if metrics.overall_score < 0.6:
            suggestions.append("ğŸš¨ ç·åˆå“è³ªãŒåŸºæº–ã‚’ä¸‹å›ã£ã¦ã„ã¾ã™ã€‚å„ªå…ˆçš„ãªæ”¹å–„ãŒå¿…è¦ã§ã™")

        return suggestions

    def _save_quality_result(self, metrics: QualityMetrics, ai_agent: str,
                           execution_time: float, target_files: List[str]) -> None:
        """å“è³ªçµæœä¿å­˜"""

        result_entry = {
            "timestamp": datetime.datetime.now().isoformat(),
            "ai_agent": ai_agent,
            "execution_time": execution_time,
            "target_files": target_files,
            "metrics": {
                "overall_score": metrics.overall_score,
                "quality_level": metrics.quality_level.value,
                "error_count": metrics.error_count,
                "warning_count": metrics.warning_count,
                "scores": {
                    "syntax": metrics.syntax_score,
                    "type_check": metrics.type_score,
                    "lint": metrics.lint_score,
                    "format": metrics.format_score,
                    "security": metrics.security_score,
                    "performance": metrics.performance_score,
                    "test": metrics.test_coverage
                }
            }
        }

        # å±¥æ­´æ›´æ–°
        history = []
        if self.history_path.exists():
            try:
                with open(self.history_path, 'r', encoding='utf-8') as f:
                    history = json.load(f)
            except:
                history = []

        history.append(result_entry)

        # å±¥æ­´ã‚µã‚¤ã‚ºåˆ¶é™
        if len(history) > 50:
            history = history[-50:]

        with open(self.history_path, 'w', encoding='utf-8') as f:
            json.dump(history, f, indent=2, ensure_ascii=False)

    def _count_file_lines(self, file_path: str) -> int:
        """ãƒ•ã‚¡ã‚¤ãƒ«è¡Œæ•°ã‚«ã‚¦ãƒ³ãƒˆ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return len(f.readlines())
        except:
            return 0

    def _load_standards(self) -> Dict[str, Any]:
        """å“è³ªåŸºæº–èª­ã¿è¾¼ã¿"""

        default_standards = {
            "thresholds": {
                "minimum_overall_score": 0.7,
                "minimum_type_score": 0.8,
                "minimum_lint_score": 0.8,
                "maximum_error_count": 10,
                "minimum_test_coverage": 0.5
            },
            "weights": {
                "syntax": 0.2,
                "type_check": 0.25,
                "lint": 0.2,
                "format": 0.1,
                "security": 0.15,
                "performance": 0.05,
                "test": 0.05
            },
            "automation": {
                "auto_fix_format": True,
                "auto_fix_simple_types": True,
                "auto_fix_imports": True
            }
        }

        if self.standards_path.exists():
            try:
                with open(self.standards_path, 'r', encoding='utf-8') as f:
                    user_standards = json.load(f)
                default_standards.update(user_standards)
            except:
                pass
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šä¿å­˜
            with open(self.standards_path, 'w', encoding='utf-8') as f:
                json.dump(default_standards, f, indent=2, ensure_ascii=False)

        return default_standards

    def comprehensive_quality_check(self, target_files: List[str],
                                   ai_agent: str = "claude",
                                   context: Optional[Dict[str, Any]] = None) -> QualityMetrics:
        """åŒ…æ‹¬çš„å“è³ªãƒã‚§ãƒƒã‚¯ï¼ˆ3å±¤æ¤œè¨¼ä½“åˆ¶å¯¾å¿œç‰ˆï¼‰

        run_comprehensive_checkã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹ + 3å±¤æ¤œè¨¼ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå¯¾å¿œ
        """

        print(f"ğŸ” åŒ…æ‹¬çš„å“è³ªãƒã‚§ãƒƒã‚¯é–‹å§‹ - 3å±¤æ¤œè¨¼ä½“åˆ¶å¯¾å¿œç‰ˆ")

        context = context or {}

        # æ—¢å­˜ã®run_comprehensive_checkã‚’å®Ÿè¡Œ
        base_metrics = self.run_comprehensive_check(target_files, ai_agent)

        # 3å±¤æ¤œè¨¼ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ã®è¿½åŠ æƒ…å ±ã‚’ä»˜ä¸
        if context.get("layer_context"):
            layer_info = context["layer_context"]

            # Layer 1ã®çµæœãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆ
            if layer_info.get("layer1_result"):
                layer1_result = layer_info["layer1_result"]
                if not layer1_result.get("passed"):
                    print("âš ï¸ Layer 1æ§‹æ–‡æ¤œè¨¼æœªé€šé - å“è³ªã‚¹ã‚³ã‚¢èª¿æ•´")
                    # æ§‹æ–‡ã‚¨ãƒ©ãƒ¼ãŒã‚ã‚‹å ´åˆã¯ã‚¹ã‚³ã‚¢ã‚’ä¸‹æ–¹ä¿®æ­£
                    base_metrics.overall_score *= 0.7

            # Layer 2ã®çµæœãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆ
            if layer_info.get("layer2_result"):
                layer2_result = layer_info["layer2_result"]
                if not layer2_result.get("passed"):
                    print("âš ï¸ Layer 2å“è³ªæ¤œè¨¼æœªé€šé - å“è³ªãƒ¬ãƒ™ãƒ«èª¿æ•´")
                    # å“è³ªæ¤œè¨¼æœªé€šéã®å ´åˆã¯ãƒ¬ãƒ™ãƒ«ã‚’ä¸‹æ–¹ä¿®æ­£
                    if base_metrics.quality_level == QualityLevel.EXCELLENT:
                        base_metrics.quality_level = QualityLevel.GOOD
                    elif base_metrics.quality_level == QualityLevel.GOOD:
                        base_metrics.quality_level = QualityLevel.ACCEPTABLE

        # 3å±¤æ¤œè¨¼å°‚ç”¨ã®æ”¹å–„ææ¡ˆã‚’è¿½åŠ 
        three_layer_suggestions = [
            "3å±¤æ¤œè¨¼ä½“åˆ¶ã§ã®å“è³ªãƒã‚§ãƒƒã‚¯å®Œäº†",
            "Layer 1ï¼ˆæ§‹æ–‡ï¼‰â†’ Layer 2ï¼ˆå“è³ªï¼‰â†’ Layer 3ï¼ˆClaudeæ‰¿èªï¼‰ã®é †åºã§å®Ÿè¡Œæ¨å¥¨"
        ]

        base_metrics.improvement_suggestions.extend(three_layer_suggestions)

        print(f"âœ… åŒ…æ‹¬çš„å“è³ªãƒã‚§ãƒƒã‚¯å®Œäº†ï¼ˆ3å±¤æ¤œè¨¼å¯¾å¿œç‰ˆï¼‰")

        return base_metrics

    def get_quality_report(self) -> Dict[str, Any]:
        """å“è³ªãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""

        if not self.history_path.exists():
            return {"error": "å“è³ªå±¥æ­´ãƒ‡ãƒ¼ã‚¿ãªã—"}

        try:
            with open(self.history_path, 'r', encoding='utf-8') as f:
                history = json.load(f)

            if not history:
                return {"error": "å“è³ªå±¥æ­´ãƒ‡ãƒ¼ã‚¿ãªã—"}

            # çµ±è¨ˆè¨ˆç®—
            recent_results = history[-10:]  # æœ€æ–°10ä»¶

            avg_overall = sum(r["metrics"]["overall_score"] for r in recent_results) / len(recent_results)
            avg_errors = sum(r["metrics"]["error_count"] for r in recent_results) / len(recent_results)

            quality_trend = []
            if len(history) >= 5:
                for i in range(len(history) - 4):
                    batch = history[i:i+5]
                    batch_avg = sum(r["metrics"]["overall_score"] for r in batch) / 5
                    quality_trend.append(batch_avg)

            return {
                "current_quality": {
                    "overall_score": recent_results[-1]["metrics"]["overall_score"],
                    "quality_level": recent_results[-1]["metrics"]["quality_level"],
                    "error_count": recent_results[-1]["metrics"]["error_count"]
                },
                "trends": {
                    "average_overall_score": avg_overall,
                    "average_error_count": avg_errors,
                    "quality_trend": quality_trend
                },
                "recommendations": self._generate_recommendations(recent_results),
                "total_checks": len(history)
            }

        except Exception as e:
            return {"error": f"ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}"}

    def _generate_recommendations(self, recent_results: List[Dict[str, Any]]) -> List[str]:
        """æ”¹å–„æ¨å¥¨äº‹é …ç”Ÿæˆ"""

        recommendations = []

        # å‚¾å‘åˆ†æ
        if len(recent_results) >= 3:
            scores = [r["metrics"]["overall_score"] for r in recent_results[-3:]]
            if all(scores[i] > scores[i+1] for i in range(len(scores)-1)):
                recommendations.append("ğŸ“‰ å“è³ªã‚¹ã‚³ã‚¢ãŒä½ä¸‹å‚¾å‘ã«ã‚ã‚Šã¾ã™ã€‚æ”¹å–„ãŒå¿…è¦ã§ã™")
            elif all(scores[i] < scores[i+1] for i in range(len(scores)-1)):
                recommendations.append("ğŸ“ˆ å“è³ªã‚¹ã‚³ã‚¢ãŒå‘ä¸Šã—ã¦ã„ã¾ã™ã€‚å¼•ãç¶šãç¶­æŒã—ã¦ãã ã•ã„")

        # ã‚¨ãƒ©ãƒ¼ç‡åˆ†æ
        recent_errors = [r["metrics"]["error_count"] for r in recent_results]
        avg_errors = sum(recent_errors) / len(recent_errors)

        if avg_errors > 20:
            recommendations.append("ğŸš¨ ã‚¨ãƒ©ãƒ¼æ•°ãŒå¤šã™ãã¾ã™ã€‚åŸºæœ¬çš„ãªå“è³ªãƒã‚§ãƒƒã‚¯ã‚’å¼·åŒ–ã—ã¦ãã ã•ã„")
        elif avg_errors > 10:
            recommendations.append("âš ï¸ ã‚¨ãƒ©ãƒ¼æ•°ãŒã‚„ã‚„å¤šã‚ã§ã™ã€‚å®šæœŸçš„ãªå“è³ªãƒã‚§ãƒƒã‚¯ã‚’æ¨å¥¨ã—ã¾ã™")

        return recommendations

    # =========================
    # 3å±¤æ¤œè¨¼ä½“åˆ¶å°‚ç”¨ãƒ¡ã‚½ãƒƒãƒ‰
    # =========================

    def validate_syntax(self, target_files: List[str]) -> Dict[str, Any]:
        """Layer 1: æ§‹æ–‡æ¤œè¨¼ï¼ˆ3å±¤æ¤œè¨¼ä½“åˆ¶ï¼‰"""

        print("ğŸ” Layer 1: æ§‹æ–‡æ¤œè¨¼é–‹å§‹...")

        # æ§‹æ–‡ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ
        syntax_result = self._check_syntax(target_files)

        # åŸºæœ¬å“è³ªãƒã‚§ãƒƒã‚¯
        type_result = self._check_types(target_files)

        # æ¤œè¨¼çµæœçµ±åˆ
        validation_passed = (
            syntax_result.passed and
            type_result.score >= 0.7  # å‹ãƒã‚§ãƒƒã‚¯70%ä»¥ä¸Š
        )

        result = {
            "layer": 1,
            "validation_type": "syntax_validation",
            "passed": validation_passed,
            "syntax_check": {
                "passed": syntax_result.passed,
                "score": syntax_result.score,
                "errors": len(syntax_result.errors),
                "details": syntax_result.details
            },
            "type_check": {
                "passed": type_result.passed,
                "score": type_result.score,
                "errors": len(type_result.errors),
                "details": type_result.details
            },
            "summary": {
                "total_files": len(target_files),
                "syntax_errors": len(syntax_result.errors),
                "type_errors": len(type_result.errors),
                "validation_passed": validation_passed
            },
            "next_layer_recommended": validation_passed,
            "timestamp": datetime.datetime.now().isoformat()
        }

        print(f"âœ… Layer 1å®Œäº†: {'PASS' if validation_passed else 'FAIL'}")
        print(f"   æ§‹æ–‡ã‚¨ãƒ©ãƒ¼: {len(syntax_result.errors)}ä»¶")
        print(f"   å‹ã‚¨ãƒ©ãƒ¼: {len(type_result.errors)}ä»¶")

        return result

    def check_code_quality(self, target_files: List[str], layer1_result: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Layer 2: å“è³ªæ¤œè¨¼ï¼ˆ3å±¤æ¤œè¨¼ä½“åˆ¶ï¼‰"""

        print("ğŸ›¡ï¸ Layer 2: å“è³ªæ¤œè¨¼é–‹å§‹...")

        # Layer 1ã®çµæœç¢ºèª
        if layer1_result and not layer1_result.get("passed", False):
            return {
                "layer": 2,
                "validation_type": "quality_validation",
                "passed": False,
                "skipped": True,
                "reason": "Layer 1æ§‹æ–‡æ¤œè¨¼ãŒå¤±æ•—ã—ãŸãŸã‚å“è³ªæ¤œè¨¼ã‚’ã‚¹ã‚­ãƒƒãƒ—",
                "timestamp": datetime.datetime.now().isoformat()
            }

        # åŒ…æ‹¬çš„å“è³ªãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ
        lint_result = self._check_lint(target_files)
        format_result = self._check_format(target_files)
        security_result = self._check_security(target_files)
        performance_result = self._check_performance(target_files)
        test_result = self._check_tests(target_files)

        # å“è³ªåŸºæº–åˆ¤å®š
        quality_scores = {
            "lint": lint_result.score,
            "format": format_result.score,
            "security": security_result.score,
            "performance": performance_result.score,
            "test": test_result.score
        }

        overall_quality_score = sum(quality_scores.values()) / len(quality_scores)
        quality_passed = overall_quality_score >= 0.75  # 75%ä»¥ä¸Šã§åˆæ ¼

        result = {
            "layer": 2,
            "validation_type": "quality_validation",
            "passed": quality_passed,
            "overall_quality_score": overall_quality_score,
            "quality_checks": {
                "lint": {
                    "passed": lint_result.passed,
                    "score": lint_result.score,
                    "warnings": len(lint_result.warnings)
                },
                "format": {
                    "passed": format_result.passed,
                    "score": format_result.score,
                    "issues": len(format_result.errors)
                },
                "security": {
                    "passed": security_result.passed,
                    "score": security_result.score,
                    "vulnerabilities": len(security_result.errors)
                },
                "performance": {
                    "passed": performance_result.passed,
                    "score": performance_result.score,
                    "bottlenecks": len(performance_result.warnings)
                },
                "test": {
                    "passed": test_result.passed,
                    "score": test_result.score,
                    "coverage": test_result.score * 100
                }
            },
            "summary": {
                "total_files": len(target_files),
                "quality_level": self._determine_quality_level(overall_quality_score),
                "claude_review_recommended": quality_passed
            },
            "next_layer_recommended": quality_passed,
            "timestamp": datetime.datetime.now().isoformat()
        }

        print(f"âœ… Layer 2å®Œäº†: {'PASS' if quality_passed else 'FAIL'}")
        print(f"   ç·åˆå“è³ªã‚¹ã‚³ã‚¢: {overall_quality_score:.3f}")
        print(f"   å“è³ªãƒ¬ãƒ™ãƒ«: {self._determine_quality_level(overall_quality_score).value}")

        return result

    def claude_final_approval(self, target_files: List[str], layer1_result: Optional[Dict[str, Any]] = None,
                            layer2_result: Optional[Dict[str, Any]] = None, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Layer 3: Claudeæœ€çµ‚æ‰¿èªï¼ˆ3å±¤æ¤œè¨¼ä½“åˆ¶ï¼‰"""

        print("ğŸ‘¨â€ğŸ’» Layer 3: Claudeæœ€çµ‚æ‰¿èªé–‹å§‹...")

        context = context or {}

        # å‰å±¤ã®çµæœç¢ºèª
        layer1_passed = layer1_result.get("passed", False) if layer1_result else False
        layer2_passed = layer2_result.get("passed", False) if layer2_result else False

        if not (layer1_passed and layer2_passed):
            return {
                "layer": 3,
                "validation_type": "claude_final_approval",
                "approved": False,
                "skipped": True,
                "reason": "å‰å±¤ã®æ¤œè¨¼ãŒæœªå®Œäº†ã¾ãŸã¯å¤±æ•—ã®ãŸã‚æœ€çµ‚æ‰¿èªã‚’ã‚¹ã‚­ãƒƒãƒ—",
                "layer1_passed": layer1_passed,
                "layer2_passed": layer2_passed,
                "timestamp": datetime.datetime.now().isoformat()
            }

        # Claudeå“è³ªåŸºæº–ã«ã‚ˆã‚‹æœ€çµ‚ãƒã‚§ãƒƒã‚¯
        final_metrics = self.run_comprehensive_check(target_files, "claude")

        # æœ€çµ‚æ‰¿èªåˆ¤å®šåŸºæº–
        approval_criteria = {
            "minimum_overall_score": 0.80,  # 80%ä»¥ä¸Š
            "maximum_critical_errors": 0,   # é‡å¤§ã‚¨ãƒ©ãƒ¼0ä»¶
            "minimum_test_coverage": 0.70,  # ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸70%ä»¥ä¸Š
            "maximum_security_issues": 0    # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å•é¡Œ0ä»¶
        }

        # åˆ¤å®šå®Ÿè¡Œ
        approval_checks = {
            "overall_score_check": final_metrics.overall_score >= approval_criteria["minimum_overall_score"],
            "critical_errors_check": final_metrics.error_count <= approval_criteria["maximum_critical_errors"],
            "test_coverage_check": final_metrics.test_coverage >= approval_criteria["minimum_test_coverage"],
            "security_check": final_metrics.security_score >= 0.95  # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã¯95%ä»¥ä¸Š
        }

        final_approved = all(approval_checks.values())

        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè€ƒæ…®ï¼ˆæ–°è¦å®Ÿè£…ã®å ´åˆã®ç‰¹åˆ¥åŸºæº–ï¼‰
        if context.get("task_type") in ["new_implementation", "hybrid_implementation", "new_feature_development"]:
            # æ–°è¦å®Ÿè£…ã¯åŸºæº–ã‚’è‹¥å¹²ç·©å’Œ
            if final_metrics.overall_score >= 0.75 and final_metrics.error_count <= 2:
                final_approved = True
                approval_checks["new_implementation_exception"] = True

        result = {
            "layer": 3,
            "validation_type": "claude_final_approval",
            "approved": final_approved,
            "final_metrics": {
                "overall_score": final_metrics.overall_score,
                "quality_level": final_metrics.quality_level.value,
                "error_count": final_metrics.error_count,
                "warning_count": final_metrics.warning_count,
                "test_coverage": final_metrics.test_coverage,
                "security_score": final_metrics.security_score
            },
            "approval_criteria": approval_criteria,
            "approval_checks": approval_checks,
            "context_considerations": {
                "task_type": context.get("task_type", "unknown"),
                "special_criteria_applied": "new_implementation_exception" in approval_checks
            },
            "recommendations": final_metrics.improvement_suggestions,
            "summary": {
                "layer1_passed": layer1_passed,
                "layer2_passed": layer2_passed,
                "layer3_approved": final_approved,
                "three_layer_verification_complete": final_approved
            },
            "timestamp": datetime.datetime.now().isoformat()
        }

        print(f"âœ… Layer 3å®Œäº†: {'APPROVED' if final_approved else 'REJECTED'}")
        print(f"   æœ€çµ‚å“è³ªã‚¹ã‚³ã‚¢: {final_metrics.overall_score:.3f}")
        print(f"   3å±¤æ¤œè¨¼çµæœ: {'å®Œå…¨é€šé' if final_approved else 'è¦æ”¹å–„'}")

        return result

    def evaluate_integration_quality(self, target_files: List[str],
                                    layer2_result: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """çµ±åˆå“è³ªè©•ä¾¡ï¼ˆ3å±¤æ¤œè¨¼ä½“åˆ¶å°‚ç”¨ï¼‰"""

        print("ğŸ”— çµ±åˆå“è³ªè©•ä¾¡é–‹å§‹...")

        # Layer 2ã®çµæœç¢ºèª
        if layer2_result and not layer2_result.get("passed", False):
            return {
                "layer": "integration",
                "validation_type": "integration_quality_evaluation",
                "passed": False,
                "skipped": True,
                "reason": "Layer 2å“è³ªæ¤œè¨¼ãŒå¤±æ•—ã—ãŸãŸã‚çµ±åˆå“è³ªè©•ä¾¡ã‚’ã‚¹ã‚­ãƒƒãƒ—",
                "timestamp": datetime.datetime.now().isoformat()
            }

        try:
            # ComprehensiveQualityValidatorã‚’å‹•çš„ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ»ä½¿ç”¨
            try:
                from .comprehensive_validator import (  # type: ignore
                    ComprehensiveQualityValidator,
                    ValidationCategory
                )
            except ImportError:
                print("âš ï¸ ComprehensiveQualityValidator ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼ - ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ãƒ¢ãƒ¼ãƒ‰å®Ÿè¡Œ")
                return self._fallback_integration_evaluation(target_files)

            comprehensive_validator = ComprehensiveQualityValidator()

            # çµ±åˆå“è³ªæ¤œè¨¼å®Ÿè¡Œ
            integration_categories = [
                ValidationCategory.INTEGRATION,
                ValidationCategory.RELIABILITY,
                ValidationCategory.SCALABILITY
            ]

            validation_result = comprehensive_validator.validate_comprehensive_quality(
                target_files,
                enterprise_mode=False,
                categories=integration_categories
            )

            summary = validation_result["summary"]

            # çµ±åˆå“è³ªåˆ¤å®šåŸºæº– (æ®µéšçš„æ”¹å–„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ)
            integration_score = summary.get("overall_score", 0.0)
            # åˆæœŸæ®µéš: 50%ä»¥ä¸Šã§åˆæ ¼ï¼ˆæ®µéšçš„ã«95%ã¾ã§å¼•ãä¸Šã’äºˆå®šï¼‰
            integration_passed = integration_score >= 0.50

            # è©³ç´°åˆ†æ
            integration_results = validation_result.get("results_by_category", {}).get("integration", [])
            reliability_results = validation_result.get("results_by_category", {}).get("reliability", [])
            scalability_results = validation_result.get("results_by_category", {}).get("scalability", [])

            # çµ±åˆãƒ†ã‚¹ãƒˆçµæœåˆ†æ
            integration_test_score = 0.0
            integration_findings = []

            if integration_results:
                test_scores = [r.score for r in integration_results]
                integration_test_score = sum(test_scores) / len(test_scores)
                integration_findings = [finding for r in integration_results for finding in r.findings]

            result = {
                "layer": "integration",
                "validation_type": "integration_quality_evaluation",
                "passed": integration_passed,
                "overall_integration_score": integration_score,
                "detailed_scores": {
                    "integration_test_score": integration_test_score,
                    "reliability_score": summary.get("category_scores", {}).get("reliability", 0.0),
                    "scalability_score": summary.get("category_scores", {}).get("scalability", 0.0)
                },
                "integration_analysis": {
                    "total_files": len(target_files),
                    "integration_tests_generated": len(integration_results),
                    "integration_findings": integration_findings[:5],  # æœ€å¤§5ä»¶
                    "reliability_issues": len(reliability_results),
                    "scalability_concerns": len(scalability_results)
                },
                "quality_assessment": {
                    "integration_readiness": integration_passed,
                    "recommended_next_steps": self._generate_integration_recommendations(
                        integration_score, integration_findings
                    ),
                    "risk_level": "low" if integration_score >= 0.7 else "medium" if integration_score >= 0.5 else "high"
                },
                "comprehensive_validation_summary": summary,
                "timestamp": datetime.datetime.now().isoformat()
            }

            print(f"âœ… çµ±åˆå“è³ªè©•ä¾¡å®Œäº†: {'PASS' if integration_passed else 'FAIL'}")
            print(f"   çµ±åˆã‚¹ã‚³ã‚¢: {integration_score:.3f}")
            print(f"   ãƒ†ã‚¹ãƒˆç”Ÿæˆæ•°: {len(integration_results)}ä»¶")
            print(f"   ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«: {result['quality_assessment']['risk_level']}")

            return result

        except ImportError as e:
            print(f"âš ï¸ ComprehensiveQualityValidator ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return self._fallback_integration_evaluation(target_files)
        except Exception as e:
            print(f"âŒ çµ±åˆå“è³ªè©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
            return {
                "layer": "integration",
                "validation_type": "integration_quality_evaluation",
                "passed": False,
                "error": True,
                "error_message": str(e),
                "fallback_used": True,
                "timestamp": datetime.datetime.now().isoformat()
            }

    def _generate_integration_recommendations(self, score: float, findings: List[str]) -> List[str]:
        """çµ±åˆå“è³ªæ”¹å–„æ¨å¥¨äº‹é …ç”Ÿæˆ"""

        recommendations = []

        if score < 0.5:
            recommendations.extend([
                "çµ±åˆå“è³ªãŒåŸºæº–ã‚’å¤§å¹…ã«ä¸‹å›ã£ã¦ã„ã¾ã™ã€‚åŸºæœ¬çš„ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«æ§‹é€ ã‚’è¦‹ç›´ã—ã¦ãã ã•ã„",
                "çµ±åˆãƒ†ã‚¹ãƒˆãŒå¤šæ•°å¤±æ•—ã—ã¦ã„ã¾ã™ã€‚ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆé–“ã®ä¾å­˜é–¢ä¿‚ã‚’ç¢ºèªã—ã¦ãã ã•ã„",
                "ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åˆ†é›¢ãƒ»ç–çµåˆè¨­è¨ˆã®å°å…¥ã‚’æ¤œè¨ã—ã¦ãã ã•ã„"
            ])
        elif score < 0.7:
            recommendations.extend([
                "çµ±åˆå“è³ªã®æ”¹å–„ãŒå¿…è¦ã§ã™ã€‚ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹è¨­è¨ˆã‚’è¦‹ç›´ã—ã¦ãã ã•ã„",
                "çµ±åˆãƒ†ã‚¹ãƒˆã®è¿½åŠ ãƒ»æ”¹å–„ã‚’æ¤œè¨ã—ã¦ãã ã•ã„",
                "ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®å¼·åŒ–ã‚’æ¨å¥¨ã—ã¾ã™"
            ])
        elif score < 0.85:
            recommendations.extend([
                "åŸºæœ¬çš„ãªçµ±åˆå“è³ªã¯ç¢ºä¿ã•ã‚Œã¦ã„ã¾ã™ã€‚ç´°ã‹ã„æ”¹å–„ã‚’ç¶™ç¶šã—ã¦ãã ã•ã„",
                "çµ±åˆãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ã®å‘ä¸Šã‚’æ¤œè¨ã—ã¦ãã ã•ã„",
                "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚’æ¤œè¨ã—ã¦ãã ã•ã„"
            ])
        else:
            recommendations.append("å„ªç§€ãªçµ±åˆå“è³ªãƒ¬ãƒ™ãƒ«ã§ã™ã€‚ç¾åœ¨ã®æ°´æº–ã‚’ç¶­æŒã—ã¦ãã ã•ã„")

        # å…·ä½“çš„ãªå•é¡Œã«åŸºã¥ãæ¨å¥¨äº‹é …
        if findings:
            recommendations.append(f"ç™ºè¦‹ã•ã‚ŒãŸå•é¡Œï¼ˆ{len(findings)}ä»¶ï¼‰ã®å„ªå…ˆçš„ãªè§£æ±ºãŒå¿…è¦ã§ã™")

        return recommendations

    def _fallback_integration_evaluation(self, target_files: List[str]) -> Dict[str, Any]:
        """ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ç”¨ã®åŸºæœ¬çš„çµ±åˆå“è³ªè©•ä¾¡"""

        print("ğŸ”„ ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ãƒ¢ãƒ¼ãƒ‰: åŸºæœ¬çš„çµ±åˆå“è³ªè©•ä¾¡ã‚’å®Ÿè¡Œ")

        # åŸºæœ¬çš„ãªãƒ•ã‚¡ã‚¤ãƒ«é–¢é€£æ€§ãƒã‚§ãƒƒã‚¯
        integration_issues = []

        for file_path in target_files:
            if not os.path.exists(file_path):
                integration_issues.append(f"ãƒ•ã‚¡ã‚¤ãƒ«æœªå­˜åœ¨: {file_path}")
                continue

            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                # åŸºæœ¬çš„ãªçµ±åˆå•é¡Œãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯
                if 'import' not in content:
                    integration_issues.append(f"ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ–‡ãªã—: {file_path}")

                if len(content.strip()) == 0:
                    integration_issues.append(f"ç©ºãƒ•ã‚¡ã‚¤ãƒ«: {file_path}")

            except Exception as e:
                integration_issues.append(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {file_path} - {str(e)}")

        # ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        total_files = len(target_files)
        issues_count = len(integration_issues)
        fallback_score = max(0.0, 1.0 - (issues_count / max(total_files, 1)))

        return {
            "layer": "integration",
            "validation_type": "integration_quality_evaluation",
            "passed": fallback_score >= 0.7,
            "overall_integration_score": fallback_score,
            "fallback_mode": True,
            "integration_analysis": {
                "total_files": total_files,
                "issues_found": issues_count,
                "integration_findings": integration_issues[:10]  # æœ€å¤§10ä»¶
            },
            "quality_assessment": {
                "integration_readiness": fallback_score >= 0.7,
                "recommended_next_steps": [
                    "ComprehensiveQualityValidatorã®è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„",
                    "è©³ç´°ãªçµ±åˆå“è³ªè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ã®å¾©æ—§ãŒå¿…è¦ã§ã™"
                ],
                "risk_level": "high" if fallback_score < 0.7 else "medium"
            },
            "timestamp": datetime.datetime.now().isoformat()
        }

    def run_three_layer_verification_with_failsafe(self, target_files: List[str],
                                                  context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """3å±¤æ¤œè¨¼ä½“åˆ¶å®Œå…¨ç‰ˆï¼ˆãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•æ©Ÿèƒ½ä»˜ãï¼‰"""

        print("ğŸ›¡ï¸ 3å±¤æ¤œè¨¼ä½“åˆ¶é–‹å§‹ï¼ˆãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•æ©Ÿèƒ½ä»˜ãï¼‰")

        context = context or {}
        verification_results: Dict[str, Any] = {
            "layer1_result": None,
            "layer2_result": None,
            "layer3_result": None,
            "integration_evaluation": None,
            "failsafe_activated": [],
            "overall_status": "pending",
            "total_execution_time": 0.0,
            "timestamp": datetime.datetime.now().isoformat()
        }

        start_time = datetime.datetime.now()

        try:
            # ========== Layer 1: æ§‹æ–‡æ¤œè¨¼ ==========
            print("ğŸ” Layer 1: æ§‹æ–‡æ¤œè¨¼å®Ÿè¡Œä¸­...")
            try:
                layer1_result = self.validate_syntax(target_files)
                verification_results["layer1_result"] = layer1_result

                if not layer1_result.get("passed", False):
                    print("âš ï¸ Layer 1å¤±æ•— - ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ãƒ¢ãƒ¼ãƒ‰: åŸºæœ¬æ§‹æ–‡ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ")
                    verification_results["failsafe_activated"].append("layer1_basic_syntax_check")

                    # ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•: åŸºæœ¬çš„ãªæ§‹æ–‡ãƒã‚§ãƒƒã‚¯
                    layer1_result = self._failsafe_basic_syntax_check(target_files)
                    verification_results["layer1_result"] = layer1_result

            except Exception as e:
                print(f"âŒ Layer 1ã‚¨ãƒ©ãƒ¼ - ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•å®Ÿè¡Œ: {e}")
                verification_results["failsafe_activated"].append("layer1_exception_handling")
                layer1_result = self._failsafe_basic_syntax_check(target_files)
                verification_results["layer1_result"] = layer1_result

            # ========== Layer 2: å“è³ªæ¤œè¨¼ ==========
            print("ğŸ›¡ï¸ Layer 2: å“è³ªæ¤œè¨¼å®Ÿè¡Œä¸­...")
            try:
                layer2_result = self.check_code_quality(target_files, verification_results["layer1_result"])
                verification_results["layer2_result"] = layer2_result

                if not layer2_result.get("passed", False):
                    print("âš ï¸ Layer 2å¤±æ•— - ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ãƒ¢ãƒ¼ãƒ‰: åŸºæœ¬å“è³ªãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ")
                    verification_results["failsafe_activated"].append("layer2_basic_quality_check")

                    # ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•: åŸºæœ¬çš„ãªå“è³ªãƒã‚§ãƒƒã‚¯
                    layer2_result = self._failsafe_basic_quality_check(target_files)
                    verification_results["layer2_result"] = layer2_result

            except Exception as e:
                print(f"âŒ Layer 2ã‚¨ãƒ©ãƒ¼ - ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•å®Ÿè¡Œ: {e}")
                verification_results["failsafe_activated"].append("layer2_exception_handling")
                layer2_result = self._failsafe_basic_quality_check(target_files)
                verification_results["layer2_result"] = layer2_result

            # ========== çµ±åˆå“è³ªè©•ä¾¡ ==========
            print("ğŸ”— çµ±åˆå“è³ªè©•ä¾¡å®Ÿè¡Œä¸­...")
            try:
                integration_result = self.evaluate_integration_quality(target_files, verification_results["layer2_result"])
                verification_results["integration_evaluation"] = integration_result

            except Exception as e:
                print(f"âŒ çµ±åˆå“è³ªè©•ä¾¡ã‚¨ãƒ©ãƒ¼ - ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•å®Ÿè¡Œ: {e}")
                verification_results["failsafe_activated"].append("integration_evaluation_exception")
                integration_result = self._fallback_integration_evaluation(target_files)
                verification_results["integration_evaluation"] = integration_result

            # ========== Layer 3: Claudeæœ€çµ‚æ‰¿èª ==========
            print("ğŸ‘¨â€ğŸ’» Layer 3: Claudeæœ€çµ‚æ‰¿èªå®Ÿè¡Œä¸­...")
            try:
                layer3_result = self.claude_final_approval(
                    target_files,
                    verification_results["layer1_result"],
                    verification_results["layer2_result"],
                    context
                )
                verification_results["layer3_result"] = layer3_result

                if not layer3_result.get("approved", False):
                    print("âš ï¸ Layer 3æ‰¿èªæœªå–å¾— - ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ãƒ¢ãƒ¼ãƒ‰: åŸºæœ¬æ‰¿èªãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ")
                    verification_results["failsafe_activated"].append("layer3_basic_approval")

                    # ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•: åŸºæœ¬æ‰¿èªãƒã‚§ãƒƒã‚¯
                    layer3_result = self._failsafe_basic_approval_check(
                        target_files,
                        verification_results["layer1_result"],
                        verification_results["layer2_result"]
                    )
                    verification_results["layer3_result"] = layer3_result

            except Exception as e:
                print(f"âŒ Layer 3ã‚¨ãƒ©ãƒ¼ - ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•å®Ÿè¡Œ: {e}")
                verification_results["failsafe_activated"].append("layer3_exception_handling")
                layer3_result = self._failsafe_basic_approval_check(
                    target_files,
                    verification_results["layer1_result"],
                    verification_results["layer2_result"]
                )
                verification_results["layer3_result"] = layer3_result

            # ========== æœ€çµ‚åˆ¤å®š ==========
            total_execution_time = (datetime.datetime.now() - start_time).total_seconds()
            verification_results["total_execution_time"] = total_execution_time

            # å…¨å±¤ã®çµæœã‹ã‚‰æœ€çµ‚ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ±ºå®š
            layer1_passed = verification_results["layer1_result"].get("passed", False)
            layer2_passed = verification_results["layer2_result"].get("passed", False)
            layer3_approved = verification_results["layer3_result"].get("approved", False)
            integration_passed = verification_results["integration_evaluation"].get("passed", False)

            overall_passed = layer1_passed and layer2_passed and layer3_approved and integration_passed

            if overall_passed:
                verification_results["overall_status"] = "fully_verified"
            elif layer3_approved:
                verification_results["overall_status"] = "approved_with_conditions"
            elif layer2_passed:
                verification_results["overall_status"] = "quality_verified"
            elif layer1_passed:
                verification_results["overall_status"] = "syntax_verified"
            else:
                verification_results["overall_status"] = "verification_failed"

            # ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ä½¿ç”¨ã®è¨˜éŒ²ã¨ãƒ­ã‚°
            if verification_results["failsafe_activated"]:
                self._log_failsafe_usage(verification_results["failsafe_activated"], target_files)
                print(f"ğŸ”„ ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•å®Ÿè¡Œå›æ•°: {len(verification_results['failsafe_activated'])}å›")

            print(f"âœ… 3å±¤æ¤œè¨¼ä½“åˆ¶å®Œäº†: {verification_results['overall_status']}")
            print(f"â±ï¸ ç·å®Ÿè¡Œæ™‚é–“: {total_execution_time:.2f}ç§’")

            return verification_results

        except Exception as e:
            total_execution_time = (datetime.datetime.now() - start_time).total_seconds()
            print(f"ğŸš¨ 3å±¤æ¤œè¨¼ä½“åˆ¶é‡å¤§ã‚¨ãƒ©ãƒ¼: {e}")

            # å®Œå…¨ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ãƒ¢ãƒ¼ãƒ‰
            return self._complete_failsafe_verification(target_files, str(e), total_execution_time)

    def _failsafe_basic_syntax_check(self, target_files: List[str]) -> Dict[str, Any]:
        """ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•: åŸºæœ¬æ§‹æ–‡ãƒã‚§ãƒƒã‚¯"""

        print("ğŸ”„ ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•: åŸºæœ¬æ§‹æ–‡ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ")

        syntax_errors = []
        for file_path in target_files:
            if not os.path.exists(file_path):
                syntax_errors.append(f"ãƒ•ã‚¡ã‚¤ãƒ«æœªå­˜åœ¨: {file_path}")
                continue

            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                # åŸºæœ¬çš„ãªæ§‹æ–‡ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯
                if not content.strip():
                    syntax_errors.append(f"ç©ºãƒ•ã‚¡ã‚¤ãƒ«: {file_path}")
                elif 'SyntaxError' in content:
                    syntax_errors.append(f"æ§‹æ–‡ã‚¨ãƒ©ãƒ¼å«æœ‰: {file_path}")

            except Exception as e:
                syntax_errors.append(f"èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {file_path} - {str(e)}")

        passed = len(syntax_errors) == 0

        return {
            "layer": 1,
            "validation_type": "failsafe_syntax_validation",
            "passed": passed,
            "failsafe_mode": True,
            "syntax_errors": syntax_errors,
            "files_checked": len(target_files),
            "timestamp": datetime.datetime.now().isoformat()
        }

    def _failsafe_basic_quality_check(self, target_files: List[str]) -> Dict[str, Any]:
        """ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•: åŸºæœ¬å“è³ªãƒã‚§ãƒƒã‚¯"""

        print("ğŸ”„ ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•: åŸºæœ¬å“è³ªãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ")

        quality_issues = []
        for file_path in target_files:
            if not os.path.exists(file_path):
                continue

            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                lines = content.split('\n')

                # åŸºæœ¬å“è³ªãƒã‚§ãƒƒã‚¯
                if len(lines) > 1000:
                    quality_issues.append(f"å¤§ãã™ãã‚‹ãƒ•ã‚¡ã‚¤ãƒ«: {file_path} ({len(lines)}è¡Œ)")

                if 'TODO' in content or 'FIXME' in content:
                    quality_issues.append(f"æœªè§£æ±ºTODO/FIXME: {file_path}")

            except Exception as e:
                quality_issues.append(f"å“è³ªãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {file_path} - {str(e)}")

        passed = len(quality_issues) <= 2  # è»½å¾®ãªå•é¡Œã¯è¨±å®¹

        return {
            "layer": 2,
            "validation_type": "failsafe_quality_validation",
            "passed": passed,
            "failsafe_mode": True,
            "quality_issues": quality_issues,
            "files_checked": len(target_files),
            "timestamp": datetime.datetime.now().isoformat()
        }

    def _failsafe_basic_approval_check(self, target_files: List[str],
                                     layer1_result: Optional[Dict[str, Any]] = None,
                                     layer2_result: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•: åŸºæœ¬æ‰¿èªãƒã‚§ãƒƒã‚¯"""

        print("ğŸ”„ ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•: åŸºæœ¬æ‰¿èªãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ")

        # æœ€ä½é™ã®æ‰¿èªæ¡ä»¶
        layer1_ok = layer1_result and layer1_result.get("passed", False)
        layer2_ok = layer2_result and layer2_result.get("passed", False)

        # ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ã§ã¯ç·©ã„åŸºæº–ã§æ‰¿èª
        basic_approved = layer1_ok or (len(target_files) > 0 and all(os.path.exists(f) for f in target_files))

        return {
            "layer": 3,
            "validation_type": "failsafe_claude_approval",
            "approved": basic_approved,
            "failsafe_mode": True,
            "approval_basis": "basic_file_existence_check" if basic_approved else "no_valid_files",
            "files_checked": len(target_files),
            "layer1_input": layer1_ok,
            "layer2_input": layer2_ok,
            "timestamp": datetime.datetime.now().isoformat()
        }

    def _complete_failsafe_verification(self, target_files: List[str],
                                      error_message: str, execution_time: float) -> Dict[str, Any]:
        """å®Œå…¨ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•æ¤œè¨¼"""

        print("ğŸ†˜ å®Œå…¨ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ãƒ¢ãƒ¼ãƒ‰å®Ÿè¡Œ")

        return {
            "layer1_result": {"passed": False, "failsafe_mode": True},
            "layer2_result": {"passed": False, "failsafe_mode": True},
            "layer3_result": {"approved": False, "failsafe_mode": True},
            "integration_evaluation": {"passed": False, "failsafe_mode": True},
            "failsafe_activated": ["complete_system_failure"],
            "overall_status": "complete_failsafe_mode",
            "error_message": error_message,
            "total_execution_time": execution_time,
            "files_attempted": len(target_files),
            "recommendations": [
                "ã‚·ã‚¹ãƒ†ãƒ è¨­å®šã®ç¢ºèªãŒå¿…è¦ã§ã™",
                "ä¾å­˜é–¢ä¿‚ã®ç¢ºèªã‚’è¡Œã£ã¦ãã ã•ã„",
                "ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’å–å¾—ã—ã¦ãã ã•ã„"
            ],
            "timestamp": datetime.datetime.now().isoformat()
        }

    def _log_failsafe_usage(self, failsafe_types: List[str], target_files: List[str]) -> None:
        """ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ä½¿ç”¨ã®ãƒ­ã‚°è¨˜éŒ²"""

        log_entry = {
            "timestamp": datetime.datetime.now().isoformat(),
            "failsafe_types": failsafe_types,
            "target_files": target_files,
            "total_files": len(target_files),
            "failsafe_count": len(failsafe_types)
        }

        # ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã«è¨˜éŒ²
        failsafe_log_path = Path("postbox/monitoring/failsafe_usage.json")
        failsafe_log_path.parent.mkdir(parents=True, exist_ok=True)

        try:
            failsafe_logs = []
            if failsafe_log_path.exists():
                with open(failsafe_log_path, 'r', encoding='utf-8') as f:
                    failsafe_logs = json.load(f)

            failsafe_logs.append(log_entry)

            # ãƒ­ã‚°ã‚µã‚¤ã‚ºåˆ¶é™ï¼ˆæœ€æ–°100ä»¶ï¼‰
            if len(failsafe_logs) > 100:
                failsafe_logs = failsafe_logs[-100:]

            with open(failsafe_log_path, 'w', encoding='utf-8') as f:
                json.dump(failsafe_logs, f, indent=2, ensure_ascii=False)

            print(f"ğŸ“ ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ãƒ­ã‚°è¨˜éŒ²: {failsafe_log_path}")

        except Exception as e:
            print(f"âš ï¸ ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ãƒ­ã‚°è¨˜éŒ²ã‚¨ãƒ©ãƒ¼: {e}")

    def execute_complete_three_layer_verification(self, target_files: List[str],
                                                context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """å®Œå…¨3å±¤æ¤œè¨¼å®Ÿè¡Œï¼ˆçµ±åˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼‰"""

        print("ğŸ¯ å®Œå…¨3å±¤æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œé–‹å§‹")
        print(f"ğŸ“ å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {len(target_files)}ä»¶")

        context = context or {}

        # ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ä»˜ã3å±¤æ¤œè¨¼ã‚’å®Ÿè¡Œ
        verification_result = self.run_three_layer_verification_with_failsafe(target_files, context)

        # çµæœã®è©³ç´°åˆ†æã¨ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        enhanced_result = self._enhance_verification_result(verification_result, target_files, context)

        # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›
        self._print_verification_summary(enhanced_result)

        # ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã¸ã®çµæœè¨˜éŒ²
        self._record_verification_metrics(enhanced_result)

        return enhanced_result

    def _enhance_verification_result(self, verification_result: Dict[str, Any],
                                   target_files: List[str], context: Dict[str, Any]) -> Dict[str, Any]:
        """æ¤œè¨¼çµæœã®æ‹¡å¼µãƒ»åˆ†æ"""

        enhanced = verification_result.copy()

        # è©³ç´°çµ±è¨ˆã®è¿½åŠ 
        enhanced["detailed_statistics"] = {
            "total_files_processed": len(target_files),
            "layer1_success_rate": 1.0 if verification_result["layer1_result"].get("passed") else 0.0,
            "layer2_success_rate": 1.0 if verification_result["layer2_result"].get("passed") else 0.0,
            "layer3_approval_rate": 1.0 if verification_result["layer3_result"].get("approved") else 0.0,
            "integration_success_rate": 1.0 if verification_result["integration_evaluation"].get("passed") else 0.0,
            "overall_success_rate": self._calculate_overall_success_rate(verification_result),
            "failsafe_usage_rate": len(verification_result["failsafe_activated"]) / 4.0  # 4å±¤ã§ã®ä½¿ç”¨ç‡
        }

        # æ”¹å–„æ¨å¥¨äº‹é …ã®ç”Ÿæˆ
        enhanced["improvement_roadmap"] = self._generate_improvement_roadmap(verification_result, context)

        # æ¬¡å›å®Ÿè¡Œã¸ã®ææ¡ˆ
        enhanced["next_execution_suggestions"] = self._generate_next_execution_suggestions(verification_result)

        # å“è³ªãƒˆãƒ¬ãƒ³ãƒ‰ã®äºˆæ¸¬
        enhanced["quality_trend_prediction"] = self._predict_quality_trend(verification_result, target_files)

        return enhanced

    def _calculate_overall_success_rate(self, verification_result: Dict[str, Any]) -> float:
        """ç·åˆæˆåŠŸç‡è¨ˆç®—"""

        success_scores = []

        # Layer 1
        if verification_result["layer1_result"].get("passed"):
            success_scores.append(1.0)
        else:
            success_scores.append(0.0)

        # Layer 2
        if verification_result["layer2_result"].get("passed"):
            success_scores.append(1.0)
        else:
            success_scores.append(0.0)

        # Layer 3
        if verification_result["layer3_result"].get("approved"):
            success_scores.append(1.0)
        else:
            success_scores.append(0.0)

        # Integration
        if verification_result["integration_evaluation"].get("passed"):
            success_scores.append(1.0)
        else:
            success_scores.append(0.0)

        return sum(success_scores) / len(success_scores) if success_scores else 0.0

    def _generate_improvement_roadmap(self, verification_result: Dict[str, Any],
                                    context: Dict[str, Any]) -> Dict[str, Any]:
        """æ”¹å–„ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—ç”Ÿæˆ"""

        roadmap: Dict[str, Any] = {
            "immediate_actions": [],
            "short_term_goals": [],
            "long_term_improvements": [],
            "priority_order": []
        }

        # Layer 1ãŒå¤±æ•—ã—ãŸå ´åˆ
        if not verification_result["layer1_result"].get("passed"):
            roadmap["immediate_actions"].extend([
                "æ§‹æ–‡ã‚¨ãƒ©ãƒ¼ã®ä¿®æ­£",
                "åŸºæœ¬çš„ãªPythonæ–‡æ³•ãƒã‚§ãƒƒã‚¯",
                "ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ–‡ã®ç¢ºèª"
            ])
            roadmap["priority_order"].append("syntax_fixes")

        # Layer 2ãŒå¤±æ•—ã—ãŸå ´åˆ
        if not verification_result["layer2_result"].get("passed"):
            roadmap["short_term_goals"].extend([
                "ã‚³ãƒ¼ãƒ‰å“è³ªåŸºæº–ã®éµå®ˆ",
                "ãƒªãƒ³ãƒˆæŒ‡æ‘˜äº‹é …ã®è§£æ±º",
                "ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆçµ±ä¸€"
            ])
            roadmap["priority_order"].append("quality_improvements")

        # Layer 3ãŒæ‰¿èªã•ã‚Œãªã‹ã£ãŸå ´åˆ
        if not verification_result["layer3_result"].get("approved"):
            roadmap["short_term_goals"].extend([
                "Claudeæ‰¿èªåŸºæº–ã®ç¢ºèª",
                "åŒ…æ‹¬çš„å“è³ªå‘ä¸Š",
                "ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸æ”¹å–„"
            ])
            roadmap["priority_order"].append("approval_requirements")

        # çµ±åˆè©•ä¾¡ãŒå¤±æ•—ã—ãŸå ´åˆ
        if not verification_result["integration_evaluation"].get("passed"):
            roadmap["long_term_improvements"].extend([
                "ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«æ§‹é€ ã®æ”¹å–„",
                "çµ±åˆãƒ†ã‚¹ãƒˆã®è¿½åŠ ",
                "ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®è¦‹ç›´ã—"
            ])
            roadmap["priority_order"].append("architecture_improvements")

        # ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ãŒä½¿ç”¨ã•ã‚ŒãŸå ´åˆ
        if verification_result["failsafe_activated"]:
            roadmap["immediate_actions"].append("ã‚·ã‚¹ãƒ†ãƒ è¨­å®šãƒ»ä¾å­˜é–¢ä¿‚ã®ç¢ºèª")
            roadmap["priority_order"].insert(0, "system_stability")

        return roadmap

    def _generate_next_execution_suggestions(self, verification_result: Dict[str, Any]) -> List[str]:
        """æ¬¡å›å®Ÿè¡Œææ¡ˆç”Ÿæˆ"""

        suggestions = []

        overall_status = verification_result.get("overall_status", "unknown")

        if overall_status == "fully_verified":
            suggestions.extend([
                "å“è³ªãƒ¬ãƒ™ãƒ«ãŒé«˜ã„ãŸã‚ã€å®šæœŸçš„ãªãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ãƒã‚§ãƒƒã‚¯æ¨å¥¨",
                "æ–°æ©Ÿèƒ½è¿½åŠ æ™‚ã®ç¶™ç¶šçš„å“è³ªç›£è¦–ã‚’æ¨å¥¨",
                "ç¾åœ¨ã®å“è³ªãƒ¬ãƒ™ãƒ«ç¶­æŒã®ãŸã‚ã®å®šæœŸãƒã‚§ãƒƒã‚¯"
            ])
        elif overall_status == "approved_with_conditions":
            suggestions.extend([
                "æ¡ä»¶ä»˜ãæ‰¿èªã®ãŸã‚ã€æŒ‡æ‘˜äº‹é …ã®ä¿®æ­£å¾Œã«å†æ¤œè¨¼æ¨å¥¨",
                "éƒ¨åˆ†çš„æ”¹å–„å¾Œã®æ®µéšçš„å†ãƒã‚§ãƒƒã‚¯",
                "ç‰¹å®šLayerå†å®Ÿè¡Œã§ã®ãƒ”ãƒ³ãƒã‚¤ãƒ³ãƒˆæ”¹å–„"
            ])
        elif overall_status in ["quality_verified", "syntax_verified"]:
            suggestions.extend([
                "æœªé€šéLayerã®é›†ä¸­çš„ãªæ”¹å–„ãŒå¿…è¦",
                "æ®µéšçš„å“è³ªå‘ä¸Šã‚¢ãƒ—ãƒ­ãƒ¼ãƒæ¨å¥¨",
                "ç‰¹åŒ–å‹å“è³ªæ”¹å–„ãƒ„ãƒ¼ãƒ«ã®æ´»ç”¨æ¤œè¨"
            ])
        else:
            suggestions.extend([
                "åŸºæœ¬çš„ãªå“è³ªç¢ºä¿ã‹ã‚‰é–‹å§‹ã™ã‚‹æ®µéšçš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒå¿…è¦",
                "è‡ªå‹•ä¿®æ­£ãƒ„ãƒ¼ãƒ«ã®ç©æ¥µæ´»ç”¨ã‚’æ¨å¥¨",
                "å¤–éƒ¨å“è³ªç›£æŸ»ãƒ„ãƒ¼ãƒ«ã¨ã®é€£æºæ¤œè¨"
            ])

        return suggestions

    def _predict_quality_trend(self, verification_result: Dict[str, Any],
                             target_files: List[str]) -> Dict[str, Any]:
        """å“è³ªãƒˆãƒ¬ãƒ³ãƒ‰äºˆæ¸¬"""

        return {
            "current_quality_level": verification_result.get("overall_status", "unknown"),
            "predicted_improvement_time": "1-2é€±é–“" if verification_result.get("overall_status") in [
                "quality_verified", "approved_with_conditions"
            ] else "1-2ãƒ¶æœˆ",
            "improvement_confidence": 0.8 if not verification_result["failsafe_activated"] else 0.6,
            "recommended_check_frequency": "é€±1å›" if verification_result.get("overall_status") == "fully_verified" else "æ¯æ—¥",
            "risk_factors": [
                "ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ä½¿ç”¨" if verification_result["failsafe_activated"] else "å®‰å®šå‹•ä½œ",
                f"å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(target_files)}",
                f"å®Ÿè¡Œæ™‚é–“: {verification_result.get('total_execution_time', 0):.1f}ç§’"
            ]
        }

    def _print_verification_summary(self, enhanced_result: Dict[str, Any]) -> None:
        """æ¤œè¨¼çµæœã‚µãƒãƒªãƒ¼å‡ºåŠ›"""

        print("\n" + "="*60)
        print("ğŸ“Š 3å±¤æ¤œè¨¼ä½“åˆ¶ å®Œå…¨å®Ÿè¡Œçµæœã‚µãƒãƒªãƒ¼")
        print("="*60)

        # åŸºæœ¬ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
        overall_status = enhanced_result.get("overall_status", "unknown")
        print(f"ğŸ¯ ç·åˆã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {overall_status}")
        print(f"â±ï¸  å®Ÿè¡Œæ™‚é–“: {enhanced_result.get('total_execution_time', 0):.2f}ç§’")

        # å„å±¤ã®çµæœ
        print(f"\nğŸ“‹ å„å±¤ã®çµæœ:")
        layer1_status = "âœ… PASS" if enhanced_result["layer1_result"].get("passed") else "âŒ FAIL"
        layer2_status = "âœ… PASS" if enhanced_result["layer2_result"].get("passed") else "âŒ FAIL"
        layer3_status = "âœ… APPROVED" if enhanced_result["layer3_result"].get("approved") else "âŒ REJECTED"
        integration_status = "âœ… PASS" if enhanced_result["integration_evaluation"].get("passed") else "âŒ FAIL"

        print(f"  ğŸ” Layer 1 (æ§‹æ–‡æ¤œè¨¼): {layer1_status}")
        print(f"  ğŸ›¡ï¸  Layer 2 (å“è³ªæ¤œè¨¼): {layer2_status}")
        print(f"  ğŸ‘¨â€ğŸ’» Layer 3 (Claudeæ‰¿èª): {layer3_status}")
        print(f"  ğŸ”— çµ±åˆå“è³ªè©•ä¾¡: {integration_status}")

        # çµ±è¨ˆæƒ…å ±
        stats = enhanced_result.get("detailed_statistics", {})
        print(f"\nğŸ“ˆ è©³ç´°çµ±è¨ˆ:")
        print(f"  ç·åˆæˆåŠŸç‡: {stats.get('overall_success_rate', 0):.1%}")
        print(f"  ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•ä½¿ç”¨ç‡: {stats.get('failsafe_usage_rate', 0):.1%}")

        # ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•æƒ…å ±
        if enhanced_result.get("failsafe_activated"):
            print(f"\nğŸ”„ ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•å®Ÿè¡Œ: {len(enhanced_result['failsafe_activated'])}å›")
            for i, fs_type in enumerate(enhanced_result['failsafe_activated'], 1):
                print(f"  {i}. {fs_type}")

        # æ”¹å–„ææ¡ˆ
        roadmap = enhanced_result.get("improvement_roadmap", {})
        if roadmap.get("immediate_actions"):
            print(f"\nğŸš€ ç·Šæ€¥æ”¹å–„äº‹é …:")
            for action in roadmap["immediate_actions"][:3]:
                print(f"  â€¢ {action}")

        print("="*60)

    def _record_verification_metrics(self, enhanced_result: Dict[str, Any]) -> None:
        """æ¤œè¨¼ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²"""

        metrics_entry = {
            "timestamp": datetime.datetime.now().isoformat(),
            "overall_status": enhanced_result.get("overall_status"),
            "execution_time": enhanced_result.get("total_execution_time"),
            "success_rates": enhanced_result.get("detailed_statistics", {}),
            "failsafe_usage": len(enhanced_result.get("failsafe_activated", [])),
            "layer_results": {
                "layer1_passed": enhanced_result["layer1_result"].get("passed"),
                "layer2_passed": enhanced_result["layer2_result"].get("passed"),
                "layer3_approved": enhanced_result["layer3_result"].get("approved"),
                "integration_passed": enhanced_result["integration_evaluation"].get("passed")
            }
        }

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã«è¨˜éŒ²
        metrics_path = Path("postbox/monitoring/three_layer_verification_metrics.json")
        metrics_path.parent.mkdir(parents=True, exist_ok=True)

        try:
            metrics_history = []
            if metrics_path.exists():
                with open(metrics_path, 'r', encoding='utf-8') as f:
                    metrics_history = json.load(f)

            metrics_history.append(metrics_entry)

            # å±¥æ­´ã‚µã‚¤ã‚ºåˆ¶é™ï¼ˆæœ€æ–°200ä»¶ï¼‰
            if len(metrics_history) > 200:
                metrics_history = metrics_history[-200:]

            with open(metrics_path, 'w', encoding='utf-8') as f:
                json.dump(metrics_history, f, indent=2, ensure_ascii=False)

            print(f"ğŸ“Š æ¤œè¨¼ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²: {metrics_path}")

        except Exception as e:
            print(f"âš ï¸ ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²ã‚¨ãƒ©ãƒ¼: {e}")

    def _determine_quality_level(self, score: float) -> QualityLevel:
        """ã‚¹ã‚³ã‚¢ã‹ã‚‰å“è³ªãƒ¬ãƒ™ãƒ«ã‚’åˆ¤å®š"""
        if score >= 0.95:
            return QualityLevel.EXCELLENT
        elif score >= 0.80:
            return QualityLevel.GOOD
        elif score >= 0.60:
            return QualityLevel.ACCEPTABLE
        elif score >= 0.40:
            return QualityLevel.POOR
        else:
            return QualityLevel.CRITICAL

    # ========== çµ±åˆãƒ†ã‚¹ãƒˆã‚·ã‚¹ãƒ†ãƒ çµ±åˆãƒ¡ã‚½ãƒƒãƒ‰ (Issue #859) ==========

    def run_integration_test_suite(self, target_files: List[str],
                                  context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆå®Ÿè¡Œ

        Args:
            target_files: ãƒ†ã‚¹ãƒˆå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆ
            context: ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ

        Returns:
            Dict[str, Any]: çµ±åˆãƒ†ã‚¹ãƒˆçµæœ
        """

        print("ğŸ§ª çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆå®Ÿè¡Œé–‹å§‹")

        if not INTEGRATION_TEST_AVAILABLE or not self.integration_test_system:
            return {
                "error": "çµ±åˆãƒ†ã‚¹ãƒˆã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“",
                "available": False,
                "timestamp": datetime.datetime.now().isoformat()
            }

        context = context or {}

        try:
            integration_results = []

            for file_path in target_files:
                if not os.path.exists(file_path):
                    print(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {file_path}")
                    continue

                # æ–°è¦å®Ÿè£…çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
                result = self.integration_test_system.test_new_implementation(
                    file_path, context
                )
                integration_results.append(result)

            # äº’æ›æ€§æ¤œè¨¼
            compatibility_result = self.integration_test_system.verify_existing_compatibility(
                target_files
            )

            # å“è³ªåŸºæº–é©åˆç¢ºèª
            quality_results = []
            for file_path in target_files:
                if os.path.exists(file_path):
                    quality_result = self.integration_test_system.validate_quality_standards(
                        file_path
                    )
                    quality_results.append(quality_result)

            # ç·åˆè©•ä¾¡
            overall_score = self._calculate_integration_overall_score(
                integration_results, compatibility_result, quality_results
            )

            suite_result = {
                "integration_tests": [
                    {
                        "test_id": r.test_id,
                        "status": r.status.value,
                        "score": r.score,
                        "target_file": r.target_files[0] if r.target_files else "",
                        "execution_time": r.execution_time,
                        "recommendations": r.recommendations
                    } for r in integration_results
                ],
                "compatibility_check": {
                    "compatibility_score": compatibility_result.compatibility_score,
                    "backward_compatible": compatibility_result.backward_compatible,
                    "forward_compatible": compatibility_result.forward_compatible,
                    "breaking_changes": compatibility_result.breaking_changes,
                    "integration_issues": compatibility_result.integration_issues
                },
                "quality_validation": [
                    {
                        "file": target_files[i] if i < len(target_files) else "unknown",
                        "overall_score": qr.overall_quality_score,
                        "quality_level": qr.quality_level,
                        "test_coverage": qr.test_coverage,
                        "improvement_suggestions": qr.improvement_suggestions
                    } for i, qr in enumerate(quality_results)
                ],
                "overall_assessment": {
                    "overall_score": overall_score,
                    "total_tests_run": len(integration_results),
                    "tests_passed": len([r for r in integration_results if r.status.value == "pass"]),
                    "tests_failed": len([r for r in integration_results if r.status.value == "fail"]),
                    "compatibility_passed": compatibility_result.compatibility_score >= 0.75,
                    "quality_standards_met": all(qr.overall_quality_score >= 0.70 for qr in quality_results)
                },
                "timestamp": datetime.datetime.now().isoformat()
            }

            print(f"âœ… çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆå®Ÿè¡Œå®Œäº†: ç·åˆã‚¹ã‚³ã‚¢ {overall_score:.3f}")

            return suite_result

        except Exception as e:
            print(f"âŒ çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            return {
                "error": str(e),
                "available": True,
                "execution_failed": True,
                "timestamp": datetime.datetime.now().isoformat()
            }

    def generate_test_coverage_report(self, target_files: List[str]) -> Dict[str, Any]:
        """ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ

        Args:
            target_files: å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆ

        Returns:
            Dict[str, Any]: ã‚«ãƒãƒ¬ãƒƒã‚¸ãƒ¬ãƒãƒ¼ãƒˆ
        """

        print("ğŸ“Š ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆé–‹å§‹")

        if not INTEGRATION_TEST_AVAILABLE or not self.test_generator:
            return {
                "error": "ãƒ†ã‚¹ãƒˆç”Ÿæˆã‚¨ãƒ³ã‚¸ãƒ³ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“",
                "available": False
            }

        try:
            # åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆç”Ÿæˆ
            test_suite = self.test_generator.generate_comprehensive_test_suite(
                target_files, GenerationStrategy.COVERAGE_DRIVEN
            )

            # ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ
            test_file_path = self.test_generator.generate_test_file(test_suite)

            coverage_report = {
                "test_suite_info": {
                    "suite_id": test_suite.suite_id,
                    "total_tests": test_suite.total_tests,
                    "estimated_coverage": test_suite.estimated_coverage,
                    "generation_strategy": test_suite.generation_strategy.value,
                    "target_modules": test_suite.target_module
                },
                "coverage_analysis": {
                    "files_analyzed": len(target_files),
                    "test_cases_generated": len(test_suite.test_cases),
                    "coverage_gaps": self._identify_coverage_gaps(target_files, test_suite),
                    "improvement_opportunities": self._suggest_coverage_improvements(test_suite)
                },
                "generated_test_file": test_file_path,
                "recommendations": [
                    "ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’å®Ÿè¡Œã—ã¦ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚’ç¢ºèªã—ã¦ãã ã•ã„",
                    "ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚®ãƒ£ãƒƒãƒ—ã‚’æ‰‹å‹•ã§è£œå®Œã™ã‚‹ã“ã¨ã‚’æ¤œè¨ã—ã¦ãã ã•ã„",
                    "å®šæœŸçš„ã«ã‚«ãƒãƒ¬ãƒƒã‚¸ãƒ¬ãƒãƒ¼ãƒˆã‚’æ›´æ–°ã—ã¦ãã ã•ã„"
                ],
                "timestamp": datetime.datetime.now().isoformat()
            }

            print(f"âœ… ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {test_suite.total_tests}ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹")

            return coverage_report

        except Exception as e:
            print(f"âŒ ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return {
                "error": str(e),
                "available": True,
                "generation_failed": True,
                "timestamp": datetime.datetime.now().isoformat()
            }

    def run_new_implementation_quality_check(self, implementation_path: str,
                                           context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """æ–°è¦å®Ÿè£…å‘ã‘å“è³ªãƒã‚§ãƒƒã‚¯ï¼ˆçµ±åˆãƒ†ã‚¹ãƒˆå«ã‚€ï¼‰

        Args:
            implementation_path: æ–°è¦å®Ÿè£…ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            context: å®Ÿè£…ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã€è¦ä»¶ç­‰ï¼‰

        Returns:
            Dict[str, Any]: å“è³ªãƒã‚§ãƒƒã‚¯çµæœ
        """

        print(f"ğŸ” æ–°è¦å®Ÿè£…å‘ã‘å“è³ªãƒã‚§ãƒƒã‚¯é–‹å§‹: {implementation_path}")

        context = context or {}

        # 1. åŸºæœ¬å“è³ªãƒã‚§ãƒƒã‚¯
        basic_quality = self.run_comprehensive_check([implementation_path], "claude")

        # 2. çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        integration_result = None
        if INTEGRATION_TEST_AVAILABLE and self.integration_test_system:
            try:
                integration_result = self.integration_test_system.test_new_implementation(
                    implementation_path, context
                )
            except Exception as e:
                print(f"âš ï¸ çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")

        # 3. ãƒ†ã‚¹ãƒˆç”Ÿæˆ
        test_generation_result = None
        if INTEGRATION_TEST_AVAILABLE and self.test_generator:
            try:
                test_generation_result = self.test_generator.generate_comprehensive_test_suite(
                    [implementation_path], GenerationStrategy.COMPREHENSIVE
                )
            except Exception as e:
                print(f"âš ï¸ ãƒ†ã‚¹ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")

        # 4. ç·åˆè©•ä¾¡
        comprehensive_result = {
            "basic_quality": {
                "overall_score": basic_quality.overall_score,
                "quality_level": basic_quality.quality_level.value,
                "error_count": basic_quality.error_count,
                "warning_count": basic_quality.warning_count,
                "improvement_suggestions": basic_quality.improvement_suggestions
            },
            "integration_test": {
                "available": integration_result is not None,
                "test_id": integration_result.test_id if integration_result else None,
                "status": integration_result.status.value if integration_result else "not_run",
                "score": integration_result.score if integration_result else 0.0,
                "execution_time": integration_result.execution_time if integration_result else 0.0,
                "recommendations": integration_result.recommendations if integration_result else []
            },
            "test_generation": {
                "available": test_generation_result is not None,
                "suite_id": test_generation_result.suite_id if test_generation_result else None,
                "total_tests": test_generation_result.total_tests if test_generation_result else 0,
                "estimated_coverage": test_generation_result.estimated_coverage if test_generation_result else 0.0
            },
            "overall_assessment": self._calculate_new_implementation_assessment(
                basic_quality, integration_result, test_generation_result, context
            ),
            "timestamp": datetime.datetime.now().isoformat()
        }

        print(f"âœ… æ–°è¦å®Ÿè£…å‘ã‘å“è³ªãƒã‚§ãƒƒã‚¯å®Œäº†")

        return comprehensive_result

    def _calculate_integration_overall_score(self, integration_results: List[Any],
                                           compatibility_result: Any,
                                           quality_results: List[Any]) -> float:
        """çµ±åˆãƒ†ã‚¹ãƒˆç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—"""

        scores = []

        # çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚³ã‚¢
        if integration_results:
            integration_avg = sum(r.score for r in integration_results) / len(integration_results)
            scores.append(integration_avg * 0.4)  # 40%ã®é‡ã¿

        # äº’æ›æ€§ã‚¹ã‚³ã‚¢
        if compatibility_result:
            scores.append(compatibility_result.compatibility_score * 0.3)  # 30%ã®é‡ã¿

        # å“è³ªã‚¹ã‚³ã‚¢
        if quality_results:
            quality_avg = sum(qr.overall_quality_score for qr in quality_results) / len(quality_results)
            scores.append(quality_avg * 0.3)  # 30%ã®é‡ã¿

        return sum(scores) if scores else 0.0

    def _identify_coverage_gaps(self, target_files: List[str], test_suite: Any) -> List[str]:
        """ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚®ãƒ£ãƒƒãƒ—ç‰¹å®š"""

        gaps = []

        for file_path in target_files:
            if not os.path.exists(file_path):
                gaps.append(f"ãƒ•ã‚¡ã‚¤ãƒ«æœªå­˜åœ¨: {file_path}")
                continue

            # ç°¡æ˜“ã‚®ãƒ£ãƒƒãƒ—åˆ†æ
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                # é–¢æ•°ãƒ»ã‚¯ãƒ©ã‚¹æ•°ã¨ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹æ•°ã®æ¯”è¼ƒ
                function_count = content.count("def ")
                class_count = content.count("class ")

                estimated_testable_units = function_count + class_count
                actual_test_cases = len(test_suite.test_cases) if test_suite else 0

                if actual_test_cases < estimated_testable_units:
                    gaps.append(f"{file_path}: æ¨å®š{estimated_testable_units}å˜ä½ã«å¯¾ã—{actual_test_cases}ãƒ†ã‚¹ãƒˆ")

            except Exception as e:
                gaps.append(f"{file_path}: åˆ†æã‚¨ãƒ©ãƒ¼ - {str(e)}")

        return gaps

    def _suggest_coverage_improvements(self, test_suite: Any) -> List[str]:
        """ã‚«ãƒãƒ¬ãƒƒã‚¸æ”¹å–„ææ¡ˆ"""

        suggestions = []

        if not test_suite:
            suggestions.append("ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
            return suggestions

        if test_suite.estimated_coverage < 0.8:
            suggestions.append("æ¨å®šã‚«ãƒãƒ¬ãƒƒã‚¸ãŒ80%æœªæº€ã§ã™ã€‚è¿½åŠ ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’æ¤œè¨ã—ã¦ãã ã•ã„")

        if test_suite.total_tests < 5:
            suggestions.append("ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹æ•°ãŒå°‘ãªã™ãã¾ã™ã€‚å¢ƒç•Œå€¤ãƒ†ã‚¹ãƒˆã‚„ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ã®è¿½åŠ ã‚’æ¨å¥¨ã—ã¾ã™")

        # ãƒ†ã‚¹ãƒˆã‚¿ã‚¤ãƒ—åˆ¥ã®ææ¡ˆ
        test_types = set()
        for test_case in test_suite.test_cases:
            test_types.add(test_case.test_type.value)

        if "boundary" not in test_types:
            suggestions.append("å¢ƒç•Œå€¤ãƒ†ã‚¹ãƒˆã®è¿½åŠ ã‚’æ¨å¥¨ã—ã¾ã™")

        if "integration" not in test_types:
            suggestions.append("çµ±åˆãƒ†ã‚¹ãƒˆã®è¿½åŠ ã‚’æ¨å¥¨ã—ã¾ã™")

        if not suggestions:
            suggestions.append("ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ã¯é©åˆ‡ã§ã™")

        return suggestions

    def _calculate_new_implementation_assessment(self, basic_quality: Any,
                                               integration_result: Any,
                                               test_generation_result: Any,
                                               context: Dict[str, Any]) -> Dict[str, Any]:
        """æ–°è¦å®Ÿè£…ç·åˆè©•ä¾¡è¨ˆç®—"""

        # ã‚¹ã‚³ã‚¢è¨ˆç®—
        quality_score = basic_quality.overall_score
        integration_score = integration_result.score if integration_result else 0.0
        test_generation_score = (test_generation_result.estimated_coverage
                               if test_generation_result else 0.0)

        # é‡ã¿ä»˜ãå¹³å‡
        weights = {"quality": 0.5, "integration": 0.3, "test_generation": 0.2}

        overall_score = (
            quality_score * weights["quality"] +
            integration_score * weights["integration"] +
            test_generation_score * weights["test_generation"]
        )

        # è©•ä¾¡åˆ¤å®š
        if overall_score >= 0.8:
            assessment = "EXCELLENT"
            recommendation = "æ–°è¦å®Ÿè£…ã¯é«˜å“è³ªã§ã™ã€‚æœ¬ç•ªãƒ‡ãƒ—ãƒ­ã‚¤ã®æº–å‚™ãŒã§ãã¦ã„ã¾ã™"
        elif overall_score >= 0.7:
            assessment = "GOOD"
            recommendation = "æ–°è¦å®Ÿè£…ã¯è‰¯å¥½ãªå“è³ªã§ã™ã€‚è»½å¾®ãªæ”¹å–„å¾Œã«ãƒ‡ãƒ—ãƒ­ã‚¤å¯èƒ½ã§ã™"
        elif overall_score >= 0.6:
            recommendation = "æ–°è¦å®Ÿè£…ã¯è¨±å®¹ç¯„å›²å†…ã§ã™ãŒã€æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚Šã¾ã™"
            assessment = "ACCEPTABLE"
        else:
            assessment = "POOR"
            recommendation = "æ–°è¦å®Ÿè£…ã¯å“è³ªåŸºæº–ã‚’ä¸‹å›ã£ã¦ã„ã¾ã™ã€‚å¤§å¹…ãªæ”¹å–„ãŒå¿…è¦ã§ã™"

        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè€ƒæ…®
        task_type = context.get("task_type", "unknown")
        if task_type in ["new_implementation", "hybrid_implementation"]:
            # æ–°è¦å®Ÿè£…ã¯è‹¥å¹²åŸºæº–ã‚’ç·©å’Œ
            if overall_score >= 0.65:
                assessment = "ACCEPTABLE_FOR_NEW_IMPLEMENTATION"

        return {
            "overall_score": overall_score,
            "assessment": assessment,
            "recommendation": recommendation,
            "score_breakdown": {
                "quality": quality_score,
                "integration": integration_score,
                "test_generation": test_generation_score
            },
            "context_applied": task_type,
            "ready_for_deployment": overall_score >= 0.7
        }

def main() -> None:
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
    qm = QualityManager()

    test_files = [
        "kumihan_formatter/core/utilities/logger.py",
        "kumihan_formatter/config/base_config.py"
    ]

    metrics = qm.run_comprehensive_check(test_files, "claude")

    print(f"\nğŸ“Š å“è³ªãƒã‚§ãƒƒã‚¯çµæœ:")
    print(f"ç·åˆã‚¹ã‚³ã‚¢: {metrics.overall_score:.3f}")
    print(f"å“è³ªãƒ¬ãƒ™ãƒ«: {metrics.quality_level.value}")
    print(f"ã‚¨ãƒ©ãƒ¼æ•°: {metrics.error_count}")
    print(f"è­¦å‘Šæ•°: {metrics.warning_count}")

    if metrics.improvement_suggestions:
        print(f"\nğŸ’¡ æ”¹å–„ææ¡ˆ:")
        for suggestion in metrics.improvement_suggestions:
            print(f"  {suggestion}")

if __name__ == "__main__":
    main()
