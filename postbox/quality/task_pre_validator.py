#!/usr/bin/env python3
"""
Task Pre-Validation System
ã‚¿ã‚¹ã‚¯äº‹å‰æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ  - ä¸è¦ã‚¿ã‚¹ã‚¯ã®äº‹å‰æ’é™¤ã«ã‚ˆã‚‹åŠ¹ç‡åŒ–
"""

import os
import json
import subprocess
import datetime
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
from dataclasses import dataclass
from enum import Enum


class ValidationResult(Enum):
    """æ¤œè¨¼çµæœ"""
    VALID = "valid"                 # å®Ÿè¡Œã™ã¹ã
    SKIP_NOT_NEEDED = "skip_not_needed"     # ä¿®æ­£ä¸è¦ã«ã‚ˆã‚Šã‚¹ã‚­ãƒƒãƒ—
    SKIP_FILE_MISSING = "skip_file_missing" # ãƒ•ã‚¡ã‚¤ãƒ«ä¸å­˜åœ¨ã«ã‚ˆã‚Šã‚¹ã‚­ãƒƒãƒ—
    SKIP_ALREADY_FIXED = "skip_already_fixed"   # æ—¢ã«ä¿®æ­£æ¸ˆã¿
    SKIP_INVALID_TARGET = "skip_invalid_target" # ç„¡åŠ¹ãªã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
    DEFER_HIGH_RISK = "defer_high_risk"     # é«˜ãƒªã‚¹ã‚¯ã«ã‚ˆã‚Šå»¶æœŸ


@dataclass
class PreValidationReport:
    """äº‹å‰æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆ"""
    task_id: str
    original_task: Dict[str, Any]
    validation_result: ValidationResult
    reason: str
    estimated_fix_count: int
    current_error_count: int
    file_states: Dict[str, str]
    risk_assessment: str
    recommendations: List[str]
    skip_reason_details: Optional[Dict[str, Any]] = None


class TaskPreValidator:
    """ã‚¿ã‚¹ã‚¯äº‹å‰æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self) -> None:
        self.validation_cache: Dict[str, Any] = {}
        self.mypy_cache: Dict[str, Dict[str, int]] = {}
        self.file_state_cache: Dict[str, str] = {}

        # æ¤œè¨¼åŸºæº–è¨­å®š
        self.validation_criteria: Dict[str, Any] = {
            "min_error_count_threshold": 1,  # æœ€å°ã‚¨ãƒ©ãƒ¼æ•°é–¾å€¤
            "max_file_size_mb": 10,         # æœ€å¤§ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º
            "supported_extensions": [".py"], # å¯¾å¿œæ‹¡å¼µå­
            "skip_test_files": True,        # ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—
            "max_file_count_per_task": 20   # ã‚¿ã‚¹ã‚¯ã‚ãŸã‚Šæœ€å¤§ãƒ•ã‚¡ã‚¤ãƒ«æ•°
        }

        print("ğŸ” TaskPreValidator åˆæœŸåŒ–å®Œäº†")

    def validate_task(self, task_data: Dict[str, Any]) -> PreValidationReport:
        """
        ã‚¿ã‚¹ã‚¯ã®äº‹å‰æ¤œè¨¼å®Ÿè¡Œ

        Args:
            task_data: æ¤œè¨¼å¯¾è±¡ã‚¿ã‚¹ã‚¯ãƒ‡ãƒ¼ã‚¿

        Returns:
            PreValidationReport: æ¤œè¨¼çµæœãƒ¬ãƒãƒ¼ãƒˆ
        """

        task_id = task_data.get("task_id", "unknown")
        task_type = task_data.get("type", "unknown")
        target_files = task_data.get("target_files", [])

        print(f"ğŸ” ã‚¿ã‚¹ã‚¯äº‹å‰æ¤œè¨¼é–‹å§‹: {task_id} ({task_type})")

        # Step 1: åŸºæœ¬çš„ãªå¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
        basic_validation = self._validate_basic_requirements(task_data)
        if basic_validation.validation_result != ValidationResult.VALID:
            return basic_validation

        # Step 2: ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ãƒ»çŠ¶æ…‹ãƒã‚§ãƒƒã‚¯
        file_validation = self._validate_file_states(task_data)
        if file_validation.validation_result != ValidationResult.VALID:
            return file_validation

        # Step 3: ã‚¨ãƒ©ãƒ¼ç¾æ³ãƒã‚§ãƒƒã‚¯ï¼ˆmypy/flake8ï¼‰
        error_validation = self._validate_current_errors(task_data)
        if error_validation.validation_result != ValidationResult.VALID:
            return error_validation

        # Step 4: ä¿®æ­£å¯èƒ½æ€§è©•ä¾¡
        fixability_validation = self._validate_fixability(task_data)
        if fixability_validation.validation_result != ValidationResult.VALID:
            return fixability_validation

        # Step 5: ãƒªã‚¹ã‚¯è©•ä¾¡
        risk_assessment = self._assess_task_risk(task_data)

        # æœ€çµ‚æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        return PreValidationReport(
            task_id=task_id,
            original_task=task_data,
            validation_result=ValidationResult.VALID,
            reason="ã™ã¹ã¦ã®æ¤œè¨¼ã‚’ã‚¯ãƒªã‚¢ - å®Ÿè¡Œæ¨å¥¨",
            estimated_fix_count=self._estimate_fix_count(task_data),
            current_error_count=self._count_current_errors(target_files, task_data.get("requirements", {}).get("error_type", "")),
            file_states=self._get_file_states(target_files),
            risk_assessment=risk_assessment,
            recommendations=self._generate_execution_recommendations(task_data)
        )

    def _validate_basic_requirements(self, task_data: Dict[str, Any]) -> PreValidationReport:
        """åŸºæœ¬è¦ä»¶æ¤œè¨¼"""

        task_id = task_data.get("task_id", "unknown")
        task_type = task_data.get("type", "")
        target_files = task_data.get("target_files", [])

        # ãƒ•ã‚¡ã‚¤ãƒ«æ•°ãƒã‚§ãƒƒã‚¯
        if len(target_files) > self.validation_criteria["max_file_count_per_task"]:
            return PreValidationReport(
                task_id=task_id,
                original_task=task_data,
                validation_result=ValidationResult.SKIP_INVALID_TARGET,
                reason=f"ãƒ•ã‚¡ã‚¤ãƒ«æ•°éå¤š: {len(target_files)}ä»¶ > {self.validation_criteria['max_file_count_per_task']}ä»¶",
                estimated_fix_count=0,
                current_error_count=0,
                file_states={},
                risk_assessment="high",
                recommendations=["ã‚¿ã‚¹ã‚¯ã‚’è¤‡æ•°ã«åˆ†å‰²ã—ã¦ãã ã•ã„"]
            )

        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒç©º
        if not target_files:
            return PreValidationReport(
                task_id=task_id,
                original_task=task_data,
                validation_result=ValidationResult.SKIP_INVALID_TARGET,
                reason="å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“",
                estimated_fix_count=0,
                current_error_count=0,
                file_states={},
                risk_assessment="high",
                recommendations=["target_filesã‚’æŒ‡å®šã—ã¦ãã ã•ã„"]
            )

        # æ‹¡å¼µå­ãƒã‚§ãƒƒã‚¯
        invalid_files = []
        for file_path in target_files:
            if not any(file_path.endswith(ext) for ext in self.validation_criteria["supported_extensions"]):
                invalid_files.append(file_path)

        if invalid_files:
            return PreValidationReport(
                task_id=task_id,
                original_task=task_data,
                validation_result=ValidationResult.SKIP_INVALID_TARGET,
                reason=f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„æ‹¡å¼µå­ã®ãƒ•ã‚¡ã‚¤ãƒ«: {invalid_files}",
                estimated_fix_count=0,
                current_error_count=0,
                file_states={},
                risk_assessment="medium",
                recommendations=["Pythonãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ.pyï¼‰ã®ã¿ã‚’æŒ‡å®šã—ã¦ãã ã•ã„"]
            )

        # ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ã‚­ãƒƒãƒ—
        if self.validation_criteria["skip_test_files"]:
            test_files = [f for f in target_files if "test" in f.lower() or f.endswith("_test.py")]
            if test_files:
                return PreValidationReport(
                    task_id=task_id,
                    original_task=task_data,
                    validation_result=ValidationResult.SKIP_NOT_NEEDED,
                    reason=f"ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãŸã‚è‡ªå‹•ã‚¹ã‚­ãƒƒãƒ—: {test_files}",
                    estimated_fix_count=0,
                    current_error_count=0,
                    file_states={f: "test_file" for f in test_files},
                    risk_assessment="low",
                    recommendations=["ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿®æ­£ã¯æ‰‹å‹•ã§è¡Œã£ã¦ãã ã•ã„"]
                )

        # åŸºæœ¬è¦ä»¶ã‚¯ãƒªã‚¢
        return PreValidationReport(
            task_id=task_id,
            original_task=task_data,
            validation_result=ValidationResult.VALID,
            reason="åŸºæœ¬è¦ä»¶ãƒã‚§ãƒƒã‚¯é€šé",
            estimated_fix_count=0,
            current_error_count=0,
            file_states={},
            risk_assessment="low",
            recommendations=[]
        )

    def _validate_file_states(self, task_data: Dict[str, Any]) -> PreValidationReport:
        """ãƒ•ã‚¡ã‚¤ãƒ«çŠ¶æ…‹æ¤œè¨¼"""

        task_id = task_data.get("task_id", "unknown")
        target_files = task_data.get("target_files", [])

        file_states = {}
        missing_files = []
        oversized_files = []

        for file_path in target_files:
            if not os.path.exists(file_path):
                missing_files.append(file_path)
                file_states[file_path] = "missing"
                continue

            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
            file_size_mb = os.path.getsize(file_path) / (1024 * 1024)
            if file_size_mb > self.validation_criteria["max_file_size_mb"]:
                oversized_files.append(file_path)
                file_states[file_path] = f"oversized_{file_size_mb:.1f}MB"
                continue

            # èª­ã¿è¾¼ã¿å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                if not content.strip():
                    file_states[file_path] = "empty"
                else:
                    file_states[file_path] = "valid"
            except UnicodeDecodeError:
                file_states[file_path] = "encoding_error"
            except PermissionError:
                file_states[file_path] = "permission_denied"
            except Exception as e:
                file_states[file_path] = f"read_error_{str(e)[:20]}"

        # ä¸å­˜åœ¨ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆ
        if missing_files:
            return PreValidationReport(
                task_id=task_id,
                original_task=task_data,
                validation_result=ValidationResult.SKIP_FILE_MISSING,
                reason=f"å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {missing_files}",
                estimated_fix_count=0,
                current_error_count=0,
                file_states=file_states,
                risk_assessment="medium",
                recommendations=["ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„", "æ–°è¦å®Ÿè£…ã‚¿ã‚¹ã‚¯ã«å¤‰æ›´ã‚’æ¤œè¨"]
            )

        # ã‚µã‚¤ã‚ºéå¤§ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆ
        if oversized_files:
            return PreValidationReport(
                task_id=task_id,
                original_task=task_data,
                validation_result=ValidationResult.DEFER_HIGH_RISK,
                reason=f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºéå¤§: {oversized_files}",
                estimated_fix_count=0,
                current_error_count=0,
                file_states=file_states,
                risk_assessment="high",
                recommendations=["å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ã¯æ‰‹å‹•ä¿®æ­£ã‚’æ¨å¥¨", "éƒ¨åˆ†çš„ãªä¿®æ­£ã‚¿ã‚¹ã‚¯ã«åˆ†å‰²"]
            )

        # ãƒ•ã‚¡ã‚¤ãƒ«çŠ¶æ…‹ãƒã‚§ãƒƒã‚¯é€šé
        return PreValidationReport(
            task_id=task_id,
            original_task=task_data,
            validation_result=ValidationResult.VALID,
            reason="ãƒ•ã‚¡ã‚¤ãƒ«çŠ¶æ…‹ãƒã‚§ãƒƒã‚¯é€šé",
            estimated_fix_count=0,
            current_error_count=0,
            file_states=file_states,
            risk_assessment="low",
            recommendations=[]
        )

    def _validate_current_errors(self, task_data: Dict[str, Any]) -> PreValidationReport:
        """ç¾åœ¨ã®ã‚¨ãƒ©ãƒ¼çŠ¶æ³æ¤œè¨¼"""

        task_id = task_data.get("task_id", "unknown")
        target_files = task_data.get("target_files", [])
        error_type = task_data.get("requirements", {}).get("error_type", "")

        current_error_count = 0
        file_error_details = {}

        for file_path in target_files:
            if not os.path.exists(file_path):
                continue

            # mypyã‚¨ãƒ©ãƒ¼ã‚«ã‚¦ãƒ³ãƒˆ
            file_errors = self._count_file_errors(file_path, error_type)
            current_error_count += file_errors
            file_error_details[file_path] = file_errors

        # ã‚¨ãƒ©ãƒ¼ãŒå­˜åœ¨ã—ãªã„å ´åˆ
        if current_error_count == 0:
            return PreValidationReport(
                task_id=task_id,
                original_task=task_data,
                validation_result=ValidationResult.SKIP_ALREADY_FIXED,
                reason=f"å¯¾è±¡ã‚¨ãƒ©ãƒ¼ï¼ˆ{error_type}ï¼‰ãŒå­˜åœ¨ã—ã¾ã›ã‚“",
                estimated_fix_count=0,
                current_error_count=0,
                file_states={f: f"no_errors" for f in target_files},
                risk_assessment="low",
                recommendations=["ã“ã®ã‚¿ã‚¹ã‚¯ã¯ä¸è¦ã§ã™"],
                skip_reason_details={"file_error_details": file_error_details}
            )

        # æœ€å°é–¾å€¤æœªæº€ã®å ´åˆ
        if current_error_count < self.validation_criteria["min_error_count_threshold"]:
            return PreValidationReport(
                task_id=task_id,
                original_task=task_data,
                validation_result=ValidationResult.SKIP_NOT_NEEDED,
                reason=f"ã‚¨ãƒ©ãƒ¼æ•°ãŒæœ€å°é–¾å€¤æœªæº€: {current_error_count}ä»¶",
                estimated_fix_count=current_error_count,
                current_error_count=current_error_count,
                file_states={f: f"{file_error_details[f]}_errors" for f in target_files if f in file_error_details},
                risk_assessment="low",
                recommendations=["æ‰‹å‹•ä¿®æ­£ã§ååˆ†ã§ã™"]
            )

        # ã‚¨ãƒ©ãƒ¼æ¤œè¨¼é€šé
        return PreValidationReport(
            task_id=task_id,
            original_task=task_data,
            validation_result=ValidationResult.VALID,
            reason=f"ä¿®æ­£å¯¾è±¡ã‚¨ãƒ©ãƒ¼ {current_error_count}ä»¶ã‚’ç¢ºèª",
            estimated_fix_count=current_error_count,
            current_error_count=current_error_count,
            file_states={f: f"{file_error_details[f]}_errors" for f in target_files if f in file_error_details},
            risk_assessment="low",
            recommendations=[]
        )

    def _validate_fixability(self, task_data: Dict[str, Any]) -> PreValidationReport:
        """ä¿®æ­£å¯èƒ½æ€§è©•ä¾¡"""

        task_id = task_data.get("task_id", "unknown")
        target_files = task_data.get("target_files", [])
        error_type = task_data.get("requirements", {}).get("error_type", "")

        fixability_score = 0.0
        fixable_files = []
        unfixable_files = []

        for file_path in target_files:
            if not os.path.exists(file_path):
                continue

            # ãƒ•ã‚¡ã‚¤ãƒ«å›ºæœ‰ã®ä¿®æ­£å¯èƒ½æ€§è©•ä¾¡
            file_fixability = self._assess_file_fixability(file_path, error_type)

            if file_fixability >= 0.6:
                fixable_files.append((file_path, file_fixability))
                fixability_score += file_fixability
            else:
                unfixable_files.append((file_path, file_fixability))

        average_fixability = fixability_score / len(fixable_files) if fixable_files else 0.0

        # ä¿®æ­£å¯èƒ½ãƒ•ã‚¡ã‚¤ãƒ«ãŒå°‘ãªã„å ´åˆ
        if len(fixable_files) == 0:
            return PreValidationReport(
                task_id=task_id,
                original_task=task_data,
                validation_result=ValidationResult.DEFER_HIGH_RISK,
                reason="ä¿®æ­£å¯èƒ½ãªãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“",
                estimated_fix_count=0,
                current_error_count=self._count_current_errors(target_files, error_type),
                file_states={f: f"unfixable_{score:.2f}" for f, score in unfixable_files},
                risk_assessment="high",
                recommendations=["æ‰‹å‹•ä¿®æ­£ã‚’æ¨å¥¨", "ã‚¿ã‚¹ã‚¯ã‚’å†è¨­è¨ˆ"]
            )

        # ä¿®æ­£å¯èƒ½æ€§ãŒä½ã„å ´åˆ
        if average_fixability < 0.4:
            return PreValidationReport(
                task_id=task_id,
                original_task=task_data,
                validation_result=ValidationResult.DEFER_HIGH_RISK,
                reason=f"ä¿®æ­£å¯èƒ½æ€§ãŒä½ã„: {average_fixability:.2f}",
                estimated_fix_count=len(fixable_files),
                current_error_count=self._count_current_errors(target_files, error_type),
                file_states={f: f"low_fixability_{score:.2f}" for f, score in fixable_files},
                risk_assessment="high",
                recommendations=["éƒ¨åˆ†çš„ãªæ‰‹å‹•ä¿®æ­£ã‚’æ¤œè¨", "è¤‡é›‘åº¦ã‚’ä¸‹ã’ã‚‹ãŸã‚ã®ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°"]
            )

        # ä¿®æ­£å¯èƒ½æ€§è©•ä¾¡é€šé
        return PreValidationReport(
            task_id=task_id,
            original_task=task_data,
            validation_result=ValidationResult.VALID,
            reason=f"ä¿®æ­£å¯èƒ½æ€§è‰¯å¥½: {average_fixability:.2f} ({len(fixable_files)}ä»¶)",
            estimated_fix_count=len(fixable_files),
            current_error_count=self._count_current_errors(target_files, error_type),
            file_states={f: f"fixable_{score:.2f}" for f, score in fixable_files},
            risk_assessment="low" if average_fixability > 0.7 else "medium",
            recommendations=[]
        )

    def _count_file_errors(self, file_path: str, error_type: str) -> int:
        """ãƒ•ã‚¡ã‚¤ãƒ«å†…ã®ç‰¹å®šã‚¨ãƒ©ãƒ¼ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""

        if file_path in self.mypy_cache:
            return self.mypy_cache[file_path].get(error_type, 0)

        try:
            # mypyå®Ÿè¡Œ
            result = subprocess.run(
                ["python3", "-m", "mypy", "--strict", file_path],
                capture_output=True,
                text=True,
                timeout=30
            )

            # ã‚¨ãƒ©ãƒ¼æ•°ã‚«ã‚¦ãƒ³ãƒˆ
            error_count = 0
            if error_type:
                error_count = result.stdout.count(f"[{error_type}]")
            else:
                error_count = result.stdout.count("error:")

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›´æ–°
            if file_path not in self.mypy_cache:
                self.mypy_cache[file_path] = {}
            self.mypy_cache[file_path][error_type] = error_count

            return error_count

        except subprocess.TimeoutExpired:
            print(f"âš ï¸ mypy timeout for {file_path}")
            return 0
        except FileNotFoundError:
            print(f"âš ï¸ mypy not found")
            return 0
        except Exception as e:
            print(f"âš ï¸ mypy error for {file_path}: {e}")
            return 0

    def _count_current_errors(self, target_files: List[str], error_type: str) -> int:
        """ç¾åœ¨ã®å…¨ã‚¨ãƒ©ãƒ¼æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""

        total_errors = 0
        for file_path in target_files:
            if os.path.exists(file_path):
                total_errors += self._count_file_errors(file_path, error_type)
        return total_errors

    def _assess_file_fixability(self, file_path: str, error_type: str) -> float:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿®æ­£å¯èƒ½æ€§ã‚’è©•ä¾¡ï¼ˆ0.0-1.0ï¼‰"""

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            lines = content.split('\n')
            line_count = len(lines)

            fixability_factors = {
                # ãƒã‚¸ãƒ†ã‚£ãƒ–è¦å› 
                "simple_function_defs": content.count('def ') * 0.1,
                "basic_imports": len([l for l in lines if l.strip().startswith('import ')]) * 0.05,
                "type_hints_present": content.count(':') * 0.02,

                # ãƒã‚¬ãƒ†ã‚£ãƒ–è¦å› 
                "complex_decorators": content.count('@') * -0.05,
                "lambda_expressions": content.count('lambda') * -0.08,
                "eval_statements": content.count('eval(') * -0.2,
                "dynamic_imports": content.count('__import__') * -0.15,
                "excessive_length": max(0, (line_count - 200) / 100) * -0.1,
            }

            base_fixability = 0.7  # åŸºæœ¬ä¿®æ­£å¯èƒ½æ€§

            # è¦å› ã«ã‚ˆã‚‹èª¿æ•´
            for factor, impact in fixability_factors.items():
                base_fixability += impact

            # ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—å›ºæœ‰ã®èª¿æ•´
            error_type_adjustments = {
                "no-untyped-def": 0.15,      # æ¯”è¼ƒçš„ä¿®æ­£ã—ã‚„ã™ã„
                "no-untyped-call": -0.1,     # ã‚„ã‚„å›°é›£
                "type-arg": 0.05,            # ä¸­ç¨‹åº¦
                "call-arg": -0.15,           # å›°é›£
                "attr-defined": -0.2,        # æœ€ã‚‚å›°é›£
            }

            base_fixability += error_type_adjustments.get(error_type, 0.0)

            return max(0.0, min(1.0, base_fixability))

        except Exception:
            return 0.3  # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ä½ã„ä¿®æ­£å¯èƒ½æ€§

    def _assess_task_risk(self, task_data: Dict[str, Any]) -> str:
        """ã‚¿ã‚¹ã‚¯ãƒªã‚¹ã‚¯è©•ä¾¡"""

        target_files = task_data.get("target_files", [])
        task_type = task_data.get("type", "")

        risk_score = 0

        # ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã«ã‚ˆã‚‹ãƒªã‚¹ã‚¯
        risk_score += min(len(target_files) * 0.5, 3)

        # ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã«ã‚ˆã‚‹ãƒªã‚¹ã‚¯
        type_risks = {
            "new_implementation": 3,
            "hybrid_implementation": 4,
            "new_feature_development": 5,
            "code_modification": 1,
            "file_code_modification": 1,
            "micro_code_modification": 0.5
        }
        risk_score += type_risks.get(task_type, 2)

        # é‡è¦ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿®æ­£
        critical_paths = ["main", "core", "__init__", "config"]
        if any(critical in file_path.lower() for file_path in target_files for critical in critical_paths):
            risk_score += 2

        # ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«æ±ºå®š
        if risk_score <= 2:
            return "low"
        elif risk_score <= 5:
            return "medium"
        else:
            return "high"

    def _estimate_fix_count(self, task_data: Dict[str, Any]) -> int:
        """ä¿®æ­£äºˆæƒ³ä»¶æ•°ã‚’æ¨å®š"""

        target_files = task_data.get("target_files", [])
        error_type = task_data.get("requirements", {}).get("error_type", "")

        return self._count_current_errors(target_files, error_type)

    def _get_file_states(self, target_files: List[str]) -> Dict[str, str]:
        """ãƒ•ã‚¡ã‚¤ãƒ«çŠ¶æ…‹å–å¾—"""

        states = {}
        for file_path in target_files:
            if file_path in self.file_state_cache:
                states[file_path] = self.file_state_cache[file_path]
            elif os.path.exists(file_path):
                states[file_path] = "exists"
                self.file_state_cache[file_path] = "exists"
            else:
                states[file_path] = "missing"
                self.file_state_cache[file_path] = "missing"

        return states

    def _generate_execution_recommendations(self, task_data: Dict[str, Any]) -> List[str]:
        """å®Ÿè¡Œæ¨å¥¨äº‹é …ç”Ÿæˆ"""

        task_type = task_data.get("type", "")
        target_files = task_data.get("target_files", [])
        error_type = task_data.get("requirements", {}).get("error_type", "")

        recommendations = []

        # åŸºæœ¬æ¨å¥¨äº‹é …
        recommendations.append("å®Ÿè¡Œå‰ã«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ä½œæˆ")
        recommendations.append("æ®µéšçš„ä¿®æ­£ã¨å„æ®µéšã§ã®å“è³ªãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ")

        # ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—å›ºæœ‰ã®æ¨å¥¨äº‹é …
        if error_type == "no-untyped-def":
            recommendations.append("æˆ»ã‚Šå€¤å‹æ³¨é‡ˆã®æ¨è«–ã‚’æ…é‡ã«å®Ÿæ–½")
            recommendations.append("æ—¢å­˜ã®å‹æ³¨é‡ˆã¨ã®æ•´åˆæ€§ç¢ºèª")
        elif error_type == "call-arg":
            recommendations.append("å¼•æ•°å‹å¤‰æ›ã®å®‰å…¨æ€§ç¢ºèª")
            recommendations.append("å®Ÿè¡Œæ™‚ã‚¨ãƒ©ãƒ¼ã®å¯èƒ½æ€§ã‚’è€ƒæ…®")

        # ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã«å¿œã˜ãŸæ¨å¥¨äº‹é …
        if len(target_files) > 5:
            recommendations.append("ãƒ•ã‚¡ã‚¤ãƒ«å˜ä½ã§ã®é †æ¬¡å®Ÿè¡Œã‚’æ¨å¥¨")
            recommendations.append("å„ãƒ•ã‚¡ã‚¤ãƒ«ä¿®æ­£å¾Œã«ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")

        return recommendations

    def batch_validate_tasks(self, task_list: List[Dict[str, Any]]) -> Dict[str, PreValidationReport]:
        """ã‚¿ã‚¹ã‚¯ã®ãƒãƒƒãƒæ¤œè¨¼"""

        print(f"ğŸ“‹ ãƒãƒƒãƒæ¤œè¨¼é–‹å§‹: {len(task_list)}ã‚¿ã‚¹ã‚¯")

        validation_results = {}

        for task_data in task_list:
            task_id = task_data.get("task_id", "unknown")
            try:
                validation_result = self.validate_task(task_data)
                validation_results[task_id] = validation_result

                print(f"  âœ… {task_id}: {validation_result.validation_result.value}")

            except Exception as e:
                # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                error_report = PreValidationReport(
                    task_id=task_id,
                    original_task=task_data,
                    validation_result=ValidationResult.DEFER_HIGH_RISK,
                    reason=f"æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {str(e)}",
                    estimated_fix_count=0,
                    current_error_count=0,
                    file_states={},
                    risk_assessment="high",
                    recommendations=["æ‰‹å‹•ã§æ¤œè¨¼ã—ã¦ãã ã•ã„"]
                )
                validation_results[task_id] = error_report
                print(f"  âŒ {task_id}: æ¤œè¨¼ã‚¨ãƒ©ãƒ¼")

        print(f"ğŸ“Š ãƒãƒƒãƒæ¤œè¨¼å®Œäº†: {len(validation_results)}ä»¶")
        return validation_results

    def generate_validation_summary(self, validation_results: Dict[str, PreValidationReport]) -> Dict[str, Any]:
        """æ¤œè¨¼ã‚µãƒãƒªãƒ¼ç”Ÿæˆ"""

        summary = {
            "total_tasks": len(validation_results),
            "validation_counts": {},
            "estimated_workload": {
                "valid_tasks": 0,
                "total_estimated_fixes": 0,
                "total_current_errors": 0
            },
            "risk_distribution": {"low": 0, "medium": 0, "high": 0},
            "skip_reasons": {},
            "recommendations": {
                "immediate_execution": [],
                "defer_execution": [],
                "skip_permanently": []
            }
        }

        for task_id, report in validation_results.items():
            # æ¤œè¨¼çµæœã®é›†è¨ˆ
            result_type = report.validation_result.value
            summary["validation_counts"][result_type] = summary["validation_counts"].get(result_type, 0) + 1

            # ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰é›†è¨ˆ
            if report.validation_result == ValidationResult.VALID:
                summary["estimated_workload"]["valid_tasks"] += 1
                summary["estimated_workload"]["total_estimated_fixes"] += report.estimated_fix_count
                summary["estimated_workload"]["total_current_errors"] += report.current_error_count
                summary["recommendations"]["immediate_execution"].append(task_id)
            elif report.validation_result == ValidationResult.DEFER_HIGH_RISK:
                summary["recommendations"]["defer_execution"].append(task_id)
            else:
                summary["recommendations"]["skip_permanently"].append(task_id)

            # ãƒªã‚¹ã‚¯åˆ†æ•£
            summary["risk_distribution"][report.risk_assessment] += 1

            # ã‚¹ã‚­ãƒƒãƒ—ç†ç”±é›†è¨ˆ
            if report.validation_result != ValidationResult.VALID:
                skip_category = report.validation_result.value
                if skip_category not in summary["skip_reasons"]:
                    summary["skip_reasons"][skip_category] = []
                summary["skip_reasons"][skip_category].append(task_id)

        return summary

    def save_validation_report(self, validation_results: Dict[str, PreValidationReport],
                             output_path: str = "tmp/task_validation_report.json") -> str:
        """æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜"""

        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        os.makedirs(os.path.dirname(output_path), exist_ok=True)

        # ã‚µãƒãƒªãƒ¼ç”Ÿæˆ
        summary = self.generate_validation_summary(validation_results)

        # ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿æ§‹ç¯‰
        report_data = {
            "validation_timestamp": datetime.datetime.now().isoformat(),
            "validation_summary": summary,
            "detailed_results": {}
        }

        # è©³ç´°çµæœã‚’è¾æ›¸å½¢å¼ã«å¤‰æ›
        for task_id, report in validation_results.items():
            report_data["detailed_results"][task_id] = {
                "task_id": report.task_id,
                "validation_result": report.validation_result.value,
                "reason": report.reason,
                "estimated_fix_count": report.estimated_fix_count,
                "current_error_count": report.current_error_count,
                "file_states": report.file_states,
                "risk_assessment": report.risk_assessment,
                "recommendations": report.recommendations,
                "skip_reason_details": report.skip_reason_details
            }

        # JSONä¿å­˜
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)

        print(f"ğŸ“Š æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜å®Œäº†: {output_path}")
        return output_path


def main():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""

    print("ğŸ§ª TaskPreValidator ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")

    validator = TaskPreValidator()

    # ãƒ†ã‚¹ãƒˆã‚¿ã‚¹ã‚¯ãƒ‡ãƒ¼ã‚¿
    test_tasks = [
        {
            "task_id": "test_task_001",
            "type": "file_code_modification",
            "target_files": ["postbox/utils/task_manager.py"],
            "requirements": {
                "error_type": "no-untyped-def",
                "fix_pattern": "add_return_type_annotations"
            }
        },
        {
            "task_id": "test_task_002",
            "type": "file_code_modification",
            "target_files": ["nonexistent_file.py"],
            "requirements": {
                "error_type": "no-untyped-def"
            }
        },
        {
            "task_id": "test_task_003",
            "type": "file_code_modification",
            "target_files": ["test_example.py"],
            "requirements": {
                "error_type": "no-untyped-def"
            }
        }
    ]

    # ãƒãƒƒãƒæ¤œè¨¼å®Ÿè¡Œ
    results = validator.batch_validate_tasks(test_tasks)

    # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
    summary = validator.generate_validation_summary(results)
    print(f"\nğŸ“Š æ¤œè¨¼ã‚µãƒãƒªãƒ¼:")
    print(f"ç·ã‚¿ã‚¹ã‚¯æ•°: {summary['total_tasks']}")
    print(f"æœ‰åŠ¹ã‚¿ã‚¹ã‚¯æ•°: {summary['estimated_workload']['valid_tasks']}")
    print(f"æ¨å®šä¿®æ­£ä»¶æ•°: {summary['estimated_workload']['total_estimated_fixes']}")

    # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
    validator.save_validation_report(results)

    print("\nâœ… TaskPreValidator ãƒ†ã‚¹ãƒˆå®Œäº†")


if __name__ == "__main__":
    main()
