#!/usr/bin/env python3
"""
Dependency Analyzer - ä¾å­˜é–¢ä¿‚è§£æã‚¨ãƒ³ã‚¸ãƒ³
Issue #870: è¤‡é›‘ã‚¿ã‚¹ã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³æ‹¡å¼µ

ãƒ•ã‚¡ã‚¤ãƒ«é–“ä¾å­˜é–¢ä¿‚ã®è‡ªå‹•æ¤œå‡ºãƒ»è§£æã‚·ã‚¹ãƒ†ãƒ 
"""

import ast
import os
import sys
import importlib.util
from typing import Dict, List, Any, Set, Tuple, Optional
from pathlib import Path
from dataclasses import dataclass
from collections import defaultdict, deque
# NetworkXçµ±åˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
try:
    import networkx as nx
    NETWORKX_AVAILABLE = True
except ImportError:
    NETWORKX_AVAILABLE = False
    nx = None

# ãƒ­ã‚¬ãƒ¼çµ±åˆ
try:
    from kumihan_formatter.core.utilities.logger import get_logger
    logger = get_logger(__name__)
except ImportError:
    import logging
    logger = logging.getLogger(__name__)


@dataclass
class ImportInfo:
    """ã‚¤ãƒ³ãƒãƒ¼ãƒˆæƒ…å ±"""
    module_name: str
    alias: Optional[str]
    imported_names: List[str]
    import_type: str  # 'import', 'from_import', 'relative_import'
    line_number: int
    is_standard_library: bool
    is_third_party: bool
    is_local: bool


@dataclass
class DependencyNode:
    """ä¾å­˜é–¢ä¿‚ãƒãƒ¼ãƒ‰"""
    file_path: str
    module_name: str
    imports: List[ImportInfo]
    exports: List[str]  # å…¬é–‹ã•ã‚Œã‚‹é–¢æ•°ãƒ»ã‚¯ãƒ©ã‚¹å
    complexity_score: int
    cyclomatic_complexity: int
    dependency_count: int


@dataclass
class DependencyGraph:
    """ä¾å­˜é–¢ä¿‚ã‚°ãƒ©ãƒ•"""
    nodes: Dict[str, DependencyNode]
    edges: List[Tuple[str, str]]  # (from_file, to_file)
    cycles: List[List[str]]  # å¾ªç’°ä¾å­˜ãƒ‘ã‚¹
    levels: Dict[str, int]  # å®Ÿè£…é †åºãƒ¬ãƒ™ãƒ«
    clusters: List[List[str]]  # é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼


class DependencyAnalyzer:
    """ä¾å­˜é–¢ä¿‚è§£æã‚¨ãƒ³ã‚¸ãƒ³"""

    def __init__(self, project_root: Optional[str] = None):
        self.project_root = Path(project_root or os.getcwd())
        self.graph = None
        self.standard_modules = self._get_standard_modules()

        logger.info("ğŸ” Dependency Analyzer åˆæœŸåŒ–å®Œäº†")
        logger.info(f"ğŸ“ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆ: {self.project_root}")

    def analyze_project_dependencies(
        self, target_paths: List[str] = None
    ) -> DependencyGraph:
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ã®ä¾å­˜é–¢ä¿‚è§£æ"""

        logger.info("ğŸ” ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä¾å­˜é–¢ä¿‚è§£æé–‹å§‹")

        if target_paths is None:
            target_paths = self._find_python_files()

        # 1. å„ãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ†æ
        nodes = {}
        for file_path in target_paths:
            try:
                node = self._analyze_file(file_path)
                if node:
                    nodes[file_path] = node
            except Exception as e:
                logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«è§£æã‚¨ãƒ©ãƒ¼ {file_path}: {e}")

        # 2. ä¾å­˜é–¢ä¿‚ã‚¨ãƒƒã‚¸æ§‹ç¯‰
        edges = self._build_dependency_edges(nodes)

        # 3. å¾ªç’°ä¾å­˜æ¤œå‡º
        cycles = self._detect_cycles(nodes, edges)

        # 4. å®Ÿè£…é †åºãƒ¬ãƒ™ãƒ«è¨ˆç®—
        levels = self._calculate_implementation_levels(nodes, edges)

        # 5. ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æ
        clusters = self._analyze_clusters(nodes, edges)

        self.graph = DependencyGraph(
            nodes=nodes,
            edges=edges,
            cycles=cycles,
            levels=levels,
            clusters=clusters
        )

        logger.info(f"âœ… ä¾å­˜é–¢ä¿‚è§£æå®Œäº†:")
        logger.info(f"  - ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(nodes)}")
        logger.info(f"  - ä¾å­˜é–¢ä¿‚æ•°: {len(edges)}")
        logger.info(f"  - å¾ªç’°ä¾å­˜: {len(cycles)}ä»¶")
        logger.info(f"  - å®Ÿè£…ãƒ¬ãƒ™ãƒ«: {max(levels.values()) if levels else 0}")

        return self.graph

    def get_implementation_order(self) -> List[List[str]]:
        """å®Ÿè£…é †åºã®å–å¾—ï¼ˆä¾å­˜é–¢ä¿‚ã‚’è€ƒæ…®ï¼‰"""

        if not self.graph:
            raise ValueError("ä¾å­˜é–¢ä¿‚è§£æãŒå®Ÿè¡Œã•ã‚Œã¦ã„ã¾ã›ã‚“")

        # ãƒ¬ãƒ™ãƒ«åˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        level_groups = defaultdict(list)
        for file_path, level in self.graph.levels.items():
            level_groups[level].append(file_path)

        # ãƒ¬ãƒ™ãƒ«é †ã«ã‚½ãƒ¼ãƒˆ
        implementation_order = []
        for level in sorted(level_groups.keys()):
            implementation_order.append(level_groups[level])

        logger.info(f"ğŸ“‹ å®Ÿè£…é †åºæ±ºå®š: {len(implementation_order)}ãƒ¬ãƒ™ãƒ«")
        return implementation_order

    def detect_circular_dependencies(self) -> List[List[str]]:
        """å¾ªç’°ä¾å­˜ã®æ¤œå‡º"""

        if not self.graph:
            raise ValueError("ä¾å­˜é–¢ä¿‚è§£æãŒå®Ÿè¡Œã•ã‚Œã¦ã„ã¾ã›ã‚“")

        if self.graph.cycles:
            logger.warning(f"ğŸ”„ å¾ªç’°ä¾å­˜æ¤œå‡º: {len(self.graph.cycles)}ä»¶")
            for i, cycle in enumerate(self.graph.cycles):
                logger.warning(f"  ã‚µã‚¤ã‚¯ãƒ«{i+1}: {' -> '.join(cycle)}")

        return self.graph.cycles

    def get_parallel_implementation_groups(self) -> List[List[str]]:
        """ä¸¦åˆ—å®Ÿè£…å¯èƒ½ã‚°ãƒ«ãƒ¼ãƒ—ã®å–å¾—"""

        if not self.graph:
            raise ValueError("ä¾å­˜é–¢ä¿‚è§£æãŒå®Ÿè¡Œã•ã‚Œã¦ã„ã¾ã›ã‚“")

        parallel_groups = []

        # åŒä¸€ãƒ¬ãƒ™ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ä¸¦åˆ—å®Ÿè£…å¯èƒ½
        level_groups = defaultdict(list)
        for file_path, level in self.graph.levels.items():
            level_groups[level].append(file_path)

        for level, files in level_groups.items():
            if len(files) > 1:
                parallel_groups.append(files)

        logger.info(f"âš¡ ä¸¦åˆ—å®Ÿè£…å¯èƒ½ã‚°ãƒ«ãƒ¼ãƒ—: {len(parallel_groups)}ã‚°ãƒ«ãƒ¼ãƒ—")
        return parallel_groups

    def analyze_file_complexity(self, file_path: str) -> Dict[str, Any]:
        """ãƒ•ã‚¡ã‚¤ãƒ«è¤‡é›‘åº¦ã®è©³ç´°åˆ†æ"""

        if not self.graph or file_path not in self.graph.nodes:
            return {"error": "ãƒ•ã‚¡ã‚¤ãƒ«ãŒè§£æã•ã‚Œã¦ã„ã¾ã›ã‚“"}

        node = self.graph.nodes[file_path]

        complexity_analysis = {
            "file_path": file_path,
            "complexity_score": node.complexity_score,
            "cyclomatic_complexity": node.cyclomatic_complexity,
            "dependency_count": node.dependency_count,
            "import_count": len(node.imports),
            "export_count": len(node.exports),
            "third_party_imports": len([
                imp for imp in node.imports if imp.is_third_party
            ]),
            "local_imports": len([
                imp for imp in node.imports if imp.is_local
            ]),
            "complexity_level": self._classify_complexity(node.complexity_score)
        }

        return complexity_analysis

    def _analyze_file(self, file_path: str) -> Optional[DependencyNode]:
        """å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¾å­˜é–¢ä¿‚è§£æ"""

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            tree = ast.parse(content, filename=file_path)

            # ã‚¤ãƒ³ãƒãƒ¼ãƒˆè§£æ
            imports = self._extract_imports(tree)

            # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆè§£æ
            exports = self._extract_exports(tree)

            # è¤‡é›‘åº¦è¨ˆç®—
            complexity_score = self._calculate_complexity_score(tree)
            cyclomatic_complexity = self._calculate_cyclomatic_complexity(tree)

            # ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åæ¨å®š
            module_name = self._path_to_module_name(file_path)

            return DependencyNode(
                file_path=file_path,
                module_name=module_name,
                imports=imports,
                exports=exports,
                complexity_score=complexity_score,
                cyclomatic_complexity=cyclomatic_complexity,
                dependency_count=len(imports)
            )

        except Exception as e:
            logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«è§£æã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
            return None

    def _extract_imports(self, tree: ast.AST) -> List[ImportInfo]:
        """ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ–‡ã®æŠ½å‡º"""

        imports = []

        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    import_info = ImportInfo(
                        module_name=alias.name,
                        alias=alias.asname,
                        imported_names=[],
                        import_type='import',
                        line_number=node.lineno,
                        is_standard_library=self._is_standard_library(alias.name),
                        is_third_party=self._is_third_party(alias.name),
                        is_local=self._is_local_import(alias.name)
                    )
                    imports.append(import_info)

            elif isinstance(node, ast.ImportFrom):
                if node.module:
                    imported_names = [alias.name for alias in node.names]
                    import_info = ImportInfo(
                        module_name=node.module,
                        alias=None,
                        imported_names=imported_names,
                        import_type='from_import' if node.level == 0 else 'relative_import',
                        line_number=node.lineno,
                        is_standard_library=self._is_standard_library(node.module),
                        is_third_party=self._is_third_party(node.module),
                        is_local=self._is_local_import(node.module)
                    )
                    imports.append(import_info)

        return imports

    def _extract_exports(self, tree: ast.AST) -> List[str]:
        """ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆå…¬é–‹è¦ç´ ï¼‰ã®æŠ½å‡º"""

        exports = []

        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                if not node.name.startswith('_'):
                    exports.append(node.name)
            elif isinstance(node, ast.ClassDef):
                if not node.name.startswith('_'):
                    exports.append(node.name)
            elif isinstance(node, ast.Assign):
                for target in node.targets:
                    if isinstance(target, ast.Name) and not target.id.startswith('_'):
                        exports.append(target.id)

        return exports

    def _calculate_complexity_score(self, tree: ast.AST) -> int:
        """è¤‡é›‘åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—"""

        score = 0

        for node in ast.walk(tree):
            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                score += 2
            elif isinstance(node, ast.ClassDef):
                score += 3
            elif isinstance(node, (ast.If, ast.While, ast.For)):
                score += 1
            elif isinstance(node, ast.Try):
                score += 2

        return score

    def _calculate_cyclomatic_complexity(self, tree: ast.AST) -> int:
        """å¾ªç’°çš„è¤‡é›‘åº¦è¨ˆç®—"""

        complexity = 1  # åŸºæº–å€¤

        for node in ast.walk(tree):
            if isinstance(node, (ast.If, ast.While, ast.For, ast.With)):
                complexity += 1
            elif isinstance(node, ast.Try):
                complexity += len(node.handlers)
            elif isinstance(node, ast.BoolOp):
                complexity += len(node.values) - 1

        return complexity

    def _build_dependency_edges(
        self, nodes: Dict[str, DependencyNode]
    ) -> List[Tuple[str, str]]:
        """ä¾å­˜é–¢ä¿‚ã‚¨ãƒƒã‚¸ã®æ§‹ç¯‰"""

        edges = []

        for file_path, node in nodes.items():
            for import_info in node.imports:
                if import_info.is_local:
                    # ãƒ­ãƒ¼ã‚«ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã®å ´åˆã€å¯¾å¿œã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
                    target_file = self._resolve_import_to_file(
                        import_info.module_name, file_path
                    )
                    if target_file and target_file in nodes:
                        edges.append((file_path, target_file))

        return edges

    def _detect_cycles(
        self, nodes: Dict[str, DependencyNode], edges: List[Tuple[str, str]]
    ) -> List[List[str]]:
        """å¾ªç’°ä¾å­˜ã®æ¤œå‡º"""

        if NETWORKX_AVAILABLE and nx:
            # NetworkXã‚’ä½¿ç”¨ã—ã¦ã‚°ãƒ©ãƒ•æ§‹ç¯‰
            G = nx.DiGraph()
            G.add_nodes_from(nodes.keys())
            G.add_edges_from(edges)

            # å¾ªç’°ä¾å­˜æ¤œå‡º
            try:
                cycles = list(nx.simple_cycles(G))
                return cycles
            except Exception:
                return []
        else:
            # ç°¡æ˜“å®Ÿè£…ï¼ˆNetworkXæœªä½¿ç”¨ï¼‰
            return self._detect_cycles_simple(nodes, edges)

    def _detect_cycles_simple(
        self, nodes: Dict[str, DependencyNode], edges: List[Tuple[str, str]]
    ) -> List[List[str]]:
        """ç°¡æ˜“å¾ªç’°ä¾å­˜æ¤œå‡ºï¼ˆNetworkXæœªä½¿ç”¨ï¼‰"""

        # DFS-based cycle detection
        visited = set()
        rec_stack = set()
        cycles = []

        def dfs(node: str, path: List[str]) -> None:
            if node in rec_stack:
                # å¾ªç’°æ¤œå‡º
                cycle_start = path.index(node)
                cycle = path[cycle_start:] + [node]
                cycles.append(cycle)
                return

            if node in visited:
                return

            visited.add(node)
            rec_stack.add(node)

            # éš£æ¥ãƒãƒ¼ãƒ‰ã‚’æ¢ç´¢
            for edge_from, edge_to in edges:
                if edge_from == node:
                    dfs(edge_to, path + [node])

            rec_stack.remove(node)

        for node in nodes.keys():
            if node not in visited:
                dfs(node, [])

        return cycles

    def _calculate_implementation_levels(
        self, nodes: Dict[str, DependencyNode], edges: List[Tuple[str, str]]
    ) -> Dict[str, int]:
        """å®Ÿè£…é †åºãƒ¬ãƒ™ãƒ«ã®è¨ˆç®—"""

        # ãƒˆãƒãƒ­ã‚¸ã‚«ãƒ«ã‚½ãƒ¼ãƒˆãƒ™ãƒ¼ã‚¹ã®å®Ÿè£…
        in_degree = {file_path: 0 for file_path in nodes.keys()}

        # å…¥æ¬¡æ•°è¨ˆç®—
        for _, target in edges:
            if target in in_degree:
                in_degree[target] += 1

        # ãƒ¬ãƒ™ãƒ«è¨ˆç®—
        levels = {}
        queue = deque([file for file, degree in in_degree.items() if degree == 0])
        current_level = 0

        while queue:
            level_size = len(queue)
            for _ in range(level_size):
                file_path = queue.popleft()
                levels[file_path] = current_level

                # ä¾å­˜å…ˆã®ãƒ•ã‚¡ã‚¤ãƒ«ã®å…¥æ¬¡æ•°ã‚’æ¸›å°‘
                for edge_from, edge_to in edges:
                    if edge_from == file_path and edge_to in in_degree:
                        in_degree[edge_to] -= 1
                        if in_degree[edge_to] == 0:
                            queue.append(edge_to)

            current_level += 1

        return levels

    def _analyze_clusters(
        self, nodes: Dict[str, DependencyNode], edges: List[Tuple[str, str]]
    ) -> List[List[str]]:
        """é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®åˆ†æ"""

        # å˜ç´”ãªã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆå…±é€šã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ™ãƒ¼ã‚¹ï¼‰
        clusters = []

        # ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ¬ãƒ™ãƒ«ã§ã®ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        module_groups = defaultdict(list)
        for file_path, node in nodes.items():
            module_prefix = node.module_name.split('.')[0] if '.' in node.module_name else node.module_name
            module_groups[module_prefix].append(file_path)

        for group_files in module_groups.values():
            if len(group_files) > 1:
                clusters.append(group_files)

        return clusters

    def _find_python_files(self) -> List[str]:
        """Pythonãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œç´¢"""

        python_files = []

        for root, dirs, files in os.walk(self.project_root):
            # é™¤å¤–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            dirs[:] = [d for d in dirs if not d.startswith('.') and d != '__pycache__']

            for file in files:
                if file.endswith('.py'):
                    file_path = os.path.join(root, file)
                    python_files.append(file_path)

        return python_files

    def _get_standard_modules(self) -> Set[str]:
        """æ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ä¸€è¦§å–å¾—"""

        standard_modules = {
            'os', 'sys', 'json', 'time', 'datetime', 'pathlib', 'typing',
            'collections', 'itertools', 'functools', 'operator', 'copy',
            'pickle', 're', 'math', 'random', 'statistics', 'decimal',
            'fractions', 'sqlite3', 'csv', 'configparser', 'logging',
            'unittest', 'doctest', 'argparse', 'subprocess', 'threading',
            'multiprocessing', 'asyncio', 'concurrent', 'queue', 'socket',
            'urllib', 'http', 'ftplib', 'smtplib', 'email', 'html', 'xml',
            'hashlib', 'hmac', 'secrets', 'ssl', 'gzip', 'bz2', 'lzma',
            'zipfile', 'tarfile', 'tempfile', 'shutil', 'glob', 'fnmatch'
        }

        return standard_modules

    def _is_standard_library(self, module_name: str) -> bool:
        """æ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‹ã©ã†ã‹ã®åˆ¤å®š"""
        return module_name.split('.')[0] in self.standard_modules

    def _is_third_party(self, module_name: str) -> bool:
        """ã‚µãƒ¼ãƒ‰ãƒ‘ãƒ¼ãƒ†ã‚£ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‹ã©ã†ã‹ã®åˆ¤å®š"""
        return not self._is_standard_library(module_name) and not self._is_local_import(module_name)

    def _is_local_import(self, module_name: str) -> bool:
        """ãƒ­ãƒ¼ã‚«ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‹ã©ã†ã‹ã®åˆ¤å®š"""
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå›ºæœ‰ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åãƒ‘ã‚¿ãƒ¼ãƒ³ã§åˆ¤å®š
        project_patterns = ['kumihan_formatter', 'postbox']
        return any(module_name.startswith(pattern) for pattern in project_patterns)

    def _path_to_module_name(self, file_path: str) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‹ã‚‰ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åã¸ã®å¤‰æ›"""

        rel_path = os.path.relpath(file_path, self.project_root)
        module_parts = rel_path.replace(os.sep, '.').replace('.py', '').split('.')

        # __init__.pyã®å ´åˆã¯è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã‚’ä½¿ç”¨
        if module_parts[-1] == '__init__':
            module_parts = module_parts[:-1]

        return '.'.join(module_parts)

    def _resolve_import_to_file(self, module_name: str, current_file: str) -> Optional[str]:
        """ã‚¤ãƒ³ãƒãƒ¼ãƒˆåã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã¸ã®è§£æ±º"""

        try:
            # åŸºæœ¬çš„ãªè§£æ±ºãƒ­ã‚¸ãƒƒã‚¯
            module_parts = module_name.split('.')

            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‹ã‚‰ã®ç›¸å¯¾ãƒ‘ã‚¹æ§‹ç¯‰
            potential_paths = [
                self.project_root / (os.sep.join(module_parts) + '.py'),
                self.project_root / os.sep.join(module_parts) / '__init__.py'
            ]

            for path in potential_paths:
                if path.exists():
                    return str(path)

            return None

        except Exception:
            return None

    def _classify_complexity(self, score: int) -> str:
        """è¤‡é›‘åº¦ãƒ¬ãƒ™ãƒ«ã®åˆ†é¡"""

        if score <= 10:
            return "simple"
        elif score <= 25:
            return "moderate"
        elif score <= 50:
            return "complex"
        else:
            return "very_complex"


if __name__ == "__main__":
    """ç°¡å˜ãªãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""

    analyzer = DependencyAnalyzer()

    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ã®ä¾å­˜é–¢ä¿‚è§£æ
    graph = analyzer.analyze_project_dependencies()

    # å®Ÿè£…é †åºã®è¡¨ç¤º
    implementation_order = analyzer.get_implementation_order()
    print(f"\nğŸ“‹ å®Ÿè£…é †åº:")
    for level, files in enumerate(implementation_order):
        print(f"  ãƒ¬ãƒ™ãƒ« {level}: {len(files)}ãƒ•ã‚¡ã‚¤ãƒ«")
        for file in files[:3]:  # æœ€åˆã®3ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿è¡¨ç¤º
            print(f"    - {file}")
        if len(files) > 3:
            print(f"    ... ä»– {len(files)-3}ãƒ•ã‚¡ã‚¤ãƒ«")

    # å¾ªç’°ä¾å­˜ã®ç¢ºèª
    cycles = analyzer.detect_circular_dependencies()
    if cycles:
        print(f"\nğŸ”„ å¾ªç’°ä¾å­˜æ¤œå‡º: {len(cycles)}ä»¶")
    else:
        print(f"\nâœ… å¾ªç’°ä¾å­˜ãªã—")

    # ä¸¦åˆ—å®Ÿè£…å¯èƒ½ã‚°ãƒ«ãƒ¼ãƒ—
    parallel_groups = analyzer.get_parallel_implementation_groups()
    if parallel_groups:
        print(f"\nâš¡ ä¸¦åˆ—å®Ÿè£…å¯èƒ½: {len(parallel_groups)}ã‚°ãƒ«ãƒ¼ãƒ—")
