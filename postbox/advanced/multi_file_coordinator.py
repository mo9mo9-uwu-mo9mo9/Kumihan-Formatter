#!/usr/bin/env python3
"""
Multi-File Coordinator - ãƒãƒ«ãƒãƒ•ã‚¡ã‚¤ãƒ«å®Ÿè£…ã‚³ãƒ¼ãƒ‡ã‚£ãƒãƒ¼ã‚¿ãƒ¼
Issue #870: è¤‡é›‘ã‚¿ã‚¹ã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³æ‹¡å¼µ

5+ãƒ•ã‚¡ã‚¤ãƒ«åŒæ™‚å®Ÿè£…ã‚’ç®¡ç†ã™ã‚‹é«˜åº¦ãªã‚³ãƒ¼ãƒ‡ã‚£ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚·ã‚¹ãƒ†ãƒ 
"""

import json
import os
import threading
import time
from typing import Dict, List, Any, Set, Optional, Tuple
from pathlib import Path
from dataclasses import dataclass, asdict
from enum import Enum
from concurrent.futures import ThreadPoolExecutor, Future
import queue

from .dependency_analyzer import DependencyAnalyzer, DependencyGraph

# ãƒ­ã‚¬ãƒ¼çµ±åˆ
try:
    from kumihan_formatter.core.utilities.logger import get_logger
    logger = get_logger(__name__)
except ImportError:
    import logging
    logger = logging.getLogger(__name__)


class ImplementationStatus(Enum):
    """å®Ÿè£…ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹"""
    PENDING = "pending"
    IN_PROGRESS = "in_progress"
    COMPLETED = "completed"
    FAILED = "failed"
    BLOCKED = "blocked"


class CoordinationStrategy(Enum):
    """å”èª¿æˆ¦ç•¥"""
    SEQUENTIAL = "sequential"      # é †æ¬¡å®Ÿè£…
    PARALLEL = "parallel"          # ä¸¦åˆ—å®Ÿè£…
    HYBRID = "hybrid"              # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰
    DEPENDENCY_DRIVEN = "dependency_driven"  # ä¾å­˜é–¢ä¿‚é§†å‹•


@dataclass
class FileImplementationTask:
    """ãƒ•ã‚¡ã‚¤ãƒ«å®Ÿè£…ã‚¿ã‚¹ã‚¯"""
    file_path: str
    module_name: str
    implementation_spec: Dict[str, Any]
    dependencies: List[str]
    dependents: List[str]
    priority: int
    estimated_complexity: int
    status: ImplementationStatus
    assigned_agent: Optional[str]
    start_time: Optional[float]
    completion_time: Optional[float]
    error_info: Optional[Dict[str, Any]]


@dataclass
class ImplementationPlan:
    """å®Ÿè£…è¨ˆç”»"""
    plan_id: str
    target_files: List[str]
    coordination_strategy: CoordinationStrategy
    implementation_phases: List[List[str]]  # ãƒ•ã‚§ãƒ¼ã‚ºåˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆ
    parallel_groups: List[List[str]]        # ä¸¦åˆ—å®Ÿè£…å¯èƒ½ã‚°ãƒ«ãƒ¼ãƒ—
    critical_path: List[str]                # ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãƒ‘ã‚¹
    estimated_duration: int                 # æ¨å®šå®Ÿè£…æ™‚é–“ï¼ˆåˆ†ï¼‰
    success_criteria: Dict[str, Any]


@dataclass
class CoordinationMetrics:
    """å”èª¿ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    total_files: int
    completed_files: int
    failed_files: int
    parallel_efficiency: float
    dependency_resolution_rate: float
    average_implementation_time: float
    bottleneck_files: List[str]


class MultiFileCoordinator:
    """ãƒãƒ«ãƒãƒ•ã‚¡ã‚¤ãƒ«å®Ÿè£…ã‚³ãƒ¼ãƒ‡ã‚£ãƒãƒ¼ã‚¿ãƒ¼"""

    def __init__(self, max_parallel_tasks: int = 4):
        self.dependency_analyzer = DependencyAnalyzer()
        self.max_parallel_tasks = max_parallel_tasks
        self.active_tasks: Dict[str, FileImplementationTask] = {}
        self.completed_tasks: Dict[str, FileImplementationTask] = {}
        self.implementation_queue = queue.PriorityQueue()
        self.coordination_lock = threading.Lock()
        self.metrics = None

        # Geminiçµ±åˆï¼ˆå°†æ¥å®Ÿè£…ï¼‰
        self.gemini_agents = []

        logger.info(f"ğŸ›ï¸ Multi-File Coordinator åˆæœŸåŒ–å®Œäº†")
        logger.info(f"âš¡ æœ€å¤§ä¸¦åˆ—ã‚¿ã‚¹ã‚¯æ•°: {max_parallel_tasks}")

    def create_implementation_plan(
        self,
        target_files: List[str],
        strategy: CoordinationStrategy = CoordinationStrategy.HYBRID,
        specifications: Dict[str, Dict[str, Any]] = None
    ) -> ImplementationPlan:
        """åŒ…æ‹¬çš„å®Ÿè£…è¨ˆç”»ä½œæˆ"""

        logger.info(f"ğŸ“‹ å®Ÿè£…è¨ˆç”»ä½œæˆé–‹å§‹: {len(target_files)}ãƒ•ã‚¡ã‚¤ãƒ«")
        logger.info(f"ğŸ¯ å”èª¿æˆ¦ç•¥: {strategy.value}")

        # 1. ä¾å­˜é–¢ä¿‚è§£æ
        dependency_graph = self.dependency_analyzer.analyze_project_dependencies(target_files)

        # 2. å®Ÿè£…ãƒ•ã‚§ãƒ¼ã‚ºæ±ºå®š
        implementation_phases = self._determine_implementation_phases(
            dependency_graph, strategy
        )

        # 3. ä¸¦åˆ—å®Ÿè£…ã‚°ãƒ«ãƒ¼ãƒ—ç‰¹å®š
        parallel_groups = self._identify_parallel_groups(
            dependency_graph, implementation_phases
        )

        # 4. ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãƒ‘ã‚¹è¨ˆç®—
        critical_path = self._calculate_critical_path(
            dependency_graph, target_files
        )

        # 5. å®Ÿè£…æ™‚é–“æ¨å®š
        estimated_duration = self._estimate_implementation_duration(
            dependency_graph, target_files, specifications or {}
        )

        # 6. æˆåŠŸåŸºæº–è¨­å®š
        success_criteria = self._define_success_criteria(target_files)

        plan_id = f"impl_plan_{int(time.time())}"

        plan = ImplementationPlan(
            plan_id=plan_id,
            target_files=target_files,
            coordination_strategy=strategy,
            implementation_phases=implementation_phases,
            parallel_groups=parallel_groups,
            critical_path=critical_path,
            estimated_duration=estimated_duration,
            success_criteria=success_criteria
        )

        logger.info(f"âœ… å®Ÿè£…è¨ˆç”»ä½œæˆå®Œäº†:")
        logger.info(f"  - å®Ÿè£…ãƒ•ã‚§ãƒ¼ã‚º: {len(implementation_phases)}")
        logger.info(f"  - ä¸¦åˆ—ã‚°ãƒ«ãƒ¼ãƒ—: {len(parallel_groups)}")
        logger.info(f"  - æ¨å®šæ™‚é–“: {estimated_duration}åˆ†")

        return plan

    def execute_implementation_plan(
        self,
        plan: ImplementationPlan,
        specifications: Dict[str, Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """å®Ÿè£…è¨ˆç”»ã®å®Ÿè¡Œ"""

        logger.info(f"ğŸš€ å®Ÿè£…è¨ˆç”»å®Ÿè¡Œé–‹å§‹: {plan.plan_id}")

        # 1. ã‚¿ã‚¹ã‚¯åˆæœŸåŒ–
        self._initialize_implementation_tasks(plan, specifications or {})

        # 2. å”èª¿æˆ¦ç•¥ã«å¿œã˜ãŸå®Ÿè¡Œ
        if plan.coordination_strategy == CoordinationStrategy.SEQUENTIAL:
            result = self._execute_sequential_implementation(plan)
        elif plan.coordination_strategy == CoordinationStrategy.PARALLEL:
            result = self._execute_parallel_implementation(plan)
        elif plan.coordination_strategy == CoordinationStrategy.HYBRID:
            result = self._execute_hybrid_implementation(plan)
        else:
            result = self._execute_dependency_driven_implementation(plan)

        # 3. ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        self.metrics = self._calculate_coordination_metrics()

        # 4. çµæœã¾ã¨ã‚
        execution_result = {
            "plan_id": plan.plan_id,
            "execution_status": result["status"],
            "completed_files": len(self.completed_tasks),
            "failed_files": len([t for t in self.active_tasks.values() if t.status == ImplementationStatus.FAILED]),
            "metrics": asdict(self.metrics) if self.metrics else None,
            "detailed_results": result
        }

        logger.info(f"âœ… å®Ÿè£…è¨ˆç”»å®Ÿè¡Œå®Œäº†: {execution_result['execution_status']}")
        return execution_result

    def monitor_implementation_progress(self) -> Dict[str, Any]:
        """å®Ÿè£…é€²æ—ã®ç›£è¦–"""

        with self.coordination_lock:
            total_tasks = len(self.active_tasks) + len(self.completed_tasks)
            completed_count = len(self.completed_tasks)
            in_progress_count = len([
                t for t in self.active_tasks.values()
                if t.status == ImplementationStatus.IN_PROGRESS
            ])
            failed_count = len([
                t for t in self.active_tasks.values()
                if t.status == ImplementationStatus.FAILED
            ])

            progress = {
                "total_files": total_tasks,
                "completed": completed_count,
                "in_progress": in_progress_count,
                "failed": failed_count,
                "completion_rate": completed_count / max(total_tasks, 1),
                "active_tasks": [
                    {
                        "file": task.file_path,
                        "status": task.status.value,
                        "agent": task.assigned_agent
                    }
                    for task in self.active_tasks.values()
                    if task.status == ImplementationStatus.IN_PROGRESS
                ]
            }

        return progress

    def handle_implementation_failure(
        self,
        file_path: str,
        error_info: Dict[str, Any]
    ) -> Dict[str, Any]:
        """å®Ÿè£…å¤±æ•—ã®å‡¦ç†"""

        logger.warning(f"âŒ å®Ÿè£…å¤±æ•—å‡¦ç†: {file_path}")

        with self.coordination_lock:
            if file_path in self.active_tasks:
                task = self.active_tasks[file_path]
                task.status = ImplementationStatus.FAILED
                task.error_info = error_info

                # ä¾å­˜é–¢ä¿‚ã«ã‚ã‚‹ã‚¿ã‚¹ã‚¯ã‚’ãƒ–ãƒ­ãƒƒã‚¯çŠ¶æ…‹ã«
                blocked_tasks = self._block_dependent_tasks(file_path)

                # ãƒªã‚«ãƒãƒªæˆ¦ç•¥æ±ºå®š
                recovery_strategy = self._determine_recovery_strategy(task, error_info)

                return {
                    "failed_file": file_path,
                    "blocked_tasks": blocked_tasks,
                    "recovery_strategy": recovery_strategy
                }

        return {"error": "ã‚¿ã‚¹ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"}

    def optimize_coordination_strategy(
        self,
        current_metrics: CoordinationMetrics
    ) -> CoordinationStrategy:
        """å”èª¿æˆ¦ç•¥ã®æœ€é©åŒ–"""

        logger.info("ğŸ¯ å”èª¿æˆ¦ç•¥æœ€é©åŒ–å®Ÿè¡Œ")

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
        if current_metrics.parallel_efficiency > 0.8:
            recommended_strategy = CoordinationStrategy.PARALLEL
        elif current_metrics.dependency_resolution_rate < 0.6:
            recommended_strategy = CoordinationStrategy.DEPENDENCY_DRIVEN
        elif len(current_metrics.bottleneck_files) > 0:
            recommended_strategy = CoordinationStrategy.HYBRID
        else:
            recommended_strategy = CoordinationStrategy.SEQUENTIAL

        logger.info(f"ğŸ“Š æ¨å¥¨æˆ¦ç•¥: {recommended_strategy.value}")
        return recommended_strategy

    def _determine_implementation_phases(
        self,
        graph: DependencyGraph,
        strategy: CoordinationStrategy
    ) -> List[List[str]]:
        """å®Ÿè£…ãƒ•ã‚§ãƒ¼ã‚ºã®æ±ºå®š"""

        if strategy == CoordinationStrategy.SEQUENTIAL:
            # å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å˜ä¸€ãƒ•ã‚§ãƒ¼ã‚ºã§é †æ¬¡å®Ÿè¡Œ
            return [list(graph.nodes.keys())]

        elif strategy == CoordinationStrategy.DEPENDENCY_DRIVEN:
            # ä¾å­˜é–¢ä¿‚ãƒ¬ãƒ™ãƒ«åˆ¥ã«ãƒ•ã‚§ãƒ¼ã‚ºåˆ†å‰²
            phases = []
            level_groups = {}

            for file_path, level in graph.levels.items():
                if level not in level_groups:
                    level_groups[level] = []
                level_groups[level].append(file_path)

            for level in sorted(level_groups.keys()):
                phases.append(level_groups[level])

            return phases

        else:
            # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰: ä¾å­˜é–¢ä¿‚ã¨ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’è€ƒæ…®
            phases = []
            level_groups = {}

            for file_path, level in graph.levels.items():
                if level not in level_groups:
                    level_groups[level] = []
                level_groups[level].append(file_path)

            # å¤§ããªã‚°ãƒ«ãƒ¼ãƒ—ã¯åˆ†å‰²
            for level in sorted(level_groups.keys()):
                group = level_groups[level]
                if len(group) > self.max_parallel_tasks:
                    # ã‚°ãƒ«ãƒ¼ãƒ—ã‚’åˆ†å‰²
                    for i in range(0, len(group), self.max_parallel_tasks):
                        phases.append(group[i:i + self.max_parallel_tasks])
                else:
                    phases.append(group)

            return phases

    def _identify_parallel_groups(
        self,
        graph: DependencyGraph,
        implementation_phases: List[List[str]]
    ) -> List[List[str]]:
        """ä¸¦åˆ—å®Ÿè£…ã‚°ãƒ«ãƒ¼ãƒ—ã®ç‰¹å®š"""

        parallel_groups = []

        for phase in implementation_phases:
            if len(phase) > 1:
                # åŒä¸€ãƒ•ã‚§ãƒ¼ã‚ºå†…ã§ä¾å­˜é–¢ä¿‚ãŒãªã„ãƒ•ã‚¡ã‚¤ãƒ«ç¾¤ã‚’ç‰¹å®š
                independent_groups = self._find_independent_groups(phase, graph)
                parallel_groups.extend(independent_groups)

        return parallel_groups

    def _calculate_critical_path(
        self,
        graph: DependencyGraph,
        target_files: List[str]
    ) -> List[str]:
        """ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãƒ‘ã‚¹è¨ˆç®—"""

        # æœ€ã‚‚æ·±ã„ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒ¼ãƒ³ã‚’ç‰¹å®š
        max_depth = 0
        critical_path = []

        for file_path in target_files:
            path = self._find_dependency_path(file_path, graph)
            if len(path) > max_depth:
                max_depth = len(path)
                critical_path = path

        return critical_path

    def _estimate_implementation_duration(
        self,
        graph: DependencyGraph,
        target_files: List[str],
        specifications: Dict[str, Dict[str, Any]]
    ) -> int:
        """å®Ÿè£…æ™‚é–“æ¨å®šï¼ˆåˆ†ï¼‰"""

        total_duration = 0

        for file_path in target_files:
            if file_path in graph.nodes:
                node = graph.nodes[file_path]

                # è¤‡é›‘åº¦ãƒ™ãƒ¼ã‚¹ã®æ™‚é–“æ¨å®š
                base_time = 15  # åŸºæœ¬å®Ÿè£…æ™‚é–“ï¼ˆåˆ†ï¼‰
                complexity_multiplier = max(1, node.complexity_score / 10)
                dependency_penalty = len(node.imports) * 2

                file_duration = int(base_time * complexity_multiplier + dependency_penalty)
                total_duration += file_duration

        # ä¸¦åˆ—åŒ–åŠ¹ç‡ã‚’è€ƒæ…®ã—ãŸèª¿æ•´
        parallel_efficiency = 0.7  # 70%ã®ä¸¦åˆ—åŠ¹ç‡ã‚’ä»®å®š
        adjusted_duration = int(total_duration * parallel_efficiency)

        return max(adjusted_duration, 30)  # æœ€ä½30åˆ†

    def _define_success_criteria(self, target_files: List[str]) -> Dict[str, Any]:
        """æˆåŠŸåŸºæº–ã®å®šç¾©"""

        return {
            "completion_rate": 0.95,  # 95%ã®ãƒ•ã‚¡ã‚¤ãƒ«å®Ÿè£…å®Œäº†
            "quality_threshold": 0.8,  # 80%ã®å“è³ªã‚¹ã‚³ã‚¢
            "max_failures": max(1, len(target_files) // 10),  # æœ€å¤§å¤±æ•—æ•°
            "time_limit_multiplier": 1.5,  # æ¨å®šæ™‚é–“ã®1.5å€ä»¥å†…
            "dependency_resolution": 0.9  # 90%ã®ä¾å­˜é–¢ä¿‚è§£æ±º
        }

    def _initialize_implementation_tasks(
        self,
        plan: ImplementationPlan,
        specifications: Dict[str, Dict[str, Any]]
    ) -> None:
        """å®Ÿè£…ã‚¿ã‚¹ã‚¯ã®åˆæœŸåŒ–"""

        with self.coordination_lock:
            self.active_tasks.clear()
            self.completed_tasks.clear()

            # ä¾å­˜é–¢ä¿‚ã‚°ãƒ©ãƒ•ã‚’å†å–å¾—
            graph = self.dependency_analyzer.analyze_project_dependencies(plan.target_files)

            for file_path in plan.target_files:
                if file_path in graph.nodes:
                    node = graph.nodes[file_path]

                    # ä¾å­˜é–¢ä¿‚æŠ½å‡º
                    dependencies = []
                    dependents = []

                    for edge_from, edge_to in graph.edges:
                        if edge_to == file_path:
                            dependencies.append(edge_from)
                        if edge_from == file_path:
                            dependents.append(edge_to)

                    # å„ªå…ˆåº¦è¨ˆç®—ï¼ˆä¾å­˜ã•ã‚Œã‚‹æ•°ãŒå¤šã„ã»ã©é«˜å„ªå…ˆåº¦ï¼‰
                    priority = len(dependents) * 10 + (100 - node.complexity_score)

                    task = FileImplementationTask(
                        file_path=file_path,
                        module_name=node.module_name,
                        implementation_spec=specifications.get(file_path, {}),
                        dependencies=dependencies,
                        dependents=dependents,
                        priority=priority,
                        estimated_complexity=node.complexity_score,
                        status=ImplementationStatus.PENDING,
                        assigned_agent=None,
                        start_time=None,
                        completion_time=None,
                        error_info=None
                    )

                    self.active_tasks[file_path] = task

    def _execute_hybrid_implementation(self, plan: ImplementationPlan) -> Dict[str, Any]:
        """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å®Ÿè£…æˆ¦ç•¥ã®å®Ÿè¡Œ"""

        logger.info("ğŸ›ï¸ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å®Ÿè£…æˆ¦ç•¥å®Ÿè¡Œ")

        results = {
            "status": "completed",
            "phase_results": [],
            "parallel_results": [],
            "failed_tasks": []
        }

        # ãƒ•ã‚§ãƒ¼ã‚ºåˆ¥å®Ÿè¡Œ
        for i, phase in enumerate(plan.implementation_phases):
            logger.info(f"ğŸ“ ãƒ•ã‚§ãƒ¼ã‚º {i+1}/{len(plan.implementation_phases)} é–‹å§‹: {len(phase)}ãƒ•ã‚¡ã‚¤ãƒ«")

            phase_result = self._execute_phase_implementation(phase)
            results["phase_results"].append(phase_result)

            # ãƒ•ã‚§ãƒ¼ã‚ºå¤±æ•—æ™‚ã®å‡¦ç†
            if phase_result["status"] == "failed":
                logger.error(f"âŒ ãƒ•ã‚§ãƒ¼ã‚º {i+1} å¤±æ•—")
                results["status"] = "failed"
                break

        return results

    def _execute_phase_implementation(self, phase_files: List[str]) -> Dict[str, Any]:
        """å˜ä¸€ãƒ•ã‚§ãƒ¼ã‚ºã®å®Ÿè£…å®Ÿè¡Œ"""

        if len(phase_files) == 1:
            # å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«å®Ÿè£…
            return self._execute_single_file_implementation(phase_files[0])
        else:
            # ä¸¦åˆ—å®Ÿè£…
            return self._execute_parallel_files_implementation(phase_files)

    def _execute_single_file_implementation(self, file_path: str) -> Dict[str, Any]:
        """å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«å®Ÿè£…"""

        logger.info(f"ğŸ“„ å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«å®Ÿè£…: {file_path}")

        with self.coordination_lock:
            if file_path not in self.active_tasks:
                return {"status": "error", "message": "ã‚¿ã‚¹ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"}

            task = self.active_tasks[file_path]
            task.status = ImplementationStatus.IN_PROGRESS
            task.start_time = time.time()

        try:
            # å®Ÿè£…å®Ÿè¡Œï¼ˆå°†æ¥ã¯Geminiçµ±åˆï¼‰
            implementation_result = self._simulate_file_implementation(task)

            with self.coordination_lock:
                if implementation_result["success"]:
                    task.status = ImplementationStatus.COMPLETED
                    task.completion_time = time.time()
                    self.completed_tasks[file_path] = task
                    del self.active_tasks[file_path]

                    return {"status": "completed", "file": file_path}
                else:
                    task.status = ImplementationStatus.FAILED
                    task.error_info = implementation_result["error"]

                    return {"status": "failed", "file": file_path, "error": implementation_result["error"]}

        except Exception as e:
            logger.error(f"å®Ÿè£…ã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
            return {"status": "error", "file": file_path, "error": str(e)}

    def _execute_parallel_files_implementation(self, phase_files: List[str]) -> Dict[str, Any]:
        """ä¸¦åˆ—ãƒ•ã‚¡ã‚¤ãƒ«å®Ÿè£…"""

        logger.info(f"âš¡ ä¸¦åˆ—å®Ÿè£…é–‹å§‹: {len(phase_files)}ãƒ•ã‚¡ã‚¤ãƒ«")

        with ThreadPoolExecutor(max_workers=min(len(phase_files), self.max_parallel_tasks)) as executor:
            futures = {
                executor.submit(self._execute_single_file_implementation, file_path): file_path
                for file_path in phase_files
            }

            results = []
            for future in futures:
                try:
                    result = future.result(timeout=300)  # 5åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
                    results.append(result)
                except Exception as e:
                    file_path = futures[future]
                    logger.error(f"ä¸¦åˆ—å®Ÿè£…ã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
                    results.append({"status": "error", "file": file_path, "error": str(e)})

        # çµæœé›†è¨ˆ
        successful = len([r for r in results if r["status"] == "completed"])
        failed = len([r for r in results if r["status"] in ["failed", "error"]])

        return {
            "status": "completed" if failed == 0 else "partial_failure",
            "successful_count": successful,
            "failed_count": failed,
            "detailed_results": results
        }

    def _simulate_file_implementation(self, task: FileImplementationTask) -> Dict[str, Any]:
        """ãƒ•ã‚¡ã‚¤ãƒ«å®Ÿè£…ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆå°†æ¥ã¯Geminiçµ±åˆï¼‰"""

        # å®Ÿè£…æˆåŠŸç‡ã‚’ãƒ•ã‚¡ã‚¤ãƒ«è¤‡é›‘åº¦ã«åŸºã¥ã„ã¦æ±ºå®š
        success_rate = max(0.7, 1.0 - (task.estimated_complexity / 100))

        import random
        if random.random() < success_rate:
            return {"success": True}
        else:
            return {
                "success": False,
                "error": {
                    "type": "implementation_error",
                    "message": f"è¤‡é›‘åº¦{task.estimated_complexity}ã«ã‚ˆã‚‹å®Ÿè£…å¤±æ•—",
                    "complexity": task.estimated_complexity
                }
            }

    def _calculate_coordination_metrics(self) -> CoordinationMetrics:
        """å”èª¿ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨ˆç®—"""

        total_files = len(self.active_tasks) + len(self.completed_tasks)
        completed_files = len(self.completed_tasks)
        failed_files = len([t for t in self.active_tasks.values() if t.status == ImplementationStatus.FAILED])

        # ä¸¦åˆ—åŠ¹ç‡è¨ˆç®—
        if completed_files > 0:
            total_time = sum([
                (t.completion_time - t.start_time)
                for t in self.completed_tasks.values()
                if t.start_time and t.completion_time
            ])
            sequential_time = total_time
            parallel_time = max([
                (t.completion_time - t.start_time)
                for t in self.completed_tasks.values()
                if t.start_time and t.completion_time
            ] + [0])

            parallel_efficiency = min(1.0, sequential_time / max(parallel_time, 1))
        else:
            parallel_efficiency = 0.0

        # ä¾å­˜é–¢ä¿‚è§£æ±ºç‡
        dependency_resolution_rate = completed_files / max(total_files, 1)

        # å¹³å‡å®Ÿè£…æ™‚é–“
        if completed_files > 0:
            avg_time = sum([
                (t.completion_time - t.start_time)
                for t in self.completed_tasks.values()
                if t.start_time and t.completion_time
            ]) / completed_files
        else:
            avg_time = 0.0

        # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ç‰¹å®š
        bottleneck_files = [
            t.file_path for t in self.active_tasks.values()
            if t.estimated_complexity > 50
        ]

        return CoordinationMetrics(
            total_files=total_files,
            completed_files=completed_files,
            failed_files=failed_files,
            parallel_efficiency=parallel_efficiency,
            dependency_resolution_rate=dependency_resolution_rate,
            average_implementation_time=avg_time,
            bottleneck_files=bottleneck_files
        )

    # ãã®ä»–ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰
    def _find_independent_groups(self, files: List[str], graph: DependencyGraph) -> List[List[str]]:
        """ç‹¬ç«‹ã‚°ãƒ«ãƒ¼ãƒ—ã®æ¤œç´¢"""
        groups = []
        remaining = set(files)

        while remaining:
            group = []
            file = remaining.pop()
            group.append(file)

            # ä¾å­˜é–¢ä¿‚ã®ãªã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åŒã˜ã‚°ãƒ«ãƒ¼ãƒ—ã«è¿½åŠ 
            to_remove = set()
            for other_file in remaining:
                has_dependency = any(
                    (file, other_file) in graph.edges or (other_file, file) in graph.edges
                    for _ in [None]  # ãƒ€ãƒŸãƒ¼ãƒ«ãƒ¼ãƒ—
                )
                if not has_dependency:
                    group.append(other_file)
                    to_remove.add(other_file)

            remaining -= to_remove
            if len(group) > 1:
                groups.append(group)

        return groups

    def _find_dependency_path(self, file_path: str, graph: DependencyGraph) -> List[str]:
        """ä¾å­˜é–¢ä¿‚ãƒ‘ã‚¹ã®æ¤œç´¢"""
        # ç°¡æ˜“å®Ÿè£…
        path = [file_path]
        current = file_path

        while True:
            dependencies = [edge[1] for edge in graph.edges if edge[0] == current]
            if not dependencies:
                break
            current = dependencies[0]  # æœ€åˆã®ä¾å­˜é–¢ä¿‚ã‚’é¸æŠ
            path.append(current)
            if len(path) > 10:  # ç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢
                break

        return path

    def _block_dependent_tasks(self, failed_file: str) -> List[str]:
        """ä¾å­˜ã‚¿ã‚¹ã‚¯ã®ãƒ–ãƒ­ãƒƒã‚¯"""
        blocked = []

        with self.coordination_lock:
            for task in self.active_tasks.values():
                if failed_file in task.dependencies:
                    task.status = ImplementationStatus.BLOCKED
                    blocked.append(task.file_path)

        return blocked

    def _determine_recovery_strategy(
        self,
        failed_task: FileImplementationTask,
        error_info: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ãƒªã‚«ãƒãƒªæˆ¦ç•¥æ±ºå®š"""

        if error_info.get("type") == "dependency_error":
            return {
                "strategy": "retry_after_dependency_fix",
                "action": "Fix dependency issues first"
            }
        elif failed_task.estimated_complexity > 70:
            return {
                "strategy": "simplify_implementation",
                "action": "Break down into smaller components"
            }
        else:
            return {
                "strategy": "retry_with_different_approach",
                "action": "Use alternative implementation pattern"
            }


if __name__ == "__main__":
    """ç°¡å˜ãªãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""

    coordinator = MultiFileCoordinator(max_parallel_tasks=3)

    # ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆ
    test_files = [
        "postbox/advanced/dependency_analyzer.py",
        "postbox/advanced/multi_file_coordinator.py",
        "postbox/workflow/dual_agent_coordinator.py"
    ]

    # å®Ÿè£…è¨ˆç”»ä½œæˆ
    plan = coordinator.create_implementation_plan(
        target_files=test_files,
        strategy=CoordinationStrategy.HYBRID
    )

    print(f"ğŸ“‹ å®Ÿè£…è¨ˆç”»ä½œæˆå®Œäº†:")
    print(f"  - å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {len(plan.target_files)}")
    print(f"  - å®Ÿè£…ãƒ•ã‚§ãƒ¼ã‚º: {len(plan.implementation_phases)}")
    print(f"  - æ¨å®šæ™‚é–“: {plan.estimated_duration}åˆ†")

    # é€²æ—ç›£è¦–ã®ãƒ†ã‚¹ãƒˆ
    progress = coordinator.monitor_implementation_progress()
    print(f"\nğŸ“Š ç¾åœ¨ã®é€²æ—: {progress['completion_rate']:.1%}")
