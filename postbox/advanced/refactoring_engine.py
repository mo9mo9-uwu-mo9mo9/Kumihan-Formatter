#!/usr/bin/env python3
"""
Refactoring Engine - è‡ªå‹•ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ã‚¨ãƒ³ã‚¸ãƒ³
Issue #870: è¤‡é›‘ã‚¿ã‚¹ã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³æ‹¡å¼µ

ã‚³ãƒ¼ãƒ‰å“è³ªå‘ä¸Šãƒ»ä¿å®ˆæ€§æ”¹å–„ã®ãŸã‚ã®è‡ªå‹•ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ 
ASTè§£æãƒ™ãƒ¼ã‚¹ã®å®‰å…¨ãªã‚³ãƒ¼ãƒ‰å¤‰æ›
"""

import ast
import os
import re
import sys
from typing import Dict, List, Any, Set, Optional, Tuple, Union, Callable
from pathlib import Path
from dataclasses import dataclass, asdict
from enum import Enum
import textwrap
import copy

# ãƒ­ã‚¬ãƒ¼çµ±åˆ
try:
    from kumihan_formatter.core.utilities.logger import get_logger
    logger = get_logger(__name__)
except ImportError:
    import logging
    logger = logging.getLogger(__name__)


class RefactoringType(Enum):
    """ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ç¨®åˆ¥"""
    EXTRACT_METHOD = "extract_method"
    EXTRACT_CLASS = "extract_class"
    INLINE_METHOD = "inline_method"
    MOVE_METHOD = "move_method"
    RENAME_VARIABLE = "rename_variable"
    RENAME_METHOD = "rename_method"
    REMOVE_DEAD_CODE = "remove_dead_code"
    SIMPLIFY_CONDITIONAL = "simplify_conditional"
    INTRODUCE_PARAMETER = "introduce_parameter"
    REPLACE_MAGIC_NUMBER = "replace_magic_number"
    SPLIT_LONG_METHOD = "split_long_method"
    MERGE_DUPLICATE_CODE = "merge_duplicate_code"
    IMPROVE_NAMING = "improve_naming"
    ADD_TYPE_HINTS = "add_type_hints"
    OPTIMIZE_IMPORTS = "optimize_imports"


class RefactoringPriority(Enum):
    """ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°å„ªå…ˆåº¦"""
    CRITICAL = "critical"    # ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ï¼ˆå³åº§å¯¾å¿œï¼‰
    HIGH = "high"           # é«˜ï¼ˆå„ªå…ˆå¯¾å¿œï¼‰
    MEDIUM = "medium"       # ä¸­ï¼ˆé€šå¸¸å¯¾å¿œï¼‰
    LOW = "low"             # ä½ï¼ˆæ™‚é–“ãŒã‚ã‚‹ã¨ãï¼‰


@dataclass
class RefactoringOpportunity:
    """ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°æ©Ÿä¼š"""
    file_path: str
    line_number: int
    end_line: Optional[int]
    refactoring_type: RefactoringType
    priority: RefactoringPriority
    description: str
    current_code: str
    suggested_code: str
    complexity_reduction: int
    maintainability_gain: float
    confidence_score: float  # ææ¡ˆã®ä¿¡é ¼åº¦
    affected_lines: List[int]
    dependencies: List[str]  # ä¾å­˜ã™ã‚‹ä»–ã®å¤‰æ›´


@dataclass
class RefactoringResult:
    """ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°çµæœ"""
    original_file: str
    refactored_file: str
    applied_refactorings: List[RefactoringOpportunity]
    skipped_refactorings: List[RefactoringOpportunity]
    errors: List[str]
    quality_improvement: float
    lines_changed: int
    complexity_reduction: int


@dataclass
class CodeMetrics:
    """ã‚³ãƒ¼ãƒ‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    cyclomatic_complexity: int
    lines_of_code: int
    number_of_methods: int
    number_of_classes: int
    depth_of_inheritance: int
    coupling_factor: float
    cohesion_score: float
    maintainability_index: float


class CodeAnalyzer:
    """ã‚³ãƒ¼ãƒ‰è§£æå™¨"""

    def __init__(self) -> None:
        self.magic_numbers: Set[Union[int, float]] = set()
        self.long_method_threshold = 20
        self.high_complexity_threshold = 10

    def analyze_file(self, file_path: str) -> Tuple[CodeMetrics, List[RefactoringOpportunity]]:
        """ãƒ•ã‚¡ã‚¤ãƒ«è§£æ"""

        logger.info(f"ğŸ” ã‚³ãƒ¼ãƒ‰è§£æé–‹å§‹: {file_path}")

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            tree = ast.parse(content, filename=file_path)

            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
            metrics = self._calculate_metrics(tree, content)

            # ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°æ©Ÿä¼šæ¤œå‡º
            opportunities = self._detect_opportunities(tree, file_path, content)

            logger.info(f"âœ… è§£æå®Œäº†: {len(opportunities)}ä»¶ã®æ”¹å–„æ©Ÿä¼š")
            return metrics, opportunities

        except Exception as e:
            logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«è§£æã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
            return CodeMetrics(0, 0, 0, 0, 0, 0.0, 0.0, 0.0), []

    def _calculate_metrics(self, tree: ast.AST, content: str) -> CodeMetrics:
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—"""

        lines = content.split('\n')
        loc = len([line for line in lines if line.strip() and not line.strip().startswith('#')])

        methods = []
        classes = []
        complexity = 0

        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                methods.append(node)
                complexity += self._calculate_cyclomatic_complexity(node)
            elif isinstance(node, ast.ClassDef):
                classes.append(node)

        # ç¶™æ‰¿ã®æ·±ã•
        max_inheritance_depth = 0
        for class_node in classes:
            depth = len(class_node.bases)
            max_inheritance_depth = max(max_inheritance_depth, depth)

        # çµåˆåº¦ãƒ»å‡é›†åº¦ï¼ˆç°¡æ˜“è¨ˆç®—ï¼‰
        coupling = min(len(methods) / max(len(classes), 1) / 10, 1.0)
        cohesion = max(0.5, 1.0 - coupling)

        # ä¿å®ˆæ€§æŒ‡æ•°ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        maintainability = max(0.0, 100 - complexity * 2 - loc / 10)

        return CodeMetrics(
            cyclomatic_complexity=complexity,
            lines_of_code=loc,
            number_of_methods=len(methods),
            number_of_classes=len(classes),
            depth_of_inheritance=max_inheritance_depth,
            coupling_factor=coupling,
            cohesion_score=cohesion,
            maintainability_index=maintainability
        )

    def _calculate_cyclomatic_complexity(self, node: ast.FunctionDef) -> int:
        """å¾ªç’°çš„è¤‡é›‘åº¦è¨ˆç®—"""
        complexity = 1  # åŸºæº–å€¤

        for child in ast.walk(node):
            if isinstance(child, (ast.If, ast.While, ast.For, ast.With)):
                complexity += 1
            elif isinstance(child, ast.Try):
                complexity += len(child.handlers)
            elif isinstance(child, ast.BoolOp):
                complexity += len(child.values) - 1

        return complexity

    def _detect_opportunities(self, tree: ast.AST, file_path: str, content: str) -> List[RefactoringOpportunity]:
        """ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°æ©Ÿä¼šæ¤œå‡º"""

        opportunities = []
        lines = content.split('\n')

        # å„ç¨®ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°æ©Ÿä¼šã‚’æ¤œå‡º
        opportunities.extend(self._detect_long_methods(tree, file_path, lines))
        opportunities.extend(self._detect_complex_methods(tree, file_path, lines))
        opportunities.extend(self._detect_magic_numbers(tree, file_path, lines))
        opportunities.extend(self._detect_duplicate_code(tree, file_path, lines))
        opportunities.extend(self._detect_naming_issues(tree, file_path, lines))
        opportunities.extend(self._detect_missing_type_hints(tree, file_path, lines))
        opportunities.extend(self._detect_dead_code(tree, file_path, lines))
        opportunities.extend(self._detect_import_issues(tree, file_path, lines))

        # å„ªå…ˆåº¦ã§ã‚½ãƒ¼ãƒˆ
        opportunities.sort(key=lambda x: (x.priority.value, -x.confidence_score))

        return opportunities

    def _detect_long_methods(self, tree: ast.AST, file_path: str, lines: List[str]) -> List[RefactoringOpportunity]:
        """é•·ã„ãƒ¡ã‚½ãƒƒãƒ‰ã®æ¤œå‡º"""
        opportunities = []

        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                method_lines = (node.end_lineno or node.lineno) - node.lineno + 1

                if method_lines > self.long_method_threshold:
                    current_code = '\n'.join(lines[node.lineno-1:(node.end_lineno or node.lineno)])

                    opportunities.append(RefactoringOpportunity(
                        file_path=file_path,
                        line_number=node.lineno,
                        end_line=node.end_lineno,
                        refactoring_type=RefactoringType.SPLIT_LONG_METHOD,
                        priority=RefactoringPriority.MEDIUM,
                        description=f"ãƒ¡ã‚½ãƒƒãƒ‰ '{node.name}' ãŒé•·ã™ãã¾ã™ ({method_lines}è¡Œ)",
                        current_code=current_code,
                        suggested_code=self._suggest_method_split(node, lines),
                        complexity_reduction=method_lines // 4,
                        maintainability_gain=0.6,
                        confidence_score=0.8,
                        affected_lines=list(range(node.lineno, node.end_lineno + 1)),
                        dependencies=[]
                    ))

        return opportunities

    def _detect_complex_methods(self, tree: ast.AST, file_path: str, lines: List[str]) -> List[RefactoringOpportunity]:
        """è¤‡é›‘ãªãƒ¡ã‚½ãƒƒãƒ‰ã®æ¤œå‡º"""
        opportunities = []

        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                complexity = self._calculate_cyclomatic_complexity(node)

                if complexity > self.high_complexity_threshold:
                    current_code = '\n'.join(lines[node.lineno-1:(node.end_lineno or node.lineno)])

                    opportunities.append(RefactoringOpportunity(
                        file_path=file_path,
                        line_number=node.lineno,
                        end_line=node.end_lineno,
                        refactoring_type=RefactoringType.SIMPLIFY_CONDITIONAL,
                        priority=RefactoringPriority.HIGH,
                        description=f"ãƒ¡ã‚½ãƒƒãƒ‰ '{node.name}' ã®è¤‡é›‘åº¦ãŒé«˜ã„ã§ã™ (CC={complexity})",
                        current_code=current_code,
                        suggested_code=self._suggest_complexity_reduction(node, lines),
                        complexity_reduction=complexity // 2,
                        maintainability_gain=0.7,
                        confidence_score=0.7,
                        affected_lines=list(range(node.lineno, node.end_lineno + 1)),
                        dependencies=[]
                    ))

        return opportunities

    def _detect_magic_numbers(self, tree: ast.AST, file_path: str, lines: List[str]) -> List[RefactoringOpportunity]:
        """ãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼ã®æ¤œå‡º"""
        opportunities = []

        for node in ast.walk(tree):
            if isinstance(node, ast.Constant):
                if isinstance(node.value, (int, float)) and node.value not in [0, 1, -1]:
                    opportunities.append(RefactoringOpportunity(
                        file_path=file_path,
                        line_number=node.lineno,
                        end_line=node.lineno,
                        refactoring_type=RefactoringType.REPLACE_MAGIC_NUMBER,
                        priority=RefactoringPriority.MEDIUM,
                        description=f"ãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼ {node.value} ã‚’å®šæ•°ã«ç½®ãæ›ãˆ",
                        current_code=str(node.value),
                        suggested_code=f"CONSTANT_VALUE = {node.value}",
                        complexity_reduction=1,
                        maintainability_gain=0.3,
                        confidence_score=0.9,
                        affected_lines=[node.lineno],
                        dependencies=[]
                    ))

        return opportunities

    def _detect_duplicate_code(self, tree: ast.AST, file_path: str, lines: List[str]) -> List[RefactoringOpportunity]:
        """é‡è¤‡ã‚³ãƒ¼ãƒ‰ã®æ¤œå‡º"""
        opportunities = []

        # ç°¡æ˜“å®Ÿè£…ï¼šåŒä¸€ã®æ–‡å­—åˆ—ã‚’æŒã¤è¡Œã‚’æ¤œå‡º
        line_counts = {}
        for i, line in enumerate(lines):
            stripped = line.strip()
            if len(stripped) > 10 and not stripped.startswith('#'):
                if stripped in line_counts:
                    line_counts[stripped].append(i + 1)
                else:
                    line_counts[stripped] = [i + 1]

        for line_content, line_numbers in line_counts.items():
            if len(line_numbers) > 1:
                opportunities.append(RefactoringOpportunity(
                    file_path=file_path,
                    line_number=line_numbers[0],
                    end_line=line_numbers[0],
                    refactoring_type=RefactoringType.MERGE_DUPLICATE_CODE,
                    priority=RefactoringPriority.MEDIUM,
                    description=f"é‡è¤‡ã‚³ãƒ¼ãƒ‰ã‚’çµ±åˆå¯èƒ½ ({len(line_numbers)}ç®‡æ‰€)",
                    current_code=line_content,
                    suggested_code=f"# å…±é€šãƒ¡ã‚½ãƒƒãƒ‰ã¨ã—ã¦æŠ½å‡ºã‚’æ¤œè¨: {line_content[:50]}...",
                    complexity_reduction=len(line_numbers),
                    maintainability_gain=0.5,
                    confidence_score=0.6,
                    affected_lines=line_numbers,
                    dependencies=[]
                ))

        return opportunities

    def _detect_naming_issues(self, tree: ast.AST, file_path: str, lines: List[str]) -> List[RefactoringOpportunity]:
        """å‘½åå•é¡Œã®æ¤œå‡º"""
        opportunities = []

        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                if len(node.name) < 3 or not node.name.islower():
                    opportunities.append(RefactoringOpportunity(
                        file_path=file_path,
                        line_number=node.lineno,
                        end_line=node.lineno,
                        refactoring_type=RefactoringType.IMPROVE_NAMING,
                        priority=RefactoringPriority.LOW,
                        description=f"é–¢æ•°å '{node.name}' ã®æ”¹å–„ã‚’æ¨å¥¨",
                        current_code=f"def {node.name}(",
                        suggested_code=f"def {self._suggest_better_name(node.name)}(",
                        complexity_reduction=0,
                        maintainability_gain=0.2,
                        confidence_score=0.5,
                        affected_lines=[node.lineno],
                        dependencies=[]
                    ))

        return opportunities

    def _detect_missing_type_hints(self, tree: ast.AST, file_path: str, lines: List[str]) -> List[RefactoringOpportunity]:
        """å‹ãƒ’ãƒ³ãƒˆä¸è¶³ã®æ¤œå‡º"""
        opportunities = []

        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                missing_hints = []

                # å¼•æ•°ã®å‹ãƒ’ãƒ³ãƒˆç¢ºèª
                for arg in node.args.args:
                    if arg.annotation is None:
                        missing_hints.append(f"å¼•æ•° {arg.arg}")

                # æˆ»ã‚Šå€¤ã®å‹ãƒ’ãƒ³ãƒˆç¢ºèª
                if node.returns is None:
                    missing_hints.append("æˆ»ã‚Šå€¤")

                if missing_hints:
                    opportunities.append(RefactoringOpportunity(
                        file_path=file_path,
                        line_number=node.lineno,
                        end_line=node.lineno,
                        refactoring_type=RefactoringType.ADD_TYPE_HINTS,
                        priority=RefactoringPriority.MEDIUM,
                        description=f"å‹ãƒ’ãƒ³ãƒˆè¿½åŠ : {', '.join(missing_hints)}",
                        current_code=f"def {node.name}(",
                        suggested_code=self._suggest_type_hints(node),
                        complexity_reduction=0,
                        maintainability_gain=0.4,
                        confidence_score=0.7,
                        affected_lines=[node.lineno],
                        dependencies=[]
                    ))

        return opportunities

    def _detect_dead_code(self, tree: ast.AST, file_path: str, lines: List[str]) -> List[RefactoringOpportunity]:
        """ãƒ‡ãƒƒãƒ‰ã‚³ãƒ¼ãƒ‰ã®æ¤œå‡º"""
        opportunities = []

        # æœªä½¿ç”¨ã‚¤ãƒ³ãƒãƒ¼ãƒˆã®æ¤œå‡º
        imports = set()
        used_names = set()

        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    imports.add(alias.name)
            elif isinstance(node, ast.ImportFrom):
                if node.module:
                    for alias in node.names:
                        imports.add(alias.name)
            elif isinstance(node, ast.Name):
                used_names.add(node.id)

        unused_imports = imports - used_names
        for unused in unused_imports:
            opportunities.append(RefactoringOpportunity(
                file_path=file_path,
                line_number=1,  # æ­£ç¢ºãªè¡Œç•ªå·ã¯åˆ¥é€”ç‰¹å®š
                end_line=1,
                refactoring_type=RefactoringType.REMOVE_DEAD_CODE,
                priority=RefactoringPriority.LOW,
                description=f"æœªä½¿ç”¨ã‚¤ãƒ³ãƒãƒ¼ãƒˆ: {unused}",
                current_code=f"import {unused}",
                suggested_code="# ã‚¤ãƒ³ãƒãƒ¼ãƒˆå‰Šé™¤",
                complexity_reduction=1,
                maintainability_gain=0.1,
                confidence_score=0.8,
                affected_lines=[],
                dependencies=[]
            ))

        return opportunities

    def _detect_import_issues(self, tree: ast.AST, file_path: str, lines: List[str]) -> List[RefactoringOpportunity]:
        """ã‚¤ãƒ³ãƒãƒ¼ãƒˆå•é¡Œã®æ¤œå‡º"""
        opportunities = []

        import_lines = []
        for i, line in enumerate(lines):
            if line.strip().startswith(('import ', 'from ')):
                import_lines.append((i + 1, line.strip()))

        if len(import_lines) > 1:
            # ã‚¤ãƒ³ãƒãƒ¼ãƒˆã®é †åºãƒã‚§ãƒƒã‚¯
            stdlib_imports = []
            third_party_imports = []
            local_imports = []

            for line_num, import_line in import_lines:
                if any(stdlib in import_line for stdlib in ['os', 'sys', 'json', 'time']):
                    stdlib_imports.append((line_num, import_line))
                elif 'kumihan_formatter' in import_line or 'postbox' in import_line:
                    local_imports.append((line_num, import_line))
                else:
                    third_party_imports.append((line_num, import_line))

            if stdlib_imports and third_party_imports and local_imports:
                opportunities.append(RefactoringOpportunity(
                    file_path=file_path,
                    line_number=import_lines[0][0],
                    end_line=import_lines[-1][0],
                    refactoring_type=RefactoringType.OPTIMIZE_IMPORTS,
                    priority=RefactoringPriority.LOW,
                    description="ã‚¤ãƒ³ãƒãƒ¼ãƒˆé †åºã®æœ€é©åŒ–",
                    current_code='\n'.join([imp[1] for imp in import_lines]),
                    suggested_code=self._suggest_import_order(stdlib_imports, third_party_imports, local_imports),
                    complexity_reduction=0,
                    maintainability_gain=0.2,
                    confidence_score=0.9,
                    affected_lines=[imp[0] for imp in import_lines],
                    dependencies=[]
                ))

        return opportunities

    def _suggest_method_split(self, node: ast.FunctionDef, lines: List[str]) -> str:
        """ãƒ¡ã‚½ãƒƒãƒ‰åˆ†å‰²ææ¡ˆ"""
        return f"# ãƒ¡ã‚½ãƒƒãƒ‰ '{node.name}' ã‚’è¤‡æ•°ã®å°ã•ãªãƒ¡ã‚½ãƒƒãƒ‰ã«åˆ†å‰²ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨"

    def _suggest_complexity_reduction(self, node: ast.FunctionDef, lines: List[str]) -> str:
        """è¤‡é›‘åº¦å‰Šæ¸›ææ¡ˆ"""
        return f"# ãƒ¡ã‚½ãƒƒãƒ‰ '{node.name}' ã®æ¡ä»¶åˆ†å²ã‚’ç°¡ç´ åŒ–ã¾ãŸã¯Strategy/Stateãƒ‘ã‚¿ãƒ¼ãƒ³ã®é©ç”¨ã‚’æ¤œè¨"

    def _suggest_better_name(self, current_name: str) -> str:
        """ã‚ˆã‚Šè‰¯ã„åå‰ã®ææ¡ˆ"""
        return f"{current_name}_improved"  # ç°¡æ˜“å®Ÿè£…

    def _suggest_type_hints(self, node: ast.FunctionDef) -> str:
        """å‹ãƒ’ãƒ³ãƒˆææ¡ˆ"""
        return f"def {node.name}(param: Any) -> Any:"  # ç°¡æ˜“å®Ÿè£…

    def _suggest_import_order(self, stdlib: List, third_party: List, local: List) -> str:
        """ã‚¤ãƒ³ãƒãƒ¼ãƒˆé †åºææ¡ˆ"""
        result = []

        if stdlib:
            result.extend([imp[1] for imp in stdlib])
            result.append("")

        if third_party:
            result.extend([imp[1] for imp in third_party])
            result.append("")

        if local:
            result.extend([imp[1] for imp in local])

        return '\n'.join(result)


class RefactoringEngine:
    """ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ã‚¨ãƒ³ã‚¸ãƒ³ - ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""

    def __init__(self, project_root: Optional[str] = None):
        self.project_root = Path(project_root or os.getcwd())
        self.analyzer = CodeAnalyzer()
        self.refactoring_history: List[RefactoringResult] = []

        logger.info("ğŸ”§ Refactoring Engine åˆæœŸåŒ–å®Œäº†")
        logger.info(f"ğŸ“ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆ: {self.project_root}")

    def analyze_project(self, target_paths: Optional[List[str]] = None) -> Dict[str, Tuple[CodeMetrics, List[RefactoringOpportunity]]]:
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“è§£æ"""

        logger.info("ğŸ” ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°è§£æé–‹å§‹")

        if target_paths is None:
            target_paths = self._find_python_files()

        results = {}
        total_opportunities = 0

        for file_path in target_paths:
            try:
                metrics, opportunities = self.analyzer.analyze_file(file_path)
                results[file_path] = (metrics, opportunities)
                total_opportunities += len(opportunities)
            except Exception as e:
                logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«è§£æã‚¨ãƒ©ãƒ¼ {file_path}: {e}")

        logger.info(f"âœ… ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè§£æå®Œäº†: {total_opportunities}ä»¶ã®æ”¹å–„æ©Ÿä¼š")
        return results

    def apply_refactorings(
        self,
        file_path: str,
        opportunities: List[RefactoringOpportunity],
        auto_apply: bool = False
    ) -> RefactoringResult:
        """ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°é©ç”¨"""

        logger.info(f"ğŸ”§ ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°é©ç”¨é–‹å§‹: {file_path}")
        logger.info(f"ğŸ“‹ é©ç”¨å€™è£œ: {len(opportunities)}ä»¶")

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                original_content = f.read()

            # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹æ±ºå®š
            output_path = self._get_refactored_file_path(file_path)

            applied = []
            skipped = []
            errors = []

            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚³ãƒ”ãƒ¼ã—ã¦å¤‰æ›´é©ç”¨
            refactored_content = original_content

            # é«˜ã„ä¿¡é ¼åº¦ã‹ã‚‰é †ã«é©ç”¨
            sorted_opportunities = sorted(opportunities, key=lambda x: -x.confidence_score)

            for opportunity in sorted_opportunities:
                try:
                    if auto_apply or opportunity.confidence_score > 0.8:
                        # è‡ªå‹•é©ç”¨
                        refactored_content = self._apply_single_refactoring(
                            refactored_content, opportunity
                        )
                        applied.append(opportunity)
                        logger.info(f"âœ… é©ç”¨: {opportunity.description}")
                    else:
                        # ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæ‰‹å‹•ç¢ºèªå¿…è¦ï¼‰
                        skipped.append(opportunity)
                        logger.info(f"â­ï¸ ã‚¹ã‚­ãƒƒãƒ—: {opportunity.description}")

                except Exception as e:
                    errors.append(f"{opportunity.description}: {str(e)}")
                    logger.error(f"âŒ é©ç”¨ã‚¨ãƒ©ãƒ¼: {e}")

            # ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°å¾Œãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(refactored_content)

            # çµæœè¨ˆç®—
            quality_improvement = sum(op.maintainability_gain for op in applied)
            lines_changed = sum(len(op.affected_lines) for op in applied)
            complexity_reduction = sum(op.complexity_reduction for op in applied)

            result = RefactoringResult(
                original_file=file_path,
                refactored_file=output_path,
                applied_refactorings=applied,
                skipped_refactorings=skipped,
                errors=errors,
                quality_improvement=quality_improvement,
                lines_changed=lines_changed,
                complexity_reduction=complexity_reduction
            )

            self.refactoring_history.append(result)

            logger.info(f"âœ… ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°å®Œäº†:")
            logger.info(f"  - é©ç”¨: {len(applied)}ä»¶")
            logger.info(f"  - ã‚¹ã‚­ãƒƒãƒ—: {len(skipped)}ä»¶")
            logger.info(f"  - ã‚¨ãƒ©ãƒ¼: {len(errors)}ä»¶")
            logger.info(f"  - å“è³ªå‘ä¸Š: {quality_improvement:.2f}")

            return result

        except Exception as e:
            logger.error(f"ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°é©ç”¨ã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
            raise

    def generate_refactoring_report(self, analysis_results: Dict[str, Tuple[CodeMetrics, List[RefactoringOpportunity]]]) -> str:
        """ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""

        logger.info("ğŸ“Š ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ")

        total_opportunities = sum(len(opportunities) for _, opportunities in analysis_results.values())
        high_priority_count = sum(
            len([op for op in opportunities if op.priority == RefactoringPriority.HIGH])
            for _, opportunities in analysis_results.values()
        )

        report_lines = [
            "# ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°è§£æãƒ¬ãƒãƒ¼ãƒˆ",
            "",
            "## æ¦‚è¦",
            f"- è§£æãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(analysis_results)}",
            f"- ç·æ”¹å–„æ©Ÿä¼š: {total_opportunities}ä»¶",
            f"- é«˜å„ªå…ˆåº¦: {high_priority_count}ä»¶",
            "",
            "## ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥è©³ç´°",
            ""
        ]

        for file_path, (metrics, opportunities) in analysis_results.items():
            rel_path = os.path.relpath(file_path, self.project_root)

            report_lines.extend([
                f"### {rel_path}",
                "",
                "#### ãƒ¡ãƒˆãƒªã‚¯ã‚¹",
                f"- å¾ªç’°çš„è¤‡é›‘åº¦: {metrics.cyclomatic_complexity}",
                f"- ã‚³ãƒ¼ãƒ‰è¡Œæ•°: {metrics.lines_of_code}",
                f"- ãƒ¡ã‚½ãƒƒãƒ‰æ•°: {metrics.number_of_methods}",
                f"- ä¿å®ˆæ€§æŒ‡æ•°: {metrics.maintainability_index:.1f}",
                "",
                "#### æ”¹å–„æ©Ÿä¼š",
                ""
            ])

            for i, opportunity in enumerate(opportunities[:5], 1):  # ä¸Šä½5ä»¶
                report_lines.extend([
                    f"{i}. **{opportunity.refactoring_type.value}** ({opportunity.priority.value})",
                    f"   - {opportunity.description}",
                    f"   - ä¿¡é ¼åº¦: {opportunity.confidence_score:.1%}",
                    f"   - ä¿å®ˆæ€§å‘ä¸Š: {opportunity.maintainability_gain:.1%}",
                    ""
                ])

            if len(opportunities) > 5:
                report_lines.append(f"   ä»– {len(opportunities) - 5}ä»¶...")

            report_lines.append("")

        # æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
        report_lines.extend([
            "## æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³",
            "",
            "1. **é«˜å„ªå…ˆåº¦é …ç›®ã®å³åº§å¯¾å¿œ**",
            "   - ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãƒ»é«˜å„ªå…ˆåº¦é …ç›®ã®ä¿®æ­£",
            "",
            "2. **æ®µéšçš„æ”¹å–„å®Ÿæ–½**",
            "   - ä¿¡é ¼åº¦80%ä»¥ä¸Šã®é …ç›®ã‹ã‚‰è‡ªå‹•é©ç”¨",
            "",
            "3. **ç¶™ç¶šçš„ç›£è¦–**",
            "   - å®šæœŸçš„ãªã‚³ãƒ¼ãƒ‰å“è³ªãƒã‚§ãƒƒã‚¯",
            ""
        ])

        return '\n'.join(report_lines)

    def get_refactoring_history(self) -> List[RefactoringResult]:
        """ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°å±¥æ­´å–å¾—"""
        return self.refactoring_history[:]

    def _find_python_files(self) -> List[str]:
        """Pythonãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢"""
        python_files = []

        for root, dirs, files in os.walk(self.project_root):
            # é™¤å¤–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            dirs[:] = [d for d in dirs if not d.startswith('.') and d != '__pycache__']

            for file in files:
                if file.endswith('.py'):
                    file_path = os.path.join(root, file)
                    python_files.append(file_path)

        return python_files

    def _get_refactored_file_path(self, original_path: str) -> str:
        """ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°å¾Œãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ç”Ÿæˆ"""
        path = Path(original_path)
        return str(self.project_root / "tmp" / f"{path.stem}_refactored{path.suffix}")

    def _apply_single_refactoring(self, content: str, opportunity: RefactoringOpportunity) -> str:
        """å˜ä¸€ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°é©ç”¨"""

        # ç°¡æ˜“å®Ÿè£…ï¼šç‰¹å®šã®æ–‡å­—åˆ—ç½®æ›
        if opportunity.refactoring_type == RefactoringType.REPLACE_MAGIC_NUMBER:
            # ãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼ç½®æ›
            return content.replace(opportunity.current_code, f"CONSTANT_{opportunity.current_code}")

        elif opportunity.refactoring_type == RefactoringType.REMOVE_DEAD_CODE:
            # ãƒ‡ãƒƒãƒ‰ã‚³ãƒ¼ãƒ‰å‰Šé™¤
            lines = content.split('\n')
            for line_num in sorted(opportunity.affected_lines, reverse=True):
                if 0 <= line_num - 1 < len(lines):
                    lines.pop(line_num - 1)
            return '\n'.join(lines)

        elif opportunity.refactoring_type == RefactoringType.OPTIMIZE_IMPORTS:
            # ã‚¤ãƒ³ãƒãƒ¼ãƒˆæœ€é©åŒ–
            lines = content.split('\n')
            import_lines = []
            other_lines = []

            for line in lines:
                if line.strip().startswith(('import ', 'from ')):
                    import_lines.append(line)
                else:
                    other_lines.append(line)

            # ã‚¤ãƒ³ãƒãƒ¼ãƒˆå†ç·¨æˆï¼ˆç°¡æ˜“ï¼‰
            optimized_imports = '\n'.join(sorted(set(import_lines)))
            return optimized_imports + '\n\n' + '\n'.join(other_lines)

        else:
            # ãã®ä»–ã®ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ã¯å°†æ¥å®Ÿè£…
            logger.warning(f"æœªå®Ÿè£…ã®ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°: {opportunity.refactoring_type.value}")
            return content


if __name__ == "__main__":
    """ç°¡å˜ãªãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""

    engine = RefactoringEngine()

    # ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®è§£æ
    test_files = [
        "postbox/advanced/dependency_analyzer.py",
        "postbox/advanced/multi_file_coordinator.py"
    ]

    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè§£æ
    analysis_results = engine.analyze_project(test_files)

    print(f"ğŸ” è§£æçµæœ:")
    for file_path, (metrics, opportunities) in analysis_results.items():
        rel_path = os.path.relpath(file_path)
        print(f"\nğŸ“„ {rel_path}:")
        print(f"  - è¤‡é›‘åº¦: {metrics.cyclomatic_complexity}")
        print(f"  - ã‚³ãƒ¼ãƒ‰è¡Œæ•°: {metrics.lines_of_code}")
        print(f"  - æ”¹å–„æ©Ÿä¼š: {len(opportunities)}ä»¶")

        # é«˜å„ªå…ˆåº¦é …ç›®è¡¨ç¤º
        high_priority = [op for op in opportunities if op.priority == RefactoringPriority.HIGH]
        if high_priority:
            print(f"  - é«˜å„ªå…ˆåº¦: {len(high_priority)}ä»¶")
            for op in high_priority[:3]:
                print(f"    â€¢ {op.description}")

    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report = engine.generate_refactoring_report(analysis_results)

    # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
    report_path = engine.project_root / "tmp" / "refactoring_report.md"
    os.makedirs(os.path.dirname(report_path), exist_ok=True)
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report)

    print(f"\nğŸ“Š ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {report_path}")
    print(f"âœ… Refactoring Engine ãƒ†ã‚¹ãƒˆå®Œäº†")
