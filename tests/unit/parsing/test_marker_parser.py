"""Marker Parser é«˜åº¦ãƒ†ã‚¹ãƒˆ - Issue #597 Week 28-29å¯¾å¿œ

ãƒãƒ¼ã‚«ãƒ¼æ§‹æ–‡æ­£è¦åŒ–ãƒ»å±æ€§æŠ½å‡ºãƒ»è¤‡åˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†å‰²ã®ç¢ºèª
"""

from unittest.mock import Mock, patch

import pytest

from kumihan_formatter.core.keyword_parsing.marker_parser import MarkerParser


class TestMarkerParserAdvanced:
    """ãƒãƒ¼ã‚«ãƒ¼ãƒ‘ãƒ¼ã‚µãƒ¼é«˜åº¦ãƒ†ã‚¹ãƒˆ"""

    def setup_method(self):
        """ãƒ†ã‚¹ãƒˆå‰ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        # ãƒ¢ãƒƒã‚¯æˆ¦ç•¥çµ±ä¸€: MarkerParserã¯è»½é‡ãªãŸã‚å®Ÿéš›ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½¿ç”¨
        self.marker_parser = MarkerParser()

        # å¿…è¦ã«å¿œã˜ã¦ãƒ¢ãƒƒã‚¯åŒ–ï¼ˆé‡ã„å‡¦ç†ã‚„å¤–éƒ¨ä¾å­˜ãŒã‚ã‚‹å ´åˆï¼‰
        self.mock_marker_parser = Mock(spec=MarkerParser)

    def test_marker_parser_initialization(self):
        """ãƒãƒ¼ã‚«ãƒ¼ãƒ‘ãƒ¼ã‚µãƒ¼åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ"""
        assert self.marker_parser is not None
        assert hasattr(self.marker_parser, "normalize_marker")
        assert hasattr(self.marker_parser, "extract_attributes")
        assert hasattr(self.marker_parser, "split_compound_keywords")

    def test_marker_normalization_comprehensive(self):
        """ãƒãƒ¼ã‚«ãƒ¼æ­£è¦åŒ–åŒ…æ‹¬ãƒ†ã‚¹ãƒˆ"""
        normalization_cases = [
            # ç©ºç™½ã®æ­£è¦åŒ–
            ("  keyword  ", "keyword"),
            ("keyword\t\n", "keyword"),
            ("  multiple   spaces  ", "multiple spaces"),
            # ç‰¹æ®Šæ–‡å­—ã®æ­£è¦åŒ–
            ("keyword+compound", "keyword+compound"),
            ("keyword[attr=value]", "keyword[attr=value]"),
            ("keyword-with-dashes", "keyword-with-dashes"),
            ("keyword_with_underscores", "keyword_with_underscores"),
            # Unicodeæ–‡å­—ã®æ­£è¦åŒ–
            ("æ—¥æœ¬èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", "æ—¥æœ¬èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"),
            ("English+æ—¥æœ¬èª", "English+æ—¥æœ¬èª"),
            ("çµµæ–‡å­—ğŸŒãƒ†ã‚¹ãƒˆ", "çµµæ–‡å­—ğŸŒãƒ†ã‚¹ãƒˆ"),
            # å¤§æ–‡å­—å°æ–‡å­—ï¼ˆå®Ÿè£…ã«ä¾å­˜ï¼‰
            ("UPPERCASE", "UPPERCASE"),  # ãã®ã¾ã¾ä¿æŒã•ã‚Œã‚‹å ´åˆ
            ("MixedCase", "MixedCase"),
        ]

        for input_marker, expected_output in normalization_cases:
            try:
                normalized = self.marker_parser.normalize_marker(input_marker)
                assert (
                    normalized == expected_output
                ), f"æ­£è¦åŒ–çµæœãŒæœŸå¾…ã¨ç•°ãªã‚‹: '{input_marker}' -> '{normalized}' (æœŸå¾…: '{expected_output}')"
            except Exception as e:
                pytest.fail(f"ãƒãƒ¼ã‚«ãƒ¼æ­£è¦åŒ–ã§ã‚¨ãƒ©ãƒ¼: '{input_marker}' -> {e}")

    def test_attribute_extraction_comprehensive(self):
        """å±æ€§æŠ½å‡ºåŒ…æ‹¬ãƒ†ã‚¹ãƒˆ"""
        attribute_cases = [
            # å˜ä¸€å±æ€§
            ("keyword[attr=value]", {"attr": "value"}),
            ("keyword[alt=ç”»åƒèª¬æ˜]", {"alt": "ç”»åƒèª¬æ˜"}),
            ("keyword[url=https://example.com]", {"url": "https://example.com"}),
            # è¤‡æ•°å±æ€§
            ("keyword[a=1,b=2,c=3]", {"a": "1", "b": "2", "c": "3"}),
            (
                "image[alt=èª¬æ˜,width=200,height=150]",
                {"alt": "èª¬æ˜", "width": "200", "height": "150"},
            ),
            # ã‚¯ã‚©ãƒ¼ãƒˆã•ã‚ŒãŸå€¤
            ("keyword[attr='quoted value']", {"attr": "quoted value"}),
            ('keyword[attr="double quoted"]', {"attr": "double quoted"}),
            (
                "keyword[mixed='single',other=\"double\"]",
                {"mixed": "single", "other": "double"},
            ),
            # ç‰¹æ®Šæ–‡å­—ã‚’å«ã‚€å±æ€§
            (
                "keyword[url='https://example.com?param=value&other=test']",
                {"url": "https://example.com?param=value&other=test"},
            ),
            (
                "keyword[style='color: red; font-size: 14px;']",
                {"style": "color: red; font-size: 14px;"},
            ),
            # Unicodeå±æ€§
            ("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰[å±æ€§=å€¤]", {"å±æ€§": "å€¤"}),
            ("keyword[æ—¥æœ¬èª=å€¤,English=value]", {"æ—¥æœ¬èª": "å€¤", "English": "value"}),
            # å±æ€§ãªã—
            ("keyword", {}),
            ("keyword[]", {}),
        ]

        for input_marker, expected_attrs in attribute_cases:
            try:
                extracted_attrs = self.marker_parser.extract_attributes(input_marker)
                assert (
                    extracted_attrs == expected_attrs
                ), f"å±æ€§æŠ½å‡ºçµæœãŒæœŸå¾…ã¨ç•°ãªã‚‹: '{input_marker}' -> {extracted_attrs} (æœŸå¾…: {expected_attrs})"
            except Exception as e:
                pytest.fail(f"å±æ€§æŠ½å‡ºã§ã‚¨ãƒ©ãƒ¼: '{input_marker}' -> {e}")

    def test_compound_keyword_splitting(self):
        """è¤‡åˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†å‰²ãƒ†ã‚¹ãƒˆ"""
        compound_cases = [
            # åŸºæœ¬çš„ãªè¤‡åˆ
            ("keyword1+keyword2", ["keyword1", "keyword2"]),
            ("å¼·èª¿+é‡è¦", ["å¼·èª¿", "é‡è¦"]),
            ("code+highlight+executable", ["code", "highlight", "executable"]),
            # å±æ€§ä»˜ãè¤‡åˆ
            ("image[alt=èª¬æ˜]+decoration", ["image[alt=èª¬æ˜]", "decoration"]),
            (
                "link[url=test]+emphasis[level=high]",
                ["link[url=test]", "emphasis[level=high]"],
            ),
            # è¤‡é›‘ãªçµ„ã¿åˆã‚ã›
            ("base+modifier[type=1]+final", ["base", "modifier[type=1]", "final"]),
            ("æ—¥æœ¬èª+English+ä¸­æ–‡", ["æ—¥æœ¬èª", "English", "ä¸­æ–‡"]),
            # å˜ä¸€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆåˆ†å‰²ãªã—ï¼‰
            ("single_keyword", ["single_keyword"]),
            ("single[attr=value]", ["single[attr=value]"]),
            # ç©ºæ–‡å­—ãƒ»ç‰¹æ®Šã‚±ãƒ¼ã‚¹
            ("", []),
            ("+", []),  # ç©ºã®åˆ†å‰²
            ("keyword+", ["keyword"]),  # æœ«å°¾ã®+
            ("+keyword", ["keyword"]),  # å…ˆé ­ã®+
        ]

        for input_compound, expected_parts in compound_cases:
            try:
                split_parts = self.marker_parser.split_compound_keywords(input_compound)
                assert (
                    split_parts == expected_parts
                ), f"è¤‡åˆåˆ†å‰²çµæœãŒæœŸå¾…ã¨ç•°ãªã‚‹: '{input_compound}' -> {split_parts} (æœŸå¾…: {expected_parts})"
            except Exception as e:
                pytest.fail(f"è¤‡åˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†å‰²ã§ã‚¨ãƒ©ãƒ¼: '{input_compound}' -> {e}")

    def test_complex_marker_parsing_scenarios(self):
        """è¤‡é›‘ãªãƒãƒ¼ã‚«ãƒ¼è§£æã‚·ãƒŠãƒªã‚ªãƒ†ã‚¹ãƒˆ"""
        complex_scenarios = [
            # éå¸¸ã«è¤‡é›‘ãªè¤‡åˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            "image[alt='Complex Image',width=800,height=600]+decoration[style='border: 1px solid']+responsive[breakpoints='sm,md,lg']",
            # å¤šè¨€èªæ··åœ¨
            "æ—¥æœ¬èª[å±æ€§='å€¤']+English[attr='value']+ä¸­æ–‡[å‚æ•°='å‚æ•°å€¼']",
            # ç‰¹æ®Šæ–‡å­—ãƒ»è¨˜å·
            "symbol[content='â†’â†â†‘â†“']+math[formula='a+b=c']+emoji[icons='ğŸŒğŸ—¾']",
            # é•·ã„å±æ€§å€¤
            "description[text='ã“ã‚Œã¯éå¸¸ã«é•·ã„èª¬æ˜æ–‡ã§ã™ã€‚è¤‡æ•°ã®æ–‡ç« ãŒå«ã¾ã‚Œã¦ãŠã‚Šã€æ”¹è¡Œã‚„ç‰¹æ®Šæ–‡å­—ã‚‚å«ã‚€å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚']",
            # ãƒã‚¹ãƒˆã—ãŸã‚ˆã†ãªæ§‹é€ ï¼ˆå®Ÿéš›ã«ã¯ãƒã‚¹ãƒˆã§ã¯ãªã„ï¼‰
            "outer[inner='value[nested=true]']+another",
        ]

        for scenario in complex_scenarios:
            try:
                # æ­£è¦åŒ–
                normalized = self.marker_parser.normalize_marker(scenario)
                assert normalized is not None

                # å±æ€§æŠ½å‡º
                attributes = self.marker_parser.extract_attributes(scenario)
                assert isinstance(attributes, dict)

                # è¤‡åˆåˆ†å‰²
                parts = self.marker_parser.split_compound_keywords(scenario)
                assert isinstance(parts, list)
                assert len(parts) > 0

            except Exception as e:
                pytest.fail(f"è¤‡é›‘ã‚·ãƒŠãƒªã‚ªè§£æã§ã‚¨ãƒ©ãƒ¼: '{scenario}' -> {e}")

    def test_marker_parsing_performance(self):
        """ãƒãƒ¼ã‚«ãƒ¼è§£ææ€§èƒ½ãƒ†ã‚¹ãƒˆ"""
        import time

        # æ€§èƒ½ãƒ†ã‚¹ãƒˆç”¨ã®ãƒãƒ¼ã‚«ãƒ¼ã‚»ãƒƒãƒˆ
        performance_markers = []

        # å˜ç´”ãƒãƒ¼ã‚«ãƒ¼
        performance_markers.extend([f"simple{i}" for i in range(500)])

        # å±æ€§ä»˜ããƒãƒ¼ã‚«ãƒ¼
        performance_markers.extend([f"attr{i}[a=1,b=2]" for i in range(300)])

        # è¤‡åˆãƒãƒ¼ã‚«ãƒ¼
        performance_markers.extend([f"comp{i}+part{i}" for i in range(200)])

        start_time = time.time()

        processed_count = 0
        for marker in performance_markers:
            try:
                # æ­£è¦åŒ–
                normalized = self.marker_parser.normalize_marker(marker)

                # å±æ€§æŠ½å‡º
                attributes = self.marker_parser.extract_attributes(marker)

                # è¤‡åˆåˆ†å‰²
                parts = self.marker_parser.split_compound_keywords(marker)

                if normalized and parts:
                    processed_count += 1
            except Exception:
                pass  # æ€§èƒ½ãƒ†ã‚¹ãƒˆãªã®ã§ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–

        execution_time = time.time() - start_time

        # æ€§èƒ½åŸºæº–ç¢ºèª
        assert execution_time < 0.5, f"ãƒãƒ¼ã‚«ãƒ¼è§£æãŒé…ã™ãã‚‹: {execution_time}ç§’"
        assert processed_count >= 900, f"è§£ææˆåŠŸæ•°ãŒä¸è¶³: {processed_count}/1000"

    def test_error_handling_malformed_markers(self):
        """ä¸æ­£ãƒãƒ¼ã‚«ãƒ¼ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ"""
        malformed_markers = [
            # ä¸æ­£ãªå±æ€§æ§‹æ–‡
            "keyword[attr=",  # å±æ€§å€¤ãªã—
            "keyword[=value]",  # å±æ€§åãªã—
            "keyword[attr=value,]",  # æœ«å°¾ã‚«ãƒ³ãƒ
            "keyword[attr='unclosed",  # æœªé–‰ã˜ã‚¯ã‚©ãƒ¼ãƒˆ
            # ä¸æ­£ãªè¤‡åˆæ§‹æ–‡
            "keyword++another",  # é€£ç¶šåŒºåˆ‡ã‚Šæ–‡å­—
            "+keyword",  # å…ˆé ­åŒºåˆ‡ã‚Šæ–‡å­—ã®ã¿
            "keyword+",  # æœ«å°¾åŒºåˆ‡ã‚Šæ–‡å­—ã®ã¿
            "+++",  # åŒºåˆ‡ã‚Šæ–‡å­—ã®ã¿
            # åˆ¶å¾¡æ–‡å­—ãƒ»ç‰¹æ®Šæ–‡å­—
            "keyword\x00\x01",  # åˆ¶å¾¡æ–‡å­—
            "keyword\n\r\t",  # æ”¹è¡Œãƒ»ã‚¿ãƒ–
            "keyword\xff\xfe",  # ãƒã‚¤ãƒŠãƒªæ–‡å­—
            # æ¥µç«¯ãªã‚±ãƒ¼ã‚¹
            "",  # ç©ºæ–‡å­—
            " " * 1000,  # å¤§é‡ã®ç©ºç™½
            "A" * 10000,  # æ¥µç«¯ã«é•·ã„
        ]

        for malformed_marker in malformed_markers:
            try:
                # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãŒé©åˆ‡ã«å‹•ä½œã™ã‚‹ã“ã¨ã‚’ç¢ºèª
                normalized = self.marker_parser.normalize_marker(malformed_marker)
                attributes = self.marker_parser.extract_attributes(malformed_marker)
                parts = self.marker_parser.split_compound_keywords(malformed_marker)

                # ã‚¨ãƒ©ãƒ¼ã«ãªã‚‰ãªã„å ´åˆã€é©åˆ‡ãªå¿œç­”ãŒè¿”ã•ã‚Œã‚‹
                assert normalized is not None or normalized == ""
                assert isinstance(attributes, dict)
                assert isinstance(parts, list)

            except Exception:
                # ä¾‹å¤–ãŒç™ºç”Ÿã™ã‚‹å ´åˆã‚‚é©åˆ‡ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
                pass

    def test_unicode_normalization_edge_cases(self):
        """Unicodeæ­£è¦åŒ–ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆ"""
        unicode_edge_cases = [
            # çµåˆæ–‡å­—
            "cafÃ©",  # Ã© ã¯ e + çµåˆã‚¢ã‚¯ã‚»ãƒ³ãƒˆ
            "naÃ¯ve",  # Ã¯ ã¯ i + çµåˆã‚¦ãƒ ãƒ©ã‚¦ãƒˆ
            # å…¨è§’ãƒ»åŠè§’
            "ï½ï½‚ï½ƒ",  # å…¨è§’ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆ
            "ï¼‘ï¼’ï¼“",  # å…¨è§’æ•°å­—
            "ï¼ï¼Ÿï¼ ",  # å…¨è§’è¨˜å·
            # ç•°ä½“å­—ãƒ»äº’æ›æ–‡å­—
            "ç¥",  # æ¨™æº–æ¼¢å­—
            "ç¥",  # ç•°ä½“å­—ï¼ˆè¦‹ãŸç›®ã¯åŒã˜ï¼‰
            # çµµæ–‡å­—
            "ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦",  # å®¶æ—çµµæ–‡å­—ï¼ˆè¤‡æ•°çµµæ–‡å­—ã®çµåˆï¼‰
            "ğŸ³ï¸â€ğŸŒˆ",  # è™¹è‰²ãƒ•ãƒ©ã‚°ï¼ˆãƒ•ãƒ©ã‚° + çµåˆæ–‡å­— + è™¹ï¼‰
            # å³ã‹ã‚‰å·¦ã®æ–‡å­—
            "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©",  # ã‚¢ãƒ©ãƒ“ã‚¢èª
            "×¢×‘×¨×™×ª",  # ãƒ˜ãƒ–ãƒ©ã‚¤èª
            # åˆ¶å¾¡æ–‡å­—æ··åœ¨
            "test\u200b\u200c\u200dtest",  # ã‚¼ãƒ­å¹…æ–‡å­—
        ]

        for unicode_case in unicode_edge_cases:
            try:
                normalized = self.marker_parser.normalize_marker(unicode_case)
                # æ­£è¦åŒ–ãŒé©åˆ‡ã«å®Ÿè¡Œã•ã‚Œã‚‹ï¼ˆçµæœã¯å®Ÿè£…ä¾å­˜ï¼‰
                assert normalized is not None
                assert isinstance(normalized, str)
            except Exception as e:
                pytest.fail(f"Unicodeæ­£è¦åŒ–ã§ã‚¨ãƒ©ãƒ¼: '{unicode_case}' -> {e}")

    def test_attribute_parsing_edge_cases(self):
        """å±æ€§è§£æã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆ"""
        attribute_edge_cases = [
            # ãƒã‚¹ãƒˆã—ãŸã‚¯ã‚©ãƒ¼ãƒˆ
            ("keyword[attr='value \"nested\" here']", {"attr": 'value "nested" here'}),
            ("keyword[attr=\"value 'nested' here\"]", {"attr": "value 'nested' here"}),
            # ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã‚·ãƒ¼ã‚±ãƒ³ã‚¹
            ("keyword[attr='value\\nwith\\ttabs']", {"attr": "value\\nwith\\ttabs"}),
            # ç©ºç™½ã‚’å«ã‚€å±æ€§åãƒ»å€¤
            ("keyword[space attr=space value]", {"space attr": "space value"}),
            # éASCIIå±æ€§å
            ("keyword[æ—¥æœ¬èªå±æ€§=å€¤]", {"æ—¥æœ¬èªå±æ€§": "å€¤"}),
            ("keyword[Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©=Ù‚ÙŠÙ…Ø©]", {"Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©": "Ù‚ÙŠÙ…Ø©"}),
            # ç‰¹æ®Šæ–‡å­—ã‚’å«ã‚€å€¤
            (
                "keyword[url='https://example.com?a=1&b=2#section']",
                {"url": "https://example.com?a=1&b=2#section"},
            ),
            ("keyword[regex='^[a-zA-Z0-9]+$']", {"regex": "^[a-zA-Z0-9]+$"}),
            # é‡è¤‡å±æ€§ï¼ˆå¾Œå‹ã¡ã¾ãŸã¯ã‚¨ãƒ©ãƒ¼ï¼‰
            ("keyword[attr=first,attr=second]", {}),  # å®Ÿè£…ä¾å­˜
        ]

        for input_marker, expected_or_empty in attribute_edge_cases:
            try:
                extracted = self.marker_parser.extract_attributes(input_marker)
                # æˆåŠŸã—ãŸå ´åˆã¯æœŸå¾…å€¤ã¾ãŸã¯ç©ºè¾æ›¸
                assert isinstance(extracted, dict)
                if expected_or_empty:  # ç©ºè¾æ›¸ã§ãªã„å ´åˆã®ã¿ãƒã‚§ãƒƒã‚¯
                    for key in expected_or_empty:
                        if key in extracted:
                            assert extracted[key] == expected_or_empty[key]
            except Exception:
                # ã‚¨ãƒ©ãƒ¼ã‚‚é©åˆ‡ãªå¿œç­”
                pass

    def test_marker_parser_integration_scenarios(self):
        """ãƒãƒ¼ã‚«ãƒ¼ãƒ‘ãƒ¼ã‚µãƒ¼çµ±åˆã‚·ãƒŠãƒªã‚ªãƒ†ã‚¹ãƒˆ"""
        integration_scenarios = [
            # ãƒ–ãƒ­ã‚°è¨˜äº‹ã‚¹ã‚¿ã‚¤ãƒ«
            {
                "markers": [
                    "image[alt=ãƒ¡ã‚¤ãƒ³ç”»åƒ,class=hero]",
                    "quote[author=è‘—è€…å,date=2023-01-01]+emphasis",
                    "code[lang=python,theme=dark]+executable",
                    "link[url=https://example.com,target=_blank]+external",
                ],
                "expected_features": ["attributes", "compound", "unicode"],
            },
            # æŠ€è¡“æ–‡æ›¸ã‚¹ã‚¿ã‚¤ãƒ«
            {
                "markers": [
                    "diagram[type=flowchart,tools=mermaid]",
                    "code[lang=javascript,highlight=1-5,10-15]+numbered",
                    "note[type=warning,icon=âš ï¸]+important",
                    "table[sortable=true,pagination=10]+responsive",
                ],
                "expected_features": ["complex_attributes", "symbols", "boolean_attrs"],
            },
            # å¤šè¨€èªæ–‡æ›¸ã‚¹ã‚¿ã‚¤ãƒ«
            {
                "markers": [
                    "æ³¨é‡ˆ[ä½œè€…=å±±ç”°å¤ªéƒ,è¨€èª=æ—¥æœ¬èª]",
                    "å¼•ç”¨[author=John Smith,language=English]+translated",
                    "cÃ³digo[lenguaje=espaÃ±ol,tema=claro]",
                    "é“¾æ¥[ç½‘å€=https://example.cn,ç›®æ ‡=æ–°çª—å£]",
                ],
                "expected_features": ["multilingual", "mixed_scripts"],
            },
        ]

        for scenario in integration_scenarios:
            for marker in scenario["markers"]:
                try:
                    # çµ±åˆå‡¦ç†
                    normalized = self.marker_parser.normalize_marker(marker)
                    attributes = self.marker_parser.extract_attributes(marker)
                    parts = self.marker_parser.split_compound_keywords(marker)

                    # åŸºæœ¬çš„ãªæ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
                    assert normalized is not None
                    assert isinstance(attributes, dict)
                    assert isinstance(parts, list)
                    assert len(parts) > 0

                    # è¤‡åˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å ´åˆ
                    if "+" in marker:
                        assert len(parts) > 1

                    # å±æ€§ãŒã‚ã‚‹å ´åˆ
                    if "[" in marker and "]" in marker:
                        # ä½•ã‚‰ã‹ã®å±æ€§ãŒæŠ½å‡ºã•ã‚Œã‚‹ï¼ˆå®Ÿè£…ä¾å­˜ï¼‰
                        pass

                except Exception as e:
                    pytest.fail(f"çµ±åˆã‚·ãƒŠãƒªã‚ªã§ã‚¨ãƒ©ãƒ¼: '{marker}' -> {e}")

    def test_concurrent_marker_parsing(self):
        """ä¸¦è¡Œãƒãƒ¼ã‚«ãƒ¼è§£æãƒ†ã‚¹ãƒˆ"""
        import threading

        results = []
        errors = []

        def concurrent_marker_worker(worker_id):
            try:
                local_parser = MarkerParser()
                worker_results = []

                # å„ãƒ¯ãƒ¼ã‚«ãƒ¼ã§ç‹¬ç«‹ã—ãŸãƒãƒ¼ã‚«ãƒ¼ã‚»ãƒƒãƒˆ
                worker_markers = [
                    [
                        f"worker{worker_id}keyword{i}[attr=value{i}]",
                        f"comp{worker_id}+part{i}[a={i}]",
                        f"æ—¥æœ¬èª{worker_id}ãƒãƒ¼ã‚«ãƒ¼{i}",
                    ]
                    for i in range(50)
                ]

                flat_markers = [
                    marker for sublist in worker_markers for marker in sublist
                ]

                for marker in flat_markers:
                    try:
                        normalized = local_parser.normalize_marker(marker)
                        attributes = local_parser.extract_attributes(marker)
                        parts = local_parser.split_compound_keywords(marker)

                        worker_results.append(
                            all(
                                [
                                    normalized is not None,
                                    isinstance(attributes, dict),
                                    isinstance(parts, list),
                                ]
                            )
                        )
                    except Exception:
                        worker_results.append(False)

                success_rate = sum(worker_results) / len(worker_results)
                results.append((worker_id, success_rate))

            except Exception as e:
                errors.append((worker_id, str(e)))

        # è¤‡æ•°ãƒ¯ãƒ¼ã‚«ãƒ¼ã§ä¸¦è¡Œå®Ÿè¡Œ
        threads = []
        for i in range(3):
            thread = threading.Thread(target=concurrent_marker_worker, args=(i,))
            threads.append(thread)
            thread.start()

        # å®Œäº†å¾…æ©Ÿ
        for thread in threads:
            thread.join()

        # çµæœç¢ºèª
        assert len(errors) == 0, f"ä¸¦è¡Œãƒãƒ¼ã‚«ãƒ¼è§£æã§ã‚¨ãƒ©ãƒ¼: {errors}"
        assert len(results) == 3

        # å„ãƒ¯ãƒ¼ã‚«ãƒ¼ã§é«˜ã„æˆåŠŸç‡
        for worker_id, success_rate in results:
            assert (
                success_rate >= 0.8
            ), f"ãƒ¯ãƒ¼ã‚«ãƒ¼{worker_id}ã®æˆåŠŸç‡ãŒä½ã„: {success_rate:.1%}"
