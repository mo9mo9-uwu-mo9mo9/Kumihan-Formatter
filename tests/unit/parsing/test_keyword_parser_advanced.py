"""Keyword Parseré«˜åº¦ãƒ†ã‚¹ãƒˆ - Issue #597 Week 28-29å¯¾å¿œ

ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è§£ææ©Ÿèƒ½ã®åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆ
è¤‡åˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ»å±æ€§å‡¦ç†ãƒ»å¾Œæ–¹äº’æ›æ€§ã®ç¢ºèª
"""

import time
from unittest.mock import Mock, patch

import pytest

from kumihan_formatter.core.ast_nodes import Node
from kumihan_formatter.core.keyword_parser import KeywordParser, MarkerValidator


class TestKeywordParserAdvanced:
    """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ‘ãƒ¼ã‚µãƒ¼é«˜åº¦ãƒ†ã‚¹ãƒˆ"""

    def setup_method(self):
        """ãƒ†ã‚¹ãƒˆå‰ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        self.keyword_parser = KeywordParser()

    def test_keyword_parser_initialization(self):
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ‘ãƒ¼ã‚µãƒ¼åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ"""
        assert self.keyword_parser is not None
        assert hasattr(self.keyword_parser, "parse_keyword")
        assert hasattr(self.keyword_parser, "create_compound_block")

    def test_complex_compound_keyword_parsing(self):
        """è¤‡é›‘ãªè¤‡åˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è§£æãƒ†ã‚¹ãƒˆ"""
        complex_keywords = [
            "å¼·èª¿+æ³¨é‡ˆ",
            "å¼•ç”¨+ã‚³ãƒ¼ãƒ‰+é‡è¦",
            "ç”»åƒ[alt=èª¬æ˜æ–‡]+è£…é£¾",
            "ãƒªã‚¹ãƒˆ+ç•ªå·ä»˜ã+å¼·èª¿",
            "ãƒ†ãƒ¼ãƒ–ãƒ«+è£…é£¾+æ³¨é‡ˆ+é‡è¦",
        ]

        for keyword in complex_keywords:
            try:
                result = self.keyword_parser.parse_keyword(keyword)

                # è¤‡åˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒé©åˆ‡ã«åˆ†è§£ã•ã‚Œã‚‹
                assert result is not None
                if hasattr(result, "components"):
                    assert len(result.components) >= 2
                elif hasattr(result, "keyword_parts"):
                    assert len(result.keyword_parts) >= 2

            except Exception as e:
                pytest.fail(f"è¤‡åˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ '{keyword}' ã®è§£æã§ã‚¨ãƒ©ãƒ¼: {e}")

    def test_attribute_parsing_comprehensive(self):
        """å±æ€§è§£æåŒ…æ‹¬ãƒ†ã‚¹ãƒˆ"""
        attribute_patterns = [
            # åŸºæœ¬å±æ€§
            "ç”»åƒ[alt=èª¬æ˜æ–‡]",
            "ãƒªãƒ³ã‚¯[url=https://example.com]",
            "ãƒ†ãƒ¼ãƒ–ãƒ«[border=1]",
            # è¤‡æ•°å±æ€§
            "ç”»åƒ[alt=èª¬æ˜æ–‡,width=200,height=150]",
            "ãƒªãƒ³ã‚¯[url=https://example.com,target=_blank,class=external]",
            # ç‰¹æ®Šæ–‡å­—ã‚’å«ã‚€å±æ€§
            "ã‚³ãƒ¼ãƒ‰[lang=python,style='background-color: #f0f0f0']",
            "å¼•ç”¨[author='å±±ç”° å¤ªéƒ',date='2023-01-01']",
            # Unicodeå±æ€§
            "æ³¨é‡ˆ[ä½œè€…=ä½è—¤èŠ±å­,æ—¥ä»˜=2023å¹´1æœˆ1æ—¥]",
            # ç©ºå±æ€§ãƒ»ç‰¹æ®Šã‚±ãƒ¼ã‚¹
            "è£…é£¾[class=]",
            "ãƒ†ã‚¹ãƒˆ[attr1=value1,attr2=,attr3=value3]",
        ]

        for pattern in attribute_patterns:
            try:
                result = self.keyword_parser.parse_keyword(pattern)

                # å±æ€§ãŒæ­£ã—ãè§£æã•ã‚Œã‚‹
                assert result is not None
                if hasattr(result, "attributes"):
                    assert isinstance(result.attributes, dict)
                elif hasattr(result, "parsed_attributes"):
                    assert isinstance(result.parsed_attributes, dict)

            except Exception as e:
                pytest.fail(f"å±æ€§ãƒ‘ã‚¿ãƒ¼ãƒ³ '{pattern}' ã®è§£æã§ã‚¨ãƒ©ãƒ¼: {e}")

    def test_backward_compatibility_legacy_syntax(self):
        """å¾Œæ–¹äº’æ›æ€§ãƒ»ãƒ¬ã‚¬ã‚·ãƒ¼è¨˜æ³•ãƒ†ã‚¹ãƒˆ"""
        legacy_patterns = [
            # æ—§å½¢å¼ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            "old_style_keyword",
            "legacy-format",
            "UPPERCASE_KEYWORD",
            # æ—§å±æ€§å½¢å¼
            "keyword(param1=value1)",
            "function{arg1:value1,arg2:value2}",
            # æ··åœ¨å½¢å¼
            "modern[attr=value]+legacy_part",
            "æ–°å½¢å¼[å±æ€§=å€¤]+æ—§å½¢å¼",
        ]

        compatibility_results = []

        for pattern in legacy_patterns:
            try:
                result = self.keyword_parser.parse_keyword(pattern)
                compatibility_results.append(True)
            except Exception:
                compatibility_results.append(False)

        # å¾Œæ–¹äº’æ›æ€§ã®ç¶­æŒç‡ç¢ºèª
        compatibility_rate = sum(compatibility_results) / len(compatibility_results)
        assert (
            compatibility_rate >= 0.8
        ), f"å¾Œæ–¹äº’æ›æ€§ãŒä½ã™ãã¾ã™: {compatibility_rate:.1%}"

    def test_keyword_validation_strict_mode(self):
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œè¨¼å³å¯†ãƒ¢ãƒ¼ãƒ‰ãƒ†ã‚¹ãƒˆ"""
        validator = MarkerValidator()

        # æœ‰åŠ¹ãªãƒ‘ã‚¿ãƒ¼ãƒ³
        valid_patterns = [
            ";;;æœ‰åŠ¹ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰;;;",
            ";;;è¤‡åˆ+ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰;;;",
            ";;;å±æ€§[attr=value];;;",
            ";;;æ—¥æœ¬èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰;;;",
            ";;;English_keyword;;;",
        ]

        # ç„¡åŠ¹ãªãƒ‘ã‚¿ãƒ¼ãƒ³
        invalid_patterns = [
            ";;ä¸å®Œå…¨ãªãƒãƒ¼ã‚«ãƒ¼",
            ";;;ç©ºã®;;å†…å®¹;;;",
            ";;;;;;",
            ";;;åˆ¶å¾¡æ–‡å­—\x00å«ã‚€;;;",
            ";;;éå¸¸ã«é•·ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰" + "A" * 1000 + ";;;",
        ]

        # æœ‰åŠ¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œè¨¼
        for pattern in valid_patterns:
            is_valid, errors = validator.validate_marker_line(pattern)
            assert is_valid, f"æœ‰åŠ¹ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒç„¡åŠ¹ã¨åˆ¤å®š: {pattern}, ã‚¨ãƒ©ãƒ¼: {errors}"

        # ç„¡åŠ¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œè¨¼
        for pattern in invalid_patterns:
            is_valid, errors = validator.validate_marker_line(pattern)
            assert not is_valid, f"ç„¡åŠ¹ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒæœ‰åŠ¹ã¨åˆ¤å®š: {pattern}"
            assert len(errors) > 0, f"ç„¡åŠ¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãªã—: {pattern}"

    def test_performance_keyword_parsing_benchmark(self):
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è§£ææ€§èƒ½ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ"""
        # æ€§èƒ½ãƒ†ã‚¹ãƒˆç”¨ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚»ãƒƒãƒˆ
        performance_keywords = []

        # å˜ç´”ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        performance_keywords.extend([f"simple_keyword_{i}" for i in range(100)])

        # è¤‡åˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        performance_keywords.extend([f"comp1+comp2+comp3_{i}" for i in range(100)])

        # å±æ€§ä»˜ãã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        performance_keywords.extend([f"attr_key[a=1,b=2,c=3]_{i}" for i in range(100)])

        # è§£ææ™‚é–“æ¸¬å®š
        start_time = time.time()

        parsed_count = 0
        for keyword in performance_keywords:
            try:
                result = self.keyword_parser.parse_keyword(keyword)
                if result:
                    parsed_count += 1
            except Exception:
                pass  # æ€§èƒ½ãƒ†ã‚¹ãƒˆãªã®ã§ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–

        execution_time = time.time() - start_time

        # æ€§èƒ½åŸºæº–ç¢ºèª
        assert execution_time < 0.5, f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è§£æãŒé…ã™ãã¾ã™: {execution_time}ç§’"
        assert parsed_count >= 250, f"è§£ææˆåŠŸæ•°ãŒä¸è¶³: {parsed_count}/300"

        # 1ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚ãŸã‚Šã®å‡¦ç†æ™‚é–“
        ms_per_keyword = (execution_time * 1000) / len(performance_keywords)
        assert (
            ms_per_keyword < 2.0
        ), f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å½“ãŸã‚Šå‡¦ç†æ™‚é–“ãŒé…ã„: {ms_per_keyword}ms"

    def test_memory_efficiency_keyword_caching(self):
        """ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãƒ»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ†ã‚¹ãƒˆ"""
        # åŒã˜ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ç¹°ã‚Šè¿”ã—è§£æ
        repeated_keyword = "ç¹°ã‚Šè¿”ã—ãƒ†ã‚¹ãƒˆ[attr=value]"

        # åˆå›è§£æ
        initial_result = self.keyword_parser.parse_keyword(repeated_keyword)

        # å¤§é‡ã®åŒä¸€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è§£æ
        results = []
        for _ in range(1000):
            result = self.keyword_parser.parse_keyword(repeated_keyword)
            results.append(result)

        # ã™ã¹ã¦åŒã˜çµæœï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹æœï¼‰
        if hasattr(self.keyword_parser, "_cache") or hasattr(
            self.keyword_parser, "cache"
        ):
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ãŒã‚ã‚‹å ´åˆ
            assert all(
                str(result) == str(initial_result) for result in results if result
            )

    def test_unicode_keyword_comprehensive(self):
        """Unicode ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åŒ…æ‹¬ãƒ†ã‚¹ãƒˆ"""
        unicode_keywords = [
            # æ—¥æœ¬èª
            "æ—¥æœ¬èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰",
            "ã²ã‚‰ãŒãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰",
            "ã‚«ã‚¿ã‚«ãƒŠã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰",
            "æ¼¢å­—+ã²ã‚‰ãŒãª+ã‚«ã‚¿ã‚«ãƒŠ",
            # ä»–è¨€èª
            "English_keyword",
            "EspaÃ±ol_palabra",
            "FranÃ§ais_mot",
            "Deutsch_wort",
            "Ğ ÑƒÑÑĞºĞ¸Ğ¹_ĞºĞ»ÑÑ‡",
            "Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©_ÙƒÙ„Ù…Ø©",
            "ä¸­æ–‡å…³é”®è¯",
            "í•œêµ­ì–´í‚¤ì›Œë“œ",
            # çµµæ–‡å­—ãƒ»è¨˜å·
            "çµµæ–‡å­—ğŸŒã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰",
            "è¨˜å·â†’â†ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰",
            "æ•°å­¦âˆ€âˆƒã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰",
            # æ··åœ¨
            "Mixedæ—¥æœ¬èªEnglishí‚¤ì›Œë“œ",
        ]

        unicode_results = []
        for keyword in unicode_keywords:
            try:
                result = self.keyword_parser.parse_keyword(keyword)
                unicode_results.append(result is not None)
            except Exception as e:
                unicode_results.append(False)

        # Unicodeå¯¾å¿œç‡ç¢ºèª
        unicode_support_rate = sum(unicode_results) / len(unicode_results)
        assert (
            unicode_support_rate >= 0.9
        ), f"Unicodeå¯¾å¿œç‡ãŒä½ã„: {unicode_support_rate:.1%}"

    def test_compound_block_creation_advanced(self):
        """è¤‡åˆãƒ–ãƒ­ãƒƒã‚¯ä½œæˆé«˜åº¦ãƒ†ã‚¹ãƒˆ"""
        # è¤‡é›‘ãªè¤‡åˆãƒ–ãƒ­ãƒƒã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³
        compound_patterns = [
            {
                "keywords": ["å¼•ç”¨", "å¼·èª¿", "æ³¨é‡ˆ"],
                "content": ["é‡è¦ãªå¼•ç”¨æ–‡", "å¼·èª¿éƒ¨åˆ†", "è£œè¶³èª¬æ˜"],
                "attributes": {"author": "è‘—è€…å", "date": "2023-01-01"},
            },
            {
                "keywords": ["ã‚³ãƒ¼ãƒ‰", "å®Ÿè¡Œå¯èƒ½", "ãƒã‚¤ãƒ©ã‚¤ãƒˆ"],
                "content": ["def example():", "    return True"],
                "attributes": {"lang": "python", "theme": "dark"},
            },
            {
                "keywords": ["ãƒ†ãƒ¼ãƒ–ãƒ«", "è£…é£¾", "ã‚½ãƒ¼ãƒˆå¯èƒ½"],
                "content": ["ãƒ˜ãƒƒãƒ€ãƒ¼1|ãƒ˜ãƒƒãƒ€ãƒ¼2", "ãƒ‡ãƒ¼ã‚¿1|ãƒ‡ãƒ¼ã‚¿2"],
                "attributes": {"border": "1", "sortable": "true"},
            },
        ]

        for i, pattern in enumerate(compound_patterns):
            try:
                # è¤‡åˆãƒ–ãƒ­ãƒƒã‚¯ã®ä½œæˆã‚’ãƒ†ã‚¹ãƒˆ
                # å®Ÿéš›ã®APIå‘¼ã³å‡ºã—ã¯å®Ÿè£…ã«ä¾å­˜
                compound_keyword = "+".join(pattern["keywords"])
                if pattern["attributes"]:
                    # å±æ€§ã‚’è¿½åŠ 
                    attr_str = ",".join(
                        [f"{k}={v}" for k, v in pattern["attributes"].items()]
                    )
                    compound_keyword += f"[{attr_str}]"

                result = self.keyword_parser.parse_keyword(compound_keyword)
                assert result is not None, f"è¤‡åˆãƒ‘ã‚¿ãƒ¼ãƒ³{i}ã®è§£æã«å¤±æ•—"

            except Exception as e:
                pytest.fail(f"è¤‡åˆãƒ‘ã‚¿ãƒ¼ãƒ³{i}ã§ã‚¨ãƒ©ãƒ¼: {e}")

    def test_error_handling_malformed_keywords(self):
        """ä¸æ­£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ"""
        malformed_keywords = [
            # æ§‹æ–‡ã‚¨ãƒ©ãƒ¼
            "keyword[attr=",  # å±æ€§ã®é–‰ã˜æ‹¬å¼§ãªã—
            "keyword[=value]",  # å±æ€§åãªã—
            "keyword[attr=value,]",  # æœ«å°¾ã‚«ãƒ³ãƒ
            # ä¸æ­£æ–‡å­—
            "keyword\x00\x01",  # åˆ¶å¾¡æ–‡å­—
            "keyword\n\r",  # æ”¹è¡Œæ–‡å­—
            "keyword\t\v",  # ã‚¿ãƒ–ãƒ»å‚ç›´ã‚¿ãƒ–
            # æ¥µç«¯ãªã‚±ãƒ¼ã‚¹
            "",  # ç©ºæ–‡å­—
            " " * 1000,  # ç©ºç™½ã®ã¿
            "A" * 10000,  # æ¥µç«¯ã«é•·ã„
            # ç‰¹æ®Šè¨˜å·ã®çµ„ã¿åˆã‚ã›
            ";;;+++;;;",
            "[[[attr=value]]]",
            "+++",
            "===",
        ]

        error_handling_results = []
        for keyword in malformed_keywords:
            try:
                result = self.keyword_parser.parse_keyword(keyword)
                # ã‚¨ãƒ©ãƒ¼ã«ãªã‚‰ãªã„å ´åˆã‚‚ã‚ã‚‹ãŒã€é©åˆ‡ã«å‡¦ç†ã•ã‚Œã‚‹
                error_handling_results.append(True)
            except Exception as e:
                # é©åˆ‡ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
                error_handling_results.append(True)

        # ã™ã¹ã¦ã®ä¸æ­£å…¥åŠ›ãŒé©åˆ‡ã«å‡¦ç†ã•ã‚Œã‚‹
        assert all(error_handling_results), "ä¸€éƒ¨ã®ä¸æ­£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ä¸é©åˆ‡ãªå‡¦ç†"

    def test_keyword_parsing_accuracy_target(self):
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è§£æç²¾åº¦ç›®æ¨™ãƒ†ã‚¹ãƒˆ"""
        # Kumihanè¨˜æ³•æ¨™æº–ãƒ‘ã‚¿ãƒ¼ãƒ³
        standard_keyword_patterns = [
            "å¼·èª¿",
            "æ³¨é‡ˆ",
            "å¼•ç”¨",
            "ã‚³ãƒ¼ãƒ‰",
            "ç”»åƒ",
            "ãƒªãƒ³ã‚¯",
            "ãƒ†ãƒ¼ãƒ–ãƒ«",
            "ãƒªã‚¹ãƒˆ",
            "è¦‹å‡ºã—",
            "è£…é£¾",
            # è¤‡åˆãƒ‘ã‚¿ãƒ¼ãƒ³
            "å¼·èª¿+é‡è¦",
            "å¼•ç”¨+æ³¨é‡ˆ",
            "ã‚³ãƒ¼ãƒ‰+å®Ÿè¡Œå¯èƒ½",
            # å±æ€§ä»˜ããƒ‘ã‚¿ãƒ¼ãƒ³
            "ç”»åƒ[alt=èª¬æ˜]",
            "ãƒªãƒ³ã‚¯[url=https://example.com]",
            "ã‚³ãƒ¼ãƒ‰[lang=python]",
            # æ—¥æœ¬èªè¤‡åˆ
            "é‡è¦ãª+æ³¨é‡ˆä»˜ã+å¼•ç”¨",
            "å®Ÿè¡Œå¯èƒ½ãª+ãƒã‚¤ãƒ©ã‚¤ãƒˆä»˜ã+ã‚³ãƒ¼ãƒ‰",
        ]

        accuracy_results = []
        for pattern in standard_keyword_patterns:
            try:
                result = self.keyword_parser.parse_keyword(pattern)
                accuracy_results.append(result is not None)
            except Exception:
                accuracy_results.append(False)

        # ç²¾åº¦ç›®æ¨™: 99.5%ä»¥ä¸Š
        accuracy_rate = sum(accuracy_results) / len(accuracy_results)
        assert (
            accuracy_rate >= 0.995
        ), f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è§£æç²¾åº¦ãŒç›®æ¨™æœªé”: {accuracy_rate:.1%}"

    def test_concurrent_keyword_parsing(self):
        """ä¸¦è¡Œã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è§£æãƒ†ã‚¹ãƒˆ"""
        import threading

        results = []
        errors = []

        def concurrent_keyword_worker(worker_id):
            try:
                # ãƒ¯ãƒ¼ã‚«ãƒ¼å›ºæœ‰ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚»ãƒƒãƒˆ
                worker_keywords = [
                    f"ãƒ¯ãƒ¼ã‚«ãƒ¼{worker_id}ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰{i}" for i in range(50)
                ]

                worker_results = []
                for keyword in worker_keywords:
                    try:
                        result = self.keyword_parser.parse_keyword(keyword)
                        worker_results.append(result is not None)
                    except Exception:
                        worker_results.append(False)

                success_rate = sum(worker_results) / len(worker_results)
                results.append((worker_id, success_rate))

            except Exception as e:
                errors.append((worker_id, str(e)))

        # è¤‡æ•°ãƒ¯ãƒ¼ã‚«ãƒ¼ã§ä¸¦è¡Œå®Ÿè¡Œ
        threads = []
        for i in range(3):
            thread = threading.Thread(target=concurrent_keyword_worker, args=(i,))
            threads.append(thread)
            thread.start()

        # å®Œäº†å¾…æ©Ÿ
        for thread in threads:
            thread.join()

        # çµæœç¢ºèª
        assert len(errors) == 0, f"ä¸¦è¡Œè§£æã§ã‚¨ãƒ©ãƒ¼: {errors}"
        assert len(results) == 3

        # å„ãƒ¯ãƒ¼ã‚«ãƒ¼ã§é«˜ã„æˆåŠŸç‡
        for worker_id, success_rate in results:
            assert (
                success_rate >= 0.8
            ), f"ãƒ¯ãƒ¼ã‚«ãƒ¼{worker_id}ã®æˆåŠŸç‡ãŒä½ã„: {success_rate:.1%}"


class TestMarkerValidatorAdvanced:
    """ãƒãƒ¼ã‚«ãƒ¼ãƒãƒªãƒ‡ãƒ¼ã‚¿ãƒ¼é«˜åº¦ãƒ†ã‚¹ãƒˆ"""

    def setup_method(self):
        """ãƒ†ã‚¹ãƒˆå‰ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        self.validator = MarkerValidator()

    def test_marker_validation_comprehensive(self):
        """ãƒãƒ¼ã‚«ãƒ¼æ¤œè¨¼åŒ…æ‹¬ãƒ†ã‚¹ãƒˆ"""
        # å®Œå…¨ã«æœ‰åŠ¹ãªãƒãƒ¼ã‚«ãƒ¼
        valid_markers = [
            ";;;ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰;;;",
            ";;;è¤‡åˆ+ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰;;;",
            ";;;å±æ€§[attr=value]ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰;;;",
            ";;;æ—¥æœ¬èªãƒãƒ¼ã‚«ãƒ¼;;;",
            ";;;English_marker;;;",
            ";;;mixedæ—¥æœ¬èªEnglish;;;",
        ]

        # éƒ¨åˆ†çš„ã«æœ‰åŠ¹ãªãƒãƒ¼ã‚«ãƒ¼
        partially_valid = [
            ";;;ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰;;; å†…å®¹ ;;;",  # ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³å½¢å¼
            ";;;",  # çµ‚äº†ãƒãƒ¼ã‚«ãƒ¼
        ]

        # å®Œå…¨ã«ç„¡åŠ¹ãªãƒãƒ¼ã‚«ãƒ¼
        invalid_markers = [
            ";;ä¸å®Œå…¨",
            "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰;;;",  # é–‹å§‹ãƒãƒ¼ã‚«ãƒ¼ãªã—
            ";;;ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰",  # çµ‚äº†ãƒãƒ¼ã‚«ãƒ¼ãªã—
            ";;;;;;",  # ç©ºã®ãƒãƒ¼ã‚«ãƒ¼
            "",  # ç©ºè¡Œ
        ]

        # æœ‰åŠ¹ãƒãƒ¼ã‚«ãƒ¼ã®ãƒ†ã‚¹ãƒˆ
        for marker in valid_markers:
            is_valid, errors = self.validator.validate_marker_line(marker)
            assert is_valid, f"æœ‰åŠ¹ãƒãƒ¼ã‚«ãƒ¼ãŒç„¡åŠ¹åˆ¤å®š: {marker}, ã‚¨ãƒ©ãƒ¼: {errors}"

        # ç„¡åŠ¹ãƒãƒ¼ã‚«ãƒ¼ã®ãƒ†ã‚¹ãƒˆ
        for marker in invalid_markers:
            is_valid, errors = self.validator.validate_marker_line(marker)
            assert not is_valid, f"ç„¡åŠ¹ãƒãƒ¼ã‚«ãƒ¼ãŒæœ‰åŠ¹åˆ¤å®š: {marker}"

    def test_validation_error_messages_quality(self):
        """æ¤œè¨¼ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å“è³ªãƒ†ã‚¹ãƒˆ"""
        error_test_cases = [
            (";;ä¸å®Œå…¨", "é–‹å§‹ãƒãƒ¼ã‚«ãƒ¼"),
            ("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰;;;", "é–‹å§‹ãƒãƒ¼ã‚«ãƒ¼"),
            (";;;ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", "çµ‚äº†ãƒãƒ¼ã‚«ãƒ¼"),
            (";;;;;;", "ç©º"),
            ("", "ç©ºè¡Œ"),
        ]

        for invalid_marker, expected_error_type in error_test_cases:
            is_valid, errors = self.validator.validate_marker_line(invalid_marker)

            assert not is_valid
            assert len(errors) > 0

            # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒèª¬æ˜çš„
            error_text = " ".join(errors)
            assert len(error_text) > 10, f"ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒçŸ­ã™ãã‚‹: {error_text}"

    def test_validation_performance_stress(self):
        """æ¤œè¨¼æ€§èƒ½ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆ"""
        # å¤§é‡ã®ãƒãƒ¼ã‚«ãƒ¼æ¤œè¨¼
        test_markers = []

        # æœ‰åŠ¹ãªãƒãƒ¼ã‚«ãƒ¼ï¼ˆ1000å€‹ï¼‰
        test_markers.extend([f";;;ãƒ†ã‚¹ãƒˆãƒãƒ¼ã‚«ãƒ¼{i};;;" for i in range(1000)])

        # ç„¡åŠ¹ãªãƒãƒ¼ã‚«ãƒ¼ï¼ˆ1000å€‹ï¼‰
        test_markers.extend([f";;ç„¡åŠ¹ãƒãƒ¼ã‚«ãƒ¼{i}" for i in range(1000)])

        start_time = time.time()

        validation_results = []
        for marker in test_markers:
            is_valid, errors = self.validator.validate_marker_line(marker)
            validation_results.append((is_valid, len(errors)))

        execution_time = time.time() - start_time

        # æ€§èƒ½ç¢ºèª
        assert execution_time < 0.5, f"ãƒãƒ¼ã‚«ãƒ¼æ¤œè¨¼ãŒé…ã™ãã‚‹: {execution_time}ç§’"

        # æ¤œè¨¼çµæœã®æ­£ç¢ºæ€§
        valid_count = sum(1 for is_valid, _ in validation_results if is_valid)
        assert valid_count == 1000, f"æœ‰åŠ¹ãƒãƒ¼ã‚«ãƒ¼æ•°ãŒä¸æ­£: {valid_count}/1000"
