# Kumihan-Formatter ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ è¨­å®šã‚¬ã‚¤ãƒ‰

> **Version**: 2025å¹´ç‰ˆ Phase 4çµ±åˆå¯¾å¿œ  
> **å¯¾è±¡**: ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºãƒ¬ãƒ™ãƒ«åŒ…æ‹¬çš„ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ   
> **æ›´æ–°æ—¥**: 2025-08-19

---

## æ¦‚è¦

Kumihan-Formatterã®ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰ãƒ»é‹ç”¨ã‚¬ã‚¤ãƒ‰ã§ã™ã€‚Phase 4ã§å®Ÿè£…ã•ã‚ŒãŸã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç›£è¦–ã€æ§‹é€ åŒ–ãƒ­ã‚°ã€ç›£æŸ»æ©Ÿèƒ½ã¨çµ±åˆã—ãŸåŒ…æ‹¬çš„ãªç›£è¦–ä½“åˆ¶ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚

### ç›£è¦–ç›®æ¨™
- **å¯ç”¨æ€§**: 99.9%ä»¥ä¸Šã®ã‚µãƒ¼ãƒ“ã‚¹ç¨¼åƒç‡
- **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹**: å‡¦ç†æ™‚é–“ãƒ»ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæœ€é©åŒ–è¿½è·¡
- **ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£**: è„…å¨æ¤œçŸ¥ãƒ»ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆå¯¾å¿œ
- **ãƒ“ã‚¸ãƒã‚¹**: åˆ©ç”¨çŠ¶æ³ãƒ»å“è³ªæŒ‡æ¨™è¿½è·¡
- **é‹ç”¨åŠ¹ç‡**: è‡ªå‹•åŒ–ãƒ»æ—©æœŸæ¤œçŸ¥ãƒ»è¿…é€Ÿå¯¾å¿œ

---

## ç›£è¦–ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

### å…¨ä½“æ§‹æˆ
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Application   â”‚â”€â”€â”€â–¶â”‚   Monitoring    â”‚â”€â”€â”€â–¶â”‚   Dashboard     â”‚
â”‚   Kumihan       â”‚    â”‚   Collection    â”‚    â”‚   & Alerting    â”‚
â”‚   Formatter     â”‚    â”‚                 â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                        â”‚                        â”‚
         â–¼                        â–¼                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Logs & Traces â”‚    â”‚   Metrics Store â”‚    â”‚   Alert Manager â”‚
â”‚   ELK Stack     â”‚    â”‚   Prometheus    â”‚    â”‚   Notification  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæ§‹æˆ

#### ãƒ‡ãƒ¼ã‚¿åé›†å±¤
- **Prometheus**: ã‚·ã‚¹ãƒ†ãƒ ãƒ»ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
- **Filebeat**: ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«åé›†ãƒ»è»¢é€
- **APM Agent**: ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³æ€§èƒ½ç›£è¦–
- **Node Exporter**: ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–

#### ãƒ‡ãƒ¼ã‚¿ä¿å­˜å±¤
- **Prometheus TSDB**: æ™‚ç³»åˆ—ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¿å­˜
- **Elasticsearch**: ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ä¿å­˜ãƒ»æ¤œç´¢
- **InfluxDB**: é«˜é »åº¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¿å­˜ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

#### å¯è¦–åŒ–ãƒ»åˆ†æå±¤
- **Grafana**: ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ»å¯è¦–åŒ–
- **Kibana**: ãƒ­ã‚°åˆ†æãƒ»æ¤œç´¢
- **AlertManager**: ã‚¢ãƒ©ãƒ¼ãƒˆç®¡ç†ãƒ»é€šçŸ¥

---

## åŸºæœ¬ç›£è¦–è¨­å®š

### 1. Prometheusã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

#### prometheus.yml
```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s
  external_labels:
    cluster: 'kumihan-formatter'
    environment: 'production'

rule_files:
  - "rules/*.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093

scrape_configs:
  # Kumihan-Formatter Application
  - job_name: 'kumihan-formatter'
    static_configs:
      - targets: ['localhost:8000']
    metrics_path: '/metrics'
    scrape_interval: 10s
    scrape_timeout: 5s

  # System Monitoring
  - job_name: 'node-exporter'
    static_configs:
      - targets: ['localhost:9100']

  # Process Monitoring
  - job_name: 'process-exporter'
    static_configs:
      - targets: ['localhost:9256']

  # Docker Monitoring
  - job_name: 'cadvisor'
    static_configs:
      - targets: ['localhost:8080']
```

#### ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨­å®š
```python
# kumihan_formatter/core/monitoring/metrics.py
from prometheus_client import Counter, Histogram, Gauge, start_http_server
import time
from functools import wraps

# ãƒ¡ãƒˆãƒªã‚¯ã‚¹å®šç¾©
REQUEST_COUNT = Counter(
    'kumihan_formatter_requests_total',
    'Total requests',
    ['method', 'endpoint', 'status']
)

REQUEST_DURATION = Histogram(
    'kumihan_formatter_request_duration_seconds',
    'Request duration',
    ['method', 'endpoint']
)

PROCESSING_TIME = Histogram(
    'kumihan_formatter_processing_duration_seconds',
    'Processing duration',
    ['operation', 'format']
)

ACTIVE_CONNECTIONS = Gauge(
    'kumihan_formatter_active_connections',
    'Active connections'
)

ERROR_RATE = Counter(
    'kumihan_formatter_errors_total',
    'Total errors',
    ['error_type', 'component']
)

def track_metrics(operation_type):
    """ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¿½è·¡ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            start_time = time.time()
            try:
                result = func(*args, **kwargs)
                REQUEST_COUNT.labels(
                    method='POST',
                    endpoint=f'/{operation_type}',
                    status='200'
                ).inc()
                return result
            except Exception as e:
                ERROR_RATE.labels(
                    error_type=type(e).__name__,
                    component=operation_type
                ).inc()
                REQUEST_COUNT.labels(
                    method='POST',
                    endpoint=f'/{operation_type}',
                    status='500'
                ).inc()
                raise
            finally:
                PROCESSING_TIME.labels(
                    operation=operation_type,
                    format='kumihan'
                ).observe(time.time() - start_time)
        return wrapper
    return decorator
```

### 2. Docker Composeçµ±åˆ
```yaml
# docker-compose.monitoring.yml
version: '3.8'

services:
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus:/etc/prometheus
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin123
      - GF_USERS_ALLOW_SIGN_UP=false

  alertmanager:
    image: prom/alertmanager:latest
    container_name: alertmanager
    ports:
      - "9093:9093"
    volumes:
      - ./monitoring/alertmanager:/etc/alertmanager

  node-exporter:
    image: prom/node-exporter:latest
    container_name: node-exporter
    ports:
      - "9100:9100"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro

volumes:
  prometheus_data:
  grafana_data:
```

---

## Phase 4 çµ±åˆç›£è¦–

### 1. æ§‹é€ åŒ–ãƒ­ã‚°ç›£è¦–

#### ELK Stackè¨­å®š
```yaml
# docker-compose.elk.yml
version: '3.8'

services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.8.0
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
    ports:
      - "9200:9200"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data

  logstash:
    image: docker.elastic.co/logstash/logstash:8.8.0
    container_name: logstash
    ports:
      - "5044:5044"
    volumes:
      - ./monitoring/logstash/pipeline:/usr/share/logstash/pipeline
      - ./monitoring/logstash/config:/usr/share/logstash/config

  kibana:
    image: docker.elastic.co/kibana/kibana:8.8.0
    container_name: kibana
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200

volumes:
  elasticsearch_data:
```

#### Logstashè¨­å®š
```ruby
# monitoring/logstash/pipeline/logstash.conf
input {
  beats {
    port => 5044
  }
  
  # Phase 4 æ§‹é€ åŒ–ãƒ­ã‚°
  file {
    path => "/app/tmp/logs/kumihan_formatter_structured.log"
    start_position => "beginning"
    codec => json
    tags => ["structured", "kumihan-formatter"]
  }
  
  # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç›£æŸ»ãƒ­ã‚°
  file {
    path => "/app/tmp/logs/security_audit.log"
    start_position => "beginning"
    codec => json
    tags => ["security", "audit"]
  }
}

filter {
  if "structured" in [tags] {
    # æ§‹é€ åŒ–ãƒ­ã‚°ã®è§£æ
    mutate {
      add_field => { "log_type" => "application" }
    }
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ½å‡º
    if [processing_time] {
      mutate {
        convert => { "processing_time" => "float" }
      }
    }
  }
  
  if "security" in [tags] {
    # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¤ãƒ™ãƒ³ãƒˆåˆ†é¡
    if [event_type] == "authentication" {
      mutate {
        add_field => { "security_category" => "auth" }
      }
    }
    
    if [event_type] == "input_validation" {
      mutate {
        add_field => { "security_category" => "input" }
      }
    }
  }
  
  # å…±é€šãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰è¿½åŠ 
  mutate {
    add_field => { "[@metadata][index_name]" => "kumihan-formatter-%{+YYYY.MM.dd}" }
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "%{[@metadata][index_name]}"
  }
  
  stdout {
    codec => rubydebug
  }
}
```

### 2. ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç›£è¦–çµ±åˆ

#### ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ¡ãƒˆãƒªã‚¯ã‚¹
```python
# kumihan_formatter/core/monitoring/security_metrics.py
from prometheus_client import Counter, Histogram, Gauge
from kumihan_formatter.core.utilities.logger import get_logger

logger = get_logger(__name__)

# ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ¡ãƒˆãƒªã‚¯ã‚¹
SECURITY_EVENTS = Counter(
    'kumihan_formatter_security_events_total',
    'Total security events',
    ['event_type', 'severity', 'source']
)

AUTH_ATTEMPTS = Counter(
    'kumihan_formatter_auth_attempts_total',
    'Authentication attempts',
    ['result', 'method']
)

INPUT_VALIDATION_FAILURES = Counter(
    'kumihan_formatter_input_validation_failures_total',
    'Input validation failures',
    ['validation_type', 'severity']
)

VULNERABILITY_SCAN_RESULTS = Gauge(
    'kumihan_formatter_vulnerability_count',
    'Vulnerability count',
    ['severity', 'component']
)

class SecurityMetricsCollector:
    """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ã‚¯ãƒ©ã‚¹"""
    
    @staticmethod
    def record_security_event(event_type: str, severity: str, source: str):
        """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¤ãƒ™ãƒ³ãƒˆè¨˜éŒ²"""
        SECURITY_EVENTS.labels(
            event_type=event_type,
            severity=severity,
            source=source
        ).inc()
        
        logger.warning(
            "Security event recorded",
            extra={
                "event_type": event_type,
                "severity": severity,
                "source": source,
                "timestamp": time.time()
            }
        )
    
    @staticmethod
    def record_auth_attempt(result: str, method: str):
        """èªè¨¼è©¦è¡Œè¨˜éŒ²"""
        AUTH_ATTEMPTS.labels(
            result=result,
            method=method
        ).inc()
    
    @staticmethod
    def record_input_validation_failure(validation_type: str, severity: str):
        """å…¥åŠ›æ¤œè¨¼å¤±æ•—è¨˜éŒ²"""
        INPUT_VALIDATION_FAILURES.labels(
            validation_type=validation_type,
            severity=severity
        ).inc()
```

---

## ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†

### 1. ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹

#### ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–
```yaml
# monitoring/prometheus/rules/system_alerts.yml
groups:
  - name: system
    rules:
      # CPUä½¿ç”¨ç‡
      - alert: HighCPUUsage
        expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High CPU usage detected"
          description: "CPU usage is above 80% for more than 5 minutes"

      # ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡
      - alert: HighMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage detected"
          description: "Memory usage is above 85% for more than 5 minutes"

      # ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨ç‡
      - alert: HighDiskUsage
        expr: (1 - node_filesystem_avail_bytes / node_filesystem_size_bytes) * 100 > 90
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "High disk usage detected"
          description: "Disk usage is above 90%"
```

### 2. ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¡ãƒˆãƒªã‚¯ã‚¹

#### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–
```yaml
# monitoring/prometheus/rules/application_alerts.yml
groups:
  - name: application
    rules:
      # å¿œç­”æ™‚é–“
      - alert: HighResponseTime
        expr: histogram_quantile(0.95, kumihan_formatter_request_duration_seconds) > 2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High response time detected"
          description: "95th percentile response time is above 2 seconds"

      # ã‚¨ãƒ©ãƒ¼ç‡
      - alert: HighErrorRate
        expr: rate(kumihan_formatter_errors_total[5m]) / rate(kumihan_formatter_requests_total[5m]) > 0.05
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "High error rate detected"
          description: "Error rate is above 5% for more than 2 minutes"

      # å‡¦ç†èƒ½åŠ›
      - alert: LowThroughput
        expr: rate(kumihan_formatter_requests_total[5m]) < 10
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Low throughput detected"
          description: "Request rate is below 10 requests/second for more than 10 minutes"
```

### 3. ã‚«ã‚¹ã‚¿ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†

#### ãƒ“ã‚¸ãƒã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹
```python
# kumihan_formatter/core/monitoring/business_metrics.py
from prometheus_client import Counter, Histogram, Gauge
import time
from typing import Dict, Any

# ãƒ“ã‚¸ãƒã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹
DOCUMENT_PROCESSING = Counter(
    'kumihan_formatter_documents_processed_total',
    'Total documents processed',
    ['format', 'size_category', 'success']
)

FEATURE_USAGE = Counter(
    'kumihan_formatter_feature_usage_total',
    'Feature usage count',
    ['feature', 'user_type']
)

CONVERSION_SUCCESS_RATE = Gauge(
    'kumihan_formatter_conversion_success_rate',
    'Conversion success rate',
    ['format_from', 'format_to']
)

ACTIVE_USERS = Gauge(
    'kumihan_formatter_active_users',
    'Number of active users',
    ['time_window']
)

class BusinessMetricsCollector:
    """ãƒ“ã‚¸ãƒã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.success_tracking: Dict[str, Dict[str, int]] = {}
    
    def track_document_processing(self, format_type: str, file_size: int, success: bool):
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‡¦ç†è¿½è·¡"""
        size_category = self._categorize_size(file_size)
        
        DOCUMENT_PROCESSING.labels(
            format=format_type,
            size_category=size_category,
            success=str(success)
        ).inc()
    
    def track_feature_usage(self, feature: str, user_type: str = "standard"):
        """æ©Ÿèƒ½åˆ©ç”¨è¿½è·¡"""
        FEATURE_USAGE.labels(
            feature=feature,
            user_type=user_type
        ).inc()
    
    def update_conversion_rate(self, format_from: str, format_to: str, success: bool):
        """å¤‰æ›æˆåŠŸç‡æ›´æ–°"""
        key = f"{format_from}_to_{format_to}"
        
        if key not in self.success_tracking:
            self.success_tracking[key] = {"success": 0, "total": 0}
        
        self.success_tracking[key]["total"] += 1
        if success:
            self.success_tracking[key]["success"] += 1
        
        rate = self.success_tracking[key]["success"] / self.success_tracking[key]["total"]
        
        CONVERSION_SUCCESS_RATE.labels(
            format_from=format_from,
            format_to=format_to
        ).set(rate)
    
    @staticmethod
    def _categorize_size(size_bytes: int) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚«ãƒ†ã‚´ãƒªåˆ†é¡"""
        if size_bytes < 1024:  # 1KBæœªæº€
            return "small"
        elif size_bytes < 1024 * 1024:  # 1MBæœªæº€
            return "medium"
        elif size_bytes < 10 * 1024 * 1024:  # 10MBæœªæº€
            return "large"
        else:
            return "xlarge"
```

---

## ãƒ­ã‚°ç›£è¦–

### 1. æ§‹é€ åŒ–ãƒ­ã‚°è¨­å®š

#### ãƒ­ã‚°è¨­å®šçµ±åˆ
```python
# kumihan_formatter/core/logging/monitoring_integration.py
import json
import time
from typing import Dict, Any
from kumihan_formatter.core.utilities.logger import get_logger
from kumihan_formatter.core.monitoring.security_metrics import SecurityMetricsCollector

logger = get_logger(__name__)

class MonitoringLogHandler:
    """ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ çµ±åˆãƒ­ã‚°ãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""
    
    def __init__(self, log_file_path: str = "tmp/logs/monitoring_events.log"):
        self.log_file_path = log_file_path
        self.security_metrics = SecurityMetricsCollector()
    
    def log_monitoring_event(self, event_type: str, data: Dict[str, Any], severity: str = "info"):
        """ç›£è¦–ã‚¤ãƒ™ãƒ³ãƒˆãƒ­ã‚°å‡ºåŠ›"""
        event = {
            "timestamp": time.time(),
            "event_type": event_type,
            "severity": severity,
            "data": data,
            "source": "kumihan_formatter"
        }
        
        # æ§‹é€ åŒ–ãƒ­ã‚°å‡ºåŠ›
        with open(self.log_file_path, "a", encoding="utf-8") as f:
            f.write(json.dumps(event, ensure_ascii=False) + "\n")
        
        # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¤ãƒ™ãƒ³ãƒˆã®å ´åˆã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
        if event_type.startswith("security_"):
            self.security_metrics.record_security_event(
                event_type=event_type,
                severity=severity,
                source="application"
            )
        
        # æ¨™æº–ãƒ­ã‚°å‡ºåŠ›
        logger.info(f"Monitoring event: {event_type}", extra=event)
    
    def log_performance_event(self, operation: str, duration: float, metadata: Dict[str, Any]):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¤ãƒ™ãƒ³ãƒˆãƒ­ã‚°"""
        self.log_monitoring_event(
            event_type="performance",
            data={
                "operation": operation,
                "duration_seconds": duration,
                "metadata": metadata
            },
            severity="info"
        )
    
    def log_security_event(self, event_type: str, details: Dict[str, Any], severity: str = "warning"):
        """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¤ãƒ™ãƒ³ãƒˆãƒ­ã‚°"""
        self.log_monitoring_event(
            event_type=f"security_{event_type}",
            data=details,
            severity=severity
        )
```

### 2. ãƒ­ã‚°åˆ†æè¨­å®š

#### Kibana ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¨­å®š
```json
{
  "version": "8.8.0",
  "objects": [
    {
      "id": "kumihan-formatter-logs",
      "type": "index-pattern",
      "attributes": {
        "title": "kumihan-formatter-*",
        "timeFieldName": "@timestamp"
      }
    },
    {
      "id": "security-events-visualization",
      "type": "visualization",
      "attributes": {
        "title": "Security Events Over Time",
        "visState": {
          "type": "histogram",
          "params": {
            "grid": {"categoryLines": false, "style": {"color": "#eee"}},
            "categoryAxes": [{"id": "CategoryAxis-1", "type": "category", "position": "bottom", "show": true, "style": {}, "scale": {"type": "linear"}, "labels": {"show": true, "truncate": 100}, "title": {}}],
            "valueAxes": [{"id": "ValueAxis-1", "name": "LeftAxis-1", "type": "value", "position": "left", "show": true, "style": {}, "scale": {"type": "linear", "mode": "normal"}, "labels": {"show": true, "rotate": 0, "filter": false, "truncate": 100}, "title": {"text": "Count"}}]
          },
          "aggs": [
            {"id": "1", "enabled": true, "type": "count", "schema": "metric", "params": {}},
            {"id": "2", "enabled": true, "type": "date_histogram", "schema": "segment", "params": {"field": "@timestamp", "interval": "auto", "customInterval": "2h", "min_doc_count": 1, "extended_bounds": {}}}
          ]
        }
      }
    }
  ]
}
```

---

## ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç›£è¦–

### 1. è„…å¨æ¤œçŸ¥ã‚·ã‚¹ãƒ†ãƒ 

#### ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š
```yaml
# monitoring/prometheus/rules/security_alerts.yml
groups:
  - name: security
    rules:
      # ç•°å¸¸ãªèªè¨¼è©¦è¡Œ
      - alert: SuspiciousAuthenticationActivity
        expr: increase(kumihan_formatter_auth_attempts_total{result="failure"}[5m]) > 10
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Suspicious authentication activity detected"
          description: "More than 10 failed authentication attempts in 5 minutes"

      # å…¥åŠ›æ¤œè¨¼å¤±æ•—æ€¥å¢—
      - alert: HighInputValidationFailures
        expr: increase(kumihan_formatter_input_validation_failures_total[1m]) > 50
        for: 30s
        labels:
          severity: warning
        annotations:
          summary: "High input validation failures"
          description: "More than 50 input validation failures in 1 minute"

      # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¤ãƒ™ãƒ³ãƒˆæ€¥å¢—
      - alert: SecurityEventSpike
        expr: increase(kumihan_formatter_security_events_total[5m]) > 20
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "Security event spike detected"
          description: "Unusual increase in security events"
```

### 2. ç›£æŸ»ãƒ­ã‚°åˆ†æ

#### ç›£æŸ»ã‚¤ãƒ™ãƒ³ãƒˆè¿½è·¡
```python
# kumihan_formatter/core/monitoring/audit_tracker.py
import json
import time
from datetime import datetime
from typing import Dict, Any, List
from dataclasses import dataclass
from kumihan_formatter.core.utilities.logger import get_logger

logger = get_logger(__name__)

@dataclass
class AuditEvent:
    """ç›£æŸ»ã‚¤ãƒ™ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    timestamp: float
    user_id: str
    event_type: str
    resource: str
    action: str
    result: str
    metadata: Dict[str, Any]

class AuditTracker:
    """ç›£æŸ»è¿½è·¡ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, audit_log_path: str = "tmp/logs/audit.log"):
        self.audit_log_path = audit_log_path
        self.events: List[AuditEvent] = []
    
    def log_audit_event(self, user_id: str, event_type: str, resource: str, 
                       action: str, result: str, metadata: Dict[str, Any] = None):
        """ç›£æŸ»ã‚¤ãƒ™ãƒ³ãƒˆãƒ­ã‚°è¨˜éŒ²"""
        if metadata is None:
            metadata = {}
        
        event = AuditEvent(
            timestamp=time.time(),
            user_id=user_id,
            event_type=event_type,
            resource=resource,
            action=action,
            result=result,
            metadata=metadata
        )
        
        # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
        audit_record = {
            "timestamp": datetime.fromtimestamp(event.timestamp).isoformat(),
            "user_id": event.user_id,
            "event_type": event.event_type,
            "resource": event.resource,
            "action": event.action,
            "result": event.result,
            "metadata": event.metadata
        }
        
        with open(self.audit_log_path, "a", encoding="utf-8") as f:
            f.write(json.dumps(audit_record, ensure_ascii=False) + "\n")
        
        self.events.append(event)
        
        # ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ é€£æº
        logger.info(f"Audit event: {event_type}", extra=audit_record)
    
    def get_security_summary(self, hours: int = 24) -> Dict[str, Any]:
        """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚µãƒãƒªãƒ¼å–å¾—"""
        cutoff_time = time.time() - (hours * 3600)
        recent_events = [e for e in self.events if e.timestamp > cutoff_time]
        
        summary = {
            "total_events": len(recent_events),
            "failed_attempts": len([e for e in recent_events if e.result == "failure"]),
            "event_types": {},
            "user_activity": {}
        }
        
        for event in recent_events:
            # ã‚¤ãƒ™ãƒ³ãƒˆã‚¿ã‚¤ãƒ—åˆ¥é›†è¨ˆ
            if event.event_type not in summary["event_types"]:
                summary["event_types"][event.event_type] = 0
            summary["event_types"][event.event_type] += 1
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ¥é›†è¨ˆ
            if event.user_id not in summary["user_activity"]:
                summary["user_activity"][event.user_id] = 0
            summary["user_activity"][event.user_id] += 1
        
        return summary
```

---

## ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š

### 1. AlertManagerè¨­å®š

#### alertmanager.yml
```yaml
global:
  smtp_smarthost: 'localhost:587'
  smtp_from: 'alerts@kumihan-formatter.com'

route:
  group_by: ['alertname']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 1h
  receiver: 'web.hook'
  routes:
  - match:
      severity: critical
    receiver: 'critical-alerts'
  - match:
      severity: warning
    receiver: 'warning-alerts'

receivers:
- name: 'web.hook'
  webhook_configs:
  - url: 'http://localhost:5001/webhook'

- name: 'critical-alerts'
  email_configs:
  - to: 'admin@kumihan-formatter.com'
    subject: 'CRITICAL: Kumihan-Formatter Alert'
    body: |
      {{ range .Alerts }}
      Alert: {{ .Annotations.summary }}
      Description: {{ .Annotations.description }}
      {{ end }}
  slack_configs:
  - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
    channel: '#alerts-critical'
    title: 'Critical Alert'
    text: |
      {{ range .Alerts }}
      Alert: {{ .Annotations.summary }}
      Description: {{ .Annotations.description }}
      {{ end }}

- name: 'warning-alerts'
  email_configs:
  - to: 'team@kumihan-formatter.com'
    subject: 'WARNING: Kumihan-Formatter Alert'
    body: |
      {{ range .Alerts }}
      Alert: {{ .Annotations.summary }}
      Description: {{ .Annotations.description }}
      {{ end }}

inhibit_rules:
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'dev', 'instance']
```

### 2. é€šçŸ¥çµ±åˆè¨­å®š

#### Slacké€šçŸ¥è¨­å®š
```python
# kumihan_formatter/core/monitoring/notifications.py
import requests
import json
from typing import Dict, Any
from kumihan_formatter.core.utilities.logger import get_logger

logger = get_logger(__name__)

class NotificationManager:
    """é€šçŸ¥ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, slack_webhook_url: str = None, email_config: Dict[str, str] = None):
        self.slack_webhook_url = slack_webhook_url
        self.email_config = email_config or {}
    
    def send_slack_alert(self, title: str, message: str, severity: str = "info"):
        """Slackã‚¢ãƒ©ãƒ¼ãƒˆé€ä¿¡"""
        if not self.slack_webhook_url:
            return
        
        color_map = {
            "critical": "danger",
            "warning": "warning",
            "info": "good"
        }
        
        payload = {
            "attachments": [{
                "color": color_map.get(severity, "good"),
                "title": f"Kumihan-Formatter Alert: {title}",
                "text": message,
                "footer": "Kumihan-Formatter Monitoring",
                "ts": int(time.time())
            }]
        }
        
        try:
            response = requests.post(
                self.slack_webhook_url,
                data=json.dumps(payload),
                headers={'Content-Type': 'application/json'},
                timeout=10
            )
            response.raise_for_status()
            logger.info(f"Slack notification sent: {title}")
        except Exception as e:
            logger.error(f"Failed to send Slack notification: {e}")
    
    def send_email_alert(self, to: str, subject: str, body: str):
        """ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ©ãƒ¼ãƒˆé€ä¿¡"""
        # Emailé€ä¿¡å®Ÿè£…ï¼ˆSMTPãƒ©ã‚¤ãƒ–ãƒ©ãƒªä½¿ç”¨ï¼‰
        pass
    
    def send_webhook_alert(self, webhook_url: str, data: Dict[str, Any]):
        """Webhooké€šçŸ¥é€ä¿¡"""
        try:
            response = requests.post(
                webhook_url,
                json=data,
                headers={'Content-Type': 'application/json'},
                timeout=10
            )
            response.raise_for_status()
            logger.info(f"Webhook notification sent to {webhook_url}")
        except Exception as e:
            logger.error(f"Failed to send webhook notification: {e}")
```

---

## ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰æ§‹ç¯‰

### 1. Grafana ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¨­å®š

#### ãƒ¡ã‚¤ãƒ³ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ JSON
```json
{
  "dashboard": {
    "id": null,
    "title": "Kumihan-Formatter Monitoring Dashboard",
    "tags": ["kumihan-formatter", "monitoring"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "title": "Request Rate",
        "type": "stat",
        "targets": [
          {
            "expr": "rate(kumihan_formatter_requests_total[5m])",
            "legendFormat": "Requests/sec"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "color": {"mode": "thresholds"},
            "thresholds": {
              "steps": [
                {"color": "red", "value": 0},
                {"color": "yellow", "value": 10},
                {"color": "green", "value": 50}
              ]
            }
          }
        },
        "gridPos": {"h": 8, "w": 6, "x": 0, "y": 0}
      },
      {
        "id": 2,
        "title": "Response Time (95th percentile)",
        "type": "stat",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, kumihan_formatter_request_duration_seconds)",
            "legendFormat": "95th percentile"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "s",
            "color": {"mode": "thresholds"},
            "thresholds": {
              "steps": [
                {"color": "green", "value": 0},
                {"color": "yellow", "value": 1},
                {"color": "red", "value": 2}
              ]
            }
          }
        },
        "gridPos": {"h": 8, "w": 6, "x": 6, "y": 0}
      },
      {
        "id": 3,
        "title": "Error Rate",
        "type": "stat",
        "targets": [
          {
            "expr": "rate(kumihan_formatter_errors_total[5m]) / rate(kumihan_formatter_requests_total[5m])",
            "legendFormat": "Error Rate"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "percentunit",
            "color": {"mode": "thresholds"},
            "thresholds": {
              "steps": [
                {"color": "green", "value": 0},
                {"color": "yellow", "value": 0.01},
                {"color": "red", "value": 0.05}
              ]
            }
          }
        },
        "gridPos": {"h": 8, "w": 6, "x": 12, "y": 0}
      },
      {
        "id": 4,
        "title": "System Resources",
        "type": "timeseries",
        "targets": [
          {
            "expr": "100 - (avg by(instance) (irate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100)",
            "legendFormat": "CPU Usage %"
          },
          {
            "expr": "(1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100",
            "legendFormat": "Memory Usage %"
          }
        ],
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 8}
      },
      {
        "id": 5,
        "title": "Security Events",
        "type": "timeseries",
        "targets": [
          {
            "expr": "rate(kumihan_formatter_security_events_total[5m])",
            "legendFormat": "Security Events/sec"
          }
        ],
        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 8}
      }
    ],
    "time": {"from": "now-1h", "to": "now"},
    "refresh": "30s"
  }
}
```

### 2. ã‚«ã‚¹ã‚¿ãƒ ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰

#### ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
```json
{
  "dashboard": {
    "title": "Security Monitoring Dashboard",
    "panels": [
      {
        "title": "Authentication Attempts",
        "type": "timeseries",
        "targets": [
          {
            "expr": "rate(kumihan_formatter_auth_attempts_total{result=\"success\"}[5m])",
            "legendFormat": "Successful"
          },
          {
            "expr": "rate(kumihan_formatter_auth_attempts_total{result=\"failure\"}[5m])",
            "legendFormat": "Failed"
          }
        ]
      },
      {
        "title": "Input Validation Failures",
        "type": "timeseries",
        "targets": [
          {
            "expr": "rate(kumihan_formatter_input_validation_failures_total[5m])",
            "legendFormat": "Validation Failures/sec"
          }
        ]
      },
      {
        "title": "Vulnerability Scan Results",
        "type": "stat",
        "targets": [
          {
            "expr": "kumihan_formatter_vulnerability_count",
            "legendFormat": "{{ severity }} vulnerabilities"
          }
        ]
      }
    ]
  }
}
```

---

## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### 1. ä¸€èˆ¬çš„ãªå•é¡Œã¨è§£æ±ºæ–¹æ³•

#### Prometheusæ¥ç¶šå•é¡Œ
```bash
# Prometheusã‚µãƒ¼ãƒ“ã‚¹ç¢ºèª
docker ps | grep prometheus
docker logs prometheus

# ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆç¢ºèª
curl http://localhost:9090/-/healthy
curl http://localhost:8000/metrics
```

#### Grafanaè¡¨ç¤ºå•é¡Œ
```bash
# Grafanaãƒ­ã‚°ç¢ºèª
docker logs grafana

# ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹æ¥ç¶šç¢ºèª
curl -X GET http://admin:admin123@localhost:3000/api/datasources

# ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç¢ºèª
curl -X GET http://admin:admin123@localhost:3000/api/dashboards/home
```

#### ELKã‚¹ã‚¿ãƒƒã‚¯å•é¡Œ
```bash
# Elasticsearchç¢ºèª
curl http://localhost:9200/_cluster/health

# Kibanaã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç¢ºèª
curl http://localhost:9200/_cat/indices?v

# Logstashè¨­å®šç¢ºèª
docker exec -it logstash /usr/share/logstash/bin/logstash --config.test_and_exit --path.config=/usr/share/logstash/pipeline/
```

### 2. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

#### Prometheusæœ€é©åŒ–è¨­å®š
```yaml
# prometheus.yml æœ€é©åŒ–è¨­å®š
global:
  scrape_interval: 30s  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ15sã‹ã‚‰å¤‰æ›´
  evaluation_interval: 30s
  external_labels:
    cluster: 'kumihan-formatter'

# ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸æœ€é©åŒ–
storage:
  tsdb:
    retention.time: 30d  # ä¿æŒæœŸé–“
    retention.size: 10GB  # æœ€å¤§ã‚µã‚¤ã‚º
```

#### Grafanaæœ€é©åŒ–
```ini
# grafana.ini æœ€é©åŒ–è¨­å®š
[database]
max_open_conn = 300
max_idle_conn = 300

[server]
enable_gzip = true

[dashboards]
default_home_dashboard_path = /etc/grafana/provisioning/dashboards/main-dashboard.json
```

### 3. ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¼·åŒ–

#### èªè¨¼ãƒ»èªå¯è¨­å®š
```yaml
# docker-compose.yml ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¼·åŒ–
services:
  grafana:
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_AUTH_ANONYMOUS_ENABLED=false
      - GF_SECURITY_COOKIE_SECURE=true
      - GF_SECURITY_COOKIE_SAMESITE=strict

  prometheus:
    command:
      - '--web.enable-admin-api'
      - '--web.enable-lifecycle'
      - '--web.external-url=https://prometheus.yourdomain.com'
```

---

## é‹ç”¨æ‰‹é †æ›¸

### 1. æ—¥å¸¸é‹ç”¨ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

#### æ¯æ—¥å®Ÿæ–½é …ç›®
- [ ] ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨ç‡ç¢ºèªï¼ˆCPU/ãƒ¡ãƒ¢ãƒª/ãƒ‡ã‚£ã‚¹ã‚¯ï¼‰
- [ ] ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼ç‡ç¢ºèª
- [ ] ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¤ãƒ™ãƒ³ãƒˆãƒ¬ãƒ“ãƒ¥ãƒ¼
- [ ] ã‚¢ãƒ©ãƒ¼ãƒˆçŠ¶æ³ç¢ºèª
- [ ] ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—çŠ¶æ³ç¢ºèª

#### é€±æ¬¡å®Ÿæ–½é …ç›®
- [ ] ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
- [ ] ãƒ­ã‚°ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³çŠ¶æ³ç¢ºèª
- [ ] è„†å¼±æ€§ã‚¹ã‚­ãƒ£ãƒ³çµæœç¢ºèª
- [ ] ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ è‡ªä½“ã®ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
- [ ] ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®šè¦‹ç›´ã—

#### æœˆæ¬¡å®Ÿæ–½é …ç›®
- [ ] ç›£è¦–ãƒ‡ãƒ¼ã‚¿ä¿æŒæœŸé–“ç®¡ç†
- [ ] ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŸºæº–å€¤è¦‹ç›´ã—
- [ ] ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒãƒªã‚·ãƒ¼ç¢ºèª
- [ ] ç½å®³å¾©æ—§ãƒ†ã‚¹ãƒˆ
- [ ] ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ

### 2. ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆå¯¾å¿œæ‰‹é †

#### é‡è¦åº¦ãƒ¬ãƒ™ãƒ«å®šç¾©
- **Critical**: ã‚µãƒ¼ãƒ“ã‚¹åœæ­¢ãƒ»ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ä¾µå®³
- **High**: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å¤§å¹…åŠ£åŒ–ãƒ»æ©Ÿèƒ½éšœå®³
- **Medium**: è»½å¾®ãªæ©Ÿèƒ½ä¸å…·åˆãƒ»è­¦å‘Šãƒ¬ãƒ™ãƒ«
- **Low**: æƒ…å ±æä¾›ãƒ»äºˆé˜²ä¿å®ˆ

#### å¯¾å¿œãƒ•ãƒ­ãƒ¼
1. **æ¤œçŸ¥ãƒ»é€šçŸ¥å—ä¿¡**
2. **åˆæœŸèª¿æŸ»ãƒ»å½±éŸ¿ç¯„å›²ç¢ºèª**
3. **ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åˆ¤æ–­**
4. **å¿œæ€¥å¯¾å¿œå®Ÿæ–½**
5. **æ ¹æœ¬åŸå› èª¿æŸ»**
6. **æ’ä¹…å¯¾ç­–å®Ÿæ–½**
7. **äº‹å¾Œãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ»æ”¹å–„**

---

## Appendix

### A. è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ

#### å®Œå…¨ãªDocker Composeè¨­å®š
```yaml
# docker-compose.monitoring.yml (å®Œå…¨ç‰ˆ)
version: '3.8'

services:
  # Prometheus
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus:/etc/prometheus
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
    restart: unless-stopped

  # Grafana
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:-admin123}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_AUTH_ANONYMOUS_ENABLED=false
    restart: unless-stopped

  # AlertManager
  alertmanager:
    image: prom/alertmanager:latest
    container_name: alertmanager
    ports:
      - "9093:9093"
    volumes:
      - ./monitoring/alertmanager:/etc/alertmanager
      - alertmanager_data:/alertmanager
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
      - '--web.external-url=http://localhost:9093'
    restart: unless-stopped

  # Node Exporter
  node-exporter:
    image: prom/node-exporter:latest
    container_name: node-exporter
    ports:
      - "9100:9100"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    restart: unless-stopped

  # Elasticsearch
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.8.0
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ports:
      - "9200:9200"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    restart: unless-stopped

  # Logstash
  logstash:
    image: docker.elastic.co/logstash/logstash:8.8.0
    container_name: logstash
    ports:
      - "5044:5044"
      - "9600:9600"
    volumes:
      - ./monitoring/logstash/pipeline:/usr/share/logstash/pipeline
      - ./monitoring/logstash/config:/usr/share/logstash/config
      - ./tmp/logs:/app/logs:ro
    depends_on:
      - elasticsearch
    restart: unless-stopped

  # Kibana
  kibana:
    image: docker.elastic.co/kibana/kibana:8.8.0
    container_name: kibana
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    depends_on:
      - elasticsearch
    restart: unless-stopped

volumes:
  prometheus_data:
  grafana_data:
  alertmanager_data:
  elasticsearch_data:

networks:
  default:
    name: monitoring_network
```

### B. åˆæœŸã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

#### setup_monitoring.sh
```bash
#!/bin/bash
# Kumihan-Formatter ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

set -e

echo "ğŸš€ Kumihan-Formatter ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—é–‹å§‹"

# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
mkdir -p monitoring/{prometheus,grafana,alertmanager,logstash}/{config,rules,dashboards,pipeline}
mkdir -p tmp/logs

# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«é…ç½®
echo "ğŸ“ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«é…ç½®ä¸­..."

# Prometheusè¨­å®š
cat > monitoring/prometheus/prometheus.yml << 'EOF'
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "rules/*.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093

scrape_configs:
  - job_name: 'kumihan-formatter'
    static_configs:
      - targets: ['host.docker.internal:8000']
  
  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']
EOF

# AlertManagerè¨­å®š
cat > monitoring/alertmanager/alertmanager.yml << 'EOF'
global:
  smtp_smarthost: 'localhost:587'
  smtp_from: 'alerts@kumihan-formatter.com'

route:
  group_by: ['alertname']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 1h
  receiver: 'web.hook'

receivers:
- name: 'web.hook'
  webhook_configs:
  - url: 'http://localhost:5001/webhook'
EOF

# Docker Composeèµ·å‹•
echo "ğŸ³ Docker Compose ã‚µãƒ¼ãƒ“ã‚¹èµ·å‹•ä¸­..."
docker-compose -f docker-compose.monitoring.yml up -d

# èµ·å‹•ç¢ºèª
echo "â³ ã‚µãƒ¼ãƒ“ã‚¹èµ·å‹•ç¢ºèªä¸­..."
sleep 30

# ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
echo "ğŸ¥ ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œä¸­..."
curl -f http://localhost:9090/-/healthy && echo "âœ… Prometheus OK"
curl -f http://localhost:3000/api/health && echo "âœ… Grafana OK"
curl -f http://localhost:9200/_cluster/health && echo "âœ… Elasticsearch OK"

echo "ğŸ‰ ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†!"
echo "ğŸ“Š Grafana: http://localhost:3000 (admin/admin123)"
echo "ğŸ“ˆ Prometheus: http://localhost:9090"
echo "ğŸ” Kibana: http://localhost:5601"
```

---

*ğŸ¯ Kumihan-Formatter ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºç›£è¦–ã‚·ã‚¹ãƒ†ãƒ è¨­å®šã‚¬ã‚¤ãƒ‰ - 2025å¹´ç‰ˆ*
*ğŸ“Š Phase 4çµ±åˆå¯¾å¿œãƒ»90% Tokenå‰Šæ¸›Claude-Geminiå”æ¥­æœ€é©åŒ–*