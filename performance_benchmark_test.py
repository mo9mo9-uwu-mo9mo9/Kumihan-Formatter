#!/usr/bin/env python3
"""
Issue #769 ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ

ç›®æ¨™: 300Kè¡Œãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚’29.17ç§’â†’5ç§’ä»¥ä¸‹ã«çŸ­ç¸®ï¼ˆ83%é«˜é€ŸåŒ–ï¼‰

ãƒ†ã‚¹ãƒˆå¯¾è±¡:
- SIMDæœ€é©åŒ–ï¼ˆNumPyæ´»ç”¨ï¼‰
- éåŒæœŸI/Oæœ€é©åŒ–ï¼ˆaiofilesæ´»ç”¨ï¼‰
- æ­£è¦è¡¨ç¾ã‚¨ãƒ³ã‚¸ãƒ³æœ€é©åŒ–ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ï¼‰
- ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³æœ€é©åŒ–
- ä¸¦åˆ—å‡¦ç†æœ€é©åŒ–
"""

import asyncio
import time
import sys
import os
from pathlib import Path
from typing import List, Dict, Any
import tempfile

# Kumihan-Formatter components
sys.path.insert(0, str(Path(__file__).parent))
from kumihan_formatter.core.utilities.performance_metrics import (
    SIMDOptimizer,
    AsyncIOOptimizer, 
    RegexOptimizer,
    MemoryOptimizer,
    PerformanceMonitor,
    monitor_performance
)
from kumihan_formatter.core.utilities.parallel_processor import ParallelChunkProcessor
from kumihan_formatter.core.utilities.logger import get_logger


class PerformanceBenchmark:
    """Issue #769å¯¾å¿œ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.logger = get_logger(__name__)
        self.results = {}
        
        # æœ€é©åŒ–ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–
        self.simd_optimizer = SIMDOptimizer()
        self.async_optimizer = AsyncIOOptimizer()
        self.regex_optimizer = RegexOptimizer()
        self.memory_optimizer = MemoryOptimizer()
        self.parallel_processor = ParallelChunkProcessor()
        
        self.logger.info("Performance benchmark system initialized")
    
    def create_test_file(self, lines_count: int = 300000, output_path: Path = None) -> Path:
        """
        ãƒ†ã‚¹ãƒˆç”¨å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ
        
        Args:
            lines_count: è¡Œæ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ300Kè¡Œï¼‰
            output_path: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            
        Returns:
            Path: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        if output_path is None:
            output_path = Path(tempfile.mktemp(suffix=".txt"))
        
        self.logger.info(f"Creating test file with {lines_count:,} lines: {output_path}")
        
        # å¤šæ§˜ãªKumihanè¨˜æ³•ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å«ã‚€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
        test_patterns = [
            "# å¤ªå­— #ã“ã‚Œã¯å¤ªå­—ã®ãƒ†ã‚¹ãƒˆã§ã™##",
            "# ã‚¤ã‚¿ãƒªãƒƒã‚¯ #ã“ã‚Œã¯ã‚¤ã‚¿ãƒªãƒƒã‚¯ä½“ã®ãƒ†ã‚¹ãƒˆã§ã™##", 
            "# ãƒã‚¤ãƒ©ã‚¤ãƒˆ color=yellow #é‡è¦ãªæƒ…å ±ã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆ##",
            "# è¦‹å‡ºã—1 #ãƒ¡ã‚¤ãƒ³ã‚¿ã‚¤ãƒˆãƒ«##",
            "# è¦‹å‡ºã—2 #ã‚µãƒ–ã‚¿ã‚¤ãƒˆãƒ«##",
            "# è¦‹å‡ºã—3 #å°è¦‹å‡ºã—##",
            "é€šå¸¸ã®ãƒ†ã‚­ã‚¹ãƒˆè¡Œã§ã™ã€‚Kumihanè¨˜æ³•ã®å‡¦ç†æ€§èƒ½ã‚’ãƒ†ã‚¹ãƒˆã—ã¾ã™ã€‚",
            "# å¤ªå­— ã‚¤ã‚¿ãƒªãƒƒã‚¯ #è¤‡åˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒ†ã‚¹ãƒˆ##",
            "# ãƒã‚¤ãƒ©ã‚¤ãƒˆ color=#ff0000 #èµ¤è‰²ãƒã‚¤ãƒ©ã‚¤ãƒˆ##",
            "# ã‚³ãƒ¼ãƒ‰ #print('Hello, World!')##",
            "# ä¸‹ç·š #ä¸‹ç·šä»˜ããƒ†ã‚­ã‚¹ãƒˆ##",
            "# å–ã‚Šæ¶ˆã—ç·š #å‰Šé™¤ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ##",
            "æ—¥æœ¬èªã‚’å«ã‚€è¤‡é›‘ãªæ–‡ç« ã€‚æ¼¢å­—ã€ã²ã‚‰ãŒãªã€ã‚«ã‚¿ã‚«ãƒŠãŒæ··åœ¨ã—ã¦ã„ã¾ã™ã€‚",
            "English text mixed with Japanese. è‹±èªã¨æ—¥æœ¬èªã®æ··åˆãƒ†ã‚­ã‚¹ãƒˆã€‚",
            "æ•°å€¤ãƒ‡ãƒ¼ã‚¿: 123456789, 3.14159, -987.654",
            "ç‰¹æ®Šæ–‡å­—: !@#$%^&*()_+-=[]{}|;':\",./<>?",
        ]
        
        with open(output_path, 'w', encoding='utf-8') as f:
            for i in range(lines_count):
                # ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã—ã¦å¤šæ§˜æ€§ã‚’ç¢ºä¿
                pattern = test_patterns[i % len(test_patterns)]
                # è¡Œç•ªå·ã‚’è¿½åŠ ã—ã¦ä¸€æ„æ€§ã‚’ä¿è¨¼
                line = f"Line{i:06d}: {pattern}\n"
                f.write(line)
        
        file_size_mb = output_path.stat().st_size / 1024 / 1024
        self.logger.info(f"Test file created: {file_size_mb:.2f}MB, {lines_count:,} lines")
        
        return output_path
    
    def benchmark_simd_optimization(self, test_file: Path) -> Dict[str, Any]:
        """SIMDæœ€é©åŒ–ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        self.logger.info("Benchmarking SIMD optimization...")
        
        results = {}
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        with open(test_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        sample_lines = lines[:10000]  # 10Kè¡Œã§ãƒ†ã‚¹ãƒˆ
        
        # å¤‰æ›é–¢æ•°ãƒªã‚¹ãƒˆ
        pattern_funcs = [
            lambda line: line.replace("#", ""),
            lambda line: line.strip(),
            lambda line: line.upper() if len(line) < 50 else line,
        ]
        
        # å¾“æ¥å‡¦ç†ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
        start_time = time.time()
        traditional_result = sample_lines.copy()
        for func in pattern_funcs:
            traditional_result = [func(line) for line in traditional_result]
        traditional_time = time.time() - start_time
        
        # SIMDæœ€é©åŒ–å‡¦ç†ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
        start_time = time.time()
        simd_result = self.simd_optimizer.vectorized_line_processing(sample_lines, pattern_funcs)
        simd_time = time.time() - start_time
        
        # çµæœæ¤œè¨¼
        results_match = len(traditional_result) == len(simd_result)
        
        results = {
            "traditional_time": traditional_time,
            "simd_time": simd_time,
            "speedup_factor": traditional_time / simd_time if simd_time > 0 else float('inf'),
            "results_match": results_match,
            "processed_lines": len(sample_lines),
            "simd_metrics": self.simd_optimizer.get_simd_metrics(),
        }
        
        self.logger.info(f"SIMD benchmark: {results['speedup_factor']:.2f}x speedup")
        return results
    
    async def benchmark_async_io(self, test_file: Path) -> Dict[str, Any]:
        """éåŒæœŸI/Oã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        self.logger.info("Benchmarking AsyncIO optimization...")
        
        results = {}
        
        # åŒæœŸèª­ã¿è¾¼ã¿ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
        start_time = time.time()
        with open(test_file, 'r', encoding='utf-8') as f:
            sync_lines = f.readlines()
        sync_time = time.time() - start_time
        
        # éåŒæœŸèª­ã¿è¾¼ã¿ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
        start_time = time.time()
        async_lines = []
        async for batch in self.async_optimizer.async_read_lines_batched(test_file, batch_size=1000):
            async_lines.extend(batch)
        async_time = time.time() - start_time
        
        # çµæœæ¤œè¨¼
        results_match = len(sync_lines) == len(async_lines)
        
        results = {
            "sync_time": sync_time,
            "async_time": async_time,
            "speedup_factor": sync_time / async_time if async_time > 0 else float('inf'),
            "results_match": results_match,
            "total_lines": len(sync_lines),
            "async_metrics": self.async_optimizer.get_async_metrics(),
        }
        
        self.logger.info(f"AsyncIO benchmark: {results['speedup_factor']:.2f}x speedup")
        return results
    
    def benchmark_regex_optimization(self, test_file: Path) -> Dict[str, Any]:
        """æ­£è¦è¡¨ç¾æœ€é©åŒ–ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        self.logger.info("Benchmarking Regex optimization...")
        
        results = {}
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆã‚µãƒ³ãƒ—ãƒ«ï¼‰
        with open(test_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        sample_text = '\n'.join(lines[:5000])  # 5Kè¡Œã§ãƒ†ã‚¹ãƒˆ
        
        # ãƒ†ã‚¹ãƒˆç”¨æ­£è¦è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³
        patterns_and_replacements = [
            (r"#\s*([^#]+?)\s*#([^#]+?)##", r"<\1>\2</\1>"),
            (r"\s+", " "),
            (r"Line\d+:", ""),
        ]
        
        # å¾“æ¥å‡¦ç†ï¼ˆæ¯å›ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ï¼‰
        start_time = time.time()
        import re
        traditional_result = sample_text
        for pattern, replacement in patterns_and_replacements:
            traditional_result = re.sub(pattern, replacement, traditional_result)
        traditional_time = time.time() - start_time
        
        # æœ€é©åŒ–å‡¦ç†ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥æ´»ç”¨ï¼‰
        start_time = time.time()
        optimized_result = sample_text
        for pattern, replacement in patterns_and_replacements:
            optimized_result = self.regex_optimizer.optimized_substitute(
                pattern, replacement, optimized_result
            )
        optimized_time = time.time() - start_time
        
        # çµæœæ¤œè¨¼
        results_match = traditional_result == optimized_result
        
        results = {
            "traditional_time": traditional_time,
            "optimized_time": optimized_time,
            "speedup_factor": traditional_time / optimized_time if optimized_time > 0 else float('inf'),
            "results_match": results_match,
            "cache_stats": self.regex_optimizer.get_cache_stats(),
        }
        
        self.logger.info(f"Regex benchmark: {results['speedup_factor']:.2f}x speedup")
        return results
    
    def benchmark_memory_optimization(self, test_file: Path) -> Dict[str, Any]:
        """ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        self.logger.info("Benchmarking Memory optimization...")
        
        results = {}
        
        # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ
        start_time = time.time()
        traditional_lines = []
        with open(test_file, 'r', encoding='utf-8') as f:
            traditional_lines = f.readlines()
        traditional_time = time.time() - start_time
        
        # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–èª­ã¿è¾¼ã¿
        start_time = time.time()
        optimized_lines = []
        for chunk in self.memory_optimizer.memory_efficient_file_reader(test_file, chunk_size=64*1024):
            optimized_lines.extend(chunk.split('\n'))
        optimized_time = time.time() - start_time
        
        # ãƒªã‚¹ãƒˆæ“ä½œæœ€é©åŒ–ãƒ†ã‚¹ãƒˆ
        test_data = list(range(50000))
        
        start_time = time.time()
        traditional_sorted = sorted(test_data)
        traditional_sort_time = time.time() - start_time
        
        start_time = time.time()
        optimized_sorted = self.memory_optimizer.optimize_list_operations(test_data, "sort")
        optimized_sort_time = time.time() - start_time
        
        results = {
            "file_read": {
                "traditional_time": traditional_time,
                "optimized_time": optimized_time,
                "speedup_factor": traditional_time / optimized_time if optimized_time > 0 else float('inf'),
            },
            "list_sort": {
                "traditional_time": traditional_sort_time,
                "optimized_time": optimized_sort_time,
                "speedup_factor": traditional_sort_time / optimized_sort_time if optimized_sort_time > 0 else float('inf'),
            },
            "memory_stats": self.memory_optimizer.get_memory_stats(),
        }
        
        self.logger.info(f"Memory optimization benchmark completed")
        return results
    
    def benchmark_parallel_processing(self, test_file: Path) -> Dict[str, Any]:
        """ä¸¦åˆ—å‡¦ç†æœ€é©åŒ–ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
        self.logger.info("Benchmarking Parallel processing optimization...")
        
        results = {}
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼‰
        with open(test_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        # å¤§å®¹é‡ãƒ‡ãƒ¼ã‚¿ã§ãƒ†ã‚¹ãƒˆï¼ˆ100Kè¡Œï¼‰
        test_lines = lines[:100000] if len(lines) > 100000 else lines
        
        # ã‚ˆã‚ŠåŠ¹ç‡çš„ãªå‡¦ç†é–¢æ•°
        def process_chunk_optimized(chunk):
            # ãƒªã‚¹ãƒˆå†…åŒ…è¡¨è¨˜ã§ã‚ˆã‚Šé«˜é€Ÿã«å‡¦ç†
            return [
                line.strip().replace('#', '').replace('å¤ªå­—', '<strong>').replace('##', '</strong>')
                for line in chunk.lines
                if line.strip()  # ç©ºè¡Œã‚’ãƒ•ã‚£ãƒ«ã‚¿
            ]
        
        # ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«å‡¦ç†ï¼ˆãƒãƒƒãƒã”ã¨ã«åˆ†å‰²ï¼‰
        start_time = time.time()
        sequential_results = []
        batch_size = 5000
        for i in range(0, len(test_lines), batch_size):
            batch = test_lines[i:i + batch_size]
            batch_results = [
                line.strip().replace('#', '').replace('å¤ªå­—', '<strong>').replace('##', '</strong>')
                for line in batch
                if line.strip()
            ]
            sequential_results.extend(batch_results)
        sequential_time = time.time() - start_time
        
        # ä¸¦åˆ—å‡¦ç†ï¼ˆæœ€é©åŒ–ï¼‰
        start_time = time.time()
        
        # CPUæ•°ã«åŸºã¥ã„ã¦é©åˆ‡ãªãƒãƒ£ãƒ³ã‚¯æ•°ã‚’è¨ˆç®—
        import os
        cpu_count = os.cpu_count() or 1
        optimal_chunk_count = min(cpu_count * 2, 16)
        
        # ãƒãƒ£ãƒ³ã‚¯ä½œæˆ
        chunks = self.parallel_processor.create_chunks_adaptive(
            test_lines, 
            target_chunk_count=optimal_chunk_count
        )
        
        # ä¸¦åˆ—å‡¦ç†å®Ÿè¡Œ
        parallel_results = []
        for result in self.parallel_processor.process_chunks_parallel_optimized(
            chunks, 
            process_chunk_optimized
        ):
            if isinstance(result, list):
                parallel_results.extend(result)
            else:
                parallel_results.append(result)
        
        parallel_time = time.time() - start_time
        
        # çµæœæ¤œè¨¼
        results_match = len(sequential_results) == len(parallel_results)
        
        results = {
            "sequential_time": sequential_time,
            "parallel_time": parallel_time,
            "speedup_factor": sequential_time / parallel_time if parallel_time > 0 else float('inf'),
            "chunks_processed": len(chunks),
            "results_match": results_match,
            "processed_lines": len(test_lines),
            "parallel_metrics": self.parallel_processor.get_parallel_metrics(),
        }
        
        self.logger.info(f"Parallel processing: {results['speedup_factor']:.2f}x speedup")
        return results
    
    async def run_comprehensive_benchmark(self, lines_count: int = 300000) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®å®Ÿè¡Œ"""
        self.logger.info(f"Starting comprehensive benchmark with {lines_count:,} lines")
        
        # ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ
        test_file = self.create_test_file(lines_count)
        
        try:
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚’é–‹å§‹
            perf_monitor = PerformanceMonitor()
            perf_monitor.start_monitoring(lines_count, "benchmark_start")
            
            # å„æœ€é©åŒ–æ‰‹æ³•ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
            results = {
                "simd_optimization": self.benchmark_simd_optimization(test_file),
                "async_io": await self.benchmark_async_io(test_file),
                "regex_optimization": self.benchmark_regex_optimization(test_file),
                "memory_optimization": self.benchmark_memory_optimization(test_file),
                "parallel_processing": self.benchmark_parallel_processing(test_file),
            }
            
            perf_monitor.stop_monitoring()
            
            # ç·åˆçµæœã®è¨ˆç®—
            total_speedup = 1.0
            for optimization, result in results.items():
                if isinstance(result, dict) and "speedup_factor" in result:
                    total_speedup *= result["speedup_factor"]
                elif isinstance(result, dict):
                    # è¤‡æ•°ã®ã‚µãƒ–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãŒã‚ã‚‹å ´åˆ
                    for sub_result in result.values():
                        if isinstance(sub_result, dict) and "speedup_factor" in sub_result:
                            total_speedup *= sub_result["speedup_factor"]
            
            results["summary"] = {
                "total_estimated_speedup": total_speedup,
                "target_speedup": 5.84,  # 29.17ç§’â†’5ç§’ã®ç›®æ¨™
                "target_achieved": total_speedup >= 5.84,
                "performance_report": perf_monitor.generate_performance_report(),
            }
            
            self.logger.info(f"Comprehensive benchmark completed:")
            self.logger.info(f"  Total estimated speedup: {total_speedup:.2f}x")
            self.logger.info(f"  Target achieved: {results['summary']['target_achieved']}")
            
            return results
        
        finally:
            # ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if test_file.exists():
                test_file.unlink()
                self.logger.info(f"Test file cleaned up: {test_file}")
    
    def print_benchmark_report(self, results: Dict[str, Any]):
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœãƒ¬ãƒãƒ¼ãƒˆã‚’å‡ºåŠ›"""
        print("\n" + "="*80)
        print("ğŸš€ Issue #769 ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ")
        print("="*80)
        
        print(f"ç›®æ¨™: 300Kè¡Œãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç† 29.17ç§’ â†’ 5ç§’ä»¥ä¸‹ (83%é«˜é€ŸåŒ–)")
        print()
        
        # å„æœ€é©åŒ–æ‰‹æ³•ã®çµæœ
        for optimization, result in results.items():
            if optimization == "summary":
                continue
                
            print(f"ğŸ“Š {optimization.replace('_', ' ').title()}")
            print("-" * 40)
            
            if isinstance(result, dict) and "speedup_factor" in result:
                speedup = result["speedup_factor"]
                print(f"  é«˜é€ŸåŒ–å€ç‡: {speedup:.2f}x")
                if "traditional_time" in result and "optimized_time" in result:
                    print(f"  å¾“æ¥å‡¦ç†æ™‚é–“: {result['traditional_time']:.4f}ç§’")
                    print(f"  æœ€é©åŒ–æ™‚é–“: {result.get('optimized_time', result.get('simd_time', result.get('async_time', result.get('parallel_time', 0)))):.4f}ç§’")
            else:
                # è¤‡æ•°ã®ã‚µãƒ–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãŒã‚ã‚‹å ´åˆ
                for sub_name, sub_result in result.items():
                    if isinstance(sub_result, dict) and "speedup_factor" in sub_result:
                        print(f"  {sub_name}: {sub_result['speedup_factor']:.2f}x speedup")
            print()
        
        # ç·åˆçµæœ
        if "summary" in results:
            summary = results["summary"]
            print("ğŸ¯ ç·åˆçµæœ")
            print("-" * 40)
            print(f"  æ¨å®šç·åˆé«˜é€ŸåŒ–: {summary['total_estimated_speedup']:.2f}x")
            print(f"  ç›®æ¨™é«˜é€ŸåŒ–å€ç‡: {summary['target_speedup']:.2f}x")
            print(f"  ç›®æ¨™é”æˆ: {'âœ… YES' if summary['target_achieved'] else 'âŒ NO'}")
            print()
            
            if summary['target_achieved']:
                estimated_new_time = 29.17 / summary['total_estimated_speedup']
                print(f"  æ¨å®šæ–°å‡¦ç†æ™‚é–“: {estimated_new_time:.2f}ç§’")
                print(f"  æ€§èƒ½å‘ä¸Š: {((29.17 - estimated_new_time) / 29.17 * 100):.1f}%")
            
            print()
            print("ğŸ“ˆ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©³ç´°:")
            print(summary['performance_report'])


async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("Issue #769 ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹")
    
    benchmark = PerformanceBenchmark()
    
    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œï¼ˆ300Kè¡Œã§ãƒ†ã‚¹ãƒˆï¼‰
    results = await benchmark.run_comprehensive_benchmark(lines_count=300000)
    
    # çµæœãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›
    benchmark.print_benchmark_report(results)
    
    print("\nãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº† ğŸ‰")


if __name__ == "__main__":
    asyncio.run(main())