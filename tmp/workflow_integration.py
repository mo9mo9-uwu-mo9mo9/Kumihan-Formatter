#!/usr/bin/env python3
"""
çµ±åˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ - æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ çµ±åˆ

æ—¢å­˜ã®Geminiå”æ¥­ã‚·ã‚¹ãƒ†ãƒ ã¨ã®çµ±åˆãƒ»ç§»è¡Œæ©Ÿèƒ½
"""

import json
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any

from workflow_database import (
    WorkflowDatabase, WorkflowAnalytics,
    ProjectInfo, OrchestrationLog, FailurePattern
)


class WorkflowIntegrator:
    """æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ çµ±åˆã‚¯ãƒ©ã‚¹"""

    def __init__(self, db_path: str = "tmp/workflow_logs.db"):
        self.db = WorkflowDatabase(db_path)
        self.analytics = WorkflowAnalytics(self.db)

    def migrate_orchestration_logs(self, log_file_path: str, project_name: str) -> int:
        """orchestration_log.jsonã‹ã‚‰ã®ç§»è¡Œ"""
        if not os.path.exists(log_file_path):
            print(f"âš ï¸ ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {log_file_path}")
            return 0

        print(f"ğŸ”„ {log_file_path} ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’ç§»è¡Œä¸­...")

        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå–å¾—ã¾ãŸã¯ä½œæˆ
        project_id = self.db.get_or_create_project(project_name)

        try:
            with open(log_file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            migrated_count = 0

            # æ§˜ã€…ãªJSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¯¾å¿œ
            if isinstance(data, list):
                # ãƒªã‚¹ãƒˆå½¢å¼
                for entry in data:
                    if self._migrate_single_entry(entry, project_id):
                        migrated_count += 1

            elif isinstance(data, dict):
                # è¾æ›¸å½¢å¼
                if 'logs' in data:
                    # { "logs": [...] } å½¢å¼
                    for entry in data['logs']:
                        if self._migrate_single_entry(entry, project_id):
                            migrated_count += 1
                elif 'entries' in data:
                    # { "entries": [...] } å½¢å¼
                    for entry in data['entries']:
                        if self._migrate_single_entry(entry, project_id):
                            migrated_count += 1
                else:
                    # å˜ä¸€ã‚¨ãƒ³ãƒˆãƒªå½¢å¼
                    if self._migrate_single_entry(data, project_id):
                        migrated_count += 1

            print(f"âœ… {migrated_count}ä»¶ã®ãƒ­ã‚°ã‚’ç§»è¡Œã—ã¾ã—ãŸ")
            return migrated_count

        except Exception as e:
            print(f"âŒ ç§»è¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            return 0

    def _migrate_single_entry(self, entry: Dict, project_id: int) -> bool:
        """å˜ä¸€ã‚¨ãƒ³ãƒˆãƒªã®ç§»è¡Œ"""
        try:
            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—å¤‰æ›
            timestamp = None
            if 'timestamp' in entry:
                if isinstance(entry['timestamp'], str):
                    timestamp = datetime.fromisoformat(entry['timestamp'].replace('Z', '+00:00'))
                elif isinstance(entry['timestamp'], (int, float)):
                    timestamp = datetime.fromtimestamp(entry['timestamp'])

            # OrchestrationLogã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ
            log = OrchestrationLog(
                project_id=project_id,
                session_id=entry.get('session_id'),
                user_request=entry.get('user_request', entry.get('request', 'Migrated entry')),
                task_type=entry.get('task_type', entry.get('type')),
                complexity=entry.get('complexity'),
                automation_level=entry.get('automation_level'),
                status=entry.get('status', 'completed'),
                success=entry.get('success', entry.get('status') == 'completed'),
                execution_time_seconds=entry.get('execution_time_seconds', entry.get('execution_time')),
                claude_tokens_input=entry.get('claude_tokens_input', entry.get('claude_tokens', {}).get('input', 0)),
                claude_tokens_output=entry.get('claude_tokens_output', entry.get('claude_tokens', {}).get('output', 0)),
                gemini_tokens_input=entry.get('gemini_tokens_input', entry.get('gemini_tokens', {}).get('input', 0)),
                gemini_tokens_output=entry.get('gemini_tokens_output', entry.get('gemini_tokens', {}).get('output', 0)),
                claude_cost=entry.get('claude_cost', 0.0),
                gemini_cost=entry.get('gemini_cost', 0.0),
                total_cost=entry.get('total_cost', 0.0),
                cost_savings=entry.get('cost_savings', 0.0),
                executed_by=entry.get('executed_by', entry.get('executor')),
                timestamp=timestamp or datetime.now()
            )

            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è¨˜éŒ²
            self.db.log_orchestration(log)

            # å¤±æ•—ã‚¨ãƒ³ãƒˆãƒªã®å ´åˆã€å¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚‚è¨˜éŒ²
            if not log.success and 'error' in entry:
                failure = FailurePattern(
                    project_id=project_id,
                    task_description=log.user_request,
                    failure_type=entry.get('error', {}).get('type', 'UNKNOWN_ERROR'),
                    error_message=entry.get('error', {}).get('message', 'No error message'),
                    error_context=json.dumps(entry.get('error', {}), ensure_ascii=False),
                    recovery_action=entry.get('recovery_action'),
                    recovery_success=entry.get('recovery_success')
                )
                self.db.log_failure_pattern(failure)

            return True

        except Exception as e:
            print(f"âš ï¸ ã‚¨ãƒ³ãƒˆãƒªç§»è¡Œå¤±æ•—: {e}")
            return False

    def scan_and_migrate_all_logs(self, base_dir: str = ".") -> Dict[str, int]:
        """æŒ‡å®šãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰å…¨ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ãƒ»ç§»è¡Œ"""
        print(f"ğŸ” {base_dir} é…ä¸‹ã®ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ä¸­...")

        log_patterns = [
            "**/orchestration_log*.json",
            "**/workflow_log*.json",
            "**/gemini_log*.json",
            "**/collaboration_log*.json",
            "**/*_log*.json"
        ]

        results = {}
        base_path = Path(base_dir)

        for pattern in log_patterns:
            for log_file in base_path.glob(pattern):
                if log_file.is_file():
                    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã‚’æ¨å®š
                    project_name = self._extract_project_name(log_file)

                    print(f"ğŸ“„ ç™ºè¦‹: {log_file} (ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: {project_name})")

                    migrated_count = self.migrate_orchestration_logs(str(log_file), project_name)
                    results[str(log_file)] = migrated_count

        total_migrated = sum(results.values())
        print(f"\nâœ… ç§»è¡Œå®Œäº†: {len(results)}ãƒ•ã‚¡ã‚¤ãƒ«ã€{total_migrated}ã‚¨ãƒ³ãƒˆãƒª")

        return results

    def _extract_project_name(self, log_file: Path) -> str:
        """ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‹ã‚‰ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã‚’æ¨å®š"""
        # ãƒ‘ã‚¹ã‹ã‚‰æ¨å®šã‚’è©¦è¡Œ
        parts = log_file.parts

        # ã‚ˆãã‚ã‚‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåãƒ‘ã‚¿ãƒ¼ãƒ³
        project_indicators = [
            'kumihan-formatter', 'kumihan_formatter',
            'claude-gemini', 'workflow-engine',
            'project', 'src', 'app'
        ]

        for part in reversed(parts):
            if any(indicator in part.lower() for indicator in project_indicators):
                return part.lower()

        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã‹ã‚‰æ¨å®š
        if len(parts) > 2:
            return parts[-2].lower()

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        return "unknown-project"

    def create_project_config_template(self, project_name: str, output_path: str = None) -> str:
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨­å®šãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä½œæˆ"""
        if not output_path:
            output_path = f"tmp/{project_name}_config_template.py"

        template_content = f'''#!/usr/bin/env python3
"""
{project_name} ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨­å®š

çµ±åˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç”¨ã®è¨­å®šå®šç¾©
"""

from dataclasses import dataclass
from typing import Dict, List, Optional
import json

@dataclass
class ProjectConfig:
    """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨­å®š"""
    name: str = "{project_name}"
    display_name: str = "{project_name.replace('-', ' ').title()}"
    base_path: str = "/path/to/{project_name}"

    # æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯
    language: str = "Python"
    tech_stack: List[str] = None

    # å“è³ªè¦ä»¶
    quality_requirements: Dict = None

    # Geminiå”æ¥­è¨­å®š
    gemini_collaboration: Dict = None

    def __post_init__(self):
        if self.tech_stack is None:
            self.tech_stack = ["Python 3.12+", "Black", "isort", "mypy"]

        if self.quality_requirements is None:
            self.quality_requirements = {{
                "mypy_strict": True,
                "black_format": True,
                "test_coverage": 80.0,
                "max_complexity": 10
            }}

        if self.gemini_collaboration is None:
            self.gemini_collaboration = {{
                "enabled": True,
                "auto_switch_threshold": 1000,  # Tokenæ•°
                "preferred_tasks": ["formatting", "type_annotation", "testing"],
                "quality_check_level": "STRICT"
            }}

    def to_json(self) -> str:
        """JSONå½¢å¼ã§å‡ºåŠ›"""
        return json.dumps({{
            "name": self.name,
            "display_name": self.display_name,
            "base_path": self.base_path,
            "language": self.language,
            "tech_stack": self.tech_stack,
            "quality_requirements": self.quality_requirements,
            "gemini_collaboration": self.gemini_collaboration
        }}, ensure_ascii=False, indent=2)


# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    config = ProjectConfig()
    print("ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨­å®š:")
    print(config.to_json())
'''

        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(template_content)

        print(f"âœ… ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨­å®šãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä½œæˆ: {output_path}")
        return output_path

    def integrate_with_orchestrator(self, orchestrator_config: Dict) -> Dict:
        """workflow_orchestrator_generic.py ã¨ã®çµ±åˆè¨­å®š"""
        integration_config = {
            "database": {
                "enabled": True,
                "db_path": self.db.db_path,
                "auto_log": True,
                "log_quality_checks": True
            },
            "analytics": {
                "enabled": True,
                "real_time_monitoring": True,
                "alert_thresholds": {
                    "success_rate_drop": 0.2,  # 20%ä½ä¸‹ã§ã‚¢ãƒ©ãƒ¼ãƒˆ
                    "cost_spike": 1.5,  # 50%å¢—åŠ ã§ã‚¢ãƒ©ãƒ¼ãƒˆ
                    "failure_rate_increase": 0.1  # 10%å¢—åŠ ã§ã‚¢ãƒ©ãƒ¼ãƒˆ
                }
            },
            "automation": {
                "auto_suggestions": True,
                "apply_suggestions": False,  # æ‰‹å‹•æ‰¿èªå¿…é ˆ
                "suggestion_confidence_threshold": 0.8
            }
        }

        # æ—¢å­˜è¨­å®šã¨ãƒãƒ¼ã‚¸
        merged_config = {**orchestrator_config, **integration_config}

        return merged_config

    def generate_migration_report(self, project_name: str) -> Dict:
        """ç§»è¡Œãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        project = self.db.get_project_by_name(project_name)
        if not project:
            return {"error": f"Project '{project_name}' not found"}

        stats = self.db.get_project_stats(project_name, days=365)  # 1å¹´åˆ†

        report = {
            "project_name": project_name,
            "migration_timestamp": datetime.now().isoformat(),
            "database_path": self.db.db_path,
            "statistics": stats,
            "data_coverage": {
                "total_logs": stats['total_executions'],
                "oldest_entry": None,
                "newest_entry": None,
                "data_quality": "Good" if stats['total_executions'] > 0 else "No data"
            },
            "recommendations": []
        }

        # ãƒ‡ãƒ¼ã‚¿ç¯„å›²ç¢ºèª
        with self.db._get_connection() as conn:
            date_range = conn.execute("""
                SELECT
                    MIN(timestamp) as oldest,
                    MAX(timestamp) as newest
                FROM orchestration_logs
                WHERE project_id = ?
            """, (project.id,)).fetchone()

            if date_range['oldest']:
                report["data_coverage"]["oldest_entry"] = date_range['oldest']
                report["data_coverage"]["newest_entry"] = date_range['newest']

        # æ¨å¥¨äº‹é …
        if stats['total_executions'] < 10:
            report["recommendations"].append("ãƒ‡ãƒ¼ã‚¿é‡ãŒå°‘ãªã„ãŸã‚ã€ã‚ˆã‚Šå¤šãã®ãƒ­ã‚°åé›†ã‚’æ¨å¥¨")

        if stats['success_rate'] < 0.8:
            report["recommendations"].append("æˆåŠŸç‡æ”¹å–„ã®ãŸã‚ã®åˆ†æãƒ»å¯¾ç­–ã‚’æ¨å¥¨")

        if stats['total_cost'] == 0:
            report["recommendations"].append("ã‚³ã‚¹ãƒˆæƒ…å ±ã®è¨˜éŒ²ã‚’é–‹å§‹ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨")

        return report


def run_integration_example():
    """çµ±åˆä¾‹ã®å®Ÿè¡Œ"""
    print("ğŸ”— æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ çµ±åˆãƒ‡ãƒ¢")
    print("="*50)

    integrator = WorkflowIntegrator("tmp/integration_demo.db")

    # 1. ã‚µãƒ³ãƒ—ãƒ«ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
    sample_log = {
        "logs": [
            {
                "timestamp": "2025-08-15T10:00:00",
                "user_request": "MyPyå‹æ³¨é‡ˆä¿®æ­£",
                "task_type": "type_annotation",
                "status": "completed",
                "success": True,
                "execution_time": 120,
                "claude_tokens": {"input": 500, "output": 200},
                "gemini_tokens": {"input": 1200, "output": 600},
                "total_cost": 0.008,
                "cost_savings": 0.032,
                "executed_by": "Hybrid"
            },
            {
                "timestamp": "2025-08-15T11:00:00",
                "user_request": "æ–°æ©Ÿèƒ½å®Ÿè£…",
                "task_type": "feature",
                "status": "failed",
                "success": False,
                "execution_time": 300,
                "claude_tokens": {"input": 2000, "output": 800},
                "total_cost": 0.024,
                "executed_by": "Claude",
                "error": {
                    "type": "SYNTAX_ERROR",
                    "message": "Invalid syntax in generated code"
                }
            }
        ]
    }

    sample_file = "tmp/sample_orchestration_log.json"
    with open(sample_file, 'w', encoding='utf-8') as f:
        json.dump(sample_log, f, ensure_ascii=False, indent=2)

    print(f"ğŸ“„ ã‚µãƒ³ãƒ—ãƒ«ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ: {sample_file}")

    # 2. ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ç§»è¡Œ
    migrated_count = integrator.migrate_orchestration_logs(sample_file, "integration-test")
    print(f"ğŸ“¥ ç§»è¡Œå®Œäº†: {migrated_count}ä»¶")

    # 3. ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨­å®šãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä½œæˆ
    config_file = integrator.create_project_config_template("integration-test")

    # 4. ç§»è¡Œãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report = integrator.generate_migration_report("integration-test")

    print(f"\nğŸ“Š ç§»è¡Œãƒ¬ãƒãƒ¼ãƒˆ:")
    print(f"  â€¢ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: {report['project_name']}")
    print(f"  â€¢ ç·ãƒ­ã‚°æ•°: {report['statistics']['total_executions']}")
    print(f"  â€¢ æˆåŠŸç‡: {report['statistics']['success_rate']:.1%}")
    print(f"  â€¢ ãƒ‡ãƒ¼ã‚¿å“è³ª: {report['data_coverage']['data_quality']}")

    if report['recommendations']:
        print(f"  â€¢ æ¨å¥¨äº‹é …:")
        for rec in report['recommendations']:
            print(f"    - {rec}")

    # 5. ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
    with open("tmp/integration_report.json", 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2, default=str)

    print(f"\nâœ… çµ±åˆãƒ‡ãƒ¢å®Œäº†ï¼")
    print(f"ğŸ“ ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«:")
    print(f"  â€¢ {sample_file}")
    print(f"  â€¢ {config_file}")
    print(f"  â€¢ tmp/integration_report.json")


if __name__ == "__main__":
    run_integration_example()
